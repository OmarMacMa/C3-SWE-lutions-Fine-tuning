[
  {
    "instruction": "Enable quiet mode/no-verbose in CLI for use in pre-commit hook\nThere seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\r\n\r\nIt would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\r\n![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\r\n\r\nThis hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n",
    "input": "     dialect_selector,\n     dialect_readout,\n )\n from sqlfluff.core.config import progress_bar_configuration\n \n from sqlfluff.core.enums import FormatType, Color\n         sys.exit(EXIT_SUCCESS)\n \n \ndef do_fixes(lnt, result, formatter=None, **kwargs):\n     \"\"\"Actually do the fixes.\"\"\"\n    click.echo(\"Persisting Changes...\")\n     res = result.persist_changes(formatter=formatter, **kwargs)\n     if all(res.values()):\n        click.echo(\"Done. Please check your files to confirm.\")\n         return True\n     # If some failed then return false\n     click.echo(\n     return False  # pragma: no cover\n \n \ndef _stdin_fix(linter, formatter, fix_even_unparsable):\n     \"\"\"Handle fixing from stdin.\"\"\"\n     exit_code = EXIT_SUCCESS\n     stdin = sys.stdin.read()\n \n \n def _paths_fix(\n    linter,\n     formatter,\n     paths,\n     processes,\n ):\n     \"\"\"Handle fixing from paths.\"\"\"\n     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n    click.echo(\"==== finding fixable violations ====\")\n     exit_code = EXIT_SUCCESS\n \n     with PathAndUserErrorHandler(formatter):\n        result = linter.lint_paths(\n             paths,\n             fix=True,\n             ignore_non_existent_files=False,\n \n     # NB: We filter to linting violations here, because they're\n     # the only ones which can be potentially fixed.\n    if result.num_violations(types=SQLLintError, fixable=True) > 0:\n        click.echo(\"==== fixing violations ====\")\n        click.echo(\n            f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable \"\n            \"linting violations found\"\n        )\n         if force:\n            if warn_force:\n                 click.echo(\n                     f\"{formatter.colorize('FORCE MODE', Color.red)}: \"\n                     \"Attempting fixes...\"\n                 )\n             success = do_fixes(\n                linter,\n                 result,\n                 formatter,\n                 types=SQLLintError,\n             c = click.getchar().lower()\n             click.echo(\"...\")\n             if c in (\"y\", \"\\r\", \"\\n\"):\n                click.echo(\"Attempting fixes...\")\n                 success = do_fixes(\n                    linter,\n                     result,\n                     formatter,\n                     types=SQLLintError,\n                 click.echo(\"Aborting...\")\n                 exit_code = EXIT_FAIL\n     else:\n        click.echo(\"==== no fixable linting violations found ====\")\n        formatter.completion_message()\n \n     error_types = [\n         (\n     ]\n     for num_violations_kwargs, message_format, error_level in error_types:\n         num_violations = result.num_violations(**num_violations_kwargs)\n        if num_violations > 0:\n             click.echo(message_format.format(num_violations))\n             exit_code = max(exit_code, error_level)\n \n     \"--force\",\n     is_flag=True,\n     help=(\n        \"skip the confirmation prompt and go straight to applying \"\n         \"fixes. **Use this with caution.**\"\n     ),\n )\n @click.option(\n     \"-x\",\n     \"--fixed-suffix\",\n     force: bool,\n     paths: Tuple[str],\n     bench: bool = False,\n     fixed_suffix: str = \"\",\n     logger: Optional[logging.Logger] = None,\n     processes: Optional[int] = None,\n     \"\"\"\n     # some quick checks\n     fixing_stdin = (\"-\",) == paths\n \n     config = get_config(\n         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n     ):\n         self._output_stream = output_stream\n         self.plain_output = self.should_produce_plain_output(nocolor)\n        self._verbosity = verbosity\n         self._filter_empty = filter_empty\n         self.output_line_length = output_line_length\n \n         \"\"\"Format the config of a `Linter`.\"\"\"\n         text_buffer = StringIO()\n         # Only show version information if verbosity is high enough\n        if self._verbosity > 0:\n             text_buffer.write(\"==== sqlfluff ====\\n\")\n             config_content = [\n                 (\"sqlfluff\", get_package_version()),\n                 (\"python\", get_python_version()),\n                 (\"implementation\", get_python_implementation()),\n                (\"verbosity\", self._verbosity),\n             ]\n             if linter.dialect:\n                 config_content.append((\"dialect\", linter.dialect.name))\n                         col_width=41,\n                     )\n                 )\n            if self._verbosity > 1:\n                 text_buffer.write(\"\\n== Raw Config:\\n\")\n                 text_buffer.write(self.format_config_vals(linter.config.iter_vals()))\n         return text_buffer.getvalue()\n     def dispatch_persist_filename(self, filename, result):\n         \"\"\"Dispatch filenames during a persist operation.\"\"\"\n         # Only show the skip records at higher levels of verbosity\n        if self._verbosity >= 2 or result != \"SKIP\":\n             self._dispatch(self.format_filename(filename=filename, success=result))\n \n     def _format_path(self, path: str) -> str:\n \n     def dispatch_path(self, path: str) -> None:\n         \"\"\"Dispatch paths for display.\"\"\"\n        if self._verbosity > 0:\n             self._dispatch(self._format_path(path))\n \n     def dispatch_template_header(\n         self, fname: str, linter_config: FluffConfig, file_config: FluffConfig\n     ) -> None:\n         \"\"\"Dispatch the header displayed before templating.\"\"\"\n        if self._verbosity > 1:\n             self._dispatch(self.format_filename(filename=fname, success=\"TEMPLATING\"))\n             # This is where we output config diffs if they exist.\n             if file_config:\n \n     def dispatch_parse_header(self, fname: str) -> None:\n         \"\"\"Dispatch the header displayed before parsing.\"\"\"\n        if self._verbosity > 1:\n             self._dispatch(self.format_filename(filename=fname, success=\"PARSING\"))\n \n     def dispatch_lint_header(self, fname: str, rules: List[str]) -> None:\n         \"\"\"Dispatch the header displayed before linting.\"\"\"\n        if self._verbosity > 1:\n             self._dispatch(\n                 self.format_filename(\n                     filename=fname, success=f\"LINTING ({', '.join(rules)})\"\n \n     def dispatch_processing_header(self, processes: int) -> None:\n         \"\"\"Dispatch the header displayed before linting.\"\"\"\n        if self._verbosity > 0:\n             self._dispatch(  # pragma: no cover\n                 f\"{self.colorize('effective configured processes: ', Color.lightgrey)} \"\n                 f\"{processes}\"\n         show = fails + warns > 0\n \n         # Only print the filename if it's either a failure or verbosity > 1\n        if self._verbosity > 0 or show:\n             text_buffer.write(self.format_filename(fname, success=fails == 0))\n             text_buffer.write(\"\\n\")\n \n         self, fname: str, linted_file: LintedFile, only_fixable: bool\n     ) -> None:\n         \"\"\"Dispatch any violations found in a file.\"\"\"\n         s = self._format_file_violations(\n             fname,\n             linted_file.get_violations(\n         if isinstance(success, str):\n             status_string = success\n         else:\n            status_string = self.colorize(\n                success_text if success else \"FAIL\",\n                Color.green if success else Color.red,\n            )\n         return f\"== [{self.colorize(filename, Color.lightgrey)}] {status_string}\"\n \n     def format_violation(\ndiff --git a/src/sqlfluff/core/linter/linted_dir.py b/src/sqlfluff/core/linter/linted_dir.py\n         for file in self.files:\n             if file.num_violations(fixable=True, **kwargs) > 0:\n                 buffer[file.path] = file.persist_tree(suffix=fixed_file_suffix)\n                result = buffer[file.path]\n             else:  # pragma: no cover TODO?\n                 buffer[file.path] = True\n                 result = \"SKIP\"",
    "output": "     dialect_selector,\n     dialect_readout,\n )\nfrom sqlfluff.core.linter import LintingResult\n from sqlfluff.core.config import progress_bar_configuration\n \n from sqlfluff.core.enums import FormatType, Color\n         sys.exit(EXIT_SUCCESS)\n \n \ndef do_fixes(\n    result: LintingResult, formatter: Optional[OutputStreamFormatter] = None, **kwargs\n):\n     \"\"\"Actually do the fixes.\"\"\"\n    if formatter and formatter.verbosity >= 0:\n        click.echo(\"Persisting Changes...\")\n     res = result.persist_changes(formatter=formatter, **kwargs)\n     if all(res.values()):\n        if formatter and formatter.verbosity >= 0:\n            click.echo(\"Done. Please check your files to confirm.\")\n         return True\n     # If some failed then return false\n     click.echo(\n     return False  # pragma: no cover\n \n \ndef _stdin_fix(linter: Linter, formatter, fix_even_unparsable):\n     \"\"\"Handle fixing from stdin.\"\"\"\n     exit_code = EXIT_SUCCESS\n     stdin = sys.stdin.read()\n \n \n def _paths_fix(\n    linter: Linter,\n     formatter,\n     paths,\n     processes,\n ):\n     \"\"\"Handle fixing from paths.\"\"\"\n     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n    if formatter.verbosity >= 0:\n        click.echo(\"==== finding fixable violations ====\")\n     exit_code = EXIT_SUCCESS\n \n     with PathAndUserErrorHandler(formatter):\n        result: LintingResult = linter.lint_paths(\n             paths,\n             fix=True,\n             ignore_non_existent_files=False,\n \n     # NB: We filter to linting violations here, because they're\n     # the only ones which can be potentially fixed.\n    num_fixable = result.num_violations(types=SQLLintError, fixable=True)\n    if num_fixable > 0:\n        if formatter.verbosity >= 0:\n            click.echo(\"==== fixing violations ====\")\n        click.echo(f\"{num_fixable} \" \"fixable linting violations found\")\n         if force:\n            if warn_force and formatter.verbosity >= 0:\n                 click.echo(\n                     f\"{formatter.colorize('FORCE MODE', Color.red)}: \"\n                     \"Attempting fixes...\"\n                 )\n             success = do_fixes(\n                 result,\n                 formatter,\n                 types=SQLLintError,\n             c = click.getchar().lower()\n             click.echo(\"...\")\n             if c in (\"y\", \"\\r\", \"\\n\"):\n                if formatter.verbosity >= 0:\n                    click.echo(\"Attempting fixes...\")\n                 success = do_fixes(\n                     result,\n                     formatter,\n                     types=SQLLintError,\n                 click.echo(\"Aborting...\")\n                 exit_code = EXIT_FAIL\n     else:\n        if formatter.verbosity >= 0:\n            click.echo(\"==== no fixable linting violations found ====\")\n            formatter.completion_message()\n \n     error_types = [\n         (\n     ]\n     for num_violations_kwargs, message_format, error_level in error_types:\n         num_violations = result.num_violations(**num_violations_kwargs)\n        if num_violations > 0 and formatter.verbosity >= 0:\n             click.echo(message_format.format(num_violations))\n             exit_code = max(exit_code, error_level)\n \n     \"--force\",\n     is_flag=True,\n     help=(\n        \"Skip the confirmation prompt and go straight to applying \"\n         \"fixes. **Use this with caution.**\"\n     ),\n )\n@click.option(\n    \"-q\",\n    \"--quiet\",\n    is_flag=True,\n    help=(\n        \"Reduces the amount of output to stdout to a minimal level. \"\n        \"This is effectively the opposite of -v. NOTE: It will only \"\n        \"take effect if -f/--force is also set.\"\n    ),\n)\n @click.option(\n     \"-x\",\n     \"--fixed-suffix\",\n     force: bool,\n     paths: Tuple[str],\n     bench: bool = False,\n    quiet: bool = False,\n     fixed_suffix: str = \"\",\n     logger: Optional[logging.Logger] = None,\n     processes: Optional[int] = None,\n     \"\"\"\n     # some quick checks\n     fixing_stdin = (\"-\",) == paths\n    if quiet:\n        if kwargs[\"verbose\"]:\n            click.echo(\n                \"ERROR: The --quiet flag can only be used if --verbose is not set.\",\n            )\n            sys.exit(EXIT_ERROR)\n        kwargs[\"verbose\"] = -1\n \n     config = get_config(\n         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n     ):\n         self._output_stream = output_stream\n         self.plain_output = self.should_produce_plain_output(nocolor)\n        self.verbosity = verbosity\n         self._filter_empty = filter_empty\n         self.output_line_length = output_line_length\n \n         \"\"\"Format the config of a `Linter`.\"\"\"\n         text_buffer = StringIO()\n         # Only show version information if verbosity is high enough\n        if self.verbosity > 0:\n             text_buffer.write(\"==== sqlfluff ====\\n\")\n             config_content = [\n                 (\"sqlfluff\", get_package_version()),\n                 (\"python\", get_python_version()),\n                 (\"implementation\", get_python_implementation()),\n                (\"verbosity\", self.verbosity),\n             ]\n             if linter.dialect:\n                 config_content.append((\"dialect\", linter.dialect.name))\n                         col_width=41,\n                     )\n                 )\n            if self.verbosity > 1:\n                 text_buffer.write(\"\\n== Raw Config:\\n\")\n                 text_buffer.write(self.format_config_vals(linter.config.iter_vals()))\n         return text_buffer.getvalue()\n     def dispatch_persist_filename(self, filename, result):\n         \"\"\"Dispatch filenames during a persist operation.\"\"\"\n         # Only show the skip records at higher levels of verbosity\n        if self.verbosity >= 2 or result != \"SKIP\":\n             self._dispatch(self.format_filename(filename=filename, success=result))\n \n     def _format_path(self, path: str) -> str:\n \n     def dispatch_path(self, path: str) -> None:\n         \"\"\"Dispatch paths for display.\"\"\"\n        if self.verbosity > 0:\n             self._dispatch(self._format_path(path))\n \n     def dispatch_template_header(\n         self, fname: str, linter_config: FluffConfig, file_config: FluffConfig\n     ) -> None:\n         \"\"\"Dispatch the header displayed before templating.\"\"\"\n        if self.verbosity > 1:\n             self._dispatch(self.format_filename(filename=fname, success=\"TEMPLATING\"))\n             # This is where we output config diffs if they exist.\n             if file_config:\n \n     def dispatch_parse_header(self, fname: str) -> None:\n         \"\"\"Dispatch the header displayed before parsing.\"\"\"\n        if self.verbosity > 1:\n             self._dispatch(self.format_filename(filename=fname, success=\"PARSING\"))\n \n     def dispatch_lint_header(self, fname: str, rules: List[str]) -> None:\n         \"\"\"Dispatch the header displayed before linting.\"\"\"\n        if self.verbosity > 1:\n             self._dispatch(\n                 self.format_filename(\n                     filename=fname, success=f\"LINTING ({', '.join(rules)})\"\n \n     def dispatch_processing_header(self, processes: int) -> None:\n         \"\"\"Dispatch the header displayed before linting.\"\"\"\n        if self.verbosity > 0:\n             self._dispatch(  # pragma: no cover\n                 f\"{self.colorize('effective configured processes: ', Color.lightgrey)} \"\n                 f\"{processes}\"\n         show = fails + warns > 0\n \n         # Only print the filename if it's either a failure or verbosity > 1\n        if self.verbosity > 0 or show:\n             text_buffer.write(self.format_filename(fname, success=fails == 0))\n             text_buffer.write(\"\\n\")\n \n         self, fname: str, linted_file: LintedFile, only_fixable: bool\n     ) -> None:\n         \"\"\"Dispatch any violations found in a file.\"\"\"\n        if self.verbosity < 0:\n            return\n         s = self._format_file_violations(\n             fname,\n             linted_file.get_violations(\n         if isinstance(success, str):\n             status_string = success\n         else:\n            status_string = success_text if success else \"FAIL\"\n\n        if status_string in (\"PASS\", \"FIXED\", success_text):\n            status_string = self.colorize(status_string, Color.green)\n        elif status_string in (\"FAIL\", \"ERROR\"):\n            status_string = self.colorize(status_string, Color.red)\n\n         return f\"== [{self.colorize(filename, Color.lightgrey)}] {status_string}\"\n \n     def format_violation(\ndiff --git a/src/sqlfluff/core/linter/linted_dir.py b/src/sqlfluff/core/linter/linted_dir.py\n         for file in self.files:\n             if file.num_violations(fixable=True, **kwargs) > 0:\n                 buffer[file.path] = file.persist_tree(suffix=fixed_file_suffix)\n                result: Union[bool, str]\n                if buffer[file.path] is True:\n                    result = \"FIXED\"\n                else:  # pragma: no cover\n                    result = buffer[file.path]\n             else:  # pragma: no cover TODO?\n                 buffer[file.path] = True\n                 result = \"SKIP\""
  },
  {
    "instruction": "fix keep adding new line on wrong place \n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nTo replicate this issue you can create a file eg. test.template.sql \r\n\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n{%- endif %}\r\n```\r\n\r\nthen run:\r\n```\r\nsqlfluff fix test.template.sql  \r\n```\r\n\r\nThis will give you:\r\n```\r\nL:   2 | P:  12 | L009 | Files must end with a trailing newline.\r\n```\r\n\r\nAnd the result of the file is now:\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n\r\n{%- endif %}\r\n```\r\n\r\nIf i run it again it will complain on the same issue and the result of the file would be: \r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n\r\n\r\n{%- endif %}\r\n```\r\n\r\nAnd so on. \n\n### Expected Behaviour\n\nThe expected behavior would be to add the new line at the end of the file, that is after `{%- endif %}` instead of adding the new line at the end of the SQL query - so the result should look like this: \r\n\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n{%- endif %}\r\n\r\n```\n\n### Observed Behaviour\n\nAdds a new line to the end of the SQL query instead of in the end of the file. \n\n### How to reproduce\n\nAlready mentioned above (in What Happened section).\n\n### Dialect\n\nsnowflake\n\n### Version\n\nsqlfluff, version 0.6.2\n\n### Configuration\n\n[sqlfluff]\r\nverbose = 1\r\ndialect = snowflake\r\ntemplater = jinja\r\nexclude_rules = L027,L031,L032,L036,L044,L046,L034\r\noutput_line_length = 121\r\nsql_file_exts=.sql\r\n\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 250\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = aliases\r\n\r\n\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L030]  # function names\r\ncapitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "     config: FluffConfig\n     fname: str\n     source_str: str\n\n\nclass EnrichedFixPatch(NamedTuple):\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    source_slice: slice\n    templated_slice: slice\n    fixed_raw: str\n    # The patch category, functions mostly for debugging and explanation\n    # than for function. It allows traceability of *why* this patch was\n    # generated.\n    patch_category: str\n    templated_str: str\n    source_str: str\n\n    def dedupe_tuple(self):\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\ndiff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n from sqlfluff.core.templaters import TemplatedFile\n \n # Classes needed only for type checking\nfrom sqlfluff.core.parser.segments.base import BaseSegment, FixPatch\n \nfrom sqlfluff.core.linter.common import NoQaDirective, EnrichedFixPatch\n \n # Instantiate the linter logger\n linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n         return not any(self.get_violations(filter_ignore=True))\n \n     @staticmethod\n    def _log_hints(\n        patch: Union[EnrichedFixPatch, FixPatch], templated_file: TemplatedFile\n    ):\n         \"\"\"Log hints for debugging during patch generation.\"\"\"\n         # This next bit is ALL FOR LOGGING AND DEBUGGING\n         max_log_length = 10\n         dedupe_buffer = []\n         # We use enumerate so that we get an index for each patch. This is entirely\n         # so when debugging logs we can find a given patch again!\n        patch: Union[EnrichedFixPatch, FixPatch]\n         for idx, patch in enumerate(\n            self.tree.iter_patches(templated_str=self.templated_file.templated_str)\n         ):\n             linter_logger.debug(\"  %s Yielded patch: %s\", idx, patch)\n             self._log_hints(patch, self.templated_file)\n \n            # Attempt to convert to source space.\n             try:\n                source_slice = self.templated_file.templated_slice_to_source_slice(\n                    patch.templated_slice,\n                )\n             except ValueError:  # pragma: no cover\n                 linter_logger.info(\n                     \"      - Skipping. Source space Value Error. i.e. attempted \"\n                 continue\n \n             # Check for duplicates\n            dedupe_tuple = (source_slice, patch.fixed_raw)\n            if dedupe_tuple in dedupe_buffer:\n                 linter_logger.info(\n                    \"      - Skipping. Source space Duplicate: %s\", dedupe_tuple\n                 )\n                 continue\n \n \n             # Get the affected raw slices.\n             local_raw_slices = self.templated_file.raw_slices_spanning_source_slice(\n                source_slice\n             )\n             local_type_list = [slc.slice_type for slc in local_raw_slices]\n \n            enriched_patch = EnrichedFixPatch(\n                source_slice=source_slice,\n                templated_slice=patch.templated_slice,\n                patch_category=patch.patch_category,\n                fixed_raw=patch.fixed_raw,\n                templated_str=self.templated_file.templated_str[patch.templated_slice],\n                source_str=self.templated_file.source_str[source_slice],\n            )\n\n             # Deal with the easy cases of 1) New code at end 2) only literals\n             if not local_type_list or set(local_type_list) == {\"literal\"}:\n                 linter_logger.info(\ndiff --git a/src/sqlfluff/core/parser/lexer.py b/src/sqlfluff/core/parser/lexer.py\n                 )\n             )\n \n         # Convert to tuple before return\n         return tuple(segment_buffer)\n \ndiff --git a/src/sqlfluff/core/parser/segments/base.py b/src/sqlfluff/core/parser/segments/base.py\n from copy import deepcopy\n from dataclasses import dataclass, field, replace\n from io import StringIO\nfrom typing import Any, Callable, Dict, Optional, List, Tuple, NamedTuple, Iterator\n import logging\n \n from tqdm import tqdm\n from sqlfluff.core.parser.matchable import Matchable\n from sqlfluff.core.parser.markers import PositionMarker\n from sqlfluff.core.parser.context import ParseContext\n \n # Instantiate the linter logger (only for use in methods involved with fixing.)\n linter_logger = logging.getLogger(\"sqlfluff.linter\")\n \n \nclass FixPatch(NamedTuple):\n     \"\"\"An edit patch for a templated file.\"\"\"\n \n     templated_slice: slice\n     fixed_raw: str\n     # The patch category, functions mostly for debugging and explanation\n     # than for function. It allows traceability of *why* this patch was\n    # generated. It has no siginificance for processing.\n     patch_category: str\n \n \n @dataclass\n class AnchorEditInfo:\n     def _log_apply_fixes_check_issue(message, *args):  # pragma: no cover\n         linter_logger.critical(message, *args)\n \n    def iter_patches(self, templated_str: str) -> Iterator[FixPatch]:\n         \"\"\"Iterate through the segments generating fix patches.\n \n         The patches are generated in TEMPLATED space. This is important\n         \"\"\"\n         # Does it match? If so we can ignore it.\n         assert self.pos_marker\n         matches = self.raw == templated_str[self.pos_marker.templated_slice]\n         if matches:\n             return\n                     insert_buff = \"\"\n \n                 # Now we deal with any changes *within* the segment itself.\n                yield from segment.iter_patches(templated_str=templated_str)\n \n                 # Once we've dealt with any patches from the segment, update\n                 # our position markers.\n             # or insert. Also valid if we still have an insertion buffer here.\n             end_diff = self.pos_marker.templated_slice.stop - templated_idx\n             if end_diff or insert_buff:\n                yield FixPatch(\n                    slice(\n                        self.pos_marker.templated_slice.stop - end_diff,\n                        self.pos_marker.templated_slice.stop,\n                    ),\n                    insert_buff,\n                     patch_category=\"end_point\",\n                 )\n \n     def edit(self, raw):\ndiff --git a/src/sqlfluff/core/rules/base.py b/src/sqlfluff/core/rules/base.py\n         space = \" \"\n         return space * self.tab_space_size if self.indent_unit == \"space\" else tab\n \n    def is_final_segment(self, context: RuleContext) -> bool:\n         \"\"\"Is the current segment the final segment in the parse tree.\"\"\"\n        if len(self.filter_meta(context.siblings_post)) > 0:\n             # This can only fail on the last segment\n             return False\n         elif len(context.segment.segments) > 0:\n             # This can only fail on the last base segment\n             return False\n        elif context.segment.is_meta:\n            # We can't fail on a meta segment\n             return False\n         else:\n             # We know we are at a leaf of the tree but not necessarily at the end of the\n             # one.\n             child_segment = context.segment\n             for parent_segment in context.parent_stack[::-1]:\n                possible_children = [\n                    s for s in parent_segment.segments if not s.is_meta\n                ]\n                 if len(possible_children) > possible_children.index(child_segment) + 1:\n                     return False\n                 child_segment = parent_segment\ndiff --git a/src/sqlfluff/core/templaters/slicers/tracer.py b/src/sqlfluff/core/templaters/slicers/tracer.py\n             # parts of the tag at a time.\n             unique_alternate_id = None\n             alternate_code = None\n            trimmed_content = \"\"\n             if elem_type.endswith(\"_end\") or elem_type == \"raw_begin\":\n                 block_type = block_types[elem_type]\n                 block_subtype = None\n                     \"endfor\",\n                     \"endif\",\n                 ):\n                     # Record potential forward jump over this block.\n                     self.raw_slice_info[result[stack[-1]]].next_slice_indices.append(\n                         block_idx\ndiff --git a/src/sqlfluff/rules/L009.py b/src/sqlfluff/rules/L009.py\n \n         \"\"\"\n         # We only care about the final segment of the parse tree.\n        if not self.is_final_segment(context):\n             return None\n \n         # Include current segment for complete stack and reverse.",
    "output": "     config: FluffConfig\n     fname: str\n     source_str: str\ndiff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n from sqlfluff.core.templaters import TemplatedFile\n \n # Classes needed only for type checking\nfrom sqlfluff.core.parser.segments.base import BaseSegment, FixPatch, EnrichedFixPatch\n \nfrom sqlfluff.core.linter.common import NoQaDirective\n \n # Instantiate the linter logger\n linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n         return not any(self.get_violations(filter_ignore=True))\n \n     @staticmethod\n    def _log_hints(patch: FixPatch, templated_file: TemplatedFile):\n         \"\"\"Log hints for debugging during patch generation.\"\"\"\n         # This next bit is ALL FOR LOGGING AND DEBUGGING\n         max_log_length = 10\n         dedupe_buffer = []\n         # We use enumerate so that we get an index for each patch. This is entirely\n         # so when debugging logs we can find a given patch again!\n        patch: FixPatch  # Could be FixPatch or its subclass, EnrichedFixPatch\n         for idx, patch in enumerate(\n            self.tree.iter_patches(templated_file=self.templated_file)\n         ):\n             linter_logger.debug(\"  %s Yielded patch: %s\", idx, patch)\n             self._log_hints(patch, self.templated_file)\n \n            # Get source_slice.\n             try:\n                enriched_patch = patch.enrich(self.templated_file)\n             except ValueError:  # pragma: no cover\n                 linter_logger.info(\n                     \"      - Skipping. Source space Value Error. i.e. attempted \"\n                 continue\n \n             # Check for duplicates\n            if enriched_patch.dedupe_tuple() in dedupe_buffer:\n                 linter_logger.info(\n                    \"      - Skipping. Source space Duplicate: %s\",\n                    enriched_patch.dedupe_tuple(),\n                 )\n                 continue\n \n \n             # Get the affected raw slices.\n             local_raw_slices = self.templated_file.raw_slices_spanning_source_slice(\n                enriched_patch.source_slice\n             )\n             local_type_list = [slc.slice_type for slc in local_raw_slices]\n \n             # Deal with the easy cases of 1) New code at end 2) only literals\n             if not local_type_list or set(local_type_list) == {\"literal\"}:\n                 linter_logger.info(\ndiff --git a/src/sqlfluff/core/parser/lexer.py b/src/sqlfluff/core/parser/lexer.py\n                 )\n             )\n \n            # Generate placeholders for any source-only slices that *follow*\n            # the last element. This happens, for example, if a Jinja templated\n            # file ends with \"{% endif %}\", and there's no trailing newline.\n            if idx == len(elements) - 1:\n                so_slices = [\n                    so\n                    for so in source_only_slices\n                    if so.source_idx >= source_slice.stop\n                ]\n                for so_slice in so_slices:\n                    segment_buffer.append(\n                        TemplateSegment(\n                            pos_marker=PositionMarker(\n                                slice(so_slice.source_idx, so_slice.end_source_idx()),\n                                slice(\n                                    element.template_slice.stop,\n                                    element.template_slice.stop,\n                                ),\n                                templated_file,\n                            ),\n                            source_str=so_slice.raw,\n                            block_type=so_slice.slice_type,\n                        )\n                    )\n\n         # Convert to tuple before return\n         return tuple(segment_buffer)\n \ndiff --git a/src/sqlfluff/core/parser/segments/base.py b/src/sqlfluff/core/parser/segments/base.py\n from copy import deepcopy\n from dataclasses import dataclass, field, replace\n from io import StringIO\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Optional,\n    List,\n    Tuple,\n    Iterator,\n    Union,\n)\n import logging\n \n from tqdm import tqdm\n from sqlfluff.core.parser.matchable import Matchable\n from sqlfluff.core.parser.markers import PositionMarker\n from sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.templaters.base import TemplatedFile\n \n # Instantiate the linter logger (only for use in methods involved with fixing.)\n linter_logger = logging.getLogger(\"sqlfluff.linter\")\n \n \n@dataclass\nclass FixPatch:\n     \"\"\"An edit patch for a templated file.\"\"\"\n \n     templated_slice: slice\n     fixed_raw: str\n     # The patch category, functions mostly for debugging and explanation\n     # than for function. It allows traceability of *why* this patch was\n    # generated. It has no significance for processing.\n     patch_category: str\n \n    def enrich(self, templated_file: TemplatedFile) -> \"EnrichedFixPatch\":\n        \"\"\"Convert patch to source space.\"\"\"\n        source_slice = templated_file.templated_slice_to_source_slice(\n            self.templated_slice,\n        )\n        return EnrichedFixPatch(\n            source_slice=source_slice,\n            templated_slice=self.templated_slice,\n            patch_category=self.patch_category,\n            fixed_raw=self.fixed_raw,\n            templated_str=templated_file.templated_str[self.templated_slice],\n            source_str=templated_file.source_str[source_slice],\n        )\n\n\n@dataclass\nclass EnrichedFixPatch(FixPatch):\n    \"\"\"An edit patch for a source file.\"\"\"\n\n    source_slice: slice\n    templated_str: str\n    source_str: str\n\n    def enrich(self, templated_file: TemplatedFile) -> \"EnrichedFixPatch\":\n        \"\"\"No-op override of base class function.\"\"\"\n        return self\n\n    def dedupe_tuple(self):\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n\n \n @dataclass\n class AnchorEditInfo:\n     def _log_apply_fixes_check_issue(message, *args):  # pragma: no cover\n         linter_logger.critical(message, *args)\n \n    def iter_patches(\n        self, templated_file: TemplatedFile\n    ) -> Iterator[Union[EnrichedFixPatch, FixPatch]]:\n         \"\"\"Iterate through the segments generating fix patches.\n \n         The patches are generated in TEMPLATED space. This is important\n         \"\"\"\n         # Does it match? If so we can ignore it.\n         assert self.pos_marker\n        templated_str = templated_file.templated_str\n         matches = self.raw == templated_str[self.pos_marker.templated_slice]\n         if matches:\n             return\n                     insert_buff = \"\"\n \n                 # Now we deal with any changes *within* the segment itself.\n                yield from segment.iter_patches(templated_file=templated_file)\n \n                 # Once we've dealt with any patches from the segment, update\n                 # our position markers.\n             # or insert. Also valid if we still have an insertion buffer here.\n             end_diff = self.pos_marker.templated_slice.stop - templated_idx\n             if end_diff or insert_buff:\n                source_slice = segment.pos_marker.source_slice\n                templated_slice = slice(\n                    self.pos_marker.templated_slice.stop - end_diff,\n                    self.pos_marker.templated_slice.stop,\n                )\n                # By returning an EnrichedFixPatch (rather than FixPatch), which\n                # includes a source_slice field, we ensure that fixes adjacent\n                # to source-only slices (e.g. {% endif %}) are placed\n                # appropriately relative to source-only slices.\n                yield EnrichedFixPatch(\n                    source_slice=source_slice,\n                    templated_slice=templated_slice,\n                     patch_category=\"end_point\",\n                    fixed_raw=insert_buff,\n                    templated_str=templated_file.templated_str[templated_slice],\n                    source_str=templated_file.source_str[source_slice],\n                 )\n \n     def edit(self, raw):\ndiff --git a/src/sqlfluff/core/rules/base.py b/src/sqlfluff/core/rules/base.py\n         space = \" \"\n         return space * self.tab_space_size if self.indent_unit == \"space\" else tab\n \n    def is_final_segment(self, context: RuleContext, filter_meta: bool = True) -> bool:\n         \"\"\"Is the current segment the final segment in the parse tree.\"\"\"\n        siblings_post = context.siblings_post\n        if filter_meta:\n            siblings_post = self.filter_meta(siblings_post)\n        if len(siblings_post) > 0:\n             # This can only fail on the last segment\n             return False\n         elif len(context.segment.segments) > 0:\n             # This can only fail on the last base segment\n             return False\n        elif filter_meta and context.segment.is_meta:\n             return False\n         else:\n             # We know we are at a leaf of the tree but not necessarily at the end of the\n             # one.\n             child_segment = context.segment\n             for parent_segment in context.parent_stack[::-1]:\n                possible_children = parent_segment.segments\n                if filter_meta:\n                    possible_children = [s for s in possible_children if not s.is_meta]\n                 if len(possible_children) > possible_children.index(child_segment) + 1:\n                     return False\n                 child_segment = parent_segment\ndiff --git a/src/sqlfluff/core/templaters/slicers/tracer.py b/src/sqlfluff/core/templaters/slicers/tracer.py\n             # parts of the tag at a time.\n             unique_alternate_id = None\n             alternate_code = None\n             if elem_type.endswith(\"_end\") or elem_type == \"raw_begin\":\n                 block_type = block_types[elem_type]\n                 block_subtype = None\n                     \"endfor\",\n                     \"endif\",\n                 ):\n                    # Replace RawSliceInfo for this slice with one that has\n                    # alternate ID and code for tracking. This ensures, for\n                    # instance, that if a file ends with \"{% endif %} (with\n                    # no newline following), that we still generate a\n                    # TemplateSliceInfo for it.\n                    unique_alternate_id = self.next_slice_id()\n                    alternate_code = f\"{result[-1].raw}\\0{unique_alternate_id}_0\"\n                    self.raw_slice_info[result[-1]] = RawSliceInfo(\n                        unique_alternate_id, alternate_code, []\n                    )\n                     # Record potential forward jump over this block.\n                     self.raw_slice_info[result[stack[-1]]].next_slice_indices.append(\n                         block_idx\ndiff --git a/src/sqlfluff/rules/L009.py b/src/sqlfluff/rules/L009.py\n \n         \"\"\"\n         # We only care about the final segment of the parse tree.\n        if not self.is_final_segment(context, filter_meta=False):\n             return None\n \n         # Include current segment for complete stack and reverse."
  },
  {
    "instruction": "L026: Rule incorrectly flag column does not exist in `FROM` clause in an UPDATE statement.\n## Expected Behaviour\r\n\r\nL026 should not fail when a subquery in an UPDATE statement references a column from the UPDATE target.\r\n\r\n## Observed Behaviour\r\n\r\nL026 failed due to reference was not found in the FROM clause with the following error printed (When using `sample.sql` content below)\r\n\r\n```\r\nL:   7 | P:  28 | L026 | Reference 'my_table.id' refers to table/view not found\r\n                       | in the FROM clause or found in parent subquery.\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\n1. Create `sample.sql` with the content below\r\n```\r\nUPDATE my_table\r\nSET row_sum = (\r\n    SELECT COUNT(*) AS row_sum\r\n    FROM\r\n        another_table\r\n    WHERE\r\n        another_table.id = my_table.id\r\n);\r\n```\r\n2. Run SQLFluff by `sqlfluff lint sample.sql`\r\n\r\n## Dialect\r\n\r\nDefault / Ansi (No dialect specified)\r\n\r\n## Version\r\n```\r\n(.venv) ~/code/sqlfluff (main) $ sqlfluff --version\r\nsqlfluff, version 0.9.0\r\n```\r\n\r\n```\r\n(.venv) ~/code/sqlfluff (main) $ python --version\r\nPython 3.9.9\r\n```\r\n\r\n## Configuration\r\nDefault. No customization.\r\n\n",
    "input": "         # We need the dialect to get the value table function names. If\n         # we don't have it, assume the clause does not have a value table\n         # function.\n        return False\n \n     for function_name in table_expr.recursive_crawl(\"function_name\"):\n         # Other rules can increase whitespace in the function name, so use strip to remove\n     if not dialect:\n         # We need the dialect to get the pivot table column names. If\n         # we don't have it, assume the clause does not have a pivot table\n        return []\n \n     fc = segment.get_child(\"from_pivot_expression\")\n     if not fc:\ndiff --git a/src/sqlfluff/core/rules/reference.py b/src/sqlfluff/core/rules/reference.py\nnew file mode 100644\ndiff --git a/src/sqlfluff/dialects/dialect_ansi.py b/src/sqlfluff/dialects/dialect_ansi.py\n             return [refs[-level]]\n         return []\n \n     @staticmethod\n     def _level_to_int(level: Union[ObjectReferenceLevel, int]) -> int:\n         # If it's an ObjectReferenceLevel, get the value. Otherwise, assume it's\n             return AliasInfo(segment.raw, segment, True, self, alias_expression, ref)\n \n         # If not return the object name (or None if there isn't one)\n        # ref = self.get_child(\"object_reference\")\n         if ref:\n             # Return the last element of the reference.\n             penultimate_ref: ObjectReferenceSegment.ObjectReferencePart = list(\ndiff --git a/src/sqlfluff/dialects/dialect_bigquery.py b/src/sqlfluff/dialects/dialect_bigquery.py\n             return [refs[1], refs[2]]\n         return super().extract_possible_references(level)  # pragma: no cover\n \n \n @bigquery_dialect.segment()\n class HyphenatedObjectReferenceSegment(ObjectReferenceSegment):  # type: ignore\ndiff --git a/src/sqlfluff/rules/L025.py b/src/sqlfluff/rules/L025.py\n     def _analyze_table_aliases(cls, query: L025Query, dialect: Dialect):\n         # Get table aliases defined in query.\n         for selectable in query.selectables:\n            select_info = get_select_statement_info(selectable.selectable, dialect)\n             if select_info:\n                 # Record the aliases.\n                 query.aliases += select_info.table_aliases\n \n                # Look at each table reference; if its an alias reference,\n                 # resolve the alias: could be an alias defined in \"query\"\n                 # itself or an \"ancestor\" query.\n                 for r in select_info.reference_buffer:\n                    for tr in r.extract_possible_references(level=r.ObjectReferenceLevel.TABLE):  # type: ignore\n                         # This function walks up the query's parent stack if necessary.\n                         cls._resolve_and_mark_reference(query, tr.part)\n \ndiff --git a/src/sqlfluff/rules/L026.py b/src/sqlfluff/rules/L026.py\n \"\"\"Implementation of Rule L026.\"\"\"\n\nfrom sqlfluff.core.rules.analysis.select import get_aliases_from_select\nfrom sqlfluff.core.rules.base import EvalResultType, LintResult, RuleContext\n from sqlfluff.core.rules.doc_decorators import document_configuration\nfrom sqlfluff.rules.L020 import Rule_L020\n \n \n @document_configuration\nclass Rule_L026(Rule_L020):\n     \"\"\"References cannot reference objects not present in ``FROM`` clause.\n \n     NB: This rule is disabled by default for BigQuery due to its use of\n \n     config_keywords = [\"force_enable\"]\n \n    @staticmethod\n    def _is_bad_tbl_ref(table_aliases, parent_select, tbl_ref):\n        \"\"\"Given a table reference, try to find what it's referring to.\"\"\"\n        # Is it referring to one of the table aliases?\n        if tbl_ref[0] in [a.ref_str for a in table_aliases]:\n            # Yes. Therefore okay.\n            return False\n\n        # Not a table alias. It it referring to a correlated subquery?\n        if parent_select:\n            parent_aliases, _ = get_aliases_from_select(parent_select)\n            if parent_aliases and tbl_ref[0] in [a[0] for a in parent_aliases]:\n                # Yes. Therefore okay.\n                return False\n\n        # It's not referring to an alias or a correlated subquery. Looks like a\n        # bad reference (i.e. referring to something unknown.)\n        return True\n\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        # A buffer to keep any violations.\n        violation_buff = []\n\n        # Check all the references that we have, do they reference present aliases?\n        for r in references:\n            tbl_refs = r.extract_possible_references(level=r.ObjectReferenceLevel.TABLE)\n            if tbl_refs and all(\n                self._is_bad_tbl_ref(table_aliases, parent_select, tbl_ref)\n                for tbl_ref in tbl_refs\n            ):\n                violation_buff.append(\n                    LintResult(\n                        # Return the first segment rather than the string\n                        anchor=tbl_refs[0].segments[0],\n                        description=f\"Reference {r.raw!r} refers to table/view \"\n                        \"not found in the FROM clause or found in parent \"\n                        \"subquery.\",\n                    )\n                )\n        return violation_buff or None\n\n     def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Override Rule L020 for dialects that use structs.\n\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n         # Config type hints\n         self.force_enable: bool\n \n         ):\n             return LintResult()\n \n        return super()._eval(context=context)",
    "output": "         # We need the dialect to get the value table function names. If\n         # we don't have it, assume the clause does not have a value table\n         # function.\n        return False  # pragma: no cover\n \n     for function_name in table_expr.recursive_crawl(\"function_name\"):\n         # Other rules can increase whitespace in the function name, so use strip to remove\n     if not dialect:\n         # We need the dialect to get the pivot table column names. If\n         # we don't have it, assume the clause does not have a pivot table\n        return []  # pragma: no cover\n \n     fc = segment.get_child(\"from_pivot_expression\")\n     if not fc:\ndiff --git a/src/sqlfluff/core/rules/reference.py b/src/sqlfluff/core/rules/reference.py\nnew file mode 100644\n\"\"\"Components for working with object and table references.\"\"\"\nfrom typing import Sequence, Tuple\n\n\ndef object_ref_matches_table(\n    possible_references: Sequence[Tuple[str, ...]], targets: Sequence[Tuple[str, ...]]\n) -> bool:\n    \"\"\"Return True if any of the possible references matches a target.\"\"\"\n    # Simple case: If there are no references, assume okay\n    # (i.e. no mismatch = good).\n    if not possible_references:\n        return True\n    # Simple case: Reference exactly matches a target.\n    if any(pr in targets for pr in possible_references):\n        return True\n    # Tricky case: If one is shorter than the other, check for a suffix match.\n    # (Note this is an \"optimistic\" check, i.e. it assumes the ignored parts of\n    # the target don't matter. In a SQL context, this is basically assuming\n    # there was an earlier \"USE <<database>>\" or similar directive.\n    for pr in possible_references:\n        for t in targets:\n            if (len(pr) < len(t) and pr == t[-len(pr) :]) or (\n                len(t) < len(pr) and t == pr[-len(t) :]\n            ):\n                return True\n    return False\ndiff --git a/src/sqlfluff/dialects/dialect_ansi.py b/src/sqlfluff/dialects/dialect_ansi.py\n             return [refs[-level]]\n         return []\n \n    def extract_possible_multipart_references(\n        self, levels: List[Union[ObjectReferenceLevel, int]]\n    ) -> List[Tuple[ObjectReferencePart, ...]]:\n        \"\"\"Extract possible multipart references, e.g. schema.table.\"\"\"\n        levels_tmp = [self._level_to_int(level) for level in levels]\n        min_level = min(levels_tmp)\n        max_level = max(levels_tmp)\n        refs = list(self.iter_raw_references())\n        if len(refs) >= max_level:\n            return [tuple(refs[-max_level : 1 - min_level])]\n        return []\n\n     @staticmethod\n     def _level_to_int(level: Union[ObjectReferenceLevel, int]) -> int:\n         # If it's an ObjectReferenceLevel, get the value. Otherwise, assume it's\n             return AliasInfo(segment.raw, segment, True, self, alias_expression, ref)\n \n         # If not return the object name (or None if there isn't one)\n         if ref:\n             # Return the last element of the reference.\n             penultimate_ref: ObjectReferenceSegment.ObjectReferencePart = list(\ndiff --git a/src/sqlfluff/dialects/dialect_bigquery.py b/src/sqlfluff/dialects/dialect_bigquery.py\n             return [refs[1], refs[2]]\n         return super().extract_possible_references(level)  # pragma: no cover\n \n    def extract_possible_multipart_references(self, levels):\n        \"\"\"Extract possible multipart references, e.g. schema.table.\"\"\"\n        levels_tmp = [self._level_to_int(level) for level in levels]\n        min_level = min(levels_tmp)\n        max_level = max(levels_tmp)\n        refs = list(self.iter_raw_references())\n        if max_level == self.ObjectReferenceLevel.SCHEMA.value and len(refs) >= 3:\n            return [tuple(refs[0 : max_level - min_level + 1])]\n        # Note we aren't handling other possible cases. We'll add these as\n        # needed.\n        return super().extract_possible_multipart_references(levels)\n\n \n @bigquery_dialect.segment()\n class HyphenatedObjectReferenceSegment(ObjectReferenceSegment):  # type: ignore\ndiff --git a/src/sqlfluff/rules/L025.py b/src/sqlfluff/rules/L025.py\n     def _analyze_table_aliases(cls, query: L025Query, dialect: Dialect):\n         # Get table aliases defined in query.\n         for selectable in query.selectables:\n            select_info = selectable.select_info\n             if select_info:\n                 # Record the aliases.\n                 query.aliases += select_info.table_aliases\n \n                # Look at each table reference; if it's an alias reference,\n                 # resolve the alias: could be an alias defined in \"query\"\n                 # itself or an \"ancestor\" query.\n                 for r in select_info.reference_buffer:\n                    for tr in r.extract_possible_references(\n                        level=r.ObjectReferenceLevel.TABLE\n                    ):\n                         # This function walks up the query's parent stack if necessary.\n                         cls._resolve_and_mark_reference(query, tr.part)\n \ndiff --git a/src/sqlfluff/rules/L026.py b/src/sqlfluff/rules/L026.py\n \"\"\"Implementation of Rule L026.\"\"\"\nfrom dataclasses import dataclass, field\nfrom typing import cast, List, Optional, Tuple\n\nfrom sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.rules.analysis.select_crawler import (\n    Query as SelectCrawlerQuery,\n    SelectCrawler,\n)\nfrom sqlfluff.core.dialects.common import AliasInfo\nfrom sqlfluff.core.rules.base import (\n    BaseRule,\n    LintResult,\n    RuleContext,\n    EvalResultType,\n)\nfrom sqlfluff.core.rules.functional import sp\n from sqlfluff.core.rules.doc_decorators import document_configuration\nfrom sqlfluff.core.rules.reference import object_ref_matches_table\n\n\n@dataclass\nclass L026Query(SelectCrawlerQuery):\n    \"\"\"SelectCrawler Query with custom L026 info.\"\"\"\n\n    aliases: List[AliasInfo] = field(default_factory=list)\n \n \n @document_configuration\nclass Rule_L026(BaseRule):\n     \"\"\"References cannot reference objects not present in ``FROM`` clause.\n \n     NB: This rule is disabled by default for BigQuery due to its use of\n \n     config_keywords = [\"force_enable\"]\n \n     def _eval(self, context: RuleContext) -> EvalResultType:\n         # Config type hints\n         self.force_enable: bool\n \n         ):\n             return LintResult()\n \n        violations: List[LintResult] = []\n        start_types = [\"select_statement\", \"delete_statement\", \"update_statement\"]\n        if context.segment.is_type(\n            *start_types\n        ) and not context.functional.parent_stack.any(sp.is_type(*start_types)):\n            dml_target_table: Optional[Tuple[str, ...]] = None\n            if not context.segment.is_type(\"select_statement\"):\n                # Extract first table reference. This will be the target\n                # table in a DELETE or UPDATE statement.\n                table_reference = next(\n                    context.segment.recursive_crawl(\"table_reference\"), None\n                )\n                if table_reference:\n                    dml_target_table = self._table_ref_as_tuple(table_reference)\n\n            # Verify table references in any SELECT statements found in or\n            # below context.segment in the parser tree.\n            crawler = SelectCrawler(\n                context.segment, context.dialect, query_class=L026Query\n            )\n            query: L026Query = cast(L026Query, crawler.query_tree)\n            self._analyze_table_references(\n                query, dml_target_table, context.dialect, violations\n            )\n        return violations or None\n\n    @classmethod\n    def _alias_info_as_tuples(cls, alias_info: AliasInfo) -> List[Tuple[str, ...]]:\n        result: List[Tuple[str, ...]] = []\n        if alias_info.aliased:\n            result.append((alias_info.ref_str,))\n        if alias_info.object_reference:\n            result.append(cls._table_ref_as_tuple(alias_info.object_reference))\n        return result\n\n    @staticmethod\n    def _table_ref_as_tuple(table_reference) -> Tuple[str, ...]:\n        return tuple(ref.part for ref in table_reference.iter_raw_references())\n\n    def _analyze_table_references(\n        self,\n        query: L026Query,\n        dml_target_table: Optional[Tuple[str, ...]],\n        dialect: Dialect,\n        violations: List[LintResult],\n    ):\n        # For each query...\n        for selectable in query.selectables:\n            select_info = selectable.select_info\n            if select_info:\n                # Record the available tables.\n                query.aliases += select_info.table_aliases\n\n                # Try and resolve each reference to a value in query.aliases (or\n                # in an ancestor query).\n                for r in select_info.reference_buffer:\n                    # This function walks up the query's parent stack if necessary.\n                    violation = self._resolve_reference(\n                        r, self._get_table_refs(r, dialect), dml_target_table, query\n                    )\n                    if violation:\n                        violations.append(violation)\n\n        # Visit children.\n        for child in query.children:\n            self._analyze_table_references(\n                cast(L026Query, child), dml_target_table, dialect, violations\n            )\n\n    @staticmethod\n    def _get_table_refs(ref, dialect):\n        \"\"\"Given ObjectReferenceSegment, determine possible table references.\"\"\"\n        tbl_refs = []\n        # First, handle any schema.table references.\n        for sr, tr in ref.extract_possible_multipart_references(\n            levels=[\n                ref.ObjectReferenceLevel.SCHEMA,\n                ref.ObjectReferenceLevel.TABLE,\n            ]\n        ):\n            tbl_refs.append((tr, (sr.part, tr.part)))\n        # Maybe check for simple table references. Two cases:\n        # - For most dialects, skip this if it's a schema+table reference -- the\n        #   reference was specific, so we shouldn't ignore that by looking\n        #   elsewhere.)\n        # - Always do this in BigQuery. BigQuery table references are frequently\n        #   ambiguous because BigQuery SQL supports structures, making some\n        #   multi-level \".\" references impossible to interpret with certainty.\n        #   We may need to genericize this code someday to support other\n        #   dialects. If so, this check should probably align somehow with\n        #   whether the dialect overrides\n        #   ObjectReferenceSegment.extract_possible_references().\n        if not tbl_refs or dialect.name in [\"bigquery\"]:\n            for tr in ref.extract_possible_references(\n                level=ref.ObjectReferenceLevel.TABLE\n            ):\n                tbl_refs.append((tr, (tr.part,)))\n        return tbl_refs\n\n    def _resolve_reference(\n        self, r, tbl_refs, dml_target_table: Optional[Tuple[str, ...]], query: L026Query\n    ):\n        # Does this query define the referenced table?\n        possible_references = [tbl_ref[1] for tbl_ref in tbl_refs]\n        targets = []\n        for alias in query.aliases:\n            targets += self._alias_info_as_tuples(alias)\n        if not object_ref_matches_table(possible_references, targets):\n            # No. Check the parent query, if there is one.\n            if query.parent:\n                return self._resolve_reference(\n                    r, tbl_refs, dml_target_table, cast(L026Query, query.parent)\n                )\n            # No parent query. If there's a DML statement at the root, check its\n            # target table.\n            elif not dml_target_table or not object_ref_matches_table(\n                possible_references, [dml_target_table]\n            ):\n                return LintResult(\n                    # Return the first segment rather than the string\n                    anchor=tbl_refs[0][0].segments[0],\n                    description=f\"Reference {r.raw!r} refers to table/view \"\n                    \"not found in the FROM clause or found in ancestor \"\n                    \"statement.\",\n                )"
  },
  {
    "instruction": "Inconsistent output depending on --processes flag when --ignore linting is used\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nDepending on the value you set for the `--processes` flag when also using `--ignore linting`, different output with different exit codes are generated.\n\n### Expected Behaviour\n\nThe same exit code should be generated, independently of the `--processes` flag. Furthermore, from https://docs.sqlfluff.com/en/stable/production.html#using-sqlfluff-on-a-whole-sql-codebase I would expect that exit codes should be either `0` or `65`, not `1`.\n\n### Observed Behaviour\n\nSee the How to reproduce section.\n\n### How to reproduce\n\nCreate a `test.sql` file with the following content:\r\n\r\n```SQL\r\nCREATE TABLE example (\r\n    id TEXT DEFAULT 'Lorem ipsum dolor sit amet, consectetur adipiscing elit. In condimentum congue est, ac orci aliquam.' PRIMARY KEY\r\n);\r\n```\r\n\r\nThe line is too long according to SQLFluff, caused by the large default value, so let's see the the output of SQLFluff.\r\n\r\nRunning\r\n\r\n```SHELL\r\nsqlfluff fix --dialect postgres --ignore linting --processes 2\r\n```\r\n\r\nresults in \r\n\r\n```\r\n==== finding fixable violations ====\r\n==== no fixable linting violations found ====                                                                                                                                                                      \r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n  [1 unfixable linting violations found]\r\n```\r\n\r\nwith exit code `1`. Running the same with one process instead:\r\n\r\n```SHELL\r\nsqlfluff fix --dialect postgres --ignore linting --processes 1\r\n```\r\n\r\nresults in\r\n\r\n```\r\n==== finding fixable violations ====\r\n==== no fixable linting violations found ====                                                                                                                                                                      \r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n```\r\n\r\nand exit code `0`\r\n\r\nSame behaviour for `lint` and `format` commands.\n\n### Dialect\n\nPostgres\n\n### Version\n\n2.2.0, Python 3.10.6\n\n### Configuration\n\nNone, it's all in the CLI flags.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "             self.line_pos = line_pos\n         super().__init__(self.desc())\n \n     def __reduce__(\n         self,\n    ) -> Tuple[Type[\"SQLBaseError\"], Tuple[Any, ...]]:  # pragma: no cover\n         \"\"\"Prepare the SQLBaseError for pickling.\"\"\"\n         return type(self), (\n             self.description,\n         segment: Optional[\"BaseSegment\"] = None,\n         line_no: int = 0,\n         line_pos: int = 0,\n     ) -> None:\n         # Store the segment on creation - we might need it later\n         self.segment = segment\n             pos=segment.pos_marker if segment else None,\n             line_no=line_no,\n             line_pos=line_pos,\n         )\n \n     def __reduce__(\n         self,\n    ) -> Tuple[Type[\"SQLParseError\"], Tuple[Any, ...]]:  # pragma: no cover\n         \"\"\"Prepare the SQLParseError for pickling.\"\"\"\n        return type(self), (self.description, self.segment, self.line_no, self.line_pos)\n \n \n class SQLLintError(SQLBaseError):\n         segment: \"BaseSegment\",\n         rule: \"BaseRule\",\n         fixes: Optional[List[\"LintFix\"]] = None,\n     ) -> None:\n        # Something about position, message and fix?\n         self.segment = segment\n         self.rule = rule\n         self.fixes = fixes or []\n         super().__init__(\n            description=description, pos=segment.pos_marker if segment else None\n         )\n \n     def __reduce__(\n         self,\n    ) -> Tuple[Type[\"SQLLintError\"], Tuple[Any, ...]]:  # pragma: no cover\n         \"\"\"Prepare the SQLLintError for pickling.\"\"\"\n        return type(self), (self.description, self.segment, self.rule, self.fixes)\n \n     @property\n     def fixable(self) -> bool:\ndiff --git a/src/sqlfluff/core/parser/markers.py b/src/sqlfluff/core/parser/markers.py\n     def __le__(self, other: \"PositionMarker\") -> bool:\n         return self.working_loc <= other.working_loc  # pragma: no cover TODO?\n \n     @property\n     def working_loc(self) -> Tuple[int, int]:\n         \"\"\"Location tuple for the working position.\"\"\"",
    "output": "             self.line_pos = line_pos\n         super().__init__(self.desc())\n \n    def __eq__(self, other) -> bool:\n        \"\"\"Errors compare equal if they are the same type and same content.\"\"\"\n        if not isinstance(other, self.__class__):\n            return False\n        return self.__dict__ == other.__dict__\n\n     def __reduce__(\n         self,\n    ) -> Tuple[Type[\"SQLBaseError\"], Tuple[Any, ...]]:\n         \"\"\"Prepare the SQLBaseError for pickling.\"\"\"\n         return type(self), (\n             self.description,\n         segment: Optional[\"BaseSegment\"] = None,\n         line_no: int = 0,\n         line_pos: int = 0,\n        ignore: bool = False,\n        fatal: bool = False,\n        warning: Optional[bool] = None,\n     ) -> None:\n         # Store the segment on creation - we might need it later\n         self.segment = segment\n             pos=segment.pos_marker if segment else None,\n             line_no=line_no,\n             line_pos=line_pos,\n            ignore=ignore,\n            fatal=fatal,\n            warning=warning,\n         )\n \n     def __reduce__(\n         self,\n    ) -> Tuple[Type[\"SQLParseError\"], Tuple[Any, ...]]:\n         \"\"\"Prepare the SQLParseError for pickling.\"\"\"\n        return type(self), (\n            self.description,\n            self.segment,\n            self.line_no,\n            self.line_pos,\n            self.ignore,\n            self.fatal,\n            self.warning,\n        )\n \n \n class SQLLintError(SQLBaseError):\n         segment: \"BaseSegment\",\n         rule: \"BaseRule\",\n         fixes: Optional[List[\"LintFix\"]] = None,\n        ignore: bool = False,\n        fatal: bool = False,\n        warning: Optional[bool] = None,\n     ) -> None:\n         self.segment = segment\n         self.rule = rule\n         self.fixes = fixes or []\n         super().__init__(\n            description=description,\n            pos=segment.pos_marker if segment else None,\n            ignore=ignore,\n            fatal=fatal,\n            warning=warning,\n         )\n \n     def __reduce__(\n         self,\n    ) -> Tuple[Type[\"SQLLintError\"], Tuple[Any, ...]]:\n         \"\"\"Prepare the SQLLintError for pickling.\"\"\"\n        return type(self), (\n            self.description,\n            self.segment,\n            self.rule,\n            self.fixes,\n            self.ignore,\n            self.fatal,\n            self.warning,\n        )\n \n     @property\n     def fixable(self) -> bool:\ndiff --git a/src/sqlfluff/core/parser/markers.py b/src/sqlfluff/core/parser/markers.py\n     def __le__(self, other: \"PositionMarker\") -> bool:\n         return self.working_loc <= other.working_loc  # pragma: no cover TODO?\n \n    def __eq__(self, other) -> bool:\n        if not isinstance(other, PositionMarker):\n            return False  # pragma: no cover\n        return self.working_loc == other.working_loc\n\n     @property\n     def working_loc(self) -> Tuple[int, int]:\n         \"\"\"Location tuple for the working position.\"\"\""
  },
  {
    "instruction": "Fatal templating error with Jinja templater. Tracer produces odd results.\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nIssue found while assessing an Airflow project.\r\n\r\nThe smallest query I can make which triggers the issue is: \r\n```sql\r\nSELECT\r\n\t{% block table_name %}a{% endblock %}.b\r\nFROM d.{{ self.table_name() }}\r\n```\r\n\r\nWhen running this query through `lint` I get an `AssertionError`, or if running on the more friendly error message PR (#3433) I get: `WARNING    Length of templated file mismatch with final slice: 21 != 19.`.\n\n### Expected Behaviour\n\nThis query should slice properly and probably eventually give a jinja error that the required variables are undefined.\n\n### Observed Behaviour\n\nI've dug a little into the error and the sliced file being produced is:\r\n\r\n```python\r\n[\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(0, 8, None), templated_slice=slice(0, 8, None)),\r\n    TemplatedFileSlice(slice_type='block_start', source_slice=slice(8, 30, None), templated_slice=slice(8, 8, None)),\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(30, 31, None), templated_slice=slice(8, 9, None)),\r\n    TemplatedFileSlice(slice_type='block_end', source_slice=slice(31, 45, None), templated_slice=slice(9, 9, None)),\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(45, 55, None), templated_slice=slice(9, 19, None)),\r\n    TemplatedFileSlice(slice_type='templated', source_slice=slice(55, 78, None), templated_slice=slice(19, 19, None)),\r\n    TemplatedFileSlice(slice_type='literal', source_slice=slice(78, 79, None), templated_slice=slice(19, 19, None))\r\n]\r\n```\r\n\r\nThe issue is that while the `source_slice` looks correct for the slices, almost all of the `templated_slices` values have zero length, and importantly the last one doesn't end at position 21.\r\n\r\nThe rendered file is `SELECT\\n\\ta.b\\nFROM d.a\\n` (I've included the escape chars) which is indeed 21 chars long.\r\n\r\n@barrywhart I might need your help to work out what's going on with the Jinja tracer here.\n\n### How to reproduce\n\nRun provided query, `main` branch. Set to the `jinja` templater.\n\n### Dialect\n\ndialect is set to `snowflake`, but I don't think we're getting far enough for that to make a difference.\n\n### Version\n\n`main` branch commit `cb6357c540d2d968f766f3a7a4fa16f231cb80e4` (and a few branches derived from it)\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "     unique_alternate_id: Optional[str]\n     alternate_code: Optional[str]\n     next_slice_indices: List[int] = field(default_factory=list)\n \n \n class JinjaTracer:\n             alt_id, content_info, literal = value\n             target_slice_idx = self.find_slice_index(alt_id)\n             slice_length = content_info if literal else len(str(content_info))\n            self.move_to_slice(target_slice_idx, slice_length)\n \n         # TRICKY: The 'append_to_templated' parameter is only used by the dbt\n         # templater, passing \"\\n\" for this parameter if we need to add one back.\n         # (The Jinja templater does not pass this parameter, so\n         # 'append_to_templated' gets the default value of \"\", empty string.)\n        # we receive the default value of \"\".) The dbt templater will\n         # For more detail, see the comments near the call to slice_file() in\n         # plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py.\n         templated_str = self.make_template(self.raw_str).render() + append_to_templated\n \n         # Internal bookkeeping\n         self.slice_id: int = 0\n        self.inside_set_or_macro: bool = False\n         self.stack: List[int] = []\n         self.idx_raw: int = 0\n \n         \"\"\"Returns a RawSliceInfo for a literal.\n \n         In the alternate template, literals are replaced with a uniquely\n        numbered, easily-to-parse literal. JinjaTracer uses this output as\n         a \"breadcrumb trail\" to deduce the execution path through the template.\n \n         This is important even if the original literal (i.e. in the raw SQL\n         \"\"\"\n         unique_alternate_id = self.next_slice_id()\n         alternate_code = f\"\\0{prefix}{unique_alternate_id}_{length}\"\n        return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n \n    def update_inside_set_or_macro(\n         self, block_type: str, trimmed_parts: List[str]\n     ) -> None:\n         \"\"\"Based on block tag, update whether we're in a set/macro section.\"\"\"\n         if block_type == \"block_start\" and trimmed_parts[0] in (\n             \"macro\",\n             \"set\",\n         ):\n             # - {% set variable = value %}\n             # - {% set variable %}value{% endset %}\n             # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n            # When the second format is used, set the field\n            # 'inside_set_or_macro' to True. This info is used elsewhere,\n            # as other code inside these regions require special handling.\n            # (Generally speaking, JinjaTracer ignores the contents of these\n            # blocks, treating them like opaque templated regions.)\n             try:\n                 # Entering a set/macro block. Build a source string consisting\n                 # of just this one Jinja command and see if it parses. If so,\n                     isinstance(e.message, str)\n                     and \"Unexpected end of template\" in e.message\n                 ):\n                    # It was opening a block, thus we're inside a set or macro.\n                    self.inside_set_or_macro = True\n                 else:\n                     raise  # pragma: no cover\n        elif block_type == \"block_end\" and (trimmed_parts[0] in (\"endmacro\", \"endset\")):\n            # Exiting a set/macro block.\n            self.inside_set_or_macro = False\n \n     def make_raw_slice_info(\n        self, unique_alternate_id: Optional[str], alternate_code: Optional[str]\n     ) -> RawSliceInfo:\n         \"\"\"Create RawSliceInfo as given, or \"empty\" if in set/macro block.\"\"\"\n         if not self.inside_set_or_macro:\n            return RawSliceInfo(unique_alternate_id, alternate_code, [])\n         else:\n            return RawSliceInfo(None, None, [])\n \n     # We decide the \"kind\" of element we're dealing with using its _closing_\n     # tag rather than its opening tag. The types here map back to similar types\n                         raw_slice_info = self.track_templated(\n                             m_open, m_close, tag_contents\n                         )\n                self.update_inside_set_or_macro(block_type, tag_contents)\n                 m_strip_right = regex.search(\n                     r\"\\s+$\", raw, regex.MULTILINE | regex.DOTALL\n                 )",
    "output": "     unique_alternate_id: Optional[str]\n     alternate_code: Optional[str]\n     next_slice_indices: List[int] = field(default_factory=list)\n    inside_block: bool = field(default=False)  # {% block %}\n \n \n class JinjaTracer:\n             alt_id, content_info, literal = value\n             target_slice_idx = self.find_slice_index(alt_id)\n             slice_length = content_info if literal else len(str(content_info))\n            target_inside_block = self.raw_slice_info[\n                self.raw_sliced[target_slice_idx]\n            ].inside_block\n            if not target_inside_block:\n                # Normal case: Walk through the template.\n                self.move_to_slice(target_slice_idx, slice_length)\n            else:\n                # {% block %} executes code elsewhere in the template but does\n                # not move there. It's a bit like macro invocation.\n                self.record_trace(slice_length, target_slice_idx)\n \n         # TRICKY: The 'append_to_templated' parameter is only used by the dbt\n         # templater, passing \"\\n\" for this parameter if we need to add one back.\n         # (The Jinja templater does not pass this parameter, so\n         # 'append_to_templated' gets the default value of \"\", empty string.)\n         # For more detail, see the comments near the call to slice_file() in\n         # plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py.\n         templated_str = self.make_template(self.raw_str).render() + append_to_templated\n \n         # Internal bookkeeping\n         self.slice_id: int = 0\n        self.inside_set_or_macro: bool = False  # {% set %} or {% macro %}\n        self.inside_block = False  # {% block %}\n         self.stack: List[int] = []\n         self.idx_raw: int = 0\n \n         \"\"\"Returns a RawSliceInfo for a literal.\n \n         In the alternate template, literals are replaced with a uniquely\n        numbered, easy-to-parse literal. JinjaTracer uses this output as\n         a \"breadcrumb trail\" to deduce the execution path through the template.\n \n         This is important even if the original literal (i.e. in the raw SQL\n         \"\"\"\n         unique_alternate_id = self.next_slice_id()\n         alternate_code = f\"\\0{prefix}{unique_alternate_id}_{length}\"\n        return self.make_raw_slice_info(\n            unique_alternate_id, alternate_code, inside_block=self.inside_block\n        )\n \n    def update_inside_set_or_macro_or_block(\n         self, block_type: str, trimmed_parts: List[str]\n     ) -> None:\n         \"\"\"Based on block tag, update whether we're in a set/macro section.\"\"\"\n         if block_type == \"block_start\" and trimmed_parts[0] in (\n            \"block\",\n             \"macro\",\n             \"set\",\n         ):\n             # - {% set variable = value %}\n             # - {% set variable %}value{% endset %}\n             # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n            # When the second format is used, set one of the fields\n            # 'inside_set_or_macro' or 'inside_block' to True. This info is\n            # used elsewhere, as other code inside these regions require\n            # special handling. (Generally speaking, JinjaAnalyzer ignores\n            # the contents of these blocks, treating them like opaque templated\n            # regions.)\n             try:\n                 # Entering a set/macro block. Build a source string consisting\n                 # of just this one Jinja command and see if it parses. If so,\n                     isinstance(e.message, str)\n                     and \"Unexpected end of template\" in e.message\n                 ):\n                    # It was opening a block, thus we're inside a set, macro, or\n                    # block.\n                    if trimmed_parts[0] == \"block\":\n                        self.inside_block = True\n                    else:\n                        self.inside_set_or_macro = True\n                 else:\n                     raise  # pragma: no cover\n        elif block_type == \"block_end\":\n            if trimmed_parts[0] in (\"endmacro\", \"endset\"):\n                # Exiting a set or macro.\n                self.inside_set_or_macro = False\n            elif trimmed_parts[0] == \"endblock\":\n                # Exiting a {% block %} block.\n                self.inside_block = False\n \n     def make_raw_slice_info(\n        self,\n        unique_alternate_id: Optional[str],\n        alternate_code: Optional[str],\n        inside_block: bool = False,\n     ) -> RawSliceInfo:\n         \"\"\"Create RawSliceInfo as given, or \"empty\" if in set/macro block.\"\"\"\n         if not self.inside_set_or_macro:\n            return RawSliceInfo(unique_alternate_id, alternate_code, [], inside_block)\n         else:\n            return RawSliceInfo(None, None, [], False)\n \n     # We decide the \"kind\" of element we're dealing with using its _closing_\n     # tag rather than its opening tag. The types here map back to similar types\n                         raw_slice_info = self.track_templated(\n                             m_open, m_close, tag_contents\n                         )\n                self.update_inside_set_or_macro_or_block(block_type, tag_contents)\n                 m_strip_right = regex.search(\n                     r\"\\s+$\", raw, regex.MULTILINE | regex.DOTALL\n                 )"
  },
  {
    "instruction": "Lint and fix throws exception when having jinja for loop inside set\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nTo reproduce the error, create test.template.sql\r\n```\r\n{% set whitelisted= [\r\n    {'name': 'COL_1'},\r\n    {'name': 'COL_2'},\r\n    {'name': 'COL_3'}\r\n] %}\r\n\r\n{% set some_part_of_the_query %}\r\n    {% for col in whitelisted %}\r\n    {{col.name}}{{ \", \" if not loop.last }}\r\n    {% endfor %}\r\n{% endset %}\r\n\r\nSELECT {{some_part_of_the_query}}\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nwhen running lint i get this error:\r\n```\r\n==== sqlfluff ====\r\nsqlfluff:               0.11.0 python:                 3.8.12\r\nimplementation:        cpython dialect:             snowflake\r\nverbosity:                   1 templater:               jinja\r\n\r\n==== readout ====\r\n\r\n=== [ path: test.template.sql ] ===\r\n\r\nWARNING    Unable to lint test.template.sql due to an internal error. Please report this as an issue with your query's contents and stacktrace below!\r\nTo hide this warning, add the failing file to .sqlfluffignore\r\nTraceback (most recent call last):\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/runner.py\", line 103, in run\r\n    yield partial()\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 666, in lint_rendered\r\n    parsed = cls.parse_rendered(rendered)\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 352, in parse_rendered\r\n    tokens, lvs, config = cls._lex_templated_file(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/linter/linter.py\", line 139, in _lex_templated_file\r\n    tokens, lex_vs = lexer.lex(templated_file)\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/parser/lexer.py\", line 321, in lex\r\n    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/parser/lexer.py\", line 348, in elements_to_segments\r\n    source_slice = templated_file.templated_slice_to_source_slice(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/templaters/base.py\", line 258, in templated_slice_to_source_slice\r\n    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\r\n  File \"lib/python3.8/site-packages/sqlfluff/core/templaters/base.py\", line 177, in _find_slice_indices_of_templated_pos\r\n    raise ValueError(\"Position Not Found\")\r\nValueError: Position Not Found\r\n \r\n==== summary ====\r\nviolations:        0 status:         PASS\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n\r\n```\r\n\r\nThis is the rendered query:\r\n```\r\n SELECT\r\n\r\n    COL_1,\r\n\r\n    COL_2,\r\n\r\n    COL_3\r\n\r\n\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nAnd when trying around to make this work i removed the new lines between the selected columns like this:\r\n```\r\n{% set whitelisted= [\r\n    {'name': 'COL_1'},\r\n    {'name': 'COL_2'},\r\n    {'name': 'COL_3'}\r\n] %}\r\n\r\n{% set some_part_of_the_query %}\r\n    {% for col in whitelisted -%}\r\n    {{col.name}}{{ \", \" if not loop.last }}\r\n    {% endfor -%}\r\n{% endset %}\r\n\r\nSELECT {{some_part_of_the_query}}\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nwhich renders:\r\n```\r\nSELECT\r\n    COL_1,\r\n    COL_2,\r\n    COL_3\r\n\r\nFROM SOME_TABLE\r\n\r\n```\r\n\r\nAnd this will make the linter pass:\r\n\r\n```\r\n==== sqlfluff ====\r\nsqlfluff:               0.11.0 python:                 3.8.12\r\nimplementation:        cpython dialect:             snowflake\r\nverbosity:                   1 templater:               jinja\r\n\r\n==== readout ====\r\n\r\n=== [ path: test.template.sql ] ===\r\n\r\n== [test.template.sql] PASS                                                                                                                          \r\n==== summary ====\r\nviolations:        0 status:         PASS\r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n\r\n```\r\n\r\n\n\n### Expected Behaviour\n\nMy expectations is that the linter and fix should pass.\n\n### Observed Behaviour\n\nRight now lint and fix throws exception (see \"What Happened\" section)\n\n### How to reproduce\n\nMentioned above.\n\n### Dialect\n\nsnowflake\n\n### Version\n\nsqlfluff, version 0.11.0\n\n### Configuration\n\n[sqlfluff]\r\nverbose = 1\r\ndialect = snowflake\r\ntemplater = jinja\r\nexclude_rules = L027,L031,L032,L036,L044,L046,L034,L050\r\noutput_line_length = 121\r\nsql_file_exts=.sql\r\n\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 250\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L042]\r\nforbid_subquery_in = both\r\n\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L030]  # function names\r\nextended_capitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "             # sliced_file to reflect the mapping of the added character(s) back\n             # to the raw SQL.\n             templated_sql = templated_sql + \"\\n\" * n_trailing_newlines\n            sliced_file.append(\n                TemplatedFileSlice(\n                    slice_type=\"literal\",\n                    source_slice=slice(\n                        len(source_dbt_sql) - n_trailing_newlines, len(source_dbt_sql)\n                    ),\n                    templated_slice=slice(\n                        len(templated_sql) - n_trailing_newlines, len(templated_sql)\n                    ),\n                 )\n            )\n         return (\n             TemplatedFile(\n                 source_str=source_dbt_sql,\ndiff --git a/src/sqlfluff/core/templaters/base.py b/src/sqlfluff/core/templaters/base.py\n         templated_str: Optional[str] = None,\n         sliced_file: Optional[List[TemplatedFileSlice]] = None,\n         raw_sliced: Optional[List[RawFileSlice]] = None,\n     ):\n         \"\"\"Initialise the TemplatedFile.\n \n         self._source_newlines = list(iter_indices_of_newlines(self.source_str))\n         self._templated_newlines = list(iter_indices_of_newlines(self.templated_str))\n \n     @classmethod\n     def from_string(cls, raw):\n         \"\"\"Create TemplatedFile from a string.\"\"\"\ndiff --git a/src/sqlfluff/core/templaters/slicers/tracer.py b/src/sqlfluff/core/templaters/slicers/tracer.py\n             except IndexError:\n                 pos2 = len(trace_template_output)\n             p = trace_template_output[pos1 + 1 : pos2]\n            is_set_or_macro = p[:3] == \"set\"\n            if is_set_or_macro:\n                p = p[3:]\n             m_id = regex.match(r\"^([0-9a-f]+)(_(\\d+))?\", p)\n             if not m_id:\n                 raise ValueError(  # pragma: no cover\n             alt_id, content_info, literal = value\n             target_slice_idx = self.find_slice_index(alt_id)\n             slice_length = content_info if literal else len(str(content_info))\n            if not is_set_or_macro:\n                self.move_to_slice(target_slice_idx, slice_length)\n            else:\n                # If we find output from a {% set %} directive or a macro,\n                # record a trace without reading or updating the program\n                # counter. Such slices are always treated as \"templated\"\n                # because they are inserted during expansion of templated\n                # code (i.e. {% set %} variable or macro defined within the\n                # file).\n                self.record_trace(\n                    slice_length, target_slice_idx, slice_type=\"templated\"\n                )\n         return JinjaTrace(\n             self.make_template(self.raw_str).render(), self.raw_sliced, self.sliced_file\n         )\n                         idx,\n                     )\n                 )\n                self.raw_slice_info[result[-1]] = self.slice_info_for_literal(\n                    len(raw), \"\" if set_idx is None else \"set\"\n                )\n                 idx += len(raw)\n                 continue\n             str_buff += raw\n                         # effects, but return a unique slice ID.\n                         if trimmed_content:\n                             assert m_open and m_close\n                            unique_id = self.next_slice_id()\n                            unique_alternate_id = unique_id\n                            prefix = \"set\" if set_idx is not None else \"\"\n                            open_ = m_open.group(1)\n                            close_ = m_close.group(1)\n                            alternate_code = (\n                                f\"\\0{prefix}{unique_alternate_id} {open_} \"\n                                f\"{trimmed_content} {close_}\"\n                            )\n                 if block_type == \"block_start\" and trimmed_content.split()[0] in (\n                     \"macro\",\n                     \"set\",\n                     # - {% set variable = value %}\n                     # - {% set variable %}value{% endset %}\n                     # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n                    # When the second format is used, set the variable 'is_set'\n                     # to a non-None value. This info is used elsewhere, as\n                     # literals inside a {% set %} block require special handling\n                     # during the trace.\n                     trimmed_content_parts = trimmed_content.split(maxsplit=2)\n                    if len(trimmed_content_parts) <= 2 or not trimmed_content_parts[\n                        2\n                    ].startswith(\"=\"):\n                         set_idx = len(result)\n                elif block_type == \"block_end\" and set_idx is not None:\n                     # Exiting a {% set %} block. Clear the indicator variable.\n                     set_idx = None\n                 m = regex.search(r\"\\s+$\", raw, regex.MULTILINE | regex.DOTALL)",
    "output": "             # sliced_file to reflect the mapping of the added character(s) back\n             # to the raw SQL.\n             templated_sql = templated_sql + \"\\n\" * n_trailing_newlines\n            if sliced_file and sliced_file[-1].templated_slice.stop != len(\n                templated_sql\n            ):\n                sliced_file.append(\n                    TemplatedFileSlice(\n                        slice_type=\"literal\",\n                        source_slice=slice(\n                            len(source_dbt_sql) - n_trailing_newlines,\n                            len(source_dbt_sql),\n                        ),\n                        templated_slice=slice(\n                            len(templated_sql) - n_trailing_newlines, len(templated_sql)\n                        ),\n                    )\n                 )\n         return (\n             TemplatedFile(\n                 source_str=source_dbt_sql,\ndiff --git a/src/sqlfluff/core/templaters/base.py b/src/sqlfluff/core/templaters/base.py\n         templated_str: Optional[str] = None,\n         sliced_file: Optional[List[TemplatedFileSlice]] = None,\n         raw_sliced: Optional[List[RawFileSlice]] = None,\n        check_consistency=True,\n     ):\n         \"\"\"Initialise the TemplatedFile.\n \n         self._source_newlines = list(iter_indices_of_newlines(self.source_str))\n         self._templated_newlines = list(iter_indices_of_newlines(self.templated_str))\n \n        # NOTE: The \"check_consistency\" flag should always be True when using\n        # SQLFluff in real life. This flag was only added because some legacy\n        # templater tests in test/core/templaters/jinja_test.py use hardcoded\n        # test data with issues that will trigger errors here. It would be cool\n        # to fix that data someday. I (Barry H.) started looking into it, but\n        # it was much trickier than I expected, because bits of the same data\n        # are shared across multiple tests.\n        if check_consistency:\n            # Sanity check raw string and slices.\n            pos = 0\n            rfs: RawFileSlice\n            for idx, rfs in enumerate(self.raw_sliced):\n                assert rfs.source_idx == pos\n                pos += len(rfs.raw)\n            assert pos == len(self.source_str)\n\n            # Sanity check templated string and slices.\n            previous_slice = None\n            tfs: Optional[TemplatedFileSlice] = None\n            for idx, tfs in enumerate(self.sliced_file):\n                if previous_slice:\n                    assert (\n                        tfs.templated_slice.start == previous_slice.templated_slice.stop\n                    )\n                else:\n                    assert tfs.templated_slice.start == 0\n                previous_slice = tfs\n            if self.sliced_file and templated_str is not None:\n                assert tfs.templated_slice.stop == len(templated_str)\n\n     @classmethod\n     def from_string(cls, raw):\n         \"\"\"Create TemplatedFile from a string.\"\"\"\ndiff --git a/src/sqlfluff/core/templaters/slicers/tracer.py b/src/sqlfluff/core/templaters/slicers/tracer.py\n             except IndexError:\n                 pos2 = len(trace_template_output)\n             p = trace_template_output[pos1 + 1 : pos2]\n             m_id = regex.match(r\"^([0-9a-f]+)(_(\\d+))?\", p)\n             if not m_id:\n                 raise ValueError(  # pragma: no cover\n             alt_id, content_info, literal = value\n             target_slice_idx = self.find_slice_index(alt_id)\n             slice_length = content_info if literal else len(str(content_info))\n            self.move_to_slice(target_slice_idx, slice_length)\n         return JinjaTrace(\n             self.make_template(self.raw_str).render(), self.raw_sliced, self.sliced_file\n         )\n                         idx,\n                     )\n                 )\n                if set_idx is None:\n                    rsi = self.slice_info_for_literal(\n                        len(raw), \"\" if set_idx is None else \"set\"\n                    )\n                else:\n                    # For \"set\" blocks, don't generate alternate ID or code.\n                    # Sometimes, dbt users use {% set %} blocks to generate\n                    # queries that get sent to actual databases, thus causing\n                    # errors if we tamper with it.\n                    rsi = RawSliceInfo(None, None, [])\n                self.raw_slice_info[result[-1]] = rsi\n                 idx += len(raw)\n                 continue\n             str_buff += raw\n                         # effects, but return a unique slice ID.\n                         if trimmed_content:\n                             assert m_open and m_close\n                            # For \"set\" blocks, don't generate alternate ID or\n                            # code. Sometimes, dbt users use {% set %} blocks to\n                            # generate queries that get sent to actual\n                            # databases, thus causing errors if we tamper with\n                            # it.\n                            if set_idx is None:\n                                unique_id = self.next_slice_id()\n                                unique_alternate_id = unique_id\n                                open_ = m_open.group(1)\n                                close_ = m_close.group(1)\n                                alternate_code = (\n                                    f\"\\0{unique_alternate_id} {open_} \"\n                                    f\"{trimmed_content} {close_}\"\n                                )\n                 if block_type == \"block_start\" and trimmed_content.split()[0] in (\n                     \"macro\",\n                     \"set\",\n                     # - {% set variable = value %}\n                     # - {% set variable %}value{% endset %}\n                     # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n                    # When the second format is used, set the variable 'set_idx'\n                     # to a non-None value. This info is used elsewhere, as\n                     # literals inside a {% set %} block require special handling\n                     # during the trace.\n                     trimmed_content_parts = trimmed_content.split(maxsplit=2)\n                    if len(trimmed_content_parts) <= 2 or (\n                        not trimmed_content_parts[1].endswith(\"=\")\n                        and not trimmed_content_parts[2].startswith(\"=\")\n                    ):\n                         set_idx = len(result)\n                elif (\n                    block_type == \"block_end\"\n                    and set_idx is not None\n                    and (\n                        trimmed_content.startswith(\"endset\")\n                        or trimmed_content.startswith(\"endmacro\")\n                    )\n                ):\n                     # Exiting a {% set %} block. Clear the indicator variable.\n                     set_idx = None\n                 m = regex.search(r\"\\s+$\", raw, regex.MULTILINE | regex.DOTALL)"
  },
  {
    "instruction": "Whitespace token is_whitespace is False\nI expect segment.is_whitespace of a Whitespace token is True, however, it is set to False.\r\n\r\n## Expected Behaviour\r\nsegment.is_whitespace return True\r\n\r\n## Observed Behaviour\r\nsegment.is_whitespace return False\r\n## Steps to Reproduce\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\n## Configuration\r\n```\r\nInclude your SQLFluff configuration here\r\n```\r\n\n",
    "input": " ansi_dialect.set_lexer_struct(\n     [\n         # name, type, pattern, kwargs\n        (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n         (\n             \"inline_comment\",\n             \"regex\",\n             dict(\n                 is_comment=True,\n                 type=\"comment\",\n                subdivide=dict(type=\"newline\", name=\"newline\", regex=r\"\\r\\n|\\n\"),\n                 trim_post_subdivide=dict(\n                    type=\"whitespace\", name=\"whitespace\", regex=r\"[\\t ]+\"\n                 ),\n             ),\n         ),\n         (\"not_equal\", \"regex\", r\"!=|<>\", dict(is_code=True)),\n         (\"greater_than_or_equal\", \"regex\", r\">=\", dict(is_code=True)),\n         (\"less_than_or_equal\", \"regex\", r\"<=\", dict(is_code=True)),\n        (\"newline\", \"regex\", r\"\\r\\n|\\n\", dict(type=\"newline\")),\n         (\"casting_operator\", \"regex\", r\"::\", dict(is_code=True)),\n         (\"concat_operator\", \"regex\", r\"\\|\\|\", dict(is_code=True)),\n         (\"equals\", \"singleton\", \"=\", dict(is_code=True)),\ndiff --git a/src/sqlfluff/core/parser/lexer.py b/src/sqlfluff/core/parser/lexer.py\n         idx = 0\n \n         if self.trim_post_subdivide:\n            trimmer = re.compile(self.trim_post_subdivide[\"regex\"], re.DOTALL)\n            TrimClass = RawSegment.make(\n                self.trim_post_subdivide[\"regex\"],\n                name=self.trim_post_subdivide[\"name\"],\n                type=self.trim_post_subdivide[\"type\"],\n            )\n \n             for trim_mat in trimmer.finditer(matched):\n                 trim_span = trim_mat.span()\n             seg_buff = ()\n             str_buff = matched\n             pos_buff = start_pos\n            divider = re.compile(self.subdivide[\"regex\"], re.DOTALL)\n            DividerClass = RawSegment.make(\n                self.subdivide[\"regex\"],\n                name=self.subdivide[\"name\"],\n                type=self.subdivide[\"type\"],\n            )\n \n             while True:\n                 # Iterate through subdividing as appropriate",
    "output": " ansi_dialect.set_lexer_struct(\n     [\n         # name, type, pattern, kwargs\n        (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\", is_whitespace=True)),\n         (\n             \"inline_comment\",\n             \"regex\",\n             dict(\n                 is_comment=True,\n                 type=\"comment\",\n                subdivide=dict(\n                    type=\"newline\", name=\"newline\", regex=r\"\\r\\n|\\n\", is_whitespace=True\n                ),\n                 trim_post_subdivide=dict(\n                    type=\"whitespace\",\n                    name=\"whitespace\",\n                    regex=r\"[\\t ]+\",\n                    is_whitespace=True,\n                 ),\n             ),\n         ),\n         (\"not_equal\", \"regex\", r\"!=|<>\", dict(is_code=True)),\n         (\"greater_than_or_equal\", \"regex\", r\">=\", dict(is_code=True)),\n         (\"less_than_or_equal\", \"regex\", r\"<=\", dict(is_code=True)),\n        (\"newline\", \"regex\", r\"\\r\\n|\\n\", dict(type=\"newline\", is_whitespace=True)),\n         (\"casting_operator\", \"regex\", r\"::\", dict(is_code=True)),\n         (\"concat_operator\", \"regex\", r\"\\|\\|\", dict(is_code=True)),\n         (\"equals\", \"singleton\", \"=\", dict(is_code=True)),\ndiff --git a/src/sqlfluff/core/parser/lexer.py b/src/sqlfluff/core/parser/lexer.py\n         idx = 0\n \n         if self.trim_post_subdivide:\n            class_kwargs = self.trim_post_subdivide.copy()\n            pattern = class_kwargs.pop(\"regex\")\n            trimmer = re.compile(pattern, re.DOTALL)\n            TrimClass = RawSegment.make(pattern, **class_kwargs)\n \n             for trim_mat in trimmer.finditer(matched):\n                 trim_span = trim_mat.span()\n             seg_buff = ()\n             str_buff = matched\n             pos_buff = start_pos\n            class_kwargs = self.subdivide.copy()\n            pattern = class_kwargs.pop(\"regex\")\n            divider = re.compile(pattern, re.DOTALL)\n            DividerClass = RawSegment.make(pattern, **class_kwargs)\n \n             while True:\n                 # Iterate through subdividing as appropriate"
  },
  {
    "instruction": "--disable_progress_bar Flag Broken for Fix\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nI ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n\n### Expected Behaviour\n\nShould run as expected, with no error and no progress bar.\n\n### Observed Behaviour\n\nExit code 2 and stderr:\r\n```\r\nUsage: sqlfluff fix [OPTIONS] [PATHS]...\r\n        Try 'sqlfluff fix -h' for help.\r\n\r\n        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\r\n```\n\n### How to reproduce\n\nSql file:\r\n```\r\nSELECT foo FROM bar;\r\n```\r\n\r\nCommand:\r\n```\r\nsqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\npython 3.10.3\r\nsqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n\n### Configuration\n\nNo special configuration. Ran hermetically with `trunk`.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n--disable_progress_bar Flag Broken for Fix\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nI ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n\n### Expected Behaviour\n\nShould run as expected, with no error and no progress bar.\n\n### Observed Behaviour\n\nExit code 2 and stderr:\r\n```\r\nUsage: sqlfluff fix [OPTIONS] [PATHS]...\r\n        Try 'sqlfluff fix -h' for help.\r\n\r\n        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\r\n```\n\n### How to reproduce\n\nSql file:\r\n```\r\nSELECT foo FROM bar;\r\n```\r\n\r\nCommand:\r\n```\r\nsqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\npython 3.10.3\r\nsqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n\n### Configuration\n\nNo special configuration. Ran hermetically with `trunk`.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "     return False  # pragma: no cover\n \n \n@cli.command()\n @common_options\n @core_options\n @click.option(\n     ),\n )\n @click.option(\n     \"--disable-progress-bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n )\n @click.option(\n     \"--FIX-EVEN-UNPARSABLE\",",
    "output": "     return False  # pragma: no cover\n \n \n@cli.command(cls=DeprecatedOptionsCommand)\n @common_options\n @core_options\n @click.option(\n     ),\n )\n @click.option(\n    \"--disable_progress_bar\",\n     \"--disable-progress-bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n    cls=DeprecatedOption,\n    deprecated=[\"--disable_progress_bar\"],\n )\n @click.option(\n     \"--FIX-EVEN-UNPARSABLE\","
  },
  {
    "instruction": "TypeError when using integer placeholder\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nAn exception occurs when trying to use integer substituents.\r\n\r\n### Expected Behaviour\r\n\r\nWork without errors.\r\n\r\n### Observed Behaviour\r\n\r\n\r\nAn exception occurs:\r\n```\r\n  ...\r\n  File \"venv/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 816, in render_file\r\n    return self.render_string(raw_file, fname, config, encoding)\r\n  File \"venv/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 787, in render_string\r\n    templated_file, templater_violations = self.templater.process(\r\n  File \"venv/lib/python3.9/site-packages/sqlfluff/core/templaters/placeholder.py\", line 183, in process\r\n    start_template_pos, start_template_pos + len(replacement), None\r\nTypeError: object of type 'int' has no len()\r\n\r\n```\r\n\r\n### How to reproduce\r\n\r\n1. Create a file `example.sql`:\r\n```\r\nSELECT 1\r\nLIMIT %(capacity)s;\r\n```\r\n2. Copy `.sqlfluff` from the Configuration section\r\n3. Run `sqlfluff lint --dialect postgres example.sql`\r\n\r\n### Dialect\r\n\r\npostgres\r\n\r\n### Version\r\n\r\nsqlfluff, version 0.13.1\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\nexclude_rules = L031\r\ntemplater = placeholder\r\n\r\n[sqlfluff:templater:placeholder]\r\nparam_style = pyformat\r\ncapacity = 15\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [ ] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\nSupport Postgres-style variable substitution\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nThe Postgres `psql` utility supports flavor of colon-style variable substitution that currently confuses sqlfluff.  E.g.,\r\n\r\n```sql\r\nALTER TABLE name:variable RENAME TO name;\r\n```\r\n\r\nRunning the above through sqlfluff produces this output:\r\n\r\n```\r\nsqlfluff lint --dialect postgres 2.sql\r\n== [2.sql] FAIL\r\nL:   1 | P:   1 |  PRS | Line 1, Position 1: Found unparsable section: 'ALTER\r\n                       | TABLE name:variable RENAME TO name...'\r\n```\n\n### Use case\n\nI would like it if in the above the string \"name:variable\" were considered a valid table name (and other identifiers similarly).\n\n### Dialect\n\nThis applies to the Postgres dialect.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": " KNOWN_STYLES = {\n     # e.g. WHERE bla = :name\n     \"colon\": regex.compile(r\"(?<![:\\w\\x5c]):(?P<param_name>\\w+)(?!:)\", regex.UNICODE),\n     # e.g. WHERE bla = :2\n     \"numeric_colon\": regex.compile(\n         r\"(?<![:\\w\\x5c]):(?P<param_name>\\d+)\", regex.UNICODE\n     \"pyformat\": regex.compile(\n         r\"(?<![:\\w\\x5c])%\\((?P<param_name>[\\w_]+)\\)s\", regex.UNICODE\n     ),\n    # e.g. WHERE bla = $name\n    \"dollar\": regex.compile(r\"(?<![:\\w\\x5c])\\$(?P<param_name>[\\w_]+)\", regex.UNICODE),\n     # e.g. WHERE bla = ?\n     \"question_mark\": regex.compile(r\"(?<![:\\w\\x5c])\\?\", regex.UNICODE),\n     # e.g. WHERE bla = $3\n                 param_name = found_param[\"param_name\"]\n             last_literal_length = span[0] - last_pos_raw\n             try:\n                replacement = context[param_name]\n             except KeyError as err:\n                 # TODO: Add a url here so people can get more help.\n                 raise SQLTemplaterError(",
    "output": " KNOWN_STYLES = {\n     # e.g. WHERE bla = :name\n     \"colon\": regex.compile(r\"(?<![:\\w\\x5c]):(?P<param_name>\\w+)(?!:)\", regex.UNICODE),\n    # e.g. WHERE bla = table:name - use with caution as more prone to false positives\n    \"colon_nospaces\": regex.compile(r\":(?P<param_name>\\w+)\", regex.UNICODE),\n     # e.g. WHERE bla = :2\n     \"numeric_colon\": regex.compile(\n         r\"(?<![:\\w\\x5c]):(?P<param_name>\\d+)\", regex.UNICODE\n     \"pyformat\": regex.compile(\n         r\"(?<![:\\w\\x5c])%\\((?P<param_name>[\\w_]+)\\)s\", regex.UNICODE\n     ),\n    # e.g. WHERE bla = $name or WHERE bla = ${name}\n    \"dollar\": regex.compile(\n        r\"(?<![:\\w\\x5c])\\${?(?P<param_name>[\\w_]+)}?\", regex.UNICODE\n    ),\n     # e.g. WHERE bla = ?\n     \"question_mark\": regex.compile(r\"(?<![:\\w\\x5c])\\?\", regex.UNICODE),\n     # e.g. WHERE bla = $3\n                 param_name = found_param[\"param_name\"]\n             last_literal_length = span[0] - last_pos_raw\n             try:\n                replacement = str(context[param_name])\n             except KeyError as err:\n                 # TODO: Add a url here so people can get more help.\n                 raise SQLTemplaterError("
  },
  {
    "instruction": "L042 loop limit on fixes reached when CTE itself contains a subquery\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nWhile running `sqlfluff fix --dialect snowflake` on a sql file, I get \r\n```\r\n==== finding fixable violations ====\r\nWARNING    Loop limit on fixes reached [10].                                                                                                                                                              \r\n==== no fixable linting violations found ====                                                                                                                                                             \r\nAll Finished \ud83d\udcdc \ud83c\udf89!\r\n  [22 unfixable linting violations found]\r\n```\r\n\r\n```\r\nINSERT OVERWRITE INTO dwh.test_table\r\n\r\nWITH cte1 AS (\r\n\tSELECT *\r\n\tFROM (SELECT\r\n\t\t*,\r\n\t\tROW_NUMBER() OVER (PARTITION BY r ORDER BY updated_at DESC) AS latest\r\n\t\tFROM mongo.temp\r\n\tWHERE latest = 1\r\n))\r\n\r\nSELECT * FROM cte1 WHERE 1=1;\r\n```\r\n\r\nAll of the 22  violations are a mix of L002, L003 and L004.\r\n\r\n### Expected Behaviour\r\n\r\n`sqlfluff` should be able to fix the violations\r\n\r\n### Observed Behaviour\r\n\r\nEven if I try to fix the violations manually, it still shows the same error.\r\n\r\n### How to reproduce\r\n\r\nI will try to generate a sql file that will be able to reproduce the issue\r\n\r\n### Dialect\r\n\r\nSnowflake\r\n\r\n### Version\r\n\r\n1.1.0\r\n\r\n### Configuration\r\n\r\n```\r\n# https://docs.sqlfluff.com/en/stable/rules.html\r\n\r\n[sqlfluff]\r\nexclude_rules = L029, L031, L034\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = true\r\nindented_using_on = true\r\n\r\n[sqlfluff:rules:L002]\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L003]\r\nhanging_indents = true\r\nindent_unit = tab\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L004]\r\nindent_unit = tab\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L011]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L012]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L016]\r\nignore_comment_clauses = true\r\nignore_comment_lines = true\r\nindent_unit = tab\r\ntab_space_size = 4\r\n\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n[sqlfluff:rules:L022]\r\ncomma_style = trailing\r\n\r\n[sqlfluff:rules:L028]\r\nsingle_table_references = unqualified\r\n\r\n[sqlfluff:rules:L030]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L040]\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L042]\r\nforbid_subquery_in = both\r\n\r\n[sqlfluff:rules:L054]\r\ngroup_by_and_order_by_style = explicit\r\n\r\n[sqlfluff:rules:L063]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L066]\r\nmin_alias_length = 3\r\nmax_alias_length = 15\r\n\r\n[sqlfluff:templater:jinja:context]\r\nparams = {\"DB\": \"DEMO\"}\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": "         \"\"\"Add an element.\"\"\"\n         self.map[self.key(value)] = value\n \n     def discard(self, value):  # MutableSet\n         \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n         self.map.pop(self.key(value), None)  # pragma: no cover\ndiff --git a/src/sqlfluff/rules/L028.py b/src/sqlfluff/rules/L028.py\n from typing import Iterator, List, Optional, Set\n \n from sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser.segments.base import BaseSegment\n from sqlfluff.core.parser.segments.raw import SymbolSegment\n from sqlfluff.utils.analysis.select_crawler import Query, SelectCrawler\n from sqlfluff.core.rules import (\n     BaseRule,\n \n         if not FunctionalContext(context).parent_stack.any(sp.is_type(*_START_TYPES)):\n             crawler = SelectCrawler(context.segment, context.dialect)\n             if crawler.query_tree:\n                 # Recursively visit and check each query in the tree.\n                return list(self._visit_queries(crawler.query_tree))\n         return None\n \n    def _visit_queries(self, query: Query) -> Iterator[LintResult]:\n         if query.selectables:\n             select_info = query.selectables[0].select_info\n             # How many table names are visible from here? If more than one then do\n                     self._fix_inconsistent_to,\n                     fixable,\n                 )\n        for child in query.children:\n            yield from self._visit_queries(child)\n \n \n def _check_references(\ndiff --git a/src/sqlfluff/rules/L042.py b/src/sqlfluff/rules/L042.py\n import copy\n from functools import partial\n from typing import (\n    Generator,\n     List,\n     NamedTuple,\n     Optional,\n     TypeVar,\n     cast,\n )\n from sqlfluff.core.dialects.base import Dialect\n from sqlfluff.core.parser.segments.base import BaseSegment\n from sqlfluff.core.parser.segments.raw import (\n     CodeSegment,\n     SymbolSegment,\n     WhitespaceSegment,\n )\nfrom sqlfluff.core.rules import BaseRule, LintFix, LintResult, RuleContext\n from sqlfluff.utils.analysis.select import get_select_statement_info\n from sqlfluff.core.rules.crawlers import SegmentSeekerCrawler\n from sqlfluff.core.rules.doc_decorators import (\n     document_configuration,\n \n \n class _NestedSubQuerySummary(NamedTuple):\n    parent_clause_type: str\n    parent_select_segments: Segments\n    clause_segments: Segments\n    subquery: BaseSegment\n \n \n @document_groups\n         \"both\": [\"join_clause\", \"from_expression_element\"],\n     }\n \n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n         \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\"\"\"\n         self.forbid_subquery_in: str\n        parent_types = self._config_mapping[self.forbid_subquery_in]\n        segment = FunctionalContext(context).segment\n        parent_stack = FunctionalContext(context).parent_stack\n         is_select_child = parent_stack.any(is_type(*_SELECT_TYPES))\n        if is_select_child:\n             # Nothing to do.\n             return None\n \n        # Gather all possible offending Elements in one crawl\n        nested_subqueries: List[_NestedSubQuerySummary] = []\n        selects = segment.recursive_crawl(*_SELECT_TYPES, recurse_into=True)\n        for select in selects.iterate_segments():\n            for res in _find_nested_subqueries(select, context.dialect):\n                if res.parent_clause_type not in parent_types:\n                    continue\n                nested_subqueries.append(res)\n \n        if not nested_subqueries:\n            return None\n         # If there are offending elements calculate fixes\n        return _calculate_fixes(\n             dialect=context.dialect,\n            root_select=segment,\n            nested_subqueries=nested_subqueries,\n            parent_stack=parent_stack,\n         )\n \n\ndef _calculate_fixes(\n    dialect: Dialect,\n    root_select: Segments,\n    nested_subqueries: List[_NestedSubQuerySummary],\n    parent_stack: Segments,\n) -> List[LintResult]:\n    \"\"\"Given the Root select and the offending subqueries calculate fixes.\"\"\"\n    is_with = root_select.all(is_type(\"with_compound_statement\"))\n    # TODO: consider if we can fix recursive CTEs\n    is_recursive = is_with and len(root_select.children(is_keyword(\"recursive\"))) > 0\n    case_preference = _get_case_preference(root_select)\n    # generate an instance which will track and shape our output CTE\n    ctes = _CTEBuilder()\n    # Init the output/final select &\n    # populate existing CTEs\n    for cte in root_select.children(is_type(\"common_table_expression\")):\n        assert isinstance(cte, CTEDefinitionSegment), \"TypeGuard\"\n        ctes.insert_cte(cte)\n\n    output_select = root_select\n    if is_with:\n        output_select = root_select.children(\n            is_type(\n                \"set_expression\",\n                \"select_statement\",\n             )\n        )\n \n    lint_results: List[LintResult] = []\n    clone_map = SegmentCloneMap(root_select[0])\n    is_new_name = False\n    new_table_ref = None\n    for parent_type, _, this_seg, subquery in nested_subqueries:\n        alias_name, is_new_name = ctes.create_cte_alias(\n            this_seg.children(is_type(\"alias_expression\"))\n        )\n        new_cte = _create_cte_seg(\n            alias_name=alias_name,\n            subquery=clone_map[subquery],\n            case_preference=case_preference,\n            dialect=dialect,\n        )\n        ctes.insert_cte(new_cte)\n        this_seg_clone = clone_map[this_seg[0]]\n        assert this_seg_clone.pos_marker, \"TypeGuard\"\n        new_table_ref = _create_table_ref(alias_name, dialect)\n        this_seg_clone.segments = (new_table_ref,)\n        anchor = subquery\n        # Grab the first keyword or symbol in the subquery to use as the\n        # anchor. This makes the lint warning less likely to be filtered out\n        # if a bit of the subquery happens to be templated.\n        for seg in subquery.recursive_crawl(\"keyword\", \"symbol\"):\n            anchor = seg\n            break\n        res = LintResult(\n            anchor=anchor,\n            description=f\"{parent_type} clauses should not contain \"\n            \"subqueries. Use CTEs instead\",\n            fixes=[],\n        )\n        lint_results.append(res)\n\n    # Issue 3617: In T-SQL (and possibly other dialects) the automated fix\n    # leaves parentheses in a location that causes a syntax error. This is an\n    # unusual corner case. For simplicity, we still generate the lint warning\n    # but don't try to generate a fix. Someone could look at this later (a\n    # correct fix would involve removing the parentheses.)\n    bracketed_ctas = [seg.type for seg in parent_stack[-2:]] == [\n        \"create_table_statement\",\n        \"bracketed\",\n    ]\n    if bracketed_ctas or ctes.has_duplicate_aliases() or is_recursive:\n        # If we have duplicate CTE names just don't fix anything\n        # Return the lint warnings anyway\n        return lint_results\n\n    # Add fixes to the last result only\n    edit = [\n        ctes.compose_select(\n            clone_map[output_select[0]],\n            case_preference=case_preference,\n        ),\n    ]\n    lint_results[-1].fixes = [\n        LintFix.replace(\n            root_select[0],\n            edit_segments=edit,\n        )\n    ]\n    if is_new_name:\n        assert lint_results[0].fixes[0].edit\n        assert new_table_ref\n        # If we're creating a new CTE name but the CTE name does not appear in\n        # the fix, discard the lint error. This prevents the rule from looping,\n        # i.e. making the same fix repeatedly.\n        if not any(\n            seg.uuid == new_table_ref.uuid for seg in edit[0].recursive_crawl_all()\n        ):\n            lint_results[-1].fixes = []\n    return lint_results\n \n \n def _get_first_select_statement_descendant(\n     return None  # pragma: no cover\n \n \ndef _get_sources_from_select(segment: BaseSegment, dialect: Dialect) -> Set[str]:\n    \"\"\"Given segment, return set of table or alias names it queries from.\"\"\"\n    result = set()\n    select = None\n    if segment.is_type(\"select_statement\"):\n        select = segment\n    elif segment.is_type(\"with_compound_statement\"):\n        # For WITH statement, process the main query underneath.\n        select = _get_first_select_statement_descendant(segment)\n    if select and select.is_type(\"select_statement\"):\n        select_info = get_select_statement_info(select, dialect)\n        if select_info:\n            for a in select_info.table_aliases:\n                # For each table in FROM, return table name and any alias.\n                if a.ref_str:\n                    result.add(a.ref_str)\n                if a.object_reference:\n                    result.add(a.object_reference.raw)\n    return result\n\n\n def _is_correlated_subquery(\n     nested_select: Segments, select_source_names: Set[str], dialect: Dialect\n ):\n \n     https://en.wikipedia.org/wiki/Correlated_subquery\n     \"\"\"\n    if not nested_select:\n        return False  # pragma: no cover\n     select_statement = _get_first_select_statement_descendant(nested_select[0])\n     if not select_statement:\n         return False  # pragma: no cover\n     return False\n \n \ndef _find_nested_subqueries(\n    select: Segments,\n    dialect: Dialect,\n) -> Generator[_NestedSubQuerySummary, None, None]:\n    \"\"\"Find possible offending elements and return enough to fix them.\"\"\"\n    select_types = [\n        \"with_compound_statement\",\n        \"set_expression\",\n        \"select_statement\",\n    ]\n    from_clause = select.children().first(is_type(\"from_clause\")).children()\n    offending_types = [\"join_clause\", \"from_expression_element\"]\n    select_source_names = _get_sources_from_select(select[0], dialect)\n\n    # Match any of the types we care about\n    for this_seg in from_clause.children(is_type(*offending_types)).iterate_segments():\n        parent_type = this_seg[0].get_type()\n        # Ensure we are at the right depth (from_expression_element)\n        if not this_seg.all(is_type(\"from_expression_element\")):\n            this_seg = this_seg.children(\n                is_type(\"from_expression_element\"),\n            )\n\n        table_expression_el = this_seg.children(\n            is_type(\"table_expression\"),\n        )\n\n        # Is it bracketed? If so, lint that instead.\n        bracketed_expression = table_expression_el.children(\n            is_type(\"bracketed\"),\n        )\n        nested_select = bracketed_expression or table_expression_el\n        # If we find a child with a \"problem\" type, raise an issue.\n        # If not, we're fine.\n        seg = nested_select.children(is_type(*select_types))\n        if not seg:\n            # If there is no match there is no error\n            continue\n        # Type, parent_select, parent_sequence\n        if not _is_correlated_subquery(nested_select, select_source_names, dialect):\n            yield _NestedSubQuerySummary(\n                parent_type, select, this_seg, table_expression_el[0]\n            )\n\n\n class _CTEBuilder:\n     \"\"\"Gather CTE parts, maintain order and track naming/aliasing.\"\"\"\n \n     def insert_cte(self, cte: CTEDefinitionSegment):\n         \"\"\"Add a new CTE to the list as late as possible but before all its parents.\"\"\"\n         # This should still have the position markers of its true position\n        inbound_subquery = Segments(cte).children().last()\n         insert_position = next(\n             (\n                 i\n \n         self.ctes.insert(insert_position, cte)\n \n    def create_cte_alias(\n        self, alias_segment: Optional[Segments] = None\n    ) -> Tuple[str, bool]:\n         \"\"\"Find or create the name for the next CTE.\"\"\"\n        if alias_segment:\n             # If we know the name use it\n            name = alias_segment.children().last()[0].raw\n            return name, False\n \n         self.name_idx = self.name_idx + 1\n         name = f\"prep_{self.name_idx}\"\n         return name, True\n \n     def get_cte_segments(self) -> List[BaseSegment]:\n        \"\"\"Return a valid list of CTES with required padding Segements.\"\"\"\n         cte_segments: List[BaseSegment] = []\n         for cte in self.ctes:\n             cte_segments = cte_segments + [\n         )\n         return new_select\n \n \n def _is_child(maybe_parent: Segments, maybe_child: Segments) -> bool:\n     \"\"\"Is the child actually between the start and end markers of the parent.\"\"\"\n    assert len(maybe_child) == 1, \"Cannot assess Childness of multiple Segments\"\n    assert len(maybe_parent) == 1, \"Cannot assess Childness of multiple Parents\"\n     child_markers = maybe_child[0].pos_marker\n     parent_pos = maybe_parent[0].pos_marker\n    if not parent_pos or not child_markers:\n        return False  # pragma: no cover\n\n     if child_markers < parent_pos.start_point_marker():\n         return False  # pragma: no cover\n \ndiff --git a/src/sqlfluff/utils/analysis/select_crawler.py b/src/sqlfluff/utils/analysis/select_crawler.py\n     \"\"\"A \"SELECT\" query segment.\"\"\"\n \n     selectable: BaseSegment\n     dialect: Dialect\n \n     @cached_property\n     def select_info(self):\n         \"\"\"Returns SelectStatementColumnsAndTables on the SELECT.\"\"\"\n         \"\"\"Find corresponding table_aliases entry (if any) matching \"table\".\"\"\"\n         alias_info = [\n             t\n            for t in self.select_info.table_aliases\n             if t.aliased and t.ref_str == table\n         ]\n         assert len(alias_info) <= 1\n     parent: Optional[\"Query\"] = field(default=None)\n     # Children (could be CTE, subselect, or other).\n     children: List[\"Query\"] = field(default_factory=list)\n     cte_name_segment: Optional[BaseSegment] = field(default=None)\n \n     def lookup_cte(self, name: str, pop: bool = True) -> Optional[\"Query\"]:\n         \"\"\"Look up a CTE by name, in the current or any parent scope.\"\"\"\n         cte = self.ctes.get(name.upper())\n             return None\n \n     def crawl_sources(\n        self, segment: BaseSegment, recurse_into=True, pop=False\n     ) -> Generator[Union[str, \"Query\"], None, None]:\n         \"\"\"Find SELECTs, table refs, or value table function calls in segment.\n \n         references or function call strings, yield those.\n         \"\"\"\n         found_nested_select = False\n        for seg in segment.recursive_crawl(\n             \"table_reference\",\n             \"set_expression\",\n             \"select_statement\",\n             \"values_clause\",\n            recurse_into=recurse_into,\n         ):\n             if seg is segment:\n                 # If the starting segment itself matches the list of types we're\n                 # searching for, recursive_crawl() will return it. Skip that.\n                 continue\n \n             if seg.is_type(\"table_reference\"):\n                if not seg.is_qualified():\n                     cte = self.lookup_cte(seg.raw, pop=pop)\n                     if cte:\n                         # It's a CTE.\n                     \"set_expression\", \"select_statement\", \"values_clause\"\n                 )\n                 found_nested_select = True\n                crawler = SelectCrawler(seg, self.dialect, parent=self)\n                 # We know this will pass because we specified parent=self above.\n                 assert crawler.query_tree\n                 yield crawler.query_tree\n             except ValueError:\n                 pass\n \n        # Stores the last CTE name we saw, so we can associate it with the\n        # corresponding Query.\n        cte_name_segment: Optional[BaseSegment] = None\n \n         # Visit segment and all its children\n         for event, path in SelectCrawler.visit_segments(segment):\n                             # added to this Query later.\n                             query = self.query_class(QueryType.Simple, dialect)\n                             append_query(query)\n                        else:\n                             # It's a select_statement or values_clause.\n                            selectable = Selectable(path[-1], dialect)\n                             # Determine if this is part of a set_expression.\n                             if len(path) >= 2 and path[-2].is_type(\"set_expression\"):\n                                 # It's part of a set_expression. Append to the\n                                 append_query(query)\n                     else:\n                         # We're processing a \"with\" statement.\n                        if cte_name_segment:\n                             # If we have a CTE name, this is the Query for that\n                             # name.\n                             query = self.query_class(\n                                 QueryType.Simple,\n                                 dialect,\n                                cte_name_segment=cte_name_segment,\n                             )\n                             if path[-1].is_type(\n                                 \"select_statement\", \"values_clause\", \"update_statement\"\n                             ):\n                                 # Add to the Query object we just created.\n                                query.selectables.append(Selectable(path[-1], dialect))\n                             else:\n                                 # Processing a set_expression. Nothing\n                                 # additional to do here; we'll add selectables\n                                 # to the Query later when we encounter those\n                                 # child segments.\n                                 pass\n                            query_stack[-1].ctes[cte_name_segment.raw_upper] = query\n                            cte_name_segment = None\n                             append_query(query)\n                         else:\n                             # There's no CTE name, so we're probably processing\n                             # interested in CTEs and \"main\" queries, i.e.\n                             # standalones or those following a block of CTEs.\n                             if not any(\n                                seg.is_type(\"from_expression_element\") for seg in path\n                             ):\n                                 if path[-1].is_type(\n                                     \"select_statement\", \"update_statement\"\n                                     # Processing a select_statement. Add it to the\n                                     # Query object on top of the stack.\n                                     query_stack[-1].selectables.append(\n                                        Selectable(path[-1], dialect)\n                                     )\n                                 else:\n                                     # Processing a set_expression. Nothing\n                 elif path[-1].is_type(\"with_compound_statement\"):\n                     # Beginning a \"with\" statement, i.e. a block of CTEs.\n                     query = self.query_class(QueryType.WithCompound, dialect)\n                    if cte_name_segment:\n                        query_stack[-1].ctes[cte_name_segment.raw_upper] = query\n                        cte_name_segment = None\n                     append_query(query)\n                 elif path[-1].is_type(\"common_table_expression\"):\n                    # This is a \"<<cte name>> AS\". Grab the name for later.\n                    cte_name_segment = path[-1].segments[0]\n             elif event == \"end\":\n                 finish_segment()\n \n         return list(query.crawl_sources(segment, True))\n \n     @classmethod\n    def visit_segments(cls, seg, path=None):\n         \"\"\"Recursively visit all segments.\"\"\"\n         if path is None:\n             path = []\n         path.append(seg)\n         yield \"start\", path\n        for seg in seg.segments:\n            yield from cls.visit_segments(seg, path)\n         yield \"end\", path\n         path.pop()",
    "output": "         \"\"\"Add an element.\"\"\"\n         self.map[self.key(value)] = value\n \n    def update(self, value):\n        \"\"\"Add elements in 'value'.\"\"\"\n        for v in value:\n            self.add(v)\n\n     def discard(self, value):  # MutableSet\n         \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n         self.map.pop(self.key(value), None)  # pragma: no cover\ndiff --git a/src/sqlfluff/rules/L028.py b/src/sqlfluff/rules/L028.py\n from typing import Iterator, List, Optional, Set\n \n from sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\nfrom sqlfluff.core.parser.segments.base import BaseSegment, IdentitySet\n from sqlfluff.core.parser.segments.raw import SymbolSegment\nfrom sqlfluff.utils.analysis.select import SelectStatementColumnsAndTables\n from sqlfluff.utils.analysis.select_crawler import Query, SelectCrawler\n from sqlfluff.core.rules import (\n     BaseRule,\n \n         if not FunctionalContext(context).parent_stack.any(sp.is_type(*_START_TYPES)):\n             crawler = SelectCrawler(context.segment, context.dialect)\n            visited: IdentitySet = IdentitySet()\n             if crawler.query_tree:\n                 # Recursively visit and check each query in the tree.\n                return list(self._visit_queries(crawler.query_tree, visited))\n         return None\n \n    def _visit_queries(\n        self, query: Query, visited: IdentitySet\n    ) -> Iterator[LintResult]:\n        select_info: Optional[SelectStatementColumnsAndTables] = None\n         if query.selectables:\n             select_info = query.selectables[0].select_info\n             # How many table names are visible from here? If more than one then do\n                     self._fix_inconsistent_to,\n                     fixable,\n                 )\n        children = list(query.children)\n        # 'query.children' includes CTEs and \"main\" queries, but not queries in\n        # the \"FROM\" list. We want to visit those as well.\n        if select_info:\n            for a in select_info.table_aliases:\n                for q in SelectCrawler.get(query, a.from_expression_element):\n                    if not isinstance(q, Query):\n                        continue\n                    # Check for previously visited selectables to avoid possible\n                    # infinite recursion, e.g.:\n                    #   WITH test1 AS (SELECT i + 1, j + 1 FROM test1)\n                    #   SELECT * FROM test1;\n                    if any(s.selectable in visited for s in q.selectables):\n                        continue\n                    visited.update(s.selectable for s in q.selectables)\n                    children.append(q)\n        for child in children:\n            yield from self._visit_queries(child, visited)\n \n \n def _check_references(\ndiff --git a/src/sqlfluff/rules/L042.py b/src/sqlfluff/rules/L042.py\n import copy\n from functools import partial\n from typing import (\n    Iterator,\n     List,\n     NamedTuple,\n     Optional,\n     TypeVar,\n     cast,\n )\n\n from sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.dialects.common import AliasInfo\n from sqlfluff.core.parser.segments.base import BaseSegment\n from sqlfluff.core.parser.segments.raw import (\n     CodeSegment,\n     SymbolSegment,\n     WhitespaceSegment,\n )\nfrom sqlfluff.core.rules import (\n    BaseRule,\n    EvalResultType,\n    LintFix,\n    LintResult,\n    RuleContext,\n)\n from sqlfluff.utils.analysis.select import get_select_statement_info\nfrom sqlfluff.utils.analysis.select_crawler import Query, Selectable, SelectCrawler\n from sqlfluff.core.rules.crawlers import SegmentSeekerCrawler\n from sqlfluff.core.rules.doc_decorators import (\n     document_configuration,\n \n \n class _NestedSubQuerySummary(NamedTuple):\n    query: Query\n    selectable: Selectable\n    table_alias: AliasInfo\n    sc: SelectCrawler\n    select_source_names: Set[str]\n \n \n @document_groups\n         \"both\": [\"join_clause\", \"from_expression_element\"],\n     }\n \n    def _eval(self, context: RuleContext) -> EvalResultType:\n         \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\"\"\"\n         self.forbid_subquery_in: str\n        functional_context = FunctionalContext(context)\n        segment = functional_context.segment\n        parent_stack = functional_context.parent_stack\n        is_select = segment.all(is_type(*_SELECT_TYPES))\n         is_select_child = parent_stack.any(is_type(*_SELECT_TYPES))\n        if not is_select or is_select_child:\n             # Nothing to do.\n             return None\n \n        crawler = SelectCrawler(context.segment, context.dialect)\n        assert crawler.query_tree\n\n        # generate an instance which will track and shape our output CTE\n        ctes = _CTEBuilder()\n        # Init the output/final select &\n        # populate existing CTEs\n        for cte in crawler.query_tree.ctes.values():\n            ctes.insert_cte(cte.cte_definition_segment)  # type: ignore\n\n        is_with = segment.all(is_type(\"with_compound_statement\"))\n        # TODO: consider if we can fix recursive CTEs\n        is_recursive = is_with and len(segment.children(is_keyword(\"recursive\"))) > 0\n        case_preference = _get_case_preference(segment)\n        output_select = segment\n        if is_with:\n            output_select = segment.children(\n                is_type(\n                    \"set_expression\",\n                    \"select_statement\",\n                )\n            )\n \n         # If there are offending elements calculate fixes\n        clone_map = SegmentCloneMap(segment[0])\n        result = self._lint_query(\n             dialect=context.dialect,\n            query=crawler.query_tree,\n            ctes=ctes,\n            case_preference=case_preference,\n            clone_map=clone_map,\n         )\n \n        if result:\n            lint_result, from_expression, alias_name, subquery_parent = result\n            assert any(\n                from_expression is seg for seg in subquery_parent.recursive_crawl_all()\n             )\n            this_seg_clone = clone_map[from_expression]\n            new_table_ref = _create_table_ref(alias_name, context.dialect)\n            this_seg_clone.segments = [new_table_ref]\n            ctes.replace_with_clone(subquery_parent, clone_map)\n\n            # Issue 3617: In T-SQL (and possibly other dialects) the automated fix\n            # leaves parentheses in a location that causes a syntax error. This is an\n            # unusual corner case. For simplicity, we still generate the lint warning\n            # but don't try to generate a fix. Someone could look at this later (a\n            # correct fix would involve removing the parentheses.)\n            bracketed_ctas = [seg.type for seg in parent_stack[-2:]] == [\n                \"create_table_statement\",\n                \"bracketed\",\n            ]\n            if bracketed_ctas or ctes.has_duplicate_aliases() or is_recursive:\n                # If we have duplicate CTE names just don't fix anything\n                # Return the lint warnings anyway\n                return lint_result\n\n            # Compute fix.\n            edit = [\n                ctes.compose_select(\n                    clone_map[output_select[0]],\n                    case_preference=case_preference,\n                ),\n            ]\n            lint_result.fixes = [\n                LintFix.replace(\n                    segment[0],\n                    edit_segments=edit,\n                )\n            ]\n            return lint_result\n        return None\n \n    def _nested_subqueries(\n        self, query: Query, dialect: Dialect\n    ) -> Iterator[_NestedSubQuerySummary]:\n        parent_types = self._config_mapping[self.forbid_subquery_in]\n        for q in [query] + list(query.ctes.values()):\n            for selectable in q.selectables:\n                if not selectable.select_info:\n                    continue  # pragma: no cover\n                select_source_names = set()\n                for a in selectable.select_info.table_aliases:\n                    # For each table in FROM, return table name and any alias.\n                    if a.ref_str:\n                        select_source_names.add(a.ref_str)\n                    if a.object_reference:\n                        select_source_names.add(a.object_reference.raw)\n                for table_alias in selectable.select_info.table_aliases:\n                    sc = SelectCrawler(table_alias.from_expression_element, dialect)\n                    if sc.query_tree:\n                        path_to = selectable.selectable.path_to(\n                            table_alias.from_expression_element\n                        )\n                        if not any(seg.is_type(*parent_types) for seg in path_to):\n                            continue\n                        if _is_correlated_subquery(\n                            Segments(sc.query_tree.selectables[0].selectable),\n                            select_source_names,\n                            dialect,\n                        ):\n                            continue\n                        yield _NestedSubQuerySummary(\n                            q, selectable, table_alias, sc, select_source_names\n                        )\n\n    def _lint_query(\n        self,\n        dialect: Dialect,\n        query: Query,\n        ctes: \"_CTEBuilder\",\n        case_preference,\n        clone_map,\n    ) -> Optional[Tuple[LintResult, BaseSegment, str, BaseSegment]]:\n        \"\"\"Given the root query, compute lint warnings.\"\"\"\n        nsq: _NestedSubQuerySummary\n        for nsq in self._nested_subqueries(query, dialect):\n            alias_name, is_new_name = ctes.create_cte_alias(nsq.table_alias)\n            anchor = nsq.table_alias.from_expression_element.segments[0]\n            new_cte = _create_cte_seg(\n                alias_name=alias_name,\n                subquery=clone_map[anchor],\n                case_preference=case_preference,\n                dialect=dialect,\n            )\n            ctes.insert_cte(new_cte)\n\n            # Grab the first keyword or symbol in the subquery to\n            # use as the anchor. This makes the lint warning less\n            # likely to be filtered out if a bit of the subquery\n            # happens to be templated.\n            anchor = next(anchor.recursive_crawl(\"keyword\", \"symbol\"))\n            res = LintResult(\n                anchor=anchor,\n                description=f\"{nsq.query.selectables[0].selectable.type} clauses \"\n                \"should not contain subqueries. Use CTEs instead\",\n                fixes=[],\n            )\n            if len(nsq.query.selectables) == 1:\n                return (\n                    res,\n                    nsq.table_alias.from_expression_element,\n                    alias_name,\n                    nsq.query.selectables[0].selectable,\n                )\n        return None\n \n \n def _get_first_select_statement_descendant(\n     return None  # pragma: no cover\n \n \n def _is_correlated_subquery(\n     nested_select: Segments, select_source_names: Set[str], dialect: Dialect\n ):\n \n     https://en.wikipedia.org/wiki/Correlated_subquery\n     \"\"\"\n     select_statement = _get_first_select_statement_descendant(nested_select[0])\n     if not select_statement:\n         return False  # pragma: no cover\n     return False\n \n \n class _CTEBuilder:\n     \"\"\"Gather CTE parts, maintain order and track naming/aliasing.\"\"\"\n \n     def insert_cte(self, cte: CTEDefinitionSegment):\n         \"\"\"Add a new CTE to the list as late as possible but before all its parents.\"\"\"\n         # This should still have the position markers of its true position\n        inbound_subquery = (\n            Segments(cte).children().last(lambda seg: bool(seg.pos_marker))\n        )\n         insert_position = next(\n             (\n                 i\n \n         self.ctes.insert(insert_position, cte)\n \n    def create_cte_alias(self, alias: Optional[AliasInfo]) -> Tuple[str, bool]:\n         \"\"\"Find or create the name for the next CTE.\"\"\"\n        if alias and alias.aliased and alias.ref_str:\n             # If we know the name use it\n            return alias.ref_str, False\n \n         self.name_idx = self.name_idx + 1\n         name = f\"prep_{self.name_idx}\"\n         return name, True\n \n     def get_cte_segments(self) -> List[BaseSegment]:\n        \"\"\"Return a valid list of CTES with required padding segments.\"\"\"\n         cte_segments: List[BaseSegment] = []\n         for cte in self.ctes:\n             cte_segments = cte_segments + [\n         )\n         return new_select\n \n    def replace_with_clone(self, segment, clone_map):\n        for idx, cte in enumerate(self.ctes):\n            if any(segment is seg for seg in cte.recursive_crawl_all()):\n                self.ctes[idx] = clone_map[self.ctes[idx]]\n                return\n\n \n def _is_child(maybe_parent: Segments, maybe_child: Segments) -> bool:\n     \"\"\"Is the child actually between the start and end markers of the parent.\"\"\"\n    assert (\n        len(maybe_child) == 1\n    ), \"Cannot assess child relationship of multiple segments\"\n    assert (\n        len(maybe_parent) == 1\n    ), \"Cannot assess child relationship of multiple parents\"\n     child_markers = maybe_child[0].pos_marker\n     parent_pos = maybe_parent[0].pos_marker\n    assert parent_pos and child_markers\n     if child_markers < parent_pos.start_point_marker():\n         return False  # pragma: no cover\n \ndiff --git a/src/sqlfluff/utils/analysis/select_crawler.py b/src/sqlfluff/utils/analysis/select_crawler.py\n     \"\"\"A \"SELECT\" query segment.\"\"\"\n \n     selectable: BaseSegment\n    parent: Optional[BaseSegment]\n     dialect: Dialect\n \n    def as_str(self) -> str:\n        \"\"\"String representation for logging/testing.\"\"\"\n        return self.selectable.raw\n\n     @cached_property\n     def select_info(self):\n         \"\"\"Returns SelectStatementColumnsAndTables on the SELECT.\"\"\"\n         \"\"\"Find corresponding table_aliases entry (if any) matching \"table\".\"\"\"\n         alias_info = [\n             t\n            for t in (self.select_info.table_aliases if self.select_info else [])\n             if t.aliased and t.ref_str == table\n         ]\n         assert len(alias_info) <= 1\n     parent: Optional[\"Query\"] = field(default=None)\n     # Children (could be CTE, subselect, or other).\n     children: List[\"Query\"] = field(default_factory=list)\n    cte_definition_segment: Optional[BaseSegment] = field(default=None)\n     cte_name_segment: Optional[BaseSegment] = field(default=None)\n \n    def as_json(self) -> Dict:\n        \"\"\"JSON representation for logging/testing.\"\"\"\n        result = {}\n        if self.query_type != QueryType.Simple:\n            result[\"query_type\"] = self.query_type.name\n        if self.selectables:\n            result[\"selectables\"] = [\n                s.as_str() for s in self.selectables\n            ]  # type: ignore\n        if self.ctes:\n            result[\"ctes\"] = {\n                k: v.as_json() for k, v in self.ctes.items()\n            }  # type: ignore\n        return result\n\n     def lookup_cte(self, name: str, pop: bool = True) -> Optional[\"Query\"]:\n         \"\"\"Look up a CTE by name, in the current or any parent scope.\"\"\"\n         cte = self.ctes.get(name.upper())\n             return None\n \n     def crawl_sources(\n        self, segment: BaseSegment, recurse_into=True, pop=False, lookup_cte=True\n     ) -> Generator[Union[str, \"Query\"], None, None]:\n         \"\"\"Find SELECTs, table refs, or value table function calls in segment.\n \n         references or function call strings, yield those.\n         \"\"\"\n         found_nested_select = False\n        types = [\n             \"table_reference\",\n             \"set_expression\",\n             \"select_statement\",\n             \"values_clause\",\n        ]\n        for event, path in SelectCrawler.visit_segments(\n            segment, recurse_into=recurse_into\n         ):\n            seg = path[-1]\n            if event == \"end\" or not seg.is_type(*types):\n                continue\n\n             if seg is segment:\n                 # If the starting segment itself matches the list of types we're\n                 # searching for, recursive_crawl() will return it. Skip that.\n                 continue\n \n             if seg.is_type(\"table_reference\"):\n                if not seg.is_qualified() and lookup_cte:\n                     cte = self.lookup_cte(seg.raw, pop=pop)\n                     if cte:\n                         # It's a CTE.\n                     \"set_expression\", \"select_statement\", \"values_clause\"\n                 )\n                 found_nested_select = True\n                seg_ = Segments(*path[1:]).first(\n                    sp.is_type(\n                        \"from_expression_element\",\n                        \"set_expression\",\n                        \"select_statement\",\n                        \"values_clause\",\n                    )\n                )[0]\n                crawler = SelectCrawler(seg_, self.dialect, parent=self)\n                 # We know this will pass because we specified parent=self above.\n                 assert crawler.query_tree\n                 yield crawler.query_tree\n             except ValueError:\n                 pass\n \n        # Stacks for CTE definition & names we've seen but haven't consumed yet,\n        # so we can associate with the corresponding Query.\n        cte_definition_segment_stack: List[BaseSegment] = []\n        cte_name_segment_stack: List[BaseSegment] = []\n \n         # Visit segment and all its children\n         for event, path in SelectCrawler.visit_segments(segment):\n                             # added to this Query later.\n                             query = self.query_class(QueryType.Simple, dialect)\n                             append_query(query)\n                        # Ignore segments under a from_expression_element.\n                        # Those will be nested queries, and we're only\n                        # interested in CTEs and \"main\" queries, i.e.\n                        # standalones or those following a block of CTEs.\n                        elif not any(\n                            seg.is_type(\"from_expression_element\") for seg in path[1:]\n                        ):\n                             # It's a select_statement or values_clause.\n                            selectable = Selectable(\n                                path[-1], path[-2] if len(path) >= 2 else None, dialect\n                            )\n                             # Determine if this is part of a set_expression.\n                             if len(path) >= 2 and path[-2].is_type(\"set_expression\"):\n                                 # It's part of a set_expression. Append to the\n                                 append_query(query)\n                     else:\n                         # We're processing a \"with\" statement.\n                        if cte_name_segment_stack:\n                             # If we have a CTE name, this is the Query for that\n                             # name.\n                             query = self.query_class(\n                                 QueryType.Simple,\n                                 dialect,\n                                cte_definition_segment=cte_definition_segment_stack[-1],\n                                cte_name_segment=cte_name_segment_stack[-1],\n                             )\n                             if path[-1].is_type(\n                                 \"select_statement\", \"values_clause\", \"update_statement\"\n                             ):\n                                 # Add to the Query object we just created.\n                                query.selectables.append(\n                                    Selectable(\n                                        path[-1],\n                                        path[-2] if len(path) >= 2 else None,\n                                        dialect,\n                                    )\n                                )\n                             else:\n                                 # Processing a set_expression. Nothing\n                                 # additional to do here; we'll add selectables\n                                 # to the Query later when we encounter those\n                                 # child segments.\n                                 pass\n                            query_stack[-1].ctes[\n                                cte_name_segment_stack[-1].raw_upper\n                            ] = query\n                            cte_definition_segment_stack.pop()\n                            cte_name_segment_stack.pop()\n                             append_query(query)\n                         else:\n                             # There's no CTE name, so we're probably processing\n                             # interested in CTEs and \"main\" queries, i.e.\n                             # standalones or those following a block of CTEs.\n                             if not any(\n                                seg.is_type(\"from_expression_element\")\n                                for seg in path[1:]\n                             ):\n                                 if path[-1].is_type(\n                                     \"select_statement\", \"update_statement\"\n                                     # Processing a select_statement. Add it to the\n                                     # Query object on top of the stack.\n                                     query_stack[-1].selectables.append(\n                                        Selectable(\n                                            path[-1],\n                                            path[-2] if len(path) >= 2 else None,\n                                            dialect,\n                                        )\n                                     )\n                                 else:\n                                     # Processing a set_expression. Nothing\n                 elif path[-1].is_type(\"with_compound_statement\"):\n                     # Beginning a \"with\" statement, i.e. a block of CTEs.\n                     query = self.query_class(QueryType.WithCompound, dialect)\n                    if cte_name_segment_stack:\n                        query_stack[-1].ctes[\n                            cte_name_segment_stack[-1].raw_upper\n                        ] = query\n                        query.cte_definition_segment = cte_definition_segment_stack[-1]\n                        cte_definition_segment_stack.pop()\n                        cte_name_segment_stack.pop()\n                     append_query(query)\n                 elif path[-1].is_type(\"common_table_expression\"):\n                    # This is a \"<<cte name>> AS\". Save definition segment and\n                    # name for later.\n                    cte_definition_segment_stack.append(path[-1])\n                    cte_name_segment_stack.append(path[-1].segments[0])\n             elif event == \"end\":\n                 finish_segment()\n \n         return list(query.crawl_sources(segment, True))\n \n     @classmethod\n    def visit_segments(cls, seg, path=None, recurse_into=True):\n         \"\"\"Recursively visit all segments.\"\"\"\n         if path is None:\n             path = []\n         path.append(seg)\n         yield \"start\", path\n        if recurse_into:\n            for seg in seg.segments:\n                yield from cls.visit_segments(seg, path, recurse_into)\n         yield \"end\", path\n         path.pop()"
  },
  {
    "instruction": "Return codes are inconsistent\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nWorking on #3431 - I noticed that we're inconsistent in our return codes.\r\n\r\nIn `commands.py` we call `sys.exit()` in 15 places (currently).\r\n\r\n- Twice we call `sys.exit(0)` on success, at the end of `parse` and `lint` (`fix` is a handled differently, see below). \u2714\ufe0f \r\n- Six times we call `sys.exit(1)` for a selection of things:\r\n  - Not having `cProfiler` installed.\r\n  - Failing to apply fixes\r\n  - User Errors and OSError (in `PathAndUserErrorHandler`)\r\n- Five times we call `sys.exit(66)` for a selection of things:\r\n  - User Errors (including unknown dialect or failing to load a dialect or config)\r\n  - If parsing failed when calling `parse`.\r\n- Once we use `handle_files_with_tmp_or_prs_errors` to determine the exit code (which returns 1 or 0)\r\n- Once we use `LintingResult.stats` to determine the exit code (which returns either 65 or 0)\r\n- Once we do a mixture of the above (see end of `fix`)\r\n\r\nThis neither DRY, or consistent ... or helpful?\r\n\r\n### Expected Behaviour\r\n\r\nWe should have consistent return codes for specific scenarios. There are up for discussion, but I would suggest:\r\n\r\n- 0 for success (obviously)\r\n- 1 for a fail which is error related: not having libraries installed, user errors etc...\r\n- 65 for a linting fail (i.e. no errors in running, but issues were found in either parsing or linting).\r\n- 66 for a fixing fail (i.e. we tried to fix errors but failed to do so for some reason).\r\n\r\nThese would be defined as constants at the top of `commands.py`.\r\n\r\n### Observed Behaviour\r\n\r\nsee above\r\n\r\n### How to reproduce\r\n\r\nsee above\r\n\r\n### Dialect\r\n\r\nN/A\r\n\r\n### Version\r\n\r\nDescription is as per code in #3431\r\n\r\n### Configuration\r\n\r\n-\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": " \"\"\"init py for cli.\"\"\"\ndiff --git a/src/sqlfluff/cli/commands.py b/src/sqlfluff/cli/commands.py\n from tqdm import tqdm\n from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n \n from sqlfluff.cli.formatters import (\n     format_linting_result_header,\n     OutputStreamFormatter,\n                     Color.red,\n                 )\n             )\n            sys.exit(1)\n         elif exc_type is SQLFluffUserError:\n             click.echo(\n                 \"\\nUser Error: \"\n                     Color.red,\n                 )\n             )\n            sys.exit(1)\n \n \n def common_options(f: Callable) -> Callable:\n                     color=Color.red,\n                 )\n             )\n            sys.exit(66)\n         except KeyError:\n             click.echo(\n                 OutputStreamFormatter.colorize_helper(\n                     color=Color.red,\n                 )\n             )\n            sys.exit(66)\n     from_root_kwargs = {}\n     if \"require_dialect\" in kwargs:\n         from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n                 color=Color.red,\n             )\n         )\n        sys.exit(66)\n \n \n def get_linter_and_formatter(\n             dialect_selector(dialect)\n     except KeyError:  # pragma: no cover\n         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n        sys.exit(66)\n     formatter = OutputStreamFormatter(\n         output_stream=output_stream or make_output_stream(cfg),\n         nocolor=cfg.get(\"nocolor\"),\n             formatter.completion_message()\n         sys.exit(result.stats()[\"exit code\"])\n     else:\n        sys.exit(0)\n \n \n def do_fixes(lnt, result, formatter=None, **kwargs):\n     verbose = config.get(\"verbose\")\n     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n \n    exit_code = 0\n \n     formatter.dispatch_config(lnt)\n \n             )\n \n         click.echo(stdout, nl=False)\n        sys.exit(1 if templater_error or unfixable_error else exit_code)\n \n     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n     click.echo(\"==== finding fixable violations ====\")\n                 fixed_file_suffix=fixed_suffix,\n             )\n             if not success:\n                sys.exit(1)  # pragma: no cover\n         else:\n             click.echo(\n                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n                     fixed_file_suffix=fixed_suffix,\n                 )\n                 if not success:\n                    sys.exit(1)  # pragma: no cover\n                 else:\n                     formatter.completion_message()\n             elif c == \"n\":\n                 click.echo(\"Aborting...\")\n                exit_code = 1\n             else:  # pragma: no cover\n                 click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n                 click.echo(\"Aborting...\")\n                exit_code = 1\n     else:\n         click.echo(\"==== no fixable linting violations found ====\")\n         formatter.completion_message()\n         (\n             dict(types=SQLLintError, fixable=False),\n             \"  [{} unfixable linting violations found]\",\n            1,\n         ),\n     ]\n     for num_violations_kwargs, message_format, error_level in error_types:\n             import cProfile\n         except ImportError:  # pragma: no cover\n             click.echo(\"The cProfiler is not available on your platform.\")\n            sys.exit(1)\n         pr = cProfile.Profile()\n         pr.enable()\n \n         click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n \n     if violations_count > 0 and not nofail:\n        sys.exit(66)  # pragma: no cover\n     else:\n        sys.exit(0)\n \n \n # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n import click\n from colorama import Style\n \n from sqlfluff.cli.helpers import (\n     get_package_version,\n     get_python_version,\n     wrap_field,\n )\n from sqlfluff.cli.outputstream import OutputStream\n from sqlfluff.core import SQLBaseError, FluffConfig, Linter, TimingSummary\n from sqlfluff.core.enums import Color\n from sqlfluff.core.linter import LintedFile, LintingResult, ParsedString\n                         color,\n                     )\n                 )\n        return 1 if num_filtered_errors else 0\n \n     def print_out_violations_and_timing(\n         self,\ndiff --git a/src/sqlfluff/core/linter/linting_result.py b/src/sqlfluff/core/linter/linting_result.py\n )\n from typing_extensions import Literal\n \n \n from sqlfluff.core.errors import (\n     CheckTuple,\n \n # Classes needed only for type checking\n from sqlfluff.core.parser.segments.base import BaseSegment\n\n\n from sqlfluff.core.linter.linted_dir import LintedDir\n \n \n             all_stats[\"unclean rate\"] = 0\n         all_stats[\"clean files\"] = all_stats[\"clean\"]\n         all_stats[\"unclean files\"] = all_stats[\"unclean\"]\n        all_stats[\"exit code\"] = 65 if all_stats[\"violations\"] > 0 else 0\n         all_stats[\"status\"] = \"FAIL\" if all_stats[\"violations\"] > 0 else \"PASS\"\n         return all_stats\n ",
    "output": " \"\"\"init py for cli.\"\"\"\n\n\nEXIT_SUCCESS = 0\nEXIT_FAIL = 1\nEXIT_ERROR = 2\ndiff --git a/src/sqlfluff/cli/commands.py b/src/sqlfluff/cli/commands.py\n from tqdm import tqdm\n from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n \nfrom sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\n from sqlfluff.cli.formatters import (\n     format_linting_result_header,\n     OutputStreamFormatter,\n                     Color.red,\n                 )\n             )\n            sys.exit(EXIT_ERROR)\n         elif exc_type is SQLFluffUserError:\n             click.echo(\n                 \"\\nUser Error: \"\n                     Color.red,\n                 )\n             )\n            sys.exit(EXIT_ERROR)\n \n \n def common_options(f: Callable) -> Callable:\n                     color=Color.red,\n                 )\n             )\n            sys.exit(EXIT_ERROR)\n         except KeyError:\n             click.echo(\n                 OutputStreamFormatter.colorize_helper(\n                     color=Color.red,\n                 )\n             )\n            sys.exit(EXIT_ERROR)\n     from_root_kwargs = {}\n     if \"require_dialect\" in kwargs:\n         from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n                 color=Color.red,\n             )\n         )\n        sys.exit(EXIT_ERROR)\n \n \n def get_linter_and_formatter(\n             dialect_selector(dialect)\n     except KeyError:  # pragma: no cover\n         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n        sys.exit(EXIT_ERROR)\n     formatter = OutputStreamFormatter(\n         output_stream=output_stream or make_output_stream(cfg),\n         nocolor=cfg.get(\"nocolor\"),\n             formatter.completion_message()\n         sys.exit(result.stats()[\"exit code\"])\n     else:\n        sys.exit(EXIT_SUCCESS)\n \n \n def do_fixes(lnt, result, formatter=None, **kwargs):\n     verbose = config.get(\"verbose\")\n     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n \n    exit_code = EXIT_SUCCESS\n \n     formatter.dispatch_config(lnt)\n \n             )\n \n         click.echo(stdout, nl=False)\n        sys.exit(EXIT_FAIL if templater_error or unfixable_error else exit_code)\n \n     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n     click.echo(\"==== finding fixable violations ====\")\n                 fixed_file_suffix=fixed_suffix,\n             )\n             if not success:\n                sys.exit(EXIT_FAIL)  # pragma: no cover\n         else:\n             click.echo(\n                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n                     fixed_file_suffix=fixed_suffix,\n                 )\n                 if not success:\n                    sys.exit(EXIT_FAIL)  # pragma: no cover\n                 else:\n                     formatter.completion_message()\n             elif c == \"n\":\n                 click.echo(\"Aborting...\")\n                exit_code = EXIT_FAIL\n             else:  # pragma: no cover\n                 click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n                 click.echo(\"Aborting...\")\n                exit_code = EXIT_FAIL\n     else:\n         click.echo(\"==== no fixable linting violations found ====\")\n         formatter.completion_message()\n         (\n             dict(types=SQLLintError, fixable=False),\n             \"  [{} unfixable linting violations found]\",\n            EXIT_FAIL,\n         ),\n     ]\n     for num_violations_kwargs, message_format, error_level in error_types:\n             import cProfile\n         except ImportError:  # pragma: no cover\n             click.echo(\"The cProfiler is not available on your platform.\")\n            sys.exit(EXIT_ERROR)\n         pr = cProfile.Profile()\n         pr.enable()\n \n         click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n \n     if violations_count > 0 and not nofail:\n        sys.exit(EXIT_FAIL)  # pragma: no cover\n     else:\n        sys.exit(EXIT_SUCCESS)\n \n \n # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n import click\n from colorama import Style\n \nfrom sqlfluff.cli import EXIT_FAIL, EXIT_SUCCESS\n from sqlfluff.cli.helpers import (\n     get_package_version,\n     get_python_version,\n     wrap_field,\n )\n from sqlfluff.cli.outputstream import OutputStream\n\n from sqlfluff.core import SQLBaseError, FluffConfig, Linter, TimingSummary\n from sqlfluff.core.enums import Color\n from sqlfluff.core.linter import LintedFile, LintingResult, ParsedString\n                         color,\n                     )\n                 )\n        return EXIT_FAIL if num_filtered_errors else EXIT_SUCCESS\n \n     def print_out_violations_and_timing(\n         self,\ndiff --git a/src/sqlfluff/core/linter/linting_result.py b/src/sqlfluff/core/linter/linting_result.py\n )\n from typing_extensions import Literal\n \nfrom sqlfluff.cli import EXIT_FAIL, EXIT_SUCCESS\n \n from sqlfluff.core.errors import (\n     CheckTuple,\n \n # Classes needed only for type checking\n from sqlfluff.core.parser.segments.base import BaseSegment\n from sqlfluff.core.linter.linted_dir import LintedDir\n \n \n             all_stats[\"unclean rate\"] = 0\n         all_stats[\"clean files\"] = all_stats[\"clean\"]\n         all_stats[\"unclean files\"] = all_stats[\"unclean\"]\n        all_stats[\"exit code\"] = (\n            EXIT_FAIL if all_stats[\"violations\"] > 0 else EXIT_SUCCESS\n        )\n         all_stats[\"status\"] = \"FAIL\" if all_stats[\"violations\"] > 0 else \"PASS\"\n         return all_stats\n "
  },
  {
    "instruction": "L027: outer-level table not found in WHERE clause sub-select\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nOuter-level table/view referenced in sub-select inside `WHERE` clause is not being detected.\r\n\r\nThis error seems to only occur when the sub-select contains joins.\r\n\r\n### Expected Behaviour\r\n\r\nNo error\r\n\r\n### Observed Behaviour\r\n\r\n```\r\nL:   7 | P:  32 | L027 | Qualified reference 'my_table.kind' not found in\r\n                       | available tables/view aliases ['other_table',\r\n                       | 'mapping_table'] in select with more than one referenced\r\n                       | table/view.\r\n```\r\n\r\n### How to reproduce\r\n\r\n```sql\r\nSELECT my_col\r\nFROM my_table\r\nWHERE EXISTS (\r\n    SELECT 1\r\n    FROM other_table\r\n    INNER JOIN mapping_table ON (mapping_table.other_fk = other_table.id_pk)\r\n    WHERE mapping_table.kind = my_table.kind\r\n);\r\n```\r\n\r\n### Dialect\r\n\r\npostgres\r\n\r\n### Version\r\n\r\nsqlfluff, version 0.12.0\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\nnocolor = True\r\ndialect = postgres\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [ ] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": "                     )\n                 )\n \n            all_table_aliases = [t.ref_str for t in table_aliases] + standalone_aliases\n\n            # For qualified references, we want to check that the alias is actually\n            # valid\n            if (\n                this_ref_type == \"qualified\"\n                and list(r.iter_raw_references())[0].part not in all_table_aliases\n            ):\n                violation_buff.append(\n                    LintResult(\n                        anchor=r,\n                        description=f\"Qualified reference {r.raw!r} not found in \"\n                        f\"available tables/view aliases {all_table_aliases} in select \"\n                        \"with more than one referenced table/view.\",\n                    )\n                )\n\n         return violation_buff or None\n \n     def _init_ignore_words_list(self):",
    "output": "                     )\n                 )\n \n         return violation_buff or None\n \n     def _init_ignore_words_list(self):"
  },
  {
    "instruction": "Standardise `--disable_progress_bar` naming\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nAs noted in https://github.com/sqlfluff/sqlfluff/pull/3610#discussion_r926014745 `--disable_progress_bar` is the only command line option using underscores instead of dashes.\r\n\r\nShould we change this?\r\n\r\nThis would be a breaking change, so do we leave until next major release?\r\nOr do we accept both options?\n\n### Expected Behaviour\n\nWe should be standard in out command line option format\n\n### Observed Behaviour\n\n`--disable_progress_bar` is the only non-standard one\n\n### How to reproduce\n\nN/A\n\n### Dialect\n\nN/A\n\n### Version\n\n1.2.1\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "diff --git a/src/sqlfluff/cli/commands.py b/src/sqlfluff/cli/commands.py\n from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n \n from sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\n from sqlfluff.cli.formatters import (\n     format_linting_result_header,\n     OutputStreamFormatter,\n         click.echo(payload)\n \n \n@cli.command()\n @common_options\n @core_options\n @click.option(\n )\n @click.option(\n     \"--disable_progress_bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n )\n @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n def lint(\n     ),\n )\n @click.option(\n    \"--disable_progress_bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n )",
    "output": "\"\"\"Allows to provide deprecated options for click's command.\"\"\"\n\nfrom typing import Any, Callable\n\nimport click\nfrom click import Context, OptionParser, echo, style\nfrom click.parser import Option, ParsingState\n\n\nclass DeprecatedOption(click.Option):\n    \"\"\"Allows to provide deprecated options for click's command.\n\n    Works with `DeprecatedOptionsCommand` (see below).\n    Expects to be provided into standard `@click.option` with:\n      * two parameter declarations arguments - old one (deprecated)\n        and new one (preferred);\n      * `cls` parameter (standard click Option) as `cls=DeprecatedOption`;\n      * `deprecated` parameter - which says which ones are deprecated,\n        like`deprecated=[\"--disable_progress_bar\"]1.\n\n    This is based on\n      * https://stackoverflow.com/a/50402799/5172513\n\n    It's somewhat hackish and may broke when click internals are changed, it is even\n    mentioned in SO:\n    > This code reaches into some private structures in the parser, but this is\n    unlikely to be an issue. This parser code was last changed 4 years ago.\n    The parser code is unlikely to undergo significant revisions.\n\n    Hopefully will be removed when\n      * https://github.com/pallets/click/issues/2263\n    is finished.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.deprecated = kwargs.pop(\"deprecated\", ())\n        self.preferred = args[0][-1]\n\n        super().__init__(*args, **kwargs)\n\n\nclass DeprecatedOptionsCommand(click.Command):\n    \"\"\"Allows to provide deprecated options for click's command.\n\n    Works with `DeprecatedOption` (see above).\n    Expects to be provided into standard `@click.command` as:\n      * `@cli.command(cls=DeprecatedOptionsCommand)`\n    \"\"\"\n\n    def make_parser(self, ctx: Context) -> OptionParser:\n        \"\"\"Hook 'make_parser' and during processing check the name.\n\n        Used to invoke the option to see if it is preferred.\n        \"\"\"\n        parser: OptionParser = super().make_parser(ctx)\n\n        # get the parser options\n        options = set(parser._short_opt.values())\n        options |= set(parser._long_opt.values())\n\n        for option in options:\n            if not isinstance(option.obj, DeprecatedOption):\n                continue\n\n            option.process = self._make_process(option)  # type: ignore\n\n        return parser\n\n    def _make_process(self, an_option: Option) -> Callable:\n        \"\"\"Construct a closure to the parser option processor.\"\"\"\n        orig_process: Callable = an_option.process\n        deprecated = getattr(an_option.obj, \"deprecated\", None)\n        preferred = getattr(an_option.obj, \"preferred\", None)\n\n        if not deprecated:\n            raise ValueError(\n                f\"Expected `deprecated` value for `{an_option.obj.name!r}`\"\n            )\n\n        def process(value: Any, state: ParsingState) -> None:\n            \"\"\"Custom process method.\n\n            The function above us on the stack used 'opt' to\n            pick option from a dict, see if it is deprecated.\n            \"\"\"\n            # reach up the stack and get 'opt'\n            import inspect\n\n            frame = inspect.currentframe()\n            try:\n                opt = frame.f_back.f_locals.get(\"opt\")  # type: ignore\n            finally:\n                del frame\n\n            if opt in deprecated:  # type: ignore\n                msg = (\n                    f\"DeprecationWarning: The option {opt!r} is deprecated, \"\n                    f\"use {preferred!r}.\"\n                )\n                echo(style(msg, fg=\"red\"), err=True)\n\n            return orig_process(value, state)\n\n        return process\ndiff --git a/src/sqlfluff/cli/commands.py b/src/sqlfluff/cli/commands.py\n from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n \n from sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\nfrom sqlfluff.cli.click_deprecated_option import (\n    DeprecatedOption,\n    DeprecatedOptionsCommand,\n)\n from sqlfluff.cli.formatters import (\n     format_linting_result_header,\n     OutputStreamFormatter,\n         click.echo(payload)\n \n \n@cli.command(cls=DeprecatedOptionsCommand)\n @common_options\n @core_options\n @click.option(\n )\n @click.option(\n     \"--disable_progress_bar\",\n    \"--disable-progress-bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n    cls=DeprecatedOption,\n    deprecated=[\"--disable_progress_bar\"],\n )\n @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n def lint(\n     ),\n )\n @click.option(\n    \"--disable-progress-bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n )"
  },
  {
    "instruction": "layout.end-of-file is the only rule in kebab case\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nOur rules are all in `snake_case`, except for `layout.end-of-file`\n\n### Expected Behaviour\n\nAll rules should be in snake case\n\n### Observed Behaviour\n\nAs above\n\n### How to reproduce\n\n-\n\n### Dialect\n\nNA\n\n### Version\n\nMain\n\n### Configuration\n\nNA\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "         # Set the bundle name to the ref.\n         _bundle_name = f\":ref:`bundle_{bundle}`\"\n         for idx, rule in enumerate(rule_bundles[bundle]):\n            aliases = \", \".join(rule.aliases[:3]) + (\n                \",\" if len(rule.aliases) > 3 else \"\"\n             )\n             name_ref = f\":sqlfluff:ref:`{rule.name}`\"\n             code_ref = f\":sqlfluff:ref:`{rule.code}`\"\n                 f\"| {code_ref : <28} | {aliases : <18} |\\n\"\n             )\n \n            j = 3\n             while True:\n                 if not rule.aliases[j:]:\n                     break\n                aliases = \", \".join(rule.aliases[j : j + 3]) + (\n                    \",\" if len(rule.aliases[j:]) > 3 else \"\"\n                 )\n                 f.write(f\"|{' ' * 42}|{' ' * 50}|{' ' * 30}| {aliases : <18} |\\n\")\n                j += 3\n \n             if idx + 1 < len(rule_bundles[bundle]):\n                 f.write(f\"|{' ' * 42}+{'-' * 50}+{'-' * 30}+{'-' * 20}+\\n\")\ndiff --git a/src/sqlfluff/rules/layout/LT12.py b/src/sqlfluff/rules/layout/LT12.py\n \n     \"\"\"\n \n    name = \"layout.end-of-file\"\n    aliases = (\"L009\",)\n     groups = (\"all\", \"core\", \"layout\")\n \n     targets_templated = True",
    "output": "         # Set the bundle name to the ref.\n         _bundle_name = f\":ref:`bundle_{bundle}`\"\n         for idx, rule in enumerate(rule_bundles[bundle]):\n            step = 1  # The number of aliases per line.\n            aliases = \", \".join(rule.aliases[:step]) + (\n                \",\" if len(rule.aliases) > step else \"\"\n             )\n             name_ref = f\":sqlfluff:ref:`{rule.name}`\"\n             code_ref = f\":sqlfluff:ref:`{rule.code}`\"\n                 f\"| {code_ref : <28} | {aliases : <18} |\\n\"\n             )\n \n            j = 1\n\n             while True:\n                 if not rule.aliases[j:]:\n                     break\n                aliases = \", \".join(rule.aliases[j : j + step]) + (\n                    \",\" if len(rule.aliases[j:]) > step else \"\"\n                 )\n                 f.write(f\"|{' ' * 42}|{' ' * 50}|{' ' * 30}| {aliases : <18} |\\n\")\n                j += step\n \n             if idx + 1 < len(rule_bundles[bundle]):\n                 f.write(f\"|{' ' * 42}+{'-' * 50}+{'-' * 30}+{'-' * 20}+\\n\")\ndiff --git a/src/sqlfluff/rules/layout/LT12.py b/src/sqlfluff/rules/layout/LT12.py\n \n     \"\"\"\n \n    name = \"layout.end_of_file\"\n    # Between 2.0.0 and 2.0.4 we supported had a kebab-case name for this rule\n    # so the old name remains here as an alias to enable backward compatibility.\n    aliases = (\"L009\", \"layout.end-of-file\")\n     groups = (\"all\", \"core\", \"layout\")\n \n     targets_templated = True"
  },
  {
    "instruction": "2.0.2 - LT02 issues when query contains \"do\" statement.\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nSQLFluff v2.0.2 gives LT02 indentation errors for the Jinja `if`-block when `template_blocks_indent` is set to `True`.\r\nThe example SQL below is a bit contrived, but it's the smallest failing example I could produce based on our real SQL.\r\n\r\nIf I remove the Jinja `do`-expression from the code, the `if` block validates without errors.\r\n\r\n### Expected Behaviour\r\n\r\nI expect the SQL to pass the linting tests.\r\n\r\n### Observed Behaviour\r\n\r\nOutput from SQLFluff v2.0.2:\r\n```\r\nL:   5 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\nL:   6 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\n```\r\n\r\n### How to reproduce\r\n\r\nSQL to reproduce:\r\n```\r\n{% set cols = ['a', 'b'] %}\r\n{% do cols.remove('a') %}\r\n\r\n{% if true %}\r\n    select a\r\n    from some_table\r\n{% endif %}\r\n```\r\n\r\n### Dialect\r\n\r\n`ansi`\r\n\r\n### Version\r\n\r\n```\r\n> sqlfluff --version\r\nsqlfluff, version 2.0.2\r\n\r\n> python --version\r\nPython 3.9.9\r\n```\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\ndialect = ansi\r\ntemplater = jinja\r\n\r\n[sqlfluff:indentation]\r\ntemplate_blocks_indent = True\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n2.0.2 - LT02 issues when query contains \"do\" statement.\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nSQLFluff v2.0.2 gives LT02 indentation errors for the Jinja `if`-block when `template_blocks_indent` is set to `True`.\r\nThe example SQL below is a bit contrived, but it's the smallest failing example I could produce based on our real SQL.\r\n\r\nIf I remove the Jinja `do`-expression from the code, the `if` block validates without errors.\r\n\r\n### Expected Behaviour\r\n\r\nI expect the SQL to pass the linting tests.\r\n\r\n### Observed Behaviour\r\n\r\nOutput from SQLFluff v2.0.2:\r\n```\r\nL:   5 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\nL:   6 | P:   1 | LT02 | Line should not be indented.\r\n                       | [layout.indent]\r\n```\r\n\r\n### How to reproduce\r\n\r\nSQL to reproduce:\r\n```\r\n{% set cols = ['a', 'b'] %}\r\n{% do cols.remove('a') %}\r\n\r\n{% if true %}\r\n    select a\r\n    from some_table\r\n{% endif %}\r\n```\r\n\r\n### Dialect\r\n\r\n`ansi`\r\n\r\n### Version\r\n\r\n```\r\n> sqlfluff --version\r\nsqlfluff, version 2.0.2\r\n\r\n> python --version\r\nPython 3.9.9\r\n```\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\ndialect = ansi\r\ntemplater = jinja\r\n\r\n[sqlfluff:indentation]\r\ntemplate_blocks_indent = True\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": "                 getattr(elem, \"indent_val\", 0)\n                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n             )\n            if indent_balance != 0:\n                 linter_logger.debug(\n                     \"Indent balance test failed for %r. Template indents will not be \"\n                     \"linted for this file.\",\n                 if token.indent_val != 0:\n                     # Don't allow it if we're not linting templating block indents.\n                     if not templating_blocks_indent:\n                        continue\n             new_tokens.append(token)\n \n         # Return new buffer\ndiff --git a/src/sqlfluff/core/templaters/slicers/tracer.py b/src/sqlfluff/core/templaters/slicers/tracer.py\n         # a block, but its behavior is basically syntactic sugar for\n         # {{ open(\"somefile).read() }}. Thus, treat it as templated code.\n         # It's a similar situation with {% import %} and {% from ... import %}.\n        if tag_name in [\"include\", \"import\", \"from\"]:\n             block_type = \"templated\"\n         elif tag_name.startswith(\"end\"):\n             block_type = \"block_end\"",
    "output": "                 getattr(elem, \"indent_val\", 0)\n                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n             )\n            if indent_balance != 0:  # pragma: no cover\n                 linter_logger.debug(\n                     \"Indent balance test failed for %r. Template indents will not be \"\n                     \"linted for this file.\",\n                 if token.indent_val != 0:\n                     # Don't allow it if we're not linting templating block indents.\n                     if not templating_blocks_indent:\n                        continue  # pragma: no cover\n             new_tokens.append(token)\n \n         # Return new buffer\ndiff --git a/src/sqlfluff/core/templaters/slicers/tracer.py b/src/sqlfluff/core/templaters/slicers/tracer.py\n         # a block, but its behavior is basically syntactic sugar for\n         # {{ open(\"somefile).read() }}. Thus, treat it as templated code.\n         # It's a similar situation with {% import %} and {% from ... import %}.\n        if tag_name in [\"include\", \"import\", \"from\", \"do\"]:\n             block_type = \"templated\"\n         elif tag_name.startswith(\"end\"):\n             block_type = \"block_end\""
  },
  {
    "instruction": "sqlfluff doesn't recognise a jinja variable set inside of \"if\" statement\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen I try to define a jinja variable using \"set\" jinja directive inside of an \"if\" jinja statement, sqlfluff complains: \r\n\"Undefined jinja template variable\".\n\n### Expected Behaviour\n\nto not have a linting issue\n\n### Observed Behaviour\n\nsqlfluff lint gives an error:\r\n\"Undefined jinja template variable\"\n\n### How to reproduce\n\ntry to create a \"temp.sql\" file with the following content\r\n\r\n```\r\n{% if True %}\r\n    {% set some_var %}1{% endset %}\r\n    SELECT {{some_var}}\r\n{% endif %}\r\n```\r\n\r\nand run:\r\n```\r\nsqlfluff lint ./temp.sql\r\n```\r\n\r\nYou will get the following error:\r\n```\r\n== [./temp.sql] FAIL                                                                                                                    \r\nL:   2 | P:  12 |  TMP | Undefined jinja template variable: 'some_var'\r\nL:   3 | P:  14 |  TMP | Undefined jinja template variable: 'some_var'\r\n```\n\n### Dialect\n\ntested on 'snowflake' dialect\n\n### Version\n\nsqlfluff, version 0.11.1\r\nPython 3.8.12\n\n### Configuration\n\n[sqlfluff]\r\nverbose = 1\r\ndialect = snowflake\r\ntemplater = jinja\r\nexclude_rules = L027,L031,L032,L036,L044,L046,L034,L050\r\noutput_line_length = 121\r\nsql_file_exts=.sql\r\n\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 250\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L042]\r\nforbid_subquery_in = both\r\n\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L030]  # function names\r\nextended_capitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "         # first Exception which serves only to catch catastrophic errors.\n         try:\n             syntax_tree = env.parse(in_str)\n            undefined_variables = meta.find_undeclared_variables(syntax_tree)\n         except Exception as err:  # pragma: no cover\n             # TODO: Add a url here so people can get more help.\n             raise SQLTemplaterError(f\"Failure in identifying Jinja variables: {err}.\")\n \n        # Get rid of any that *are* actually defined.\n        for val in live_context:\n            if val in undefined_variables:\n                undefined_variables.remove(val)\n \n        if undefined_variables:\n            # Lets go through and find out where they are:\n            for val in self._crawl_tree(syntax_tree, undefined_variables, in_str):\n                violations.append(val)\n \n         try:\n             # NB: Passing no context. Everything is loaded when the template is loaded.\n                 config=config,\n                 make_template=make_template,\n             )\n             return (\n                 TemplatedFile(\n                     source_str=in_str,",
    "output": "         # first Exception which serves only to catch catastrophic errors.\n         try:\n             syntax_tree = env.parse(in_str)\n            potentially_undefined_variables = meta.find_undeclared_variables(\n                syntax_tree\n            )\n         except Exception as err:  # pragma: no cover\n             # TODO: Add a url here so people can get more help.\n             raise SQLTemplaterError(f\"Failure in identifying Jinja variables: {err}.\")\n \n        undefined_variables = set()\n\n        class Undefined:\n            \"\"\"Similar to jinja2.StrictUndefined, but remembers, not fails.\"\"\"\n\n            def __init__(self, name):\n                self.name = name\n\n            def __str__(self):\n                \"\"\"Treat undefined vars as empty, but remember for later.\"\"\"\n                undefined_variables.add(self.name)\n                return \"\"\n\n            def __getattr__(self, item):\n                undefined_variables.add(self.name)\n                return Undefined(f\"{self.name}.{item}\")\n \n        for val in potentially_undefined_variables:\n            if val not in live_context:\n                live_context[val] = Undefined(name=val)\n \n         try:\n             # NB: Passing no context. Everything is loaded when the template is loaded.\n                 config=config,\n                 make_template=make_template,\n             )\n            if undefined_variables:\n                # Lets go through and find out where they are:\n                for val in self._crawl_tree(syntax_tree, undefined_variables, in_str):\n                    violations.append(val)\n             return (\n                 TemplatedFile(\n                     source_str=in_str,"
  },
  {
    "instruction": "Misleading path does not exist message\nIt looks like if _at least one_ of the paths provided to sqlfluff do not exist, it will display an error message implying that _all_ of the supplied paths do not exist:\r\n\r\n```bash\r\ndbt@b54bee9ced88:/workspaces/dbt-dutchie$ sqlfluff fix models/shared/dispensaries.sql models/shares/dispensary_chains.sql\r\n==== finding fixable violations ====\r\n=== [dbt templater] Compiling dbt project...\r\n== [models/shared/dispensaries.sql] FAIL\r\nL:   6 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:   8 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:   9 | P:   3 | L003 | Line over-indented compared to line #376\r\nL:  10 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:  12 | P:   2 | L003 | Indentation not consistent with line #376\r\nL:  13 | P:   3 | L003 | Line over-indented compared to line #376\r\nL:  14 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  15 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  16 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  17 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  18 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  19 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  20 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  21 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  22 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  23 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  24 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  25 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  26 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  27 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  28 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  29 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  30 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  31 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  32 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  33 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  34 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  58 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL:  35 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  36 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  37 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  38 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  39 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  40 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  41 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  42 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  43 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  44 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  45 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  46 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  47 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  48 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  49 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  50 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  51 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  52 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  53 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  54 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  55 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  56 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  57 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  58 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  59 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  60 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  61 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  62 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  63 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  64 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  65 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  66 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  67 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  68 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  69 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  70 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  71 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  72 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  73 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  74 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  75 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  76 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  77 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  78 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  79 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  80 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  81 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  82 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  83 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  84 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  85 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  86 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  87 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  88 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  89 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  90 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  91 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  92 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  92 | P:  44 | L001 | Unnecessary trailing whitespace.\r\nL:  93 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  94 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  95 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  96 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  97 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  98 | P:   4 | L003 | Line over-indented compared to line #376\r\nL:  99 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 100 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 101 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 102 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 103 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 104 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 105 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 106 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 107 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 108 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 109 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 110 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 111 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 112 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 113 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 114 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 115 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 116 | P:   3 | L003 | Line over-indented compared to line #376\r\nL: 235 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 117 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 118 | P:   3 | L003 | Line over-indented compared to line #376\r\nL: 119 | P:   4 | L003 | Line over-indented compared to line #376\r\nL: 120 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 121 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 122 | P:   2 | L003 | Indentation not consistent with line #376\r\nL: 339 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 343 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 347 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 351 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 355 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 358 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 361 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 364 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 367 | P:   1 | L004 | Incorrect indentation type found in file.\r\nL: 370 | P:   1 | L004 | Incorrect indentation type found in file.\r\nThe path(s) ('models/shared/dispensaries.sql', 'models/shares/dispensary_chains.sql') could not be accessed. Check it/they exist(s).\r\n```\r\n\r\n## Expected Behaviour\r\nI would expect only the unaccessible paths to be included in the error message.\r\n\r\n## Observed Behaviour\r\nSee above\r\n\r\n## Version\r\n```bash\r\ndbt@b54bee9ced88:/workspaces/dbt-dutchie$ sqlfluff --version\r\nsqlfluff, version 0.5.2\r\n```\r\n\r\n```bash\r\ndbt@b54bee9ced88:/workspaces/dbt-dutchie$ python --version\r\nPython 3.8.6\r\n```\r\n\r\n## Configuration\r\n```\r\n[sqlfluff]\r\ndialect = snowflake\r\ntemplater = dbt\r\nrules = L001,L002,L003,L004,L005,L006\r\nignore = parsing,templating\r\n\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\ncomma_style = trailing\r\n\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = upper\r\n```\r\n\n",
    "input": " class PathAndUserErrorHandler:\n     \"\"\"Make an API call but with error handling for the CLI.\"\"\"\n \n    def __init__(self, formatter, paths):\n         self.formatter = formatter\n        self.paths = paths\n \n     def __enter__(self):\n         return self\n \n     def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is OSError:\n            click.echo(\n                self.formatter.colorize(\n                    f\"The path(s) { self.paths } could not be \"\n                    \"accessed. Check it/they exist(s).\",\n                    Color.red,\n                )\n            )\n            sys.exit(EXIT_ERROR)\n        elif exc_type is SQLFluffUserError:\n             click.echo(\n                 \"\\nUser Error: \"\n                 + self.formatter.colorize(\n     if verbose >= 1:\n         click.echo(format_linting_result_header())\n \n    with PathAndUserErrorHandler(formatter, paths):\n         # add stdin if specified via lone '-'\n         if (\"-\",) == paths:\n             result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n     click.echo(\"==== finding fixable violations ====\")\n \n    with PathAndUserErrorHandler(formatter, paths):\n         result = lnt.lint_paths(\n             paths,\n             fix=True,\n     t0 = time.monotonic()\n \n     # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter, path):\n         if \"-\" == path:\n             parsed_strings = [\n                 lnt.parse_string(\n     )\n \n     # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter, path):\n         if \"-\" == path:\n             raw_sql = sys.stdin.read()\n             fname = \"stdin\"\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     SQLLintError,\n     SQLParseError,\n     SQLFluffSkipFile,\n )\n from sqlfluff.core.parser import Lexer, Parser, RegexLexer\n from sqlfluff.core.file_helpers import get_encoding\n             if ignore_non_existent_files:\n                 return []\n             else:\n                raise OSError(\"Specified path does not exist\")\n \n         # Files referred to exactly are also ignored if\n         # matched, but we warn the users when that happens",
    "output": " class PathAndUserErrorHandler:\n     \"\"\"Make an API call but with error handling for the CLI.\"\"\"\n \n    def __init__(self, formatter):\n         self.formatter = formatter\n \n     def __enter__(self):\n         return self\n \n     def __exit__(self, exc_type, exc_val, exc_tb):\n        if exc_type is SQLFluffUserError:\n             click.echo(\n                 \"\\nUser Error: \"\n                 + self.formatter.colorize(\n     if verbose >= 1:\n         click.echo(format_linting_result_header())\n \n    with PathAndUserErrorHandler(formatter):\n         # add stdin if specified via lone '-'\n         if (\"-\",) == paths:\n             result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n     click.echo(\"==== finding fixable violations ====\")\n \n    with PathAndUserErrorHandler(formatter):\n         result = lnt.lint_paths(\n             paths,\n             fix=True,\n     t0 = time.monotonic()\n \n     # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter):\n         if \"-\" == path:\n             parsed_strings = [\n                 lnt.parse_string(\n     )\n \n     # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter):\n         if \"-\" == path:\n             raw_sql = sys.stdin.read()\n             fname = \"stdin\"\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     SQLLintError,\n     SQLParseError,\n     SQLFluffSkipFile,\n    SQLFluffUserError,\n )\n from sqlfluff.core.parser import Lexer, Parser, RegexLexer\n from sqlfluff.core.file_helpers import get_encoding\n             if ignore_non_existent_files:\n                 return []\n             else:\n                raise SQLFluffUserError(\n                    f\"Specified path does not exist. Check it/they exist(s): {path}.\"\n                )\n \n         # Files referred to exactly are also ignored if\n         # matched, but we warn the users when that happens"
  },
  {
    "instruction": "Enhance rule L036 to put all columns on separate lines if any of them are\nThe current description is ambiguous, but after discussion, we decided to update the rule and keep the description at least _similar_ to what it is currently.. See discussion on #769.\n",
    "input": "         )\n \n     def _eval_multiple_select_target_elements(self, select_targets_info, segment):\n        if select_targets_info.first_new_line_idx == -1:\n            # there are multiple select targets but no new lines\n\n            # Find and delete any whitespace between \"SELECT\" and its targets.\n            ws_to_delete = segment.select_children(\n                start_seg=segment.segments[select_targets_info.select_idx],\n                select_if=lambda s: s.is_type(\"whitespace\"),\n                loop_while=lambda s: s.is_type(\"whitespace\") or s.is_meta,\n             )\n            fixes = [LintFix(\"delete\", ws) for ws in ws_to_delete]\n            # Insert newline before the first select target.\n            ins = self.make_newline(\n                pos_marker=segment.pos_marker.advance_by(segment.raw)\n            )\n            fixes.append(LintFix(\"create\", select_targets_info.select_targets[0], ins))\n             return LintResult(anchor=segment, fixes=fixes)\n \n     def _eval_single_select_target_element(",
    "output": "         )\n \n     def _eval_multiple_select_target_elements(self, select_targets_info, segment):\n        \"\"\"Multiple select targets. Ensure each is on a separate line.\"\"\"\n        # Insert newline before every select target.\n        fixes = []\n        for i, select_target in enumerate(select_targets_info.select_targets):\n            base_segment = (\n                segment if not i else select_targets_info.select_targets[i - 1]\n             )\n            if base_segment.pos_marker.line_no == select_target.pos_marker.line_no:\n                # Find and delete any whitespace before the select target.\n                ws_to_delete = segment.select_children(\n                    start_seg=segment.segments[select_targets_info.select_idx]\n                    if not i\n                    else select_targets_info.select_targets[i - 1],\n                    select_if=lambda s: s.is_type(\"whitespace\"),\n                    loop_while=lambda s: s.is_type(\"whitespace\", \"comma\") or s.is_meta,\n                )\n                fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n                ins = self.make_newline(pos_marker=select_target.pos_marker)\n                fixes.append(LintFix(\"create\", select_target, ins))\n        if fixes:\n             return LintResult(anchor=segment, fixes=fixes)\n \n     def _eval_single_select_target_element("
  },
  {
    "instruction": "TSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n## Expected Behaviour\r\n\r\nBoth of these queries should pass, the only difference is the addition of a table alias 'a':\r\n\r\n1/ no alias\r\n\r\n```\r\nSELECT [hello]\r\nFROM\r\n    mytable\r\n```\r\n\r\n2/ same query with alias\r\n\r\n```\r\nSELECT a.[hello]\r\nFROM\r\n    mytable AS a\r\n```\r\n\r\n## Observed Behaviour\r\n\r\n1/ passes\r\n2/ fails with: L031: Avoid using aliases in join condition.\r\n\r\nBut there is no join condition :-)\r\n\r\n## Steps to Reproduce\r\n\r\nLint queries above\r\n\r\n## Dialect\r\n\r\nTSQL\r\n\r\n## Version\r\n\r\nsqlfluff 0.6.9\r\nPython 3.6.9\r\n\r\n## Configuration\r\n\r\nN/A\n",
    "input": "             violation_buff.append(\n                 LintResult(\n                     anchor=alias_info.alias_identifier_ref,\n                    description=\"Avoid using aliases in join condition\",\n                     fixes=fixes,\n                 )\n             )",
    "output": "             violation_buff.append(\n                 LintResult(\n                     anchor=alias_info.alias_identifier_ref,\n                    description=\"Avoid aliases in from clauses and join conditions.\",\n                     fixes=fixes,\n                 )\n             )"
  },
  {
    "instruction": "`AnySetOf` grammar\n<!--Note: This is for general enhancements to the project. Please use the Bug report template instead to raise parsing/linting/syntax issues for existing supported dialects-->\r\nI know this has been talked about before in PRs so making an issue to formally track.\r\n\r\nIn many grammars there's a common situation where we have to denote several options that can be specified in any order but they cannot be specified more than once.\r\n\r\nOur general approach to this in the project has been denote this using `AnyNumberOf` as this allows for the different orderings:\r\n```python\r\nAnyNumberOf(\r\n    <option_1_grammar>,\r\n    <option_2_grammar>,\r\n    ...\r\n)\r\n```\r\nHowever, the issue with this is that it places no limit on how many times each option can be specified.\r\n\r\nThis means that sqlfluff allows certain invalid statements to parse e.g.\r\n```sql\r\nCREATE TABLE ktw_account_binding (\r\n    ktw_id VARCHAR(32) NOT NULL REFERENCES ref_table(bla)\r\n    ON DELETE RESTRICT ON DELETE CASCADE ON DELETE CASCADE ON DELETE CASCADE\r\n);\r\n```\r\nhttps://github.com/sqlfluff/sqlfluff/pull/2315#issuecomment-1013847846\r\n\r\nWe've accepted this limitation for the time being as it's more important to get the statements parsing for linting/formatting purposes rather than exactly reflecting the grammar (we'd expect a general degree of common sense when it comes to repeating these options).\r\n\r\nThat being said it would be nice to address this to refine our grammar and reduce dealing with contributor confusion.\r\n\r\n`AnySetOf` would essentially allow all of it's grammar arguments to be parsed in any order a maximum of 1 time each. Hopefully we can inherit from `AnyNumberOf` to simplify this.\n",
    "input": "     Delimited,\n     Bracketed,\n     AnyNumberOf,\n     Ref,\n     Anything,\n     Nothing,\n     \"Delimited\",\n     \"Bracketed\",\n     \"AnyNumberOf\",\n     \"Ref\",\n     \"Anything\",\n     \"Nothing\",\ndiff --git a/src/sqlfluff/core/parser/grammar/__init__.py b/src/sqlfluff/core/parser/grammar/__init__.py\n \"\"\"Definitions of grammars.\"\"\"\n \n from sqlfluff.core.parser.grammar.base import Ref, Anything, Nothing\nfrom sqlfluff.core.parser.grammar.anyof import AnyNumberOf, OneOf, OptionallyBracketed\n from sqlfluff.core.parser.grammar.delimited import Delimited\n from sqlfluff.core.parser.grammar.greedy import GreedyUntil, StartsWith\n from sqlfluff.core.parser.grammar.sequence import Sequence, Bracketed\n     \"Anything\",\n     \"Nothing\",\n     \"AnyNumberOf\",\n     \"OneOf\",\n     \"OptionallyBracketed\",\n     \"Delimited\",\ndiff --git a/src/sqlfluff/core/parser/grammar/anyof.py b/src/sqlfluff/core/parser/grammar/anyof.py\n     def __init__(self, *args, **kwargs):\n         self.max_times = kwargs.pop(\"max_times\", None)\n         self.min_times = kwargs.pop(\"min_times\", 0)\n         # Any patterns to _prevent_ a match.\n         self.exclude = kwargs.pop(\"exclude\", None)\n         super().__init__(*args, **kwargs)\n \n     def _match_once(\n         self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n    ) -> MatchResult:\n         \"\"\"Match the forward segments against the available elements once.\n \n         This serves as the main body of OneOf, but also a building block\n             return MatchResult.from_unmatched(segments)\n \n         with parse_context.deeper_match() as ctx:\n            match, _ = self._longest_trimmed_match(\n                 segments,\n                 available_options,\n                 parse_context=ctx,\n                 trim_noncode=False,\n             )\n \n        return match\n \n     @match_wrapper()\n     @allow_ephemeral\n         matched_segments: MatchResult = MatchResult.from_empty()\n         unmatched_segments: Tuple[BaseSegment, ...] = segments\n         n_matches = 0\n         while True:\n             if self.max_times and n_matches >= self.max_times:\n                 # We've matched as many times as we can\n             else:\n                 pre_seg = ()  # empty tuple\n \n            match = self._match_once(unmatched_segments, parse_context=parse_context)\n             if match:\n                 matched_segments += pre_seg + match.matched_segments\n                 unmatched_segments = match.unmatched_segments\n             args[0] if len(args) == 1 else Sequence(*args),\n             **kwargs,\n         )\ndiff --git a/src/sqlfluff/dialects/dialect_ansi.py b/src/sqlfluff/dialects/dialect_ansi.py\n from sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\n from sqlfluff.core.parser import (\n     AnyNumberOf,\n     Anything,\n     BaseFileSegment,\n     BaseSegment,\n             ),\n             optional=True,\n         ),\n        AnyNumberOf(\n             # ON DELETE clause, e.g. ON DELETE NO ACTION\n             Sequence(\n                 \"ON\",",
    "output": "     Delimited,\n     Bracketed,\n     AnyNumberOf,\n    AnySetOf,\n     Ref,\n     Anything,\n     Nothing,\n     \"Delimited\",\n     \"Bracketed\",\n     \"AnyNumberOf\",\n    \"AnySetOf\",\n     \"Ref\",\n     \"Anything\",\n     \"Nothing\",\ndiff --git a/src/sqlfluff/core/parser/grammar/__init__.py b/src/sqlfluff/core/parser/grammar/__init__.py\n \"\"\"Definitions of grammars.\"\"\"\n \n from sqlfluff.core.parser.grammar.base import Ref, Anything, Nothing\nfrom sqlfluff.core.parser.grammar.anyof import (\n    AnyNumberOf,\n    AnySetOf,\n    OneOf,\n    OptionallyBracketed,\n)\n from sqlfluff.core.parser.grammar.delimited import Delimited\n from sqlfluff.core.parser.grammar.greedy import GreedyUntil, StartsWith\n from sqlfluff.core.parser.grammar.sequence import Sequence, Bracketed\n     \"Anything\",\n     \"Nothing\",\n     \"AnyNumberOf\",\n    \"AnySetOf\",\n     \"OneOf\",\n     \"OptionallyBracketed\",\n     \"Delimited\",\ndiff --git a/src/sqlfluff/core/parser/grammar/anyof.py b/src/sqlfluff/core/parser/grammar/anyof.py\n     def __init__(self, *args, **kwargs):\n         self.max_times = kwargs.pop(\"max_times\", None)\n         self.min_times = kwargs.pop(\"min_times\", 0)\n        self.max_times_per_element = kwargs.pop(\"max_times_per_element\", None)\n         # Any patterns to _prevent_ a match.\n         self.exclude = kwargs.pop(\"exclude\", None)\n         super().__init__(*args, **kwargs)\n \n     def _match_once(\n         self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n    ) -> Tuple[MatchResult, Optional[\"MatchableType\"]]:\n         \"\"\"Match the forward segments against the available elements once.\n \n         This serves as the main body of OneOf, but also a building block\n             return MatchResult.from_unmatched(segments)\n \n         with parse_context.deeper_match() as ctx:\n            match, matched_option = self._longest_trimmed_match(\n                 segments,\n                 available_options,\n                 parse_context=ctx,\n                 trim_noncode=False,\n             )\n \n        return match, matched_option\n \n     @match_wrapper()\n     @allow_ephemeral\n         matched_segments: MatchResult = MatchResult.from_empty()\n         unmatched_segments: Tuple[BaseSegment, ...] = segments\n         n_matches = 0\n\n        # Keep track of the number of times each option has been matched.\n        available_options, _ = self._prune_options(\n            segments, parse_context=parse_context\n        )\n        available_option_counter = {str(o): 0 for o in available_options}\n\n         while True:\n             if self.max_times and n_matches >= self.max_times:\n                 # We've matched as many times as we can\n             else:\n                 pre_seg = ()  # empty tuple\n \n            match, matched_option = self._match_once(\n                unmatched_segments, parse_context=parse_context\n            )\n\n            # Increment counter for matched option.\n            if matched_option and (str(matched_option) in available_option_counter):\n                available_option_counter[str(matched_option)] += 1\n                # Check if we have matched an option too many times.\n                if (\n                    self.max_times_per_element\n                    and available_option_counter[str(matched_option)]\n                    > self.max_times_per_element\n                ):\n                    return MatchResult(\n                        matched_segments.matched_segments, unmatched_segments\n                    )\n\n             if match:\n                 matched_segments += pre_seg + match.matched_segments\n                 unmatched_segments = match.unmatched_segments\n             args[0] if len(args) == 1 else Sequence(*args),\n             **kwargs,\n         )\n\n\nclass AnySetOf(AnyNumberOf):\n    \"\"\"Match any number of the elements but each element can only be matched once.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, max_times_per_element=1, **kwargs)\ndiff --git a/src/sqlfluff/dialects/dialect_ansi.py b/src/sqlfluff/dialects/dialect_ansi.py\n from sqlfluff.core.dialects.common import AliasInfo, ColumnAliasInfo\n from sqlfluff.core.parser import (\n     AnyNumberOf,\n    AnySetOf,\n     Anything,\n     BaseFileSegment,\n     BaseSegment,\n             ),\n             optional=True,\n         ),\n        AnySetOf(\n             # ON DELETE clause, e.g. ON DELETE NO ACTION\n             Sequence(\n                 \"ON\","
  },
  {
    "instruction": "Config for fix_even_unparsable not being applied\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nWhen setting the any config file to `fix_even_unparsable = True` the config get's overriden by the default (or lack thereof) on the @click.option decorator for the fix command.\r\n\r\n### Expected Behaviour\r\n\r\nWhen setting the config `fix_even_unparsable` it should be captured by the fix command as well.\r\n\r\n### Observed Behaviour\r\n\r\nThe `fix_even_unparsable` command is not being captured by the fix command\r\n\r\n### How to reproduce\r\n\r\nCreate a config file and include `fix_even_unparsable`\r\nRun `sqlfluff fix`\r\nNote that `fix_even_unparsable` is set to False at runtime\r\n\r\n### Dialect\r\n\r\nAny\r\n\r\n### Version\r\n\r\n0.13.0\r\n\r\n### Configuration\r\n\r\n`pyproject.toml`\r\n\r\n```\r\n[tool.sqlfluff.core]\r\nverbose = 2\r\ndialect = \"snowflake\"\r\nfix_even_unparsable = true\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": " @click.option(\n     \"--FIX-EVEN-UNPARSABLE\",\n     is_flag=True,\n     help=(\n         \"Enables fixing of files that have templating or parse errors. \"\n         \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n             )\n             click.echo(\n                 colorize(\n                    \"Use --fix-even-unparsable' to attempt to fix the SQL anyway.\",\n                     Color.red,\n                 ),\n                 err=True,",
    "output": " @click.option(\n     \"--FIX-EVEN-UNPARSABLE\",\n     is_flag=True,\n    default=None,\n     help=(\n         \"Enables fixing of files that have templating or parse errors. \"\n         \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n             )\n             click.echo(\n                 colorize(\n                    \"Use --FIX-EVEN-UNPARSABLE' to attempt to fix the SQL anyway.\",\n                     Color.red,\n                 ),\n                 err=True,"
  },
  {
    "instruction": "Rule L060 could give a specific error message\nAt the moment rule L060 flags something like this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\r\n```\r\n\r\nSince we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\r\n\r\nThat is it should flag this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\r\n```\r\n Or this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\r\n```\r\n\r\nAs appropriate.\r\n\r\nWhat do you think @jpy-git ?\r\n\n",
    "input": "             ],\n         )\n \n        return LintResult(context.segment, [fix])",
    "output": "             ],\n         )\n \n        return LintResult(\n            anchor=context.segment,\n            fixes=[fix],\n            description=f\"Use 'COALESCE' instead of '{context.segment.raw_upper}'.\",\n        )"
  },
  {
    "instruction": "Commented dash character converted to non utf-8 character\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nUpon fixing a query containing a multi-line comment, SQLFluff attempts to fix a commented line.\r\n\r\nThis:\r\n```sql\r\n/*\r\nTODO\r\n - tariff scenario \u2014> dm_tariff_scenario\r\n*/\r\n```\r\n\r\nBecame:\r\n```sql\r\n/*\r\nTODO\r\n - tariff scenario \u0097> dm_tariff_scenario\r\n*/\r\n``` \r\nThis in an invisible char represented as `<97>`\r\n\r\nThis causes an issue with dbt which can not compile with this char present\r\n\r\nNote this comment comes at the end of the file.\r\n\r\n### Expected Behaviour\r\n\r\nDoes not replace/fix anything that is commented\r\n\r\n### Observed Behaviour\r\n\r\n```bash\r\n $  sqlfluff fix dbt/models/marts/core/f_utility_statements.sql                                                                                                                                                                                               \r\n==== finding fixable violations ====                                                                                                                                                                                                                          \r\n=== [dbt templater] Sorting Nodes...                                                                                                                                                                                                                          \r\n=== [dbt templater] Compiling dbt project...                                                                                                                                                                                                                  \r\n=== [dbt templater] Project Compiled.                                                                                                                                                                                                                         \r\n== [dbt/models/marts/core/f_utility_statements.sql] FAIL                                                                                                                                                                                                      \r\nL:   1 | P:   5 | L001 | Unnecessary trailing whitespace.                                                                                                                                                                                                     \r\nL:   2 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   3 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \r\nL:   4 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   4 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \r\nL:   6 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   7 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \r\nL:   8 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:   8 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \r\nL:  10 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:  11 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                               \r\nL:  12 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]                                                                                                                                                                               \r\nL:  12 | P:   6 | L019 | Found trailing comma. Expected only leading.                                                                                                                                                                                         \r\nL:  15 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]   \r\nL:  16 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]                                                                                                                                                                      [0/47960]\r\nL:  17 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  18 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  19 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  20 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  20 | P:  36 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  21 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  21 | P:  32 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  22 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  22 | P:   6 | L019 | Found trailing comma. Expected only leading.\r\nL:  24 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  26 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  26 | P:  15 | L001 | Unnecessary trailing whitespace.\r\nL:  27 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  28 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  29 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  30 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  31 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  32 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  32 | P:  24 | L011 | Implicit/explicit aliasing of table.\r\nL:  32 | P:  24 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  33 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  33 | P:  49 | L011 | Implicit/explicit aliasing of table.\r\nL:  33 | P:  49 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  33 | P:  52 | L001 | Unnecessary trailing whitespace.\r\nL:  34 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  36 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  37 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  37 | P:   6 | L019 | Found trailing comma. Expected only leading.\r\nL:  39 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  41 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  41 | P:   9 | L034 | Select wildcards then simple targets before calculations\r\n                       | and aggregates.\r\nL:  43 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  46 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  47 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  48 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  51 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  52 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  53 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  54 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  57 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  58 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  61 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  62 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  64 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  65 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  68 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  69 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  70 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  71 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  73 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  73 | P:  36 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  74 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  74 | P:  56 | L031 | Avoid aliases in from clauses and join conditions.\r\nL:  75 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  76 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  76 | P:  28 | L001 | Unnecessary trailing whitespace.\r\nL:  77 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  80 | P:   9 | L003 | Expected 0 indentations, found 2 [compared to line 01]\r\nL:  81 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  83 | P:  13 | L003 | Expected 0 indentations, found 3 [compared to line 01]\r\nL:  84 | P:   5 | L003 | Expected 0 indentations, found 1 [compared to line 01]\r\nL:  94 | P:   1 | L009 | Files must end with a single trailing newline.\r\n```\r\n\r\n### How to reproduce\r\n\r\n`sqlfluff fix` with provided `.sqlfluff` configuration\r\n\r\nSQL contains proprietary code and I am, likely, unable to provide a full snippet of the SQL \r\n\r\n### Dialect\r\n\r\nSnowflake\r\n\r\n### Version\r\n\r\n0.13.0 and 0.11.1\r\n\r\n### Configuration\r\n\r\n`.sqlfluff`:\r\n```\r\n[sqlfluff]\r\ntemplater = dbt\r\ndialect = snowflake\r\n\r\n[sqlfluff:templater:dbt]\r\nproject_dir = dbt/\r\n\r\n# Defaults on anything not specified explicitly: https://docs.sqlfluff.com/en/stable/configuration.html#default-configuration\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\ncomma_style = leading\r\n\r\n# Keyword capitalisation\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = lower\r\n\r\n# TODO: this supports pascal but not snake\r\n# TODO: this inherits throwing violation on all unquoted identifiers... we can limit to aliases or column aliases\r\n# [sqlfluff:rules:L014]\r\n# extended_capitalisation_policy = pascal\r\n\r\n# TODO: not 100% certain that this default is correct\r\n# [sqlfluff:rules:L029]\r\n## Keywords should not be used as identifiers.\r\n# unquoted_identifiers_policy = aliases\r\n# quoted_identifiers_policy = none\r\n## Comma separated list of words to ignore for this rule\r\n# ignore_words = None\r\n\r\n# Function name capitalisation\r\n[sqlfluff:rules:L030]\r\nextended_capitalisation_policy = lower\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": "     )(f)\n     f = click.option(\n         \"--encoding\",\n        default=\"autodetect\",\n         help=(\n             \"Specify encoding to use when reading and writing files. Defaults to \"\n             \"autodetect.\"",
    "output": "     )(f)\n     f = click.option(\n         \"--encoding\",\n        default=None,\n         help=(\n             \"Specify encoding to use when reading and writing files. Defaults to \"\n             \"autodetect.\""
  },
  {
    "instruction": "ValueError: Position Not Found for lint/parse/fix, not clear why\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nI have admittedly messy dbt sql model that gets the following error when I try to lint, parse or fix it with sqlfluff - every other model can be processed using the same settings, but this one throws the same error below even if I only run a single rule e.g. L009.\r\n\r\nUnfortunately I cannot share the model itself but I can describe some notable features:\r\n- begins with a dbt incremental config\r\n- then sets three variables, each a list of strings\r\n- Has two `for` loops with nested `if` conditions\r\n- Has one very long line doing arithmetic operations involving both hardcoded values and columns from a two joined CTEs\r\n\r\n### Expected Behaviour\r\n\r\nNot the above error\r\n\r\n### Observed Behaviour\r\n\r\n```\r\nWARNING    Unable to lint models/ltv_prediction_model/ltv_prediction.sql due to an internal error. Please report this as an issue w\r\nith your query's contents and stacktrace below!\r\nTo hide this warning, add the failing file to .sqlfluffignore\r\nTraceback (most recent call last):\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/runner.py\", line 103, in run\r\n    yield partial()\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 666, in lint_rendered\r\n    parsed = cls.parse_rendered(rendered)\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 352, in parse_rendere\r\n\r\nd\r\n    tokens, lvs, config = cls._lex_templated_file(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 139, in _lex_template\r\nd_file\r\n    tokens, lex_vs = lexer.lex(templated_file)\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/parser/lexer.py\", line 321, in lex\r\n    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/parser/lexer.py\", line 348, in elements_to_se\r\ngments\r\n    source_slice = templated_file.templated_slice_to_source_slice(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 294, in templated_s\r\nlice_to_source_slice\r\n    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\r\n  File \"/Users/dlyons/.pyenv/versions/3.9.4/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 180, in _find_slice\r\n_indices_of_templated_pos\r\n    raise ValueError(\"Position Not Found\")\r\nValueError: Position Not Found\r\n```\r\n\r\n### How to reproduce\r\n\r\n```\r\n{{\r\n    config(\r\n        materialized='incremental',\r\n        unique_key='md5_surrogate_key_main'\r\n    )\r\n}}\r\n\r\n{%- set first_list = [\"value1\", \"value2\", \"value3\"] -%}\r\n{%- set second_list = [\"value4\", \"value5\", \"value6\"] -%}\r\n{%- set third_list = [\"value7\", \"value8\", \"value9\"] -%}\r\n\r\nwith fill_na_values as (\r\n    select\r\n        id,\r\n        run_date,\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {%- if features in third_list %}\r\n                coalesce({{features}}, (select feature_mode from {{ ref('second_list') }} where features = '{{features}}')) as {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- else -%}\r\n                coalesce({{features}}, (select feature_mean from {{ ref('second_list') }} where features = '{{features}}')) as {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- endif -%}\r\n        {%- endfor %}\r\n    from {{ ref('training_dataset') }}\r\n    {%- if is_incremental() %}\r\n    where current_date >= (select max(run_date) from {{ this }})\r\n    {%- else %}\r\n    where run_date >= '2021-01-01'\r\n    {%- endif %}\r\n),\r\n\r\nwinsorize_data as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {%- if features in first_list %}\r\n                case\r\n                    when {{features}} < (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    then (select fifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    when {{features}} > (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    then (select ninetyfifth_percentile from {{ ref('first_list') }} where winsorize_column = '{{features}}')\r\n                    else {{features}}\r\n                end as {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- else %}\r\n                {{features}}\r\n                {%- if not loop.last -%},{% endif %}\r\n            {%- endif %}\r\n        {%- endfor %}\r\n    from fill_na_values\r\n),\r\n\r\nscaling_data as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            ({{features}} - (select feature_mean from {{ ref('second_list') }} where features = '{{features}}'))/(select feature_std from {{ ref('second_list') }} where features = '{{features}}') as {{features}}\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- endfor %}\r\n    from winsorize_data\r\n),\r\n\r\napply_ceofficients as (\r\n    select\r\n        md5_surrogate_key_main,\r\n        {%- for features in second_list %}\r\n            {{features}} * (select coefficients from {{ ref('second_list') }} where features = '{{features}}') as {{features}}_coef\r\n            {%- if not loop.last -%},{% endif %}\r\n        {%- endfor %}\r\n    from scaling_data\r\n),\r\n\r\nlogistic_prediction as (\r\n    select\r\n        fan.*,\r\n        1/(1+EXP(-(0.24602303+coef1+coef2+coef3+coef4+coef5+coef6+coef7+coef8+coef9+available_balance_coef+coef10+coef11+coef12+coef13+coef14))) as prediction_probability,\r\n        case when prediction_probability < .5 then 0 else 1 end as prediction_class\r\n    from apply_ceofficients ac\r\n    inner join fill_na_values fan\r\n        on ac.md5_surrogate_key_main = fan.md5_surrogate_key_main\r\n)\r\n\r\nselect * from logistic_prediction\r\n```\r\n\r\n### Dialect\r\n\r\nSnowflake\r\n\r\n### Version\r\n\r\n0.10.1\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\n# verbose is an integer (0-2) indicating the level of log output\r\nverbose = 0\r\n# Turn off color formatting of output\r\nnocolor = False\r\ndialect = snowflake\r\ntemplater = jinja\r\n# Comma separated list of rules to check, or None for all\r\nrules = L001,L002,L003,L004,L005,L009,L010,L013,L014,L015,L017,L018,L019,L020,L021,L022,L023,L024,L026,L027,L028,L030,L036,L037,L038,L039,L040,L044,L045,L046,L050,L051,L058,L061\r\n# Comma separated list of rules to exclude, or None\r\nexclude_rules = L006,L008,L011,L012,L025,L029,L031,L034,L035,L041,L042,L043,L052\r\n# The depth to recursively parse to (0 for unlimited)\r\nrecurse = 0\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\n# Number of passes to run before admitting defeat\r\nrunaway_limit = 10\r\n# Ignore errors by category (one or more of the following, separated by commas: lexing,linting,parsing,templating)\r\nignore = None\r\n# Ignore linting errors found within sections of code coming directly from\r\n# templated code (e.g. from within Jinja curly braces. Note that it does not\r\n# ignore errors from literal code found within template loops.\r\nignore_templated_areas = True\r\n# can either be autodetect or a valid encoding e.g. utf-8, utf-8-sig\r\nencoding = autodetect\r\n# Ignore inline overrides (e.g. to test if still required)\r\ndisable_noqa = False\r\n# Comma separated list of file extensions to lint\r\n# NB: This config will only apply in the root folder\r\nsql_file_exts = .sql,.sql.j2,.dml,.ddl\r\n# Allow fix to run on files, even if they contain parsing errors\r\n# Note altering this is NOT RECOMMENDED as can corrupt SQL\r\nfix_even_unparsable = False\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindented_joins = False\r\nindented_ctes = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 80\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# Some rules have their own specific config\r\n[sqlfluff:rules:L007]\r\noperator_new_lines = after\r\n\r\n[sqlfluff:rules:L010]\r\n# Keywords\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L011]\r\n# Aliasing preference for tables\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L012]\r\n# Aliasing preference for columns\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L014]\r\n# Unquoted identifiers\r\nextended_capitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L016]\r\n# Line length\r\nignore_comment_lines = False\r\nignore_comment_clauses = False\r\n\r\n[sqlfluff:rules:L026]\r\n# References must be in FROM clause\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L028]\r\n# References must be consistently used\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]\r\n# Keywords should not be used as identifiers.\r\nunquoted_identifiers_policy = aliases\r\nquoted_identifiers_policy = none\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L030]\r\n# Function names\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L038]\r\n# Trailing commas\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:L040]\r\n# Null & Boolean Literals\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\n\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses\r\nforbid_subquery_in = join\r\n\r\n[sqlfluff:rules:L047]\r\n# Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n[sqlfluff:rules:L052]\r\n# Semi-colon formatting approach\r\nmultiline_newline = False\r\nrequire_final_semicolon = False\r\n\r\n[sqlfluff:rules:L054]\r\n# GROUP BY/ORDER BY column references\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:L057]\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\nquoted_identifiers_policy = all\r\nallow_space_in_identifier = False\r\nadditional_allowed_characters = \"\"\r\n\r\n[sqlfluff:rules:L059]\r\n# Policy on quoted and unquoted identifiers\r\nprefer_quoted_identifiers = False\r\n\r\n[sqlfluff:rules:L062]\r\n# Comma separated list of blocked words that should not be used\r\nblocked_words = None\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n```\n",
    "input": "         self.slice_id += 1\n         return result\n \n     def _slice_template(self) -> List[RawFileSlice]:\n         \"\"\"Slice template in jinja.\n \n         for _, elem_type, raw in self.env.lex(self.raw_str):\n             # Replace literal text with a unique ID.\n             if elem_type == \"data\":\n                if set_idx is None:\n                    unique_alternate_id = self.next_slice_id()\n                    alternate_code = f\"\\0{unique_alternate_id}_{len(raw)}\"\n                else:\n                    unique_alternate_id = self.next_slice_id()\n                    alternate_code = f\"\\0set{unique_alternate_id}_{len(raw)}\"\n                 result.append(\n                     RawFileSlice(\n                         raw,\n                         idx,\n                     )\n                 )\n                self.raw_slice_info[result[-1]] = RawSliceInfo(\n                    unique_alternate_id, alternate_code, []\n                 )\n                 idx += len(raw)\n                 continue\n                         )\n                     # Treat the skipped whitespace as a literal.\n                     result.append(RawFileSlice(skipped_str, \"literal\", idx))\n                    self.raw_slice_info[result[-1]] = RawSliceInfo(\"\", \"\", [])\n                     idx += num_chars_skipped\n \n             # raw_end and raw_begin behave a little differently in\n                     # returns, it has simply grouped them differently than we\n                     # want.\n                     trailing_chars = len(m.group(0))\n                    if block_type.startswith(\"block_\"):\n                        alternate_code = self._remove_block_whitespace_control(\n                            str_buff[:-trailing_chars]\n                        )\n                     result.append(\n                         RawFileSlice(\n                             str_buff[:-trailing_chars],\n                             idx,\n                         )\n                     )\n                    self.raw_slice_info[result[-1]] = RawSliceInfo(\"\", \"\", [])\n                     idx += trailing_chars\n                 else:\n                    if block_type.startswith(\"block_\"):\n                        alternate_code = self._remove_block_whitespace_control(str_buff)\n                     result.append(\n                         RawFileSlice(\n                             str_buff,\n                     stack.pop()\n                 str_buff = \"\"\n         return result\n\n    @classmethod\n    def _remove_block_whitespace_control(cls, in_str: str) -> Optional[str]:\n        \"\"\"Removes whitespace control from a Jinja block start or end.\n\n        Use of Jinja whitespace stripping (e.g. `{%-` or `-%}`) causes the\n        template to produce less output. This makes JinjaTracer's job harder,\n        because it uses the \"bread crumb trail\" of output to deduce the\n        execution path through the template. This change has no impact on the\n        actual Jinja output, which uses the original, unmodified code.\n        \"\"\"\n        result = regex.sub(r\"^{%-\", \"{%\", in_str)\n        result = regex.sub(r\"-%}$\", \"%}\", result)\n        return result if result != in_str else None",
    "output": "         self.slice_id += 1\n         return result\n \n    def slice_info_for_literal(self, length, prefix=\"\") -> RawSliceInfo:\n        \"\"\"Returns a RawSliceInfo for a literal.\n\n        In the alternate template, literals are replaced with a uniquely\n        numbered, easily-to-parse literal. JinjaTracer uses this output as\n        a \"breadcrumb trail\" to deduce the execution path through the template.\n\n        This is important even if the original literal (i.e. in the raw SQL\n        file) was empty, as is the case when Jinja whitespace control is used\n        (e.g. \"{%- endif -%}\"), because fewer breadcrumbs means JinjaTracer has\n        to *guess* the path, in which case it assumes simple, straight-line\n        execution, which can easily be wrong with loops and conditionals.\n        \"\"\"\n        unique_alternate_id = self.next_slice_id()\n        alternate_code = f\"\\0{prefix}{unique_alternate_id}_{length}\"\n        return RawSliceInfo(unique_alternate_id, alternate_code, [])\n\n     def _slice_template(self) -> List[RawFileSlice]:\n         \"\"\"Slice template in jinja.\n \n         for _, elem_type, raw in self.env.lex(self.raw_str):\n             # Replace literal text with a unique ID.\n             if elem_type == \"data\":\n                 result.append(\n                     RawFileSlice(\n                         raw,\n                         idx,\n                     )\n                 )\n                self.raw_slice_info[result[-1]] = self.slice_info_for_literal(\n                    len(raw), \"\" if set_idx is None else \"set\"\n                 )\n                 idx += len(raw)\n                 continue\n                         )\n                     # Treat the skipped whitespace as a literal.\n                     result.append(RawFileSlice(skipped_str, \"literal\", idx))\n                    self.raw_slice_info[result[-1]] = self.slice_info_for_literal(0)\n                     idx += num_chars_skipped\n \n             # raw_end and raw_begin behave a little differently in\n                     # returns, it has simply grouped them differently than we\n                     # want.\n                     trailing_chars = len(m.group(0))\n                     result.append(\n                         RawFileSlice(\n                             str_buff[:-trailing_chars],\n                             idx,\n                         )\n                     )\n                    self.raw_slice_info[result[-1]] = self.slice_info_for_literal(0)\n                     idx += trailing_chars\n                 else:\n                     result.append(\n                         RawFileSlice(\n                             str_buff,\n                     stack.pop()\n                 str_buff = \"\"\n         return result"
  },
  {
    "instruction": "Number of processes configurable in .sqlfluff\nBeing able to set the number of processes to run with in .sqlfluff might be useful to avoid having to pass it in the CLI every time.\n",
    "input": "     \"-p\",\n     \"--processes\",\n     type=int,\n    default=1,\n    help=\"The number of parallel processes to run.\",\n )\n @click.option(\n     \"--disable_progress_bar\",\n @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n def lint(\n     paths: Tuple[str],\n    processes: int,\n     format: str,\n     write_output: Optional[str],\n     annotation_level: str,\n     disregard_sqlfluffignores: bool,\n     logger: Optional[logging.Logger] = None,\n     bench: bool = False,\n     disable_progress_bar: Optional[bool] = False,\n     extra_config_path: Optional[str] = None,\n     ignore_local_config: bool = False,\n     \"-p\",\n     \"--processes\",\n     type=int,\n    default=1,\n    help=\"The number of parallel processes to run.\",\n )\n @click.option(\n     \"--disable_progress_bar\",\n def fix(\n     force: bool,\n     paths: Tuple[str],\n    processes: int,\n     bench: bool = False,\n     fixed_suffix: str = \"\",\n     logger: Optional[logging.Logger] = None,\n     disable_progress_bar: Optional[bool] = False,\n     extra_config_path: Optional[str] = None,\n     ignore_local_config: bool = False,\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n             f\"=== [{self.colorize(templater, Color.lightgrey)}] {message}\"\n         )  # pragma: no cover\n \n     def dispatch_dialect_warning(self, dialect) -> None:\n         \"\"\"Dispatch a warning for dialects.\"\"\"\n         self._dispatch(self.format_dialect_warning(dialect))  # pragma: no cover\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n        processes: int = 1,\n     ) -> LintedDir:\n         \"\"\"Lint a path.\"\"\"\n         linted_path = LintedDir(path)\n             )\n         )\n \n         # to avoid circular import\n         from sqlfluff.core.linter.runner import get_runner\n \n        runner = get_runner(\n             self,\n             self.config,\n             processes=processes,\n             allow_process_parallelism=self.allow_process_parallelism,\n         )\n \n         # Show files progress bar only when there is more than one.\n         files_count = len(fnames)\n         progress_bar_files = tqdm(\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n        processes: int = 1,\n     ) -> LintingResult:\n         \"\"\"Lint an iterable of paths.\"\"\"\n         paths_count = len(paths)\ndiff --git a/src/sqlfluff/core/linter/runner.py b/src/sqlfluff/core/linter/runner.py\n import bdb\n import functools\n import logging\n import multiprocessing.dummy\n import signal\n import sys\n     config: FluffConfig,\n     processes: int,\n     allow_process_parallelism: bool = True,\n) -> BaseRunner:\n    \"\"\"Generate a runner instance based on parallel and system configuration.\"\"\"\n     if processes > 1:\n         # Process parallelism isn't really supported during testing\n         # so this flag allows us to fall back to a threaded runner\n         # in those cases.\n         if allow_process_parallelism:\n            return MultiProcessRunner(linter, config, processes=processes)\n         else:\n            return MultiThreadRunner(linter, config, processes=processes)\n     else:\n        return SequentialRunner(linter, config)",
    "output": "     \"-p\",\n     \"--processes\",\n     type=int,\n    default=None,\n    help=(\n        \"The number of parallel processes to run. Positive numbers work as \"\n        \"expected. Zero and negative numbers will work as number_of_cpus - \"\n        \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n    ),\n )\n @click.option(\n     \"--disable_progress_bar\",\n @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n def lint(\n     paths: Tuple[str],\n     format: str,\n     write_output: Optional[str],\n     annotation_level: str,\n     disregard_sqlfluffignores: bool,\n     logger: Optional[logging.Logger] = None,\n     bench: bool = False,\n    processes: Optional[int] = None,\n     disable_progress_bar: Optional[bool] = False,\n     extra_config_path: Optional[str] = None,\n     ignore_local_config: bool = False,\n     \"-p\",\n     \"--processes\",\n     type=int,\n    default=None,\n    help=(\n        \"The number of parallel processes to run. Positive numbers work as \"\n        \"expected. Zero and negative numbers will work as number_of_cpus - \"\n        \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n    ),\n )\n @click.option(\n     \"--disable_progress_bar\",\n def fix(\n     force: bool,\n     paths: Tuple[str],\n     bench: bool = False,\n     fixed_suffix: str = \"\",\n     logger: Optional[logging.Logger] = None,\n    processes: Optional[int] = None,\n     disable_progress_bar: Optional[bool] = False,\n     extra_config_path: Optional[str] = None,\n     ignore_local_config: bool = False,\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n             f\"=== [{self.colorize(templater, Color.lightgrey)}] {message}\"\n         )  # pragma: no cover\n \n    def dispatch_processing_header(self, processes: int) -> None:\n        \"\"\"Dispatch the header displayed before linting.\"\"\"\n        if self._verbosity > 0:\n            self._dispatch(  # pragma: no cover\n                f\"{self.colorize('effective configured processes: ', Color.lightgrey)} \"\n                f\"{processes}\"\n            )\n\n     def dispatch_dialect_warning(self, dialect) -> None:\n         \"\"\"Dispatch a warning for dialects.\"\"\"\n         self._dispatch(self.format_dialect_warning(dialect))  # pragma: no cover\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n        processes: Optional[int] = None,\n     ) -> LintedDir:\n         \"\"\"Lint a path.\"\"\"\n         linted_path = LintedDir(path)\n             )\n         )\n \n        if processes is None:\n            processes = self.config.get(\"processes\", default=1)\n\n         # to avoid circular import\n         from sqlfluff.core.linter.runner import get_runner\n \n        runner, effective_processes = get_runner(\n             self,\n             self.config,\n             processes=processes,\n             allow_process_parallelism=self.allow_process_parallelism,\n         )\n \n        if self.formatter and effective_processes != 1:\n            self.formatter.dispatch_processing_header(effective_processes)\n\n         # Show files progress bar only when there is more than one.\n         files_count = len(fnames)\n         progress_bar_files = tqdm(\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n        processes: Optional[int] = None,\n     ) -> LintingResult:\n         \"\"\"Lint an iterable of paths.\"\"\"\n         paths_count = len(paths)\ndiff --git a/src/sqlfluff/core/linter/runner.py b/src/sqlfluff/core/linter/runner.py\n import bdb\n import functools\n import logging\nimport multiprocessing\n import multiprocessing.dummy\n import signal\n import sys\n     config: FluffConfig,\n     processes: int,\n     allow_process_parallelism: bool = True,\n) -> Tuple[BaseRunner, int]:\n    \"\"\"Generate a runner instance based on parallel and system configuration.\n\n    The processes argument can be positive or negative.\n    - If positive, the integer is interpreted as the number of processes.\n    - If negative or zero, the integer is interpreted as number_of_cpus - processes.\n\n    e.g.\n    -1 = all cpus but one.\n    0 = all cpus\n    1 = 1 cpu\n\n    \"\"\"\n    if processes <= 0:\n        processes = max(multiprocessing.cpu_count() + processes, 1)\n\n     if processes > 1:\n         # Process parallelism isn't really supported during testing\n         # so this flag allows us to fall back to a threaded runner\n         # in those cases.\n         if allow_process_parallelism:\n            return MultiProcessRunner(linter, config, processes=processes), processes\n         else:\n            return MultiThreadRunner(linter, config, processes=processes), processes\n     else:\n        return SequentialRunner(linter, config), processes"
  },
  {
    "instruction": "Validate layout configurations on load\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nAs raised in this comment: https://github.com/sqlfluff/sqlfluff/pull/4558#discussion_r1142745101\r\n\r\nAt the moment, the layout configs are being validated _on use_ which is potentially flaky and convoluted. Better would be to validate configs _on load_.\n\n### Use case\n\n_No response_\n\n### Dialect\n\nall\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "             fname = path\n \n     # Get file specific config\n    file_config.process_raw_file_for_config(raw_sql)\n     rendered = lnt.render_string(raw_sql, fname, file_config, \"utf8\")\n \n     if rendered.templater_violations:\ndiff --git a/src/sqlfluff/core/config.py b/src/sqlfluff/core/config.py\n ConfigElemType = Tuple[Tuple[str, ...], Any]\n \n \n @dataclass\n class _RemovedConfig:\n     old_path: Tuple[str, ...]\n     def _validate_configs(\n         configs: Iterable[ConfigElemType], file_path\n     ) -> List[ConfigElemType]:\n        \"\"\"Validate config elements against removed list.\"\"\"\n         config_map = {cfg.old_path: cfg for cfg in REMOVED_CONFIGS}\n         # Materialise the configs into a list to we can iterate twice.\n         new_configs = list(configs)\n         defined_keys = {k for k, _ in new_configs}\n         validated_configs = []\n         for k, v in new_configs:\n             if k in config_map.keys():\n                 formatted_key = \":\".join(k)\n                 removed_option = config_map[k]\n                 else:\n                     # Raise an error.\n                     raise SQLFluffUserError(\n                        f\"Config file {file_path} set an outdated config \"\n                         f\"value {formatted_key}.\\n\\n{removed_option.warning}\\n\\n\"\n                         \"See https://docs.sqlfluff.com/en/stable/configuration.html\"\n                         \" for more details.\"\n                     )\n \n             validated_configs.append((k, v))\n         return validated_configs\n \n                 for idnt, key, val in self.iter_vals(cfg=cfg[k]):\n                     yield (idnt + 1, key, val)\n \n    def process_inline_config(self, config_line: str):\n         \"\"\"Process an inline config command and update self.\"\"\"\n         # Strip preceding comment marks\n         if config_line.startswith(\"--\"):\n         config_line = config_line[9:].strip()\n         # Divide on colons\n         config_path = [elem.strip() for elem in config_line.split(\":\")]\n         # Set the value\n        self.set_value(config_path[:-1], config_path[-1])\n         # If the config is for dialect, initialise the dialect\n         if config_path[:-1] == [\"dialect\"]:\n             self._initialise_dialect(config_path[-1])\n \n    def process_raw_file_for_config(self, raw_str: str):\n         \"\"\"Process a full raw file for inline config and update self.\"\"\"\n         # Scan the raw file for config commands.\n         for raw_line in raw_str.splitlines():\n            if raw_line.startswith(\"-- sqlfluff\"):\n                 # Found a in-file config command\n                self.process_inline_config(raw_line)\n \n \n class ProgressBarConfiguration:\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n             raw_file = target_file.read()\n         # Scan the raw file for config commands.\n        file_config.process_raw_file_for_config(raw_file)\n         # Return the raw file and config\n         return raw_file, file_config, encoding\n \n         config = config or self.config\n \n         # Scan the raw file for config commands.\n        config.process_raw_file_for_config(in_str)\n         rendered = self.render_string(in_str, fname, config, encoding)\n         violations += rendered.templater_violations\n ",
    "output": "             fname = path\n \n     # Get file specific config\n    file_config.process_raw_file_for_config(raw_sql, fname)\n     rendered = lnt.render_string(raw_sql, fname, file_config, \"utf8\")\n \n     if rendered.templater_violations:\ndiff --git a/src/sqlfluff/core/config.py b/src/sqlfluff/core/config.py\n ConfigElemType = Tuple[Tuple[str, ...], Any]\n \n \nALLOWABLE_LAYOUT_CONFIG_KEYS = (\n    \"spacing_before\",\n    \"spacing_after\",\n    \"spacing_within\",\n    \"line_position\",\n    \"align_within\",\n    \"align_scope\",\n)\n\n\n @dataclass\n class _RemovedConfig:\n     old_path: Tuple[str, ...]\n     def _validate_configs(\n         configs: Iterable[ConfigElemType], file_path\n     ) -> List[ConfigElemType]:\n        \"\"\"Validate config elements.\n\n        We validate in two ways:\n        1. Are these config settings removed or deprecated.\n        2. Are these config elements in the layout section _valid_.\n        \"\"\"\n         config_map = {cfg.old_path: cfg for cfg in REMOVED_CONFIGS}\n         # Materialise the configs into a list to we can iterate twice.\n         new_configs = list(configs)\n         defined_keys = {k for k, _ in new_configs}\n         validated_configs = []\n         for k, v in new_configs:\n            # First validate against the removed option list.\n             if k in config_map.keys():\n                 formatted_key = \":\".join(k)\n                 removed_option = config_map[k]\n                 else:\n                     # Raise an error.\n                     raise SQLFluffUserError(\n                        f\"Config file {file_path!r} set an outdated config \"\n                         f\"value {formatted_key}.\\n\\n{removed_option.warning}\\n\\n\"\n                         \"See https://docs.sqlfluff.com/en/stable/configuration.html\"\n                         \" for more details.\"\n                     )\n \n            # Second validate any layout configs for validity.\n            # NOTE: For now we don't check that the \"type\" is a valid one\n            # to reference, or that the values are valid. For the values,\n            # these are likely to be rejected by the layout routines at\n            # runtime. The last risk area is validating that the type is\n            # a valid one.\n            if k and k[0] == \"layout\":\n                # Check for:\n                # - Key length\n                # - Key values\n                if (\n                    # Key length must be 4\n                    (len(k) != 4)\n                    # Second value must (currently) be \"type\"\n                    or (k[1] != \"type\")\n                    # Last key value must be one of the allowable options.\n                    or (k[3] not in ALLOWABLE_LAYOUT_CONFIG_KEYS)\n                ):\n                    raise SQLFluffUserError(\n                        f\"Config file {file_path!r} set an invalid `layout` option \"\n                        f\"value {':'.join(k)}.\\n\"\n                        \"See https://docs.sqlfluff.com/en/stable/layout.html\"\n                        \"#configuring-layout for more details.\"\n                    )\n\n             validated_configs.append((k, v))\n         return validated_configs\n \n                 for idnt, key, val in self.iter_vals(cfg=cfg[k]):\n                     yield (idnt + 1, key, val)\n \n    def process_inline_config(self, config_line: str, fname: str):\n         \"\"\"Process an inline config command and update self.\"\"\"\n         # Strip preceding comment marks\n         if config_line.startswith(\"--\"):\n         config_line = config_line[9:].strip()\n         # Divide on colons\n         config_path = [elem.strip() for elem in config_line.split(\":\")]\n        config_val = (tuple(config_path[:-1]), config_path[-1])\n        # Validate the value\n        ConfigLoader._validate_configs([config_val], fname)\n         # Set the value\n        self.set_value(*config_val)\n         # If the config is for dialect, initialise the dialect\n         if config_path[:-1] == [\"dialect\"]:\n             self._initialise_dialect(config_path[-1])\n \n    def process_raw_file_for_config(self, raw_str: str, fname: str):\n         \"\"\"Process a full raw file for inline config and update self.\"\"\"\n         # Scan the raw file for config commands.\n         for raw_line in raw_str.splitlines():\n            # With or without a space.\n            if raw_line.startswith((\"-- sqlfluff\", \"--sqlfluff\")):\n                 # Found a in-file config command\n                self.process_inline_config(raw_line, fname)\n \n \n class ProgressBarConfiguration:\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n             raw_file = target_file.read()\n         # Scan the raw file for config commands.\n        file_config.process_raw_file_for_config(raw_file, fname)\n         # Return the raw file and config\n         return raw_file, file_config, encoding\n \n         config = config or self.config\n \n         # Scan the raw file for config commands.\n        config.process_raw_file_for_config(in_str, fname)\n         rendered = self.render_string(in_str, fname, config, encoding)\n         violations += rendered.templater_violations\n "
  },
  {
    "instruction": "BigQuery: Accessing `STRUCT` elements evades triggering L027\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nAccessing unreferenced `STRUCT` elements using BigQuery dot notation in a multi table query does not trigger L027.\n\n### Expected Behaviour\n\nL027 gets triggered.\n\n### Observed Behaviour\n\nL027 does not get triggered.\n\n### How to reproduce\n\n```sql\r\nSELECT\r\n    t1.col1,\r\n    t2.col2,\r\n    events.id\r\nFROM t_table1 AS t1\r\nLEFT JOIN t_table2 AS t2\r\n    ON TRUE\r\n```\n\n### Dialect\n\nBigQUery\n\n### Version\n\n`0.11.2` using online.sqlfluff.com\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "                     )\n                 )\n \n         return violation_buff or None",
    "output": "                     )\n                 )\n \n            all_table_aliases = [t.ref_str for t in table_aliases] + standalone_aliases\n\n            # For qualified references, we want to check that the alias is actually\n            # valid\n            if (\n                this_ref_type == \"qualified\"\n                and list(r.iter_raw_references())[0].part not in all_table_aliases\n            ):\n                violation_buff.append(\n                    LintResult(\n                        anchor=r,\n                        description=f\"Qualified reference {r.raw!r} not found in \"\n                        f\"available tables/view aliases {all_table_aliases} in select \"\n                        \"with more than one referenced table/view.\",\n                    )\n                )\n\n         return violation_buff or None"
  },
  {
    "instruction": "Extra space when first field moved to new line in a WITH statement\nNote, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.\r\n\r\nGiven the following SQL:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Expected Behaviour\r\n\r\nafter running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n        my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Observed Behaviour\r\n\r\nafter running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n         my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\nNoted above. Create a file with the initial SQL and fun `sqfluff fix` on it.\r\n\r\n## Dialect\r\n\r\nRunning with default config.\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\nsqlfluff, version 0.7.0\r\nPython 3.7.5\r\n\r\n## Configuration\r\n\r\nDefault config.\r\n\n",
    "input": "                 # This is to avoid indents\n                 if not prev_newline:\n                     prev_whitespace = seg\n                prev_newline = False\n             elif seg.is_type(\"comment\"):\n                 prev_newline = False\n                 prev_whitespace = None",
    "output": "                 # This is to avoid indents\n                 if not prev_newline:\n                     prev_whitespace = seg\n                # We won't set prev_newline to False, just for whitespace\n                # in case there's multiple indents, inserted by other rule\n                # fixes (see #1713)\n             elif seg.is_type(\"comment\"):\n                 prev_newline = False\n                 prev_whitespace = None"
  },
  {
    "instruction": "dbt & JinjaTracer results in passing invalid query to database (was: DBT Call statement() block causes invalid query generated)\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen using the call statement() to run a query during compile time, the query generated is garbled causing the following sql error:\r\n```\r\n{% call statement('variables', fetch_result=true) %}\r\n\r\nselect 1 as test;\r\n\r\n{% endcall %}\r\n\r\n{% set test = load_result('variables')['table'].columns.TEST.values()[0] %}\r\n```\r\n\r\nThis results in the following error:\r\n\r\ndbt.exceptions.DatabaseException: Database Error\r\n  001003 (42000): SQL compilation error:\r\n  syntax error line 1 at position 0 unexpected '0'.\r\n\r\nThe query ran looks like this when looking at the query runner history in snowflake:\r\n\r\n```\r\n\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a_0\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a_8\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a\u263a_0\r\n```\r\n\r\nWhereas it should show:\r\n```\r\nselect 1 as test;\r\n```\n\n### Expected Behaviour\n\nExpected that the query runs properly.\n\n### Observed Behaviour\n\n```\r\n=== [dbt templater] Compiling dbt project...\r\n=== [dbt templater] Project Compiled.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 219, in exception_handler\r\n    yield\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 70, in add_query\r\n    cursor.execute(sql, bindings)\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/cursor.py\", line 794, in execute\r\n    Error.errorhandler_wrapper(self.connection, self, error_class, errvalue)\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 273, in errorhandler_wrapper\r\n    handed_over = Error.hand_to_other_handler(\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 328, in hand_to_other_handler\r\n    cursor.errorhandler(connection, cursor, error_class, error_value)\r\n  File \"/usr/local/lib/python3.9/site-packages/snowflake/connector/errors.py\", line 207, in default_errorhandler\r\n    raise error_class(\r\nsnowflake.connector.errors.ProgrammingError: 001003 (42000): SQL compilation error:\r\nsyntax error line 1 at position 0 unexpected '0'.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/sqlfluff\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/cli/commands.py\", line 1008, in parse\r\n    parsed_strings = list(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 1171, in parse_path\r\n    yield self.parse_string(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 835, in parse_string\r\n    rendered = self.render_string(in_str, fname, config, encoding)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/linter/linter.py\", line 784, in render_string\r\n    templated_file, templater_violations = self.templater.process(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/base.py\", line 47, in _wrapped\r\n    return func(self, in_str=in_str, fname=fname, config=config, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff_templater_dbt/templater.py\", line 331, in process\r\n    processed_result = self._unsafe_process(fname_absolute_path, in_str, config)\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff_templater_dbt/templater.py\", line 552, in _unsafe_process\r\n    raw_sliced, sliced_file, templated_sql = self.slice_file(\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/jinja.py\", line 462, in slice_file\r\n    trace = tracer.trace(append_to_templated=kwargs.pop(\"append_to_templated\", \"\"))\r\n  File \"/usr/local/lib/python3.9/site-packages/sqlfluff/core/templaters/slicers/tracer.py\", line 77, in trace\r\n    trace_template_output = trace_template.render()\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 1090, in render\r\n    self.environment.handle_exception()\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception\r\n    reraise(*rewrite_traceback_stack(source=source))\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"<template>\", line 16, in top-level template code\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/sandbox.py\", line 462, in call\r\n    return __context.call(__obj, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/clients/jinja.py\", line 321, in __call__\r\n    return self.call_macro(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/clients/jinja.py\", line 248, in call_macro\r\n    return macro(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/runtime.py\", line 679, in _invoke\r\n    rv = self._func(*arguments)\r\n  File \"<template>\", line 10, in template\r\n  File \"/usr/local/lib/python3.9/site-packages/jinja2/sandbox.py\", line 462, in call\r\n    return __context.call(__obj, *args, **kwargs)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/base/impl.py\", line 235, in execute\r\n    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 122, in execute\r\n    _, cursor = self.add_query(sql, auto_begin)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 458, in add_query\r\n    connection, cursor = super().add_query(\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/sql/connections.py\", line 78, in add_query\r\n    return connection, cursor\r\n  File \"/usr/local/Cellar/python@3.9/3.9.13_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 137, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.9/site-packages/dbt/adapters/snowflake/connections.py\", line 238, in exception_handler\r\n    raise DatabaseException(msg)\r\ndbt.exceptions.DatabaseException: Database Error\r\n  001003 (42000): SQL compilation error:\r\n  syntax error line 1 at position 0 unexpected '0'.\r\n```\n\n### How to reproduce\n\nUse the statement() block described here:\r\nhttps://docs.getdbt.com/reference/dbt-jinja-functions/statement-blocks\r\n\r\n\n\n### Dialect\n\nSnowflake\n\n### Version\n\n1.2.0 with the dbt formatter\n\n### Configuration\n\n```\r\n[sqlfluff]\r\n# verbose is an integer (0-2) indicating the level of log output\r\nverbose = 2\r\n# Turn off color formatting of output\r\nnocolor = False\r\n# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\r\n# Or run 'sqlfluff dialects'\r\ndialect = snowflake\r\n# One of [raw|jinja|python|placeholder]\r\ntemplater = dbt\r\n# Comma separated list of rules to check, default to all\r\nrules = all\r\n# Comma separated list of rules to exclude, or None\r\nexclude_rules = None\r\n# The depth to recursively parse to (0 for unlimited)\r\nrecurse = 0\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\n# Number of passes to run before admitting defeat\r\nrunaway_limit = 10\r\n# Ignore errors by category (one or more of the following, separated by commas: lexing,linting,parsing,templating)\r\nignore = None\r\n# Ignore linting errors found within sections of code coming directly from\r\n# templated code (e.g. from within Jinja curly braces. Note that it does not\r\n# ignore errors from literal code found within template loops.\r\nignore_templated_areas = True\r\n# can either be autodetect or a valid encoding e.g. utf-8, utf-8-sig\r\nencoding = autodetect\r\n# Ignore inline overrides (e.g. to test if still required)\r\ndisable_noqa = False\r\n# Comma separated list of file extensions to lint\r\n# NB: This config will only apply in the root folder\r\nsql_file_exts = .sql,.sql.j2,.dml,.ddl\r\n# Allow fix to run on files, even if they contain parsing errors\r\n# Note altering this is NOT RECOMMENDED as can corrupt SQL\r\nfix_even_unparsable = False\r\n# Very large files can make the parser effectively hang.\r\n# This limit skips files over a certain character length\r\n# and warns the user what has happened.\r\n# Set this to 0 to disable.\r\nlarge_file_skip_char_limit = 20000\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindented_joins = False\r\nindented_ctes = False\r\nindented_using_on = True\r\nindented_on_contents = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\nload_macros_from_path = macros/\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n[sqlfluff:templater:dbt]\r\nproject_dir = ./\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 120\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# Some rules have their own specific config\r\n[sqlfluff:rules:L003]\r\nhanging_indents = True\r\n\r\n[sqlfluff:rules:L007]\r\noperator_new_lines = after\r\n\r\n[sqlfluff:rules:L010]\r\n# Keywords\r\ncapitalisation_policy = lower\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L011]\r\n# Aliasing preference for tables\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L012]\r\n# Aliasing preference for columns\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:L014]\r\n# Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L016]\r\n# Line length\r\nignore_comment_lines = False\r\nignore_comment_clauses = False\r\n\r\n[sqlfluff:rules:L027]\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L026]\r\n# References must be in FROM clause\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L028]\r\n# References must be consistently used\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]\r\n# Keywords should not be used as identifiers.\r\nunquoted_identifiers_policy = aliases\r\nquoted_identifiers_policy = none\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L030]\r\n# Function names\r\nextended_capitalisation_policy = lower\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L031]\r\n# Avoid table aliases in from clauses and join conditions.\r\n# Disabled for some dialects (e.g. bigquery)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L036]\r\nwildcard_policy = single\r\n\r\n[sqlfluff:rules:L038]\r\n# Trailing commas\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:L040]\r\n# Null & Boolean Literals\r\ncapitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses\r\nforbid_subquery_in = join\r\n\r\n[sqlfluff:rules:L047]\r\n# Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n[sqlfluff:rules:L051]\r\n# Fully qualify JOIN clause\r\nfully_qualify_join_types = inner\r\n\r\n[sqlfluff:rules:L052]\r\n# Semi-colon formatting approach\r\nmultiline_newline = False\r\nrequire_final_semicolon = False\r\n\r\n[sqlfluff:rules:L054]\r\n# GROUP BY/ORDER BY column references\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:L057]\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\nquoted_identifiers_policy = all\r\nallow_space_in_identifier = False\r\nadditional_allowed_characters = \"\"\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L059]\r\n# Policy on quoted and unquoted identifiers\r\nprefer_quoted_identifiers = False\r\nignore_words = None\r\nignore_words_regex = None\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L062]\r\n# Comma separated list of blocked words that should not be used\r\nblocked_words = None\r\nblocked_regex = None\r\n\r\n[sqlfluff:rules:L063]\r\n# Data Types\r\nextended_capitalisation_policy = consistent\r\n# Comma separated list of words to ignore for this rule\r\nignore_words = None\r\nignore_words_regex = None\r\n\r\n[sqlfluff:rules:L064]\r\n# Consistent usage of preferred quotes for quoted literals\r\npreferred_quoted_literal_style = consistent\r\n# Disabled for dialects that do not support single and double quotes for quoted literals (e.g. Postgres)\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L066]\r\nmin_alias_length = None\r\nmax_alias_length = None\r\n```\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": " \n         # Internal bookkeeping\n         self.slice_id: int = 0\n        self.inside_set_or_macro: bool = False  # {% set %} or {% macro %}\n         self.inside_block = False  # {% block %}\n         self.stack: List[int] = []\n         self.idx_raw: int = 0\n             unique_alternate_id, alternate_code, inside_block=self.inside_block\n         )\n \n    def update_inside_set_or_macro_or_block(\n        self, block_type: str, trimmed_parts: List[str]\n    ) -> None:\n        \"\"\"Based on block tag, update whether we're in a set/macro section.\"\"\"\n         if block_type == \"block_start\" and trimmed_parts[0] in (\n             \"block\",\n             \"macro\",\n             \"set\",\n         ):\n                     if trimmed_parts[0] == \"block\":\n                         self.inside_block = True\n                     else:\n                        self.inside_set_or_macro = True\n                 else:\n                     raise  # pragma: no cover\n         elif block_type == \"block_end\":\n            if trimmed_parts[0] in (\"endmacro\", \"endset\"):\n                # Exiting a set or macro.\n                self.inside_set_or_macro = False\n             elif trimmed_parts[0] == \"endblock\":\n                 # Exiting a {% block %} block.\n                 self.inside_block = False\n \n     def make_raw_slice_info(\n         self,\n         inside_block: bool = False,\n     ) -> RawSliceInfo:\n         \"\"\"Create RawSliceInfo as given, or \"empty\" if in set/macro block.\"\"\"\n        if not self.inside_set_or_macro:\n             return RawSliceInfo(unique_alternate_id, alternate_code, [], inside_block)\n         else:\n             return RawSliceInfo(None, None, [], False)\n             # raw_end and raw_begin behave a little differently in\n             # that the whole tag shows up in one go rather than getting\n             # parts of the tag at a time.\n             if elem_type.endswith(\"_end\") or elem_type == \"raw_begin\":\n                 block_type = self.block_types[elem_type]\n                 block_subtype = None\n                         raw_slice_info = self.track_templated(\n                             m_open, m_close, tag_contents\n                         )\n                self.update_inside_set_or_macro_or_block(block_type, tag_contents)\n                 m_strip_right = regex.search(\n                     r\"\\s+$\", raw, regex.MULTILINE | regex.DOTALL\n                 )\n                     slice_idx = len(self.raw_sliced) - 1\n                     self.idx_raw += len(str_buff)\n                 if block_type.startswith(\"block\"):\n                     self.track_block_end(block_type, tag_contents[0])\n                     self.update_next_slice_indices(\n                         slice_idx, block_type, tag_contents[0]\n         )\n         return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n \n     def track_literal(self, raw: str, block_idx: int) -> None:\n         \"\"\"Set up tracking for a Jinja literal.\"\"\"\n         self.raw_sliced.append(\n             trimmed_parts = trimmed_content.split()\n         return trimmed_parts\n \n     def track_block_end(self, block_type: str, tag_name: str) -> None:\n         \"\"\"On ending a 'for' or 'if' block, set up tracking.\"\"\"\n         if block_type == \"block_end\" and tag_name in (\n             \"endfor\",\n             \"endif\",\n         ):\n            if not self.inside_set_or_macro:\n                 # Record potential forward jump over this block.\n                 self.raw_slice_info[\n                     self.raw_sliced[self.stack[-1]]",
    "output": " \n         # Internal bookkeeping\n         self.slice_id: int = 0\n        # {% set %} or {% macro %} or {% call %}\n        self.inside_set_macro_or_call: bool = False\n         self.inside_block = False  # {% block %}\n         self.stack: List[int] = []\n         self.idx_raw: int = 0\n             unique_alternate_id, alternate_code, inside_block=self.inside_block\n         )\n \n    def update_inside_set_call_macro_or_block(\n        self,\n        block_type: str,\n        trimmed_parts: List[str],\n        m_open: Optional[regex.Match],\n        m_close: Optional[regex.Match],\n        tag_contents: List[str],\n    ) -> Optional[RawSliceInfo]:\n        \"\"\"Based on block tag, update whether in a set/call/macro/block section.\"\"\"\n         if block_type == \"block_start\" and trimmed_parts[0] in (\n             \"block\",\n            \"call\",\n             \"macro\",\n             \"set\",\n         ):\n                     if trimmed_parts[0] == \"block\":\n                         self.inside_block = True\n                     else:\n                        result = None\n                        if trimmed_parts[0] == \"call\":\n                            assert m_open and m_close\n                            result = self.track_call(m_open, m_close, tag_contents)\n                        self.inside_set_macro_or_call = True\n                        return result\n                 else:\n                     raise  # pragma: no cover\n         elif block_type == \"block_end\":\n            if trimmed_parts[0] in (\"endcall\", \"endmacro\", \"endset\"):\n                # Exiting a set or macro or block.\n                self.inside_set_macro_or_call = False\n             elif trimmed_parts[0] == \"endblock\":\n                 # Exiting a {% block %} block.\n                 self.inside_block = False\n        return None\n \n     def make_raw_slice_info(\n         self,\n         inside_block: bool = False,\n     ) -> RawSliceInfo:\n         \"\"\"Create RawSliceInfo as given, or \"empty\" if in set/macro block.\"\"\"\n        if not self.inside_set_macro_or_call:\n             return RawSliceInfo(unique_alternate_id, alternate_code, [], inside_block)\n         else:\n             return RawSliceInfo(None, None, [], False)\n             # raw_end and raw_begin behave a little differently in\n             # that the whole tag shows up in one go rather than getting\n             # parts of the tag at a time.\n            m_open = None\n            m_close = None\n             if elem_type.endswith(\"_end\") or elem_type == \"raw_begin\":\n                 block_type = self.block_types[elem_type]\n                 block_subtype = None\n                         raw_slice_info = self.track_templated(\n                             m_open, m_close, tag_contents\n                         )\n                raw_slice_info_temp = self.update_inside_set_call_macro_or_block(\n                    block_type, tag_contents, m_open, m_close, tag_contents\n                )\n                if raw_slice_info_temp:\n                    raw_slice_info = raw_slice_info_temp\n                 m_strip_right = regex.search(\n                     r\"\\s+$\", raw, regex.MULTILINE | regex.DOTALL\n                 )\n                     slice_idx = len(self.raw_sliced) - 1\n                     self.idx_raw += len(str_buff)\n                 if block_type.startswith(\"block\"):\n                    self.track_block_start(block_type, tag_contents[0])\n                     self.track_block_end(block_type, tag_contents[0])\n                     self.update_next_slice_indices(\n                         slice_idx, block_type, tag_contents[0]\n         )\n         return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n \n    def track_call(\n        self, m_open: regex.Match, m_close: regex.Match, tag_contents: List[str]\n    ):\n        \"\"\"Set up tracking for \"{% call ... %}\".\"\"\"\n        unique_alternate_id = self.next_slice_id()\n        open_ = m_open.group(1)\n        close_ = m_close.group(1)\n        # Here, we still need to evaluate the original tag contents, e.g. in\n        # case it has intentional side effects, but also return a slice ID\n        # for tracking.\n        alternate_code = (\n            f\"\\0{unique_alternate_id} {open_} \" f\"{''.join(tag_contents)} {close_}\"\n        )\n        return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n\n     def track_literal(self, raw: str, block_idx: int) -> None:\n         \"\"\"Set up tracking for a Jinja literal.\"\"\"\n         self.raw_sliced.append(\n             trimmed_parts = trimmed_content.split()\n         return trimmed_parts\n \n    def track_block_start(self, block_type: str, tag_name: str) -> None:\n        \"\"\"On starting a 'call' block, set slice_type to \"templated\".\"\"\"\n        if block_type == \"block_start\" and tag_name == \"call\":\n            # Replace RawSliceInfo for this slice with one that has block_type\n            # \"templated\".\n            old_raw_file_slice = self.raw_sliced[-1]\n            self.raw_sliced[-1] = old_raw_file_slice._replace(slice_type=\"templated\")\n\n            # Move existing raw_slice_info entry since it's keyed by RawFileSlice.\n            self.raw_slice_info[self.raw_sliced[-1]] = self.raw_slice_info[\n                old_raw_file_slice\n            ]\n            del self.raw_slice_info[old_raw_file_slice]\n\n     def track_block_end(self, block_type: str, tag_name: str) -> None:\n         \"\"\"On ending a 'for' or 'if' block, set up tracking.\"\"\"\n         if block_type == \"block_end\" and tag_name in (\n             \"endfor\",\n             \"endif\",\n         ):\n            if not self.inside_set_macro_or_call:\n                 # Record potential forward jump over this block.\n                 self.raw_slice_info[\n                     self.raw_sliced[self.stack[-1]]"
  },
  {
    "instruction": "add ability to render the compiled sql\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nIt would be nice to see the compiled sql in which any templates are rendered. I would be happy to work on this but it may be a struggle and would need some guidance.\n\n### Use case\n\n It would help debug linting errors around jinja templates.\r\nIt would also make it easier to copy and use the query in the bigquery ui, for example. We process our queries through Airflow so currently I can start a dag run and look at the rendered template to get this effect. That's not very efficient though :)\r\n\n\n### Dialect\n\nWe use bigquery but this could apply to all dialects.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "         sys.exit(EXIT_SUCCESS)\n \n \n # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n # simplifies the use of cProfile, e.g.:\n # python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     # These are the building blocks of the linting process.\n \n     @staticmethod\n    def _load_raw_file_and_config(\n         fname: str, root_config: FluffConfig\n     ) -> Tuple[str, FluffConfig, str]:\n         \"\"\"Load a raw file and the associated config.\"\"\"\n     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n         \"\"\"Load and render a file with relevant config.\"\"\"\n         # Load the raw file.\n        raw_file, config, encoding = self._load_raw_file_and_config(fname, root_config)\n         # Render the file\n         return self.render_string(raw_file, fname, config, encoding)\n \n                 self.formatter.dispatch_path(path)\n             # Load the file with the config and yield the result.\n             try:\n                raw_file, config, encoding = self._load_raw_file_and_config(\n                     fname, self.config\n                 )\n             except SQLFluffSkipFile as s:",
    "output": "         sys.exit(EXIT_SUCCESS)\n \n \n@cli.command()\n@common_options\n@core_options\n@click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\ndef render(\n    path: str,\n    bench: bool,\n    logger: Optional[logging.Logger] = None,\n    extra_config_path: Optional[str] = None,\n    ignore_local_config: bool = False,\n    **kwargs,\n) -> None:\n    \"\"\"Render SQL files and just spit out the result.\n\n    PATH is the path to a sql file. This should be either a single file\n    file ('path/to/file.sql') or a single ('-') character to indicate reading\n    from *stdin*.\n    \"\"\"\n    c = get_config(\n        extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n    )\n    # We don't want anything else to be logged if we want json or yaml output\n    # unless we're writing to a file.\n    output_stream = make_output_stream(c, None, None)\n    lnt, formatter = get_linter_and_formatter(c, output_stream)\n    verbose = c.get(\"verbose\")\n\n    progress_bar_configuration.disable_progress_bar = True\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(\n        verbosity=verbose,\n        formatter=formatter,\n        logger=logger,\n        stderr_output=False,\n    )\n\n    # handle stdin if specified via lone '-'\n    with PathAndUserErrorHandler(formatter, path):\n        if \"-\" == path:\n            raw_sql = sys.stdin.read()\n            fname = \"stdin\"\n            file_config = lnt.config\n        else:\n            raw_sql, file_config, _ = lnt.load_raw_file_and_config(path, lnt.config)\n            fname = path\n\n    # Get file specific config\n    file_config.process_raw_file_for_config(raw_sql)\n    rendered = lnt.render_string(raw_sql, fname, file_config, \"utf8\")\n\n    if rendered.templater_violations:\n        for v in rendered.templater_violations:\n            click.echo(formatter.format_violation(v))\n        sys.exit(EXIT_FAIL)\n    else:\n        click.echo(rendered.templated_file.templated_str)\n        sys.exit(EXIT_SUCCESS)\n\n\n # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n # simplifies the use of cProfile, e.g.:\n # python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     # These are the building blocks of the linting process.\n \n     @staticmethod\n    def load_raw_file_and_config(\n         fname: str, root_config: FluffConfig\n     ) -> Tuple[str, FluffConfig, str]:\n         \"\"\"Load a raw file and the associated config.\"\"\"\n     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n         \"\"\"Load and render a file with relevant config.\"\"\"\n         # Load the raw file.\n        raw_file, config, encoding = self.load_raw_file_and_config(fname, root_config)\n         # Render the file\n         return self.render_string(raw_file, fname, config, encoding)\n \n                 self.formatter.dispatch_path(path)\n             # Load the file with the config and yield the result.\n             try:\n                raw_file, config, encoding = self.load_raw_file_and_config(\n                     fname, self.config\n                 )\n             except SQLFluffSkipFile as s:"
  },
  {
    "instruction": "noqa is ignored for jinja templated lines\n## Expected Behaviour\r\nLine with `noqa: TMP` should be ignored (despite of evaluation error)\r\n\r\n## Observed Behaviour\r\ntrying to lint airflow sql-template for AWS Athena query\r\nsetting up inline `-- noqa` or `--noqa: TMP` for jinja templated line not silenting templating error (typecasting error due to unable to pass datetime object while linting into template context):\r\n```\r\n== [transform/airflow/dags/queries/sfmc/player_balance.sql] FAIL\r\nL:   0 | P:   0 |  TMP | Unrecoverable failure in Jinja templating: unsupported operand type(s) for -: 'int' and 'datetime.timedelta'. Have you configured your variables?\r\n                       | https://docs.sqlfluff.com/en/latest/configuration.html\r\n```\r\n\r\n## Steps to Reproduce\r\ntemplated file:\r\n```sql\r\nselect *, row_number() over (partition by player_id order by balance_change_date desc)  as rnk\r\nfrom raw\r\nwhere\r\n    balance_change_date >= cast(from_iso8601_timestamp('{{ execution_date - macros.timedelta(hours=2, minutes=10) }}') as timestamp)  and  --noqa: TMP\r\n    balance_change_date < cast(from_iso8601_timestamp('{{ next_execution_date - macros.timedelta(minutes=10) }}') as timestamp) --noqa: TMP\r\n```\r\nrun:\r\n```bash\r\nsqlfluff lint transform/airflow/dags/queries/sfmc/player_balance.sql\r\n```\r\n\r\n## Dialect\r\npostgres (used for AWS Athena)\r\n\r\n## Version\r\ndatalake % sqlfluff --version\r\nsqlfluff, version 0.8.1\r\ndatalake % python3 --version\r\nPython 3.9.8\r\n\r\n## Configuration\r\n```ini\r\n# tox.ini\r\n[sqlfluff]\r\ntemplater = jinja\r\noutput_line_length = 180\r\nexclude_rules = L011,L012,L022,L031,L034\r\ndialect = postgres\r\n\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\n\r\n[sqlfluff:templater:jinja]\r\nlibrary_path = operation/deploy/lint\r\napply_dbt_builtins = false\r\n\r\n[sqlfluff:templater:jinja:context]\r\nds = 2021-11-11\r\nds_nodash = 20211111\r\nstart_date = 2021-11-11\r\nend_date = 2021-11-11\r\ninterval = 1\r\n# passed as int due to inabliity to pass datetime obkject \r\ndata_interval_start = 1636588800\r\ndata_interval_end = 1636588800\r\n```\r\n\r\n```python\r\n# operation/deploy/lint/macro.py\r\nfrom datetime import datetime, timedelta  # noqa: F401\r\n\r\nimport dateutil  # noqa: F401\r\n```\n",
    "input": "     time_dict: Dict[str, float]\n     fname: str\n     encoding: str\n \n \n class ParsedString(NamedTuple):\n     templated_file: TemplatedFile\n     config: FluffConfig\n     fname: str\n \n \n class EnrichedFixPatch(NamedTuple):\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     SQLParseError,\n     SQLTemplaterSkipFile,\n )\nfrom sqlfluff.core.parser import Lexer, Parser\n from sqlfluff.core.file_helpers import get_encoding\n from sqlfluff.core.templaters import TemplatedFile\n from sqlfluff.core.rules import get_ruleset\n         cls,\n         rendered: RenderedFile,\n         recurse: bool = True,\n    ):\n         \"\"\"Parse a rendered file.\"\"\"\n         t0 = time.monotonic()\n         violations = cast(List[SQLBaseError], rendered.templater_violations)\n             rendered.templated_file,\n             rendered.config,\n             rendered.fname,\n         )\n \n     @classmethod\n         return result\n \n     @classmethod\n    def extract_ignore_mask(\n         cls,\n         tree: BaseSegment,\n         rule_codes: List[str],\n             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n         return ignore_buff, violations\n \n     @classmethod\n     def lint_fix_parsed(\n         cls,\n         # Look for comment segments which might indicate lines to ignore.\n         if not config.get(\"disable_noqa\"):\n             rule_codes = [r.code for r in rule_set]\n            ignore_buff, ivs = cls.extract_ignore_mask(tree, rule_codes)\n             all_linting_errors += ivs\n         else:\n             ignore_buff = []\n             # If no parsed tree, set to None\n             tree = None\n             ignore_buff = []\n \n         # We process the ignore config here if appropriate\n         for violation in violations:\n         time_dict = {\"templating\": time.monotonic() - t0}\n \n         return RenderedFile(\n            templated_file, templater_violations, config, time_dict, fname, encoding\n         )\n \n     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\ndiff --git a/src/sqlfluff/core/templaters/jinja.py b/src/sqlfluff/core/templaters/jinja.py\n                         \"Unrecoverable failure in Jinja templating: {}. Have you \"\n                         \"configured your variables? \"\n                         \"https://docs.sqlfluff.com/en/latest/configuration.html\"\n                    ).format(err)\n                 )\n             )\n             return None, violations",
    "output": "     time_dict: Dict[str, float]\n     fname: str\n     encoding: str\n    source_str: str\n \n \n class ParsedString(NamedTuple):\n     templated_file: TemplatedFile\n     config: FluffConfig\n     fname: str\n    source_str: str\n \n \n class EnrichedFixPatch(NamedTuple):\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     SQLParseError,\n     SQLTemplaterSkipFile,\n )\nfrom sqlfluff.core.parser import Lexer, Parser, RegexLexer\n from sqlfluff.core.file_helpers import get_encoding\n from sqlfluff.core.templaters import TemplatedFile\n from sqlfluff.core.rules import get_ruleset\n         cls,\n         rendered: RenderedFile,\n         recurse: bool = True,\n    ) -> ParsedString:\n         \"\"\"Parse a rendered file.\"\"\"\n         t0 = time.monotonic()\n         violations = cast(List[SQLBaseError], rendered.templater_violations)\n             rendered.templated_file,\n             rendered.config,\n             rendered.fname,\n            rendered.source_str,\n         )\n \n     @classmethod\n         return result\n \n     @classmethod\n    def extract_ignore_mask_tree(\n         cls,\n         tree: BaseSegment,\n         rule_codes: List[str],\n             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n         return ignore_buff, violations\n \n    @classmethod\n    def extract_ignore_mask_source(\n        cls,\n        source: str,\n        inline_comment_regex: RegexLexer,\n        rule_codes: List[str],\n    ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n        \"\"\"Look for inline ignore comments and return NoQaDirectives.\n\n        Very similar to extract_ignore_mask_tree(), but can be run on raw source\n        (i.e. does not require the code to have parsed successfully).\n        \"\"\"\n        ignore_buff: List[NoQaDirective] = []\n        violations: List[SQLBaseError] = []\n        for idx, line in enumerate(source.split(\"\\n\")):\n            match = inline_comment_regex.search(line) if line else None\n            if match:\n                ignore_entry = cls.parse_noqa(\n                    line[match[0] : match[1]], idx + 1, rule_codes\n                )\n                if isinstance(ignore_entry, SQLParseError):\n                    violations.append(ignore_entry)  # pragma: no cover\n                elif ignore_entry:\n                    ignore_buff.append(ignore_entry)\n        if ignore_buff:\n            linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n        return ignore_buff, violations\n\n     @classmethod\n     def lint_fix_parsed(\n         cls,\n         # Look for comment segments which might indicate lines to ignore.\n         if not config.get(\"disable_noqa\"):\n             rule_codes = [r.code for r in rule_set]\n            ignore_buff, ivs = cls.extract_ignore_mask_tree(tree, rule_codes)\n             all_linting_errors += ivs\n         else:\n             ignore_buff = []\n             # If no parsed tree, set to None\n             tree = None\n             ignore_buff = []\n            if not parsed.config.get(\"disable_noqa\"):\n                # Templating and/or parsing have failed. Look for \"noqa\"\n                # comments (the normal path for identifying these comments\n                # requires access to the parse tree, and because of the failure,\n                # we don't have a parse tree).\n                ignore_buff, ignore_violations = cls.extract_ignore_mask_source(\n                    parsed.source_str,\n                    [\n                        lm\n                        for lm in parsed.config.get(\"dialect_obj\").lexer_matchers\n                        if lm.name == \"inline_comment\"\n                    ][0],\n                    [r.code for r in rule_set],\n                )\n                violations += ignore_violations\n \n         # We process the ignore config here if appropriate\n         for violation in violations:\n         time_dict = {\"templating\": time.monotonic() - t0}\n \n         return RenderedFile(\n            templated_file,\n            templater_violations,\n            config,\n            time_dict,\n            fname,\n            encoding,\n            in_str,\n         )\n \n     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\ndiff --git a/src/sqlfluff/core/templaters/jinja.py b/src/sqlfluff/core/templaters/jinja.py\n                         \"Unrecoverable failure in Jinja templating: {}. Have you \"\n                         \"configured your variables? \"\n                         \"https://docs.sqlfluff.com/en/latest/configuration.html\"\n                    ).format(err),\n                    # We don't have actual line number information, but specify\n                    # line 1 so users can ignore with \"noqa\" if they want. (The\n                    # default is line 0, which can't be ignored because it's not\n                    # a valid line number.)\n                    line_no=1,\n                    line_pos=1,\n                 )\n             )\n             return None, violations"
  },
  {
    "instruction": "Exception thrown when SELECT DISTINCT not on the same line\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nCheck a file containing this request:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nIt fails this way:\r\n\r\n```log\r\nCRITICAL   [RF01] Applying rule RF01 to 'file.sql' threw an Exception:  \r\nTraceback (most recent call last):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/rules/base.py\", line 864, in crawl\r\n    res = self._eval(context=context)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 107, in _eval\r\n    self._analyze_table_references(\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 152, in _analyze_table_references\r\n    if not self._should_ignore_reference(r, selectable):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 168, in _should_ignore_reference\r\n    ref_path = selectable.selectable.path_to(reference)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 1184, in path_to\r\n    elif not self.get_start_loc() <= midpoint.get_start_loc() <= self.get_end_loc():\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 877, in get_start_loc\r\n    assert self.pos_marker\r\nAssertionError\r\n== [file.sql] FAIL\r\nL:   1 | P:   1 | LT09 | Select targets should be on a new line unless there is\r\n                       | only one select target. [layout.select_targets]\r\nL:   1 | P:   1 | LT10 | 'SELECT' modifiers (e.g. 'DISTINCT') must be on the same\r\n                       | line as 'SELECT'. [layout.select_modifiers]\r\nL:   1 | P:   1 | RF01 | Unexpected exception: ;\r\nCould you open an issue at\r\n                       | https://github.com/sqlfluff/sqlfluff/issues ?\r\nYou can\r\n                       | ignore this exception for now, by adding '-- noqa: RF01'\r\n                       | at the end\r\nof line 1\r\n [references.from]\r\nL:   2 | P:   1 | LT02 | Line should not be indented. [layout.indent]\r\nL:   3 | P:  13 | LT12 | Files must end with a single trailing newline.\r\n                       | [layout.end_of_file]\r\nAll Finished!\r\n```\r\n\r\nChecking the following request does not throw an exception (move `DISTINCT` on same line than `SELECT`):\r\n\r\n```sql\r\nSELECT DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nAdditionally, I'd like to add that checking the first request on https://online.sqlfluff.com/fluffed leads to the same exception. But if you check this request:\r\n```sql\r\nSELECT \r\nDISTINCT\r\n`FIELD`\r\nFROM `TABLE`;\r\n```\r\nThen the website crashes.\r\n\r\n### Expected Behaviour\r\n\r\nI would expect not to have an exception.\r\n\r\n### Observed Behaviour\r\n\r\nAn exception was thrown whereas, I think, there is no reason to throw it.\r\n\r\n### How to reproduce\r\n\r\nCheck the following SQL:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\n### Dialect\r\n\r\nMySQL\r\n\r\n### Version\r\n\r\n2.3.2\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\n# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\r\ndialect = mysql\r\nencoding = utf-8\r\n# Exclude rule LT01/layout.spacing: it expects a space even after type of fields (i.e. \"INT (11)\")\r\n# Exclude rule ST05/structure.subquery: MySQL badly supports CTEs.\r\nexclude_rules = LT01, ST05\r\nignore = parsing\r\nmax_line_length = 120\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\ntemplater = raw\r\nverbose = 0\r\n\r\n[sqlfluff:layout:type:binary_operator]\r\nline_position = leading\r\n\r\n[sqlfluff:layout:type:comma]\r\nline_position = trailing\r\nspacing_before = touch\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindent_unit = space\r\nindented_joins = True\r\nindented_using_on = True\r\ntab_space_size = 4\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\nallow_scalar = True\r\nquoted_identifiers_policy = none\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:aliasing.column]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:aliasing.table]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:ambiguous.column_references]\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:capitalisation.functions]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.identifiers]\r\nextended_capitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.keywords]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.literals]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.types]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:convention.count_rows]\r\nprefer_count_0 = False\r\nprefer_count_1 = True\r\n\r\n[sqlfluff:rules:convention.select_trailing_comma]\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:convention.terminator]\r\nmultiline_newline = False\r\nrequire_final_semicolon = True\r\n\r\n[sqlfluff:rules:layout.long_lines]\r\nignore_comment_lines = True\r\n\r\n[sqlfluff:rules:references.keywords]\r\nignore_words = None\r\nquoted_identifiers_policy = none\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:convention.quoted_literals]\r\npreferred_quoted_literal_style = single_quotes\r\n\r\n[sqlfluff:rules:references.quoting]\r\nprefer_quoted_identifiers = True\r\n\r\n[sqlfluff:rules:references.special_chars]\r\nadditional_allowed_characters = \"\"\r\nallow_space_in_identifier = False\r\nquoted_identifiers_policy = all\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\nException thrown when SELECT DISTINCT not on the same line\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\n\r\n### What Happened\r\n\r\nCheck a file containing this request:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nIt fails this way:\r\n\r\n```log\r\nCRITICAL   [RF01] Applying rule RF01 to 'file.sql' threw an Exception:  \r\nTraceback (most recent call last):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/rules/base.py\", line 864, in crawl\r\n    res = self._eval(context=context)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 107, in _eval\r\n    self._analyze_table_references(\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 152, in _analyze_table_references\r\n    if not self._should_ignore_reference(r, selectable):\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/rules/references/RF01.py\", line 168, in _should_ignore_reference\r\n    ref_path = selectable.selectable.path_to(reference)\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 1184, in path_to\r\n    elif not self.get_start_loc() <= midpoint.get_start_loc() <= self.get_end_loc():\r\n  File \"/app/.venv/lib/python3.9/site-packages/sqlfluff/core/parser/segments/base.py\", line 877, in get_start_loc\r\n    assert self.pos_marker\r\nAssertionError\r\n== [file.sql] FAIL\r\nL:   1 | P:   1 | LT09 | Select targets should be on a new line unless there is\r\n                       | only one select target. [layout.select_targets]\r\nL:   1 | P:   1 | LT10 | 'SELECT' modifiers (e.g. 'DISTINCT') must be on the same\r\n                       | line as 'SELECT'. [layout.select_modifiers]\r\nL:   1 | P:   1 | RF01 | Unexpected exception: ;\r\nCould you open an issue at\r\n                       | https://github.com/sqlfluff/sqlfluff/issues ?\r\nYou can\r\n                       | ignore this exception for now, by adding '-- noqa: RF01'\r\n                       | at the end\r\nof line 1\r\n [references.from]\r\nL:   2 | P:   1 | LT02 | Line should not be indented. [layout.indent]\r\nL:   3 | P:  13 | LT12 | Files must end with a single trailing newline.\r\n                       | [layout.end_of_file]\r\nAll Finished!\r\n```\r\n\r\nChecking the following request does not throw an exception (move `DISTINCT` on same line than `SELECT`):\r\n\r\n```sql\r\nSELECT DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\nAdditionally, I'd like to add that checking the first request on https://online.sqlfluff.com/fluffed leads to the same exception. But if you check this request:\r\n```sql\r\nSELECT \r\nDISTINCT\r\n`FIELD`\r\nFROM `TABLE`;\r\n```\r\nThen the website crashes.\r\n\r\n### Expected Behaviour\r\n\r\nI would expect not to have an exception.\r\n\r\n### Observed Behaviour\r\n\r\nAn exception was thrown whereas, I think, there is no reason to throw it.\r\n\r\n### How to reproduce\r\n\r\nCheck the following SQL:\r\n\r\n```sql\r\nSELECT\r\n    DISTINCT `FIELD`\r\nFROM `TABLE`;\r\n```\r\n\r\n### Dialect\r\n\r\nMySQL\r\n\r\n### Version\r\n\r\n2.3.2\r\n\r\n### Configuration\r\n\r\n```\r\n[sqlfluff]\r\n# Supported dialects https://docs.sqlfluff.com/en/stable/dialects.html\r\ndialect = mysql\r\nencoding = utf-8\r\n# Exclude rule LT01/layout.spacing: it expects a space even after type of fields (i.e. \"INT (11)\")\r\n# Exclude rule ST05/structure.subquery: MySQL badly supports CTEs.\r\nexclude_rules = LT01, ST05\r\nignore = parsing\r\nmax_line_length = 120\r\n# Below controls SQLFluff output, see max_line_length for SQL output\r\noutput_line_length = 80\r\ntemplater = raw\r\nverbose = 0\r\n\r\n[sqlfluff:layout:type:binary_operator]\r\nline_position = leading\r\n\r\n[sqlfluff:layout:type:comma]\r\nline_position = trailing\r\nspacing_before = touch\r\n\r\n[sqlfluff:indentation]\r\n# See https://docs.sqlfluff.com/en/stable/indentation.html\r\nindent_unit = space\r\nindented_joins = True\r\nindented_using_on = True\r\ntab_space_size = 4\r\n\r\n# Some rules can be configured directly from the config common to other rules\r\n[sqlfluff:rules]\r\nallow_scalar = True\r\nquoted_identifiers_policy = none\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:aliasing.column]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:aliasing.table]\r\naliasing = explicit\r\n\r\n[sqlfluff:rules:ambiguous.column_references]\r\ngroup_by_and_order_by_style = consistent\r\n\r\n[sqlfluff:rules:capitalisation.functions]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.identifiers]\r\nextended_capitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.keywords]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.literals]\r\ncapitalisation_policy = upper\r\nignore_words = None\r\n\r\n[sqlfluff:rules:capitalisation.types]\r\nextended_capitalisation_policy = upper\r\n\r\n[sqlfluff:rules:convention.count_rows]\r\nprefer_count_0 = False\r\nprefer_count_1 = True\r\n\r\n[sqlfluff:rules:convention.select_trailing_comma]\r\nselect_clause_trailing_comma = forbid\r\n\r\n[sqlfluff:rules:convention.terminator]\r\nmultiline_newline = False\r\nrequire_final_semicolon = True\r\n\r\n[sqlfluff:rules:layout.long_lines]\r\nignore_comment_lines = True\r\n\r\n[sqlfluff:rules:references.keywords]\r\nignore_words = None\r\nquoted_identifiers_policy = none\r\nunquoted_identifiers_policy = all\r\n\r\n[sqlfluff:rules:convention.quoted_literals]\r\npreferred_quoted_literal_style = single_quotes\r\n\r\n[sqlfluff:rules:references.quoting]\r\nprefer_quoted_identifiers = True\r\n\r\n[sqlfluff:rules:references.special_chars]\r\nadditional_allowed_characters = \"\"\r\nallow_space_in_identifier = False\r\nquoted_identifiers_policy = all\r\n# Special characters in identifiers\r\nunquoted_identifiers_policy = all\r\n```\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": " import logging\n import weakref\n from collections import defaultdict\nfrom copy import copy\n from dataclasses import dataclass\n from io import StringIO\n from itertools import chain\n             )\n \n     def copy(\n        self, segments: Optional[Tuple[\"BaseSegment\", ...]] = None\n     ) -> \"BaseSegment\":\n         \"\"\"Copy the segment recursively, with appropriate copying of references.\n \n         Optionally provide child segments which have already been dealt\n         with to avoid another copy operation.\n         \"\"\"\n        new_seg = copy(self)\n         # Position markers are immutable, and it's important that we keep\n         # a reference to the same TemplatedFile, so keep the same position\n        # marker.\n        new_seg.pos_marker = self.pos_marker\n         # If segments were provided, use them.\n        if segments:\n            new_seg.segments = segments\n        # Otherwise copy them.\n        elif self.segments:\n            new_seg.segments = tuple(seg.copy() for seg in self.segments)\n        return new_seg\n \n     def as_record(self, **kwargs: bool) -> Optional[RecordSerialisedSegment]:\n         \"\"\"Return the segment as a structurally simplified record.\n                                 # of a create_before/create_after pair, also add\n                                 # this segment before the edit.\n                                 seg_buffer.append(seg)\n                                seg.set_parent(self)\n \n                             # We're doing a replacement (it could be a single\n                             # segment or an iterable)\n                             consumed_pos = False\n                             for s in f.edit:\n                                 seg_buffer.append(s)\n                                s.set_parent(self)\n                                 # If one of them has the same raw representation\n                                 # then the first that matches gets to take the\n                                 # original position marker.\n                                 # in the case of a creation before, also add this\n                                 # segment on the end\n                                 seg_buffer.append(seg)\n                                seg.set_parent(self)\n \n                         else:  # pragma: no cover\n                             raise ValueError(\n                             )\n                 else:\n                     seg_buffer.append(seg)\n                    seg.set_parent(self)\n             # Invalidate any caches\n             self.invalidate_caches()\n \n             # Pass through any additional kwargs\n             **{k: getattr(self, k) for k in self.additional_kwargs},\n         )\n         # Only validate if there's a match_grammar. Otherwise we may get\n         # strange results (for example with the BracketedSegment).\n         if requires_validate and (\ndiff --git a/src/sqlfluff/core/rules/base.py b/src/sqlfluff/core/rules/base.py\n             if root_segment\n             else None\n         )\n        assert path\n         for seg in path[::-1]:\n             # If the segment allows non code ends, then no problem.\n             # We're done. This is usually the outer file segment.",
    "output": " import logging\n import weakref\n from collections import defaultdict\n from dataclasses import dataclass\n from io import StringIO\n from itertools import chain\n             )\n \n     def copy(\n        self,\n        segments: Optional[Tuple[\"BaseSegment\", ...]] = None,\n        parent: Optional[\"BaseSegment\"] = None,\n     ) -> \"BaseSegment\":\n         \"\"\"Copy the segment recursively, with appropriate copying of references.\n \n         Optionally provide child segments which have already been dealt\n         with to avoid another copy operation.\n\n        NOTE: In the copy operation it's really important that we get\n        a clean segregation so that we can't go backward and mutate the\n        source object, but at the same time we should be mindful of what\n        _needs_ to be copied to avoid a deep copy where one isn't required.\n         \"\"\"\n        cls = self.__class__\n        new_segment = cls.__new__(cls)\n         # Position markers are immutable, and it's important that we keep\n         # a reference to the same TemplatedFile, so keep the same position\n        # marker. By updating from the source dict, we achieve that.\n        # By using the __dict__ object we also transfer the _cache_ too\n        # which is stored there by @cached_property.\n        new_segment.__dict__.update(self.__dict__)\n\n        # Reset the parent if provided.\n        if parent:\n            new_segment.set_parent(parent)\n\n        # If the segment doesn't have a segments property, we're done.\n        # NOTE: This is a proxy way of understanding whether it's a RawSegment\n        # of not. Typically will _have_ a `segments` attribute, but it's an\n        # empty tuple.\n        if not self.__dict__.get(\"segments\", None):\n            assert (\n                not segments\n            ), f\"Cannot provide `segments` argument to {cls.__name__} `.copy()`\\n\"\n         # If segments were provided, use them.\n        elif segments:\n            new_segment.segments = segments\n        # Otherwise we should handle recursive segment coping.\n        # We use the native .copy() method (this method!) appropriately\n        # so that the same logic is applied in recursion.\n        # We set the parent for children directly on the copy method\n        # to ensure those line up properly.\n        else:\n            new_segment.segments = tuple(\n                seg.copy(parent=new_segment) for seg in self.segments\n            )\n\n        return new_segment\n \n     def as_record(self, **kwargs: bool) -> Optional[RecordSerialisedSegment]:\n         \"\"\"Return the segment as a structurally simplified record.\n                                 # of a create_before/create_after pair, also add\n                                 # this segment before the edit.\n                                 seg_buffer.append(seg)\n \n                             # We're doing a replacement (it could be a single\n                             # segment or an iterable)\n                             consumed_pos = False\n                             for s in f.edit:\n                                 seg_buffer.append(s)\n                                 # If one of them has the same raw representation\n                                 # then the first that matches gets to take the\n                                 # original position marker.\n                                 # in the case of a creation before, also add this\n                                 # segment on the end\n                                 seg_buffer.append(seg)\n \n                         else:  # pragma: no cover\n                             raise ValueError(\n                             )\n                 else:\n                     seg_buffer.append(seg)\n\n             # Invalidate any caches\n             self.invalidate_caches()\n \n             # Pass through any additional kwargs\n             **{k: getattr(self, k) for k in self.additional_kwargs},\n         )\n        new_seg.set_as_parent(recurse=False)\n         # Only validate if there's a match_grammar. Otherwise we may get\n         # strange results (for example with the BracketedSegment).\n         if requires_validate and (\ndiff --git a/src/sqlfluff/core/rules/base.py b/src/sqlfluff/core/rules/base.py\n             if root_segment\n             else None\n         )\n        assert path, f\"No path found from {root_segment} to {segment}!\"\n         for seg in path[::-1]:\n             # If the segment allows non code ends, then no problem.\n             # We're done. This is usually the outer file segment."
  },
  {
    "instruction": "\"ValueError: Position Not Found\" with macro spanning entire file\n## Expected Behaviour\r\n\r\n`sqlfluff parse` should probably not fail with an exception and stack trace.\r\n\r\n## Observed Behaviour\r\n\r\n`sqlfluff parse` throws an exception, given an input file which is entirely spanned by a Jinja macro.\r\n\r\n## Steps to Reproduce\r\n\r\n```console\r\n$ echo -n '{% macro foo() %}{% endmacro %}' | sqlfluff parse -\r\nTraceback (most recent call last):\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/bin/sqlfluff\", line 33, in <module>\r\n    sys.exit(load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')())\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/vladimir/work/extern/sqlfluff/venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/cli/commands.py\", line 701, in parse\r\n    lnt.parse_string(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 596, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 294, in parse_rendered\r\n    tokens, lvs, config = cls._lex_templated_file(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 127, in _lex_templated_file\r\n    tokens, lex_vs = lexer.lex(templated_file)\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/parser/lexer.py\", line 319, in lex\r\n    segments: Tuple[RawSegment, ...] = self.elements_to_segments(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/parser/lexer.py\", line 346, in elements_to_segments\r\n    source_slice = templated_file.templated_slice_to_source_slice(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/templaters/base.py\", line 319, in templated_slice_to_source_slice\r\n    ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\r\n  File \"/home/vladimir/work/extern/sqlfluff/src/sqlfluff/core/templaters/base.py\", line 214, in _find_slice_indices_of_templated_pos\r\n    raise ValueError(\"Position Not Found\")\r\nValueError: Position Not Found\r\n```\r\n\r\nNote: the issue does not occur if the file ends with a newline. \r\n\r\nThe contents of the macro also doesn't matter.\r\n\r\n## Dialect\r\n\r\nNone specified\r\n\r\n## Version\r\nSQLFluff 6011bdbe05669b075045e8127cdf18cc537686d4, Python 3.9.6\r\n\r\n## Configuration\r\n\r\nNone\n",
    "input": "         is the same as the source view.\n         \"\"\"\n         self.source_str = source_str\n        self.templated_str = templated_str or source_str\n         # If no fname, we assume this is from a string or stdin.\n         self.fname = fname\n         # Assume that no sliced_file, means the file is not templated\n                     break\n \n         subslices = self.sliced_file[\n            # Ver inclusive slice\n             min(ts_start_sf_start, ts_stop_sf_start) : max(\n                 ts_start_sf_stop, ts_stop_sf_stop\n             )",
    "output": "         is the same as the source view.\n         \"\"\"\n         self.source_str = source_str\n        # An empty string is still allowed as the templated string.\n        self.templated_str = source_str if templated_str is None else templated_str\n         # If no fname, we assume this is from a string or stdin.\n         self.fname = fname\n         # Assume that no sliced_file, means the file is not templated\n                     break\n \n         subslices = self.sliced_file[\n            # Very inclusive slice\n             min(ts_start_sf_start, ts_stop_sf_start) : max(\n                 ts_start_sf_stop, ts_stop_sf_stop\n             )"
  },
  {
    "instruction": "Double backticks in Lint description\n![image](https://user-images.githubusercontent.com/80432516/150420352-57452c80-ad25-423b-8251-645e541579ad.png)\r\n(n.b. this affects a lot more rules than L051)\r\n\r\nThis was introduced in #2234 in which docstrings such as\r\n```\r\n`INNER JOIN` must be fully qualified.\r\n```\r\nwere replaced with \r\n```\r\n``INNER JOIN`` must be fully qualified.\r\n```\r\nso that they appear as code blocks in Sphinx for docs.\r\n![image](https://user-images.githubusercontent.com/80432516/150420294-eb9d3127-db1d-457c-a637-d614e0267277.png)\r\n\r\nHowever, our rules will use the first line of these docstrings in the event that no `description` is provided to the lint results.\r\n\r\nThis doesn't look great on the CLI so we should fix this. As far as I'm aware there are two approaches for this:\r\n1. Pass a `description` to all the `LintResult`s.\r\n2. Update the code that gets the default description from the docstring to do something like, replace the double backticks with a single one, or remove them, or do something clever like make them bold for the CLI and remove them for non-CLI.\r\n\r\nMy strong preference is number 2, but I'm open to discussion as to how exactly we do this \ud83d\ude04 \r\n\r\n@barrywhart @tunetheweb \n",
    "input": " \n         plugin_name, code = rule_name_match.groups()\n         # If the docstring is multiline, then we extract just summary.\n        description = cls.__doc__.split(\"\\n\")[0]\n \n         if plugin_name:\n             code = f\"{plugin_name}_{code}\"",
    "output": " \n         plugin_name, code = rule_name_match.groups()\n         # If the docstring is multiline, then we extract just summary.\n        description = cls.__doc__.replace(\"``\", \"'\").split(\"\\n\")[0]\n \n         if plugin_name:\n             code = f\"{plugin_name}_{code}\""
  },
  {
    "instruction": "Update warning for parsing errors found on the ansi dialect\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nIn the past specifying a dialect was **optional**. If unspecified, the dialect defaulted to `ansi`. Because of this there is a warning presented when sqlfluff runs in parse mode and the dialect is set to ansi and parsing errors are encountered.\r\n\r\n`WARNING: Parsing errors found and dialect is set to 'ansi'. Have you configured your dialect?`\r\n\r\nCurrently, specifying a dialect is **mandatory**. Therefore this warning is perhaps not needed... and certainly not needed in its current form.\r\n\r\nI opened this issue to document the idea and solicit feedback. \r\n1. The simplest improvement to make the message more appropriate is to just change it to this:\r\n\r\n`WARNING: Parsing errors found and dialect is set to 'ansi'. Is 'ansi' the correct dialect?`\r\n\r\n2. On the other hand, we know that the user explicitly set the dialect to `ansi`. So why bother asking if it was intentional? We don't ask if you meant postgres or tsql. There's an argument to simply remove the warning altogether.\r\n\r\n3. Finally, we could potentially differentiate between `--dialect ansi` passed on the command line vs the dialect being picked up from a `.sqlfluff` config file. Perhaps the warning should be displayed only the in the case where the dialect was picked up implicitly from the config file.\n\n### Use case\n\n_No response_\n\n### Dialect\n\nansi\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "             output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n         for v in parsed_string.violations:\n             output_stream.write(format_violation(v))  # pragma: no cover\n        if parsed_string.violations and parsed_string.config.get(\"dialect\") == \"ansi\":\n            output_stream.write(format_dialect_warning())  # pragma: no cover\n \n         if verbose >= 2:\n             output_stream.write(\"==== timings ====\")\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n     return text_buffer.getvalue()\n \n \ndef format_dialect_warning():  # pragma: no cover\n    \"\"\"Output a warning for parsing errors found on the ansi dialect.\"\"\"\n     return colorize(\n         (\n             \"WARNING: Parsing errors found and dialect is set to \"\n            \"'ansi'. Have you configured your dialect?\"\n         ),\n         Color.lightgrey,\n     )\n             f\"=== [{colorize(templater, Color.lightgrey)}] {message}\"\n         )  # pragma: no cover\n \n    def dispatch_dialect_warning(self) -> None:\n         \"\"\"Dispatch a warning for dialects.\"\"\"\n        self._dispatch(format_dialect_warning())  # pragma: no cover\n \n     def _format_file_violations(\n         self, fname: str, violations: List[SQLBaseError]\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n             )\n \n         # Safety flag for unset dialects\n        if parsed.config.get(\"dialect\") == \"ansi\" and linted_file.get_violations(\n             fixable=True if fix else None, types=SQLParseError\n         ):\n             if formatter:  # pragma: no cover TODO?\n                formatter.dispatch_dialect_warning()\n \n         return linted_file\n ",
    "output": "             output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n         for v in parsed_string.violations:\n             output_stream.write(format_violation(v))  # pragma: no cover\n        if parsed_string.violations:\n            output_stream.write(\n                format_dialect_warning(parsed_string.config.get(\"dialect\"))\n            )\n \n         if verbose >= 2:\n             output_stream.write(\"==== timings ====\")\ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n     return text_buffer.getvalue()\n \n \ndef format_dialect_warning(dialect):\n    \"\"\"Output a warning for parsing errors.\"\"\"\n     return colorize(\n         (\n             \"WARNING: Parsing errors found and dialect is set to \"\n            f\"'{dialect}'. Have you configured your dialect correctly?\"\n         ),\n         Color.lightgrey,\n     )\n             f\"=== [{colorize(templater, Color.lightgrey)}] {message}\"\n         )  # pragma: no cover\n \n    def dispatch_dialect_warning(self, dialect) -> None:\n         \"\"\"Dispatch a warning for dialects.\"\"\"\n        self._dispatch(format_dialect_warning(dialect))  # pragma: no cover\n \n     def _format_file_violations(\n         self, fname: str, violations: List[SQLBaseError]\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n             )\n \n         # Safety flag for unset dialects\n        if linted_file.get_violations(\n             fixable=True if fix else None, types=SQLParseError\n         ):\n             if formatter:  # pragma: no cover TODO?\n                formatter.dispatch_dialect_warning(parsed.config.get(\"dialect\"))\n \n         return linted_file\n "
  },
  {
    "instruction": "L045:  Unused CTEs are not automatically detected when using jinja/dbt as a templater\n## Expected Behaviour\r\nWhen unused CTEs are used with jinja or dbt as a templater, these are detected by L045. \r\n\r\n## Observed Behaviour\r\nWhen ref() statements are included in a SQL file and dbt is used as a templater, these seem to interfere with the ability for rule L045 to detect the unused CTEs.  The same behavior is observed when Jinja is included under the \"FROM\" statement of the relevant queries.\r\n\r\n## Steps to Reproduce\r\n(1). Generate a valid dbt project with at least two models with one variable each.  For the purposes of this reproduction example, I am going to assume that one model is 'foo' with variable 'var_foo' and one model is 'bar' with variable 'var_bar'.\r\n \r\n(2) Using DBT as a templater and BigQuery as a dialect, run dbt lint on the following SQL file:\r\n\r\n```sql\r\nWITH\r\nrandom_gibberish AS (\r\n    SELECT var_foo\r\n    FROM\r\n        {{ ref('foo') }}\r\n)\r\n\r\nSELECT var_bar\r\nFROM\r\n    {{ ref('bar') }}\r\n```\r\n\r\nIf the templater is switched to Jinja, L045 again doesn't produce any errors.\r\n\r\n## Dialect\r\nBigquery\r\n\r\n## Version\r\nSQLFluff version is 0.10.0.  Python version is 3.8.10.\r\nI'm using dbt 1.0.1 but the same issue occurs when Jinja is used as a templater.\r\n\r\n## Configuration\r\n```\r\n[sqlfluff]\r\ndialect = bigquery\r\nexclude_rules = L003,L008,L011,L014,L016,L029,L031,L034\r\n\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\ncomma_style = leading\r\n\r\n[sqlfluff:rules:L010]\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L030]\r\ncapitalisation_policy = upper\r\n```\n",
    "input": "     parent: Optional[\"Query\"] = field(default=None)\n     # Children (could be CTE, subselect, or other).\n     children: List[\"Query\"] = field(default_factory=list)\n \n     def lookup_cte(self, name: str, pop: bool = True) -> Optional[\"Query\"]:\n         \"\"\"Look up a CTE by name, in the current or any parent scope.\"\"\"\n \n         # Stores the last CTE name we saw, so we can associate it with the\n         # corresponding Query.\n        cte_name = None\n \n         # Visit segment and all its children\n         for event, path in SelectCrawler.visit_segments(segment):\n                                 append_query(query)\n                     else:\n                         # We're processing a \"with\" statement.\n                        if cte_name:\n                             # If we have a CTE name, this is the Query for that\n                             # name.\n                            query = self.query_class(QueryType.Simple, dialect)\n                             if path[-1].is_type(\"select_statement\", \"values_clause\"):\n                                 # Add to the Query object we just created.\n                                 query.selectables.append(Selectable(path[-1], dialect))\n                                 # to the Query later when we encounter those\n                                 # child segments.\n                                 pass\n                            query_stack[-1].ctes[cte_name] = query\n                            cte_name = None\n                             append_query(query)\n                         else:\n                             # There's no CTE name, so we're probably processing\n                 elif path[-1].is_type(\"with_compound_statement\"):\n                     # Beginning a \"with\" statement, i.e. a block of CTEs.\n                     query = self.query_class(QueryType.WithCompound, dialect)\n                    if cte_name:\n                        query_stack[-1].ctes[cte_name] = query\n                        cte_name = None\n                     append_query(query)\n                 elif path[-1].is_type(\"common_table_expression\"):\n                     # This is a \"<<cte name>> AS\". Grab the name for later.\n                    cte_name = path[-1].segments[0].raw\n             elif event == \"end\":\n                 finish_segment()\n \ndiff --git a/src/sqlfluff/rules/L045.py b/src/sqlfluff/rules/L045.py\n \"\"\"Implementation of Rule L045.\"\"\"\nfrom typing import Optional\n\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, RuleContext\n from sqlfluff.core.rules.analysis.select_crawler import Query, SelectCrawler\n \n \n                 if isinstance(source, Query):\n                     cls._visit_sources(source)\n \n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n         if context.segment.is_type(\"statement\"):\n             crawler = SelectCrawler(context.segment, context.dialect)\n             if crawler.query_tree:\n                 # Begin analysis at the final, outer query (key=None).\n                 self._visit_sources(crawler.query_tree)\n                 if crawler.query_tree.ctes:\n                    return LintResult(anchor=context.segment)\n         return None",
    "output": "     parent: Optional[\"Query\"] = field(default=None)\n     # Children (could be CTE, subselect, or other).\n     children: List[\"Query\"] = field(default_factory=list)\n    cte_name_segment: Optional[BaseSegment] = field(default=None)\n \n     def lookup_cte(self, name: str, pop: bool = True) -> Optional[\"Query\"]:\n         \"\"\"Look up a CTE by name, in the current or any parent scope.\"\"\"\n \n         # Stores the last CTE name we saw, so we can associate it with the\n         # corresponding Query.\n        cte_name_segment: Optional[BaseSegment] = None\n \n         # Visit segment and all its children\n         for event, path in SelectCrawler.visit_segments(segment):\n                                 append_query(query)\n                     else:\n                         # We're processing a \"with\" statement.\n                        if cte_name_segment:\n                             # If we have a CTE name, this is the Query for that\n                             # name.\n                            query = self.query_class(\n                                QueryType.Simple,\n                                dialect,\n                                cte_name_segment=cte_name_segment,\n                            )\n                             if path[-1].is_type(\"select_statement\", \"values_clause\"):\n                                 # Add to the Query object we just created.\n                                 query.selectables.append(Selectable(path[-1], dialect))\n                                 # to the Query later when we encounter those\n                                 # child segments.\n                                 pass\n                            query_stack[-1].ctes[cte_name_segment.raw] = query\n                            cte_name_segment = None\n                             append_query(query)\n                         else:\n                             # There's no CTE name, so we're probably processing\n                 elif path[-1].is_type(\"with_compound_statement\"):\n                     # Beginning a \"with\" statement, i.e. a block of CTEs.\n                     query = self.query_class(QueryType.WithCompound, dialect)\n                    if cte_name_segment:\n                        query_stack[-1].ctes[cte_name_segment.raw] = query\n                        cte_name_segment = None\n                     append_query(query)\n                 elif path[-1].is_type(\"common_table_expression\"):\n                     # This is a \"<<cte name>> AS\". Grab the name for later.\n                    cte_name_segment = path[-1].segments[0]\n             elif event == \"end\":\n                 finish_segment()\n \ndiff --git a/src/sqlfluff/rules/L045.py b/src/sqlfluff/rules/L045.py\n \"\"\"Implementation of Rule L045.\"\"\"\nfrom sqlfluff.core.rules.base import BaseRule, EvalResultType, LintResult, RuleContext\n from sqlfluff.core.rules.analysis.select_crawler import Query, SelectCrawler\n \n \n                 if isinstance(source, Query):\n                     cls._visit_sources(source)\n \n    def _eval(self, context: RuleContext) -> EvalResultType:\n         if context.segment.is_type(\"statement\"):\n             crawler = SelectCrawler(context.segment, context.dialect)\n             if crawler.query_tree:\n                 # Begin analysis at the final, outer query (key=None).\n                 self._visit_sources(crawler.query_tree)\n                 if crawler.query_tree.ctes:\n                    return [\n                        LintResult(\n                            anchor=query.cte_name_segment,\n                            description=f\"Query defines CTE \"\n                            f'\"{query.cte_name_segment.raw}\" '\n                            f\"but does not use it.\",\n                        )\n                        for query in crawler.query_tree.ctes.values()\n                        if query.cte_name_segment\n                    ]\n         return None"
  },
  {
    "instruction": "`fix` per file linted instead of at the end\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI am just testing sqlfluff on a small example project.\r\nWe have configured it as part of `pre-commit`.\r\n\r\n```\r\n-   repo: https://github.com/sqlfluff/sqlfluff\r\n    rev: 1.0.0\r\n    hooks:\r\n    -   id: sqlfluff-fix\r\n        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\r\n        files: \\.(sql)$\r\n        exclude: sp_whoisactive.sql\r\n```\r\n\r\nProcessing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\r\n\r\nAt the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n\n### Use case\n\nInstead of writing all formatted sql at the end, I would like to see files written sequentially.\n\n### Dialect\n\nmost likely all, i am working with t-sql.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n`fix` per file linted instead of at the end\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI am just testing sqlfluff on a small example project.\r\nWe have configured it as part of `pre-commit`.\r\n\r\n```\r\n-   repo: https://github.com/sqlfluff/sqlfluff\r\n    rev: 1.0.0\r\n    hooks:\r\n    -   id: sqlfluff-fix\r\n        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\r\n        files: \\.(sql)$\r\n        exclude: sp_whoisactive.sql\r\n```\r\n\r\nProcessing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\r\n\r\nAt the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n\n### Use case\n\nInstead of writing all formatted sql at the end, I would like to see files written sequentially.\n\n### Dialect\n\nmost likely all, i am working with t-sql.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": " \n \n def do_fixes(\n    result: LintingResult, formatter: Optional[OutputStreamFormatter] = None, **kwargs\n ):\n     \"\"\"Actually do the fixes.\"\"\"\n     if formatter and formatter.verbosity >= 0:\n         click.echo(\"Persisting Changes...\")\n    res = result.persist_changes(formatter=formatter, **kwargs)\n     if all(res.values()):\n         if formatter and formatter.verbosity >= 0:\n             click.echo(\"Done. Please check your files to confirm.\")\n         click.echo(\"==== finding fixable violations ====\")\n     exit_code = EXIT_SUCCESS\n \n     with PathAndUserErrorHandler(formatter):\n         result: LintingResult = linter.lint_paths(\n             paths,\n             fix=True,\n             ignore_non_existent_files=False,\n             processes=processes,\n         )\n \n     if not fix_even_unparsable:\n     # NB: We filter to linting violations here, because they're\n     # the only ones which can be potentially fixed.\n     num_fixable = result.num_violations(types=SQLLintError, fixable=True)\n     if num_fixable > 0:\n        if formatter.verbosity >= 0:\n             click.echo(\"==== fixing violations ====\")\n         click.echo(f\"{num_fixable} \" \"fixable linting violations found\")\n        if force:\n            if warn_force and formatter.verbosity >= 0:\n                click.echo(\n                    f\"{formatter.colorize('FORCE MODE', Color.red)}: \"\n                    \"Attempting fixes...\"\n                )\n            success = do_fixes(\n                result,\n                formatter,\n                types=SQLLintError,\n                fixed_file_suffix=fixed_suffix,\n            )\n            if not success:\n                sys.exit(EXIT_FAIL)  # pragma: no cover\n        else:\n             click.echo(\n                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n             )\n                 success = do_fixes(\n                     result,\n                     formatter,\n                    types=SQLLintError,\n                     fixed_file_suffix=fixed_suffix,\n                 )\n                 if not success:\n     is_flag=True,\n     help=(\n         \"Skip the confirmation prompt and go straight to applying \"\n        \"fixes. **Use this with caution.**\"\n     ),\n )\n @click.option(\ndiff --git a/src/sqlfluff/core/linter/linted_dir.py b/src/sqlfluff/core/linter/linted_dir.py\n         )\n \n     def persist_changes(\n        self, formatter: Any = None, fixed_file_suffix: str = \"\", **kwargs\n     ) -> Dict[str, Union[bool, str]]:\n         \"\"\"Persist changes to files in the given path.\n \n         # Run all the fixes for all the files and return a dict\n         buffer: Dict[str, Union[bool, str]] = {}\n         for file in self.files:\n            if file.num_violations(fixable=True, **kwargs) > 0:\n                buffer[file.path] = file.persist_tree(suffix=fixed_file_suffix)\n                result: Union[bool, str]\n                if buffer[file.path] is True:\n                    result = \"FIXED\"\n                else:  # pragma: no cover\n                    result = buffer[file.path]\n            else:  # pragma: no cover TODO?\n                buffer[file.path] = True\n                result = \"SKIP\"\n\n            if formatter:\n                formatter.dispatch_persist_filename(filename=file.path, result=result)\n         return buffer\n \n     @property\ndiff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n from sqlfluff.core.errors import (\n     SQLBaseError,\n     SQLLintError,\n     CheckTuple,\n )\n from sqlfluff.core.templaters import TemplatedFile, RawFileSlice\n # Instantiate the linter logger\n linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n \n \n @dataclass\n class FileTimings:\n                 str_buff += raw_source_string[source_slice]\n         return str_buff\n \n    def persist_tree(self, suffix: str = \"\") -> bool:\n         \"\"\"Persist changes to the given path.\"\"\"\n        write_buff, success = self.fix_string()\n\n        if success:\n            fname = self.path\n            # If there is a suffix specified, then use it.s\n            if suffix:\n                root, ext = os.path.splitext(fname)\n                fname = root + suffix + ext\n            self._safe_create_replace_file(self.path, fname, write_buff, self.encoding)\n         return success\n \n     @staticmethod\n     def _safe_create_replace_file(\n         input_path: str, output_path: str, write_buff: str, encoding: str\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     NoQaDirective,\n     RenderedFile,\n )\nfrom sqlfluff.core.linter.linted_file import LintedFile, FileTimings\n from sqlfluff.core.linter.linted_dir import LintedDir\n from sqlfluff.core.linter.linting_result import LintingResult\n \n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n         processes: Optional[int] = None,\n     ) -> LintingResult:\n         \"\"\"Lint an iterable of paths.\"\"\"\n         # If no paths specified - assume local\n                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                 break\n \n             # Progress bar for files is rendered only when there is more than one file.\n             # Additionally, as it's updated after each loop, we need to get file name\n             # from the next loop. This is why `enumerate` starts with `1` and there\ndiff --git a/src/sqlfluff/core/linter/linting_result.py b/src/sqlfluff/core/linter/linting_result.py\n \n from sqlfluff.core.errors import (\n     CheckTuple,\n    SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n )\n \n from sqlfluff.core.timing import TimingSummary, RuleTimingSummary\n # Classes needed only for type checking\n from sqlfluff.core.parser.segments.base import BaseSegment\n from sqlfluff.core.linter.linted_dir import LintedDir\n \n \n class LintingResult:\n             if violations\n         ]\n \n    def persist_changes(self, formatter, **kwargs) -> dict:\n         \"\"\"Run all the fixes for all the files and return a dict.\"\"\"\n         return self.combine_dicts(\n             *(\n                path.persist_changes(formatter=formatter, **kwargs)\n                 for path in self.paths\n             )\n         )\n             )\n         return self.paths[0].tree\n \n    TMP_PRS_ERROR_TYPES = (SQLTemplaterError, SQLParseError)\n\n     def count_tmp_prs_errors(self) -> Tuple[int, int]:\n         \"\"\"Count templating or parse errors before and after filtering.\"\"\"\n         total_errors = self.num_violations(\n            types=self.TMP_PRS_ERROR_TYPES, filter_ignore=False, filter_warning=False\n         )\n         num_filtered_errors = 0\n         for linted_dir in self.paths:\n             for linted_file in linted_dir.files:\n                 num_filtered_errors += linted_file.num_violations(\n                    types=self.TMP_PRS_ERROR_TYPES\n                 )\n         return total_errors, num_filtered_errors\n \n     def discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors(self) -> None:\n         \"\"\"Discard lint fixes for files with templating or parse errors.\"\"\"\n         total_errors = self.num_violations(\n            types=self.TMP_PRS_ERROR_TYPES, filter_ignore=False, filter_warning=False\n         )\n         if total_errors:\n             for linted_dir in self.paths:\n                 for linted_file in linted_dir.files:\n                    num_errors = linted_file.num_violations(\n                        types=self.TMP_PRS_ERROR_TYPES,\n                        filter_ignore=False,\n                        filter_warning=False,\n                    )\n                    if num_errors:\n                        # File has errors. Discard all the SQLLintError fixes:\n                        # they are potentially unsafe.\n                        for violation in linted_file.violations:\n                            if isinstance(violation, SQLLintError):\n                                violation.fixes = []",
    "output": " \n \n def do_fixes(\n    result: LintingResult,\n    formatter: Optional[OutputStreamFormatter] = None,\n    fixed_file_suffix: str = \"\",\n ):\n     \"\"\"Actually do the fixes.\"\"\"\n     if formatter and formatter.verbosity >= 0:\n         click.echo(\"Persisting Changes...\")\n    res = result.persist_changes(\n        formatter=formatter, fixed_file_suffix=fixed_file_suffix\n    )\n     if all(res.values()):\n         if formatter and formatter.verbosity >= 0:\n             click.echo(\"Done. Please check your files to confirm.\")\n         click.echo(\"==== finding fixable violations ====\")\n     exit_code = EXIT_SUCCESS\n \n    if force and warn_force and formatter.verbosity >= 0:\n        click.echo(\n            f\"{formatter.colorize('FORCE MODE', Color.red)}: \" \"Attempting fixes...\"\n        )\n\n     with PathAndUserErrorHandler(formatter):\n         result: LintingResult = linter.lint_paths(\n             paths,\n             fix=True,\n             ignore_non_existent_files=False,\n             processes=processes,\n            # If --force is set, then apply the changes as we go rather\n            # than waiting until the end.\n            apply_fixes=force,\n            fixed_file_suffix=fixed_suffix,\n            fix_even_unparsable=fix_even_unparsable,\n         )\n \n     if not fix_even_unparsable:\n     # NB: We filter to linting violations here, because they're\n     # the only ones which can be potentially fixed.\n     num_fixable = result.num_violations(types=SQLLintError, fixable=True)\n\n     if num_fixable > 0:\n        if not force and formatter.verbosity >= 0:\n             click.echo(\"==== fixing violations ====\")\n\n         click.echo(f\"{num_fixable} \" \"fixable linting violations found\")\n\n        if not force:\n             click.echo(\n                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n             )\n                 success = do_fixes(\n                     result,\n                     formatter,\n                     fixed_file_suffix=fixed_suffix,\n                 )\n                 if not success:\n     is_flag=True,\n     help=(\n         \"Skip the confirmation prompt and go straight to applying \"\n        \"fixes. Fixes will also be applied file by file, during the \"\n        \"linting process, rather than waiting until all files are \"\n        \"linted before fixing. **Use this with caution.**\"\n     ),\n )\n @click.option(\ndiff --git a/src/sqlfluff/core/linter/linted_dir.py b/src/sqlfluff/core/linter/linted_dir.py\n         )\n \n     def persist_changes(\n        self, formatter: Any = None, fixed_file_suffix: str = \"\"\n     ) -> Dict[str, Union[bool, str]]:\n         \"\"\"Persist changes to files in the given path.\n \n         # Run all the fixes for all the files and return a dict\n         buffer: Dict[str, Union[bool, str]] = {}\n         for file in self.files:\n            buffer[file.path] = file.persist_tree(\n                suffix=fixed_file_suffix, formatter=formatter\n            )\n         return buffer\n \n     @property\ndiff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n from sqlfluff.core.errors import (\n     SQLBaseError,\n     SQLLintError,\n    SQLParseError,\n    SQLTemplaterError,\n     CheckTuple,\n )\n from sqlfluff.core.templaters import TemplatedFile, RawFileSlice\n # Instantiate the linter logger\n linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n \nTMP_PRS_ERROR_TYPES = (SQLTemplaterError, SQLParseError)\n\n \n @dataclass\n class FileTimings:\n                 str_buff += raw_source_string[source_slice]\n         return str_buff\n \n    def persist_tree(self, suffix: str = \"\", formatter: Any = None) -> bool:\n         \"\"\"Persist changes to the given path.\"\"\"\n        if self.num_violations(fixable=True) > 0:\n            write_buff, success = self.fix_string()\n\n            if success:\n                fname = self.path\n                # If there is a suffix specified, then use it.s\n                if suffix:\n                    root, ext = os.path.splitext(fname)\n                    fname = root + suffix + ext\n                self._safe_create_replace_file(\n                    self.path, fname, write_buff, self.encoding\n                )\n                result_label = \"FIXED\"\n            else:  # pragma: no cover\n                result_label = \"FAIL\"\n        else:\n            result_label = \"SKIP\"\n            success = True\n\n        if formatter:\n            formatter.dispatch_persist_filename(filename=self.path, result=result_label)\n\n         return success\n \n    def discard_fixes_if_tmp_or_prs_errors(self) -> None:\n        \"\"\"Discard lint fixes for files with templating or parse errors.\"\"\"\n        num_errors = self.num_violations(\n            types=TMP_PRS_ERROR_TYPES,\n            filter_ignore=False,\n            filter_warning=False,\n        )\n        if num_errors:\n            # File has errors. Discard all the SQLLintError fixes:\n            # they are potentially unsafe.\n            for violation in self.violations:\n                if isinstance(violation, SQLLintError):\n                    violation.fixes = []\n\n     @staticmethod\n     def _safe_create_replace_file(\n         input_path: str, output_path: str, write_buff: str, encoding: str\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n     NoQaDirective,\n     RenderedFile,\n )\nfrom sqlfluff.core.linter.linted_file import (\n    LintedFile,\n    FileTimings,\n    TMP_PRS_ERROR_TYPES,\n)\n from sqlfluff.core.linter.linted_dir import LintedDir\n from sqlfluff.core.linter.linting_result import LintingResult\n \n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n         processes: Optional[int] = None,\n        apply_fixes: bool = False,\n        fixed_file_suffix: str = \"\",\n        fix_even_unparsable: bool = False,\n     ) -> LintingResult:\n         \"\"\"Lint an iterable of paths.\"\"\"\n         # If no paths specified - assume local\n                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                 break\n \n            # If we're applying fixes, then do that here.\n            if apply_fixes:\n                num_tmp_prs_errors = linted_file.num_violations(\n                    types=TMP_PRS_ERROR_TYPES,\n                    filter_ignore=False,\n                    filter_warning=False,\n                )\n                if fix_even_unparsable or num_tmp_prs_errors == 0:\n                    linted_file.persist_tree(\n                        suffix=fixed_file_suffix, formatter=self.formatter\n                    )\n\n             # Progress bar for files is rendered only when there is more than one file.\n             # Additionally, as it's updated after each loop, we need to get file name\n             # from the next loop. This is why `enumerate` starts with `1` and there\ndiff --git a/src/sqlfluff/core/linter/linting_result.py b/src/sqlfluff/core/linter/linting_result.py\n \n from sqlfluff.core.errors import (\n     CheckTuple,\n )\n \n from sqlfluff.core.timing import TimingSummary, RuleTimingSummary\n # Classes needed only for type checking\n from sqlfluff.core.parser.segments.base import BaseSegment\n from sqlfluff.core.linter.linted_dir import LintedDir\nfrom sqlfluff.core.linter.linted_file import TMP_PRS_ERROR_TYPES\n \n \n class LintingResult:\n             if violations\n         ]\n \n    def persist_changes(self, formatter, fixed_file_suffix: str = \"\") -> dict:\n         \"\"\"Run all the fixes for all the files and return a dict.\"\"\"\n         return self.combine_dicts(\n             *(\n                path.persist_changes(\n                    formatter=formatter, fixed_file_suffix=fixed_file_suffix\n                )\n                 for path in self.paths\n             )\n         )\n             )\n         return self.paths[0].tree\n \n     def count_tmp_prs_errors(self) -> Tuple[int, int]:\n         \"\"\"Count templating or parse errors before and after filtering.\"\"\"\n         total_errors = self.num_violations(\n            types=TMP_PRS_ERROR_TYPES,\n            filter_ignore=False,\n            filter_warning=False,\n         )\n         num_filtered_errors = 0\n         for linted_dir in self.paths:\n             for linted_file in linted_dir.files:\n                 num_filtered_errors += linted_file.num_violations(\n                    types=TMP_PRS_ERROR_TYPES\n                 )\n         return total_errors, num_filtered_errors\n \n     def discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors(self) -> None:\n         \"\"\"Discard lint fixes for files with templating or parse errors.\"\"\"\n         total_errors = self.num_violations(\n            types=TMP_PRS_ERROR_TYPES,\n            filter_ignore=False,\n            filter_warning=False,\n         )\n         if total_errors:\n             for linted_dir in self.paths:\n                 for linted_file in linted_dir.files:\n                    linted_file.discard_fixes_if_tmp_or_prs_errors()"
  },
  {
    "instruction": "\"Dropped elements in sequence matching\" when doubled semicolon\n## Expected Behaviour\r\nFrankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.\r\n## Observed Behaviour\r\n```console\r\n(.venv) ?master ~/prod/_inne/sqlfluff> echo \"select id from tbl;;\" | sqlfluff lint -\r\nTraceback (most recent call last):\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff\", line 11, in <module>\r\n    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py\", line 347, in lint\r\n    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 789, in lint_string_wrapped\r\n    linted_path.add(self.lint_string(string, fname=fname, fix=fix))\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 668, in lint_string\r\n    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 607, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 313, in parse_rendered\r\n    parsed, pvs = cls._parse_tokens(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 190, in _parse_tokens\r\n    parsed: Optional[BaseSegment] = parser.parse(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py\", line 32, in parse\r\n    parsed = root_segment.parse(parse_context=ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py\", line 821, in parse\r\n    check_still_complete(segments, m.matched_segments, m.unmatched_segments)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py\", line 30, in check_still_complete\r\n    raise RuntimeError(\r\nRuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'\r\n\r\n```\r\n## Steps to Reproduce\r\nRun \r\n```console\r\necho \"select id from tbl;;\" | sqlfluff lint -\r\n```\r\n## Dialect\r\ndefault (ansi)\r\n## Version\r\n```\r\nsqlfluff, version 0.6.6\r\nPython 3.9.5\r\n```\r\n## Configuration\r\nNone\r\n\n",
    "input": " \n from typing import Tuple, List, Any, Iterator, TYPE_CHECKING\n \n from sqlfluff.core.string_helpers import curtail_string\n \n if TYPE_CHECKING:\n     \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n     initial_str = join_segments_raw(segments_in)\n     current_str = join_segments_raw(matched_segments + unmatched_segments)\n    if initial_str != current_str:  # pragma: no cover\n        raise RuntimeError(\n            \"Dropped elements in sequence matching! {!r} != {!r}\".format(\n                initial_str, current_str\n            )\n         )\n     return True\n ",
    "output": " \n from typing import Tuple, List, Any, Iterator, TYPE_CHECKING\n \nfrom sqlfluff.core.errors import SQLParseError\n from sqlfluff.core.string_helpers import curtail_string\n \n if TYPE_CHECKING:\n     \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n     initial_str = join_segments_raw(segments_in)\n     current_str = join_segments_raw(matched_segments + unmatched_segments)\n\n    if initial_str != current_str:\n        raise SQLParseError(\n            f\"Could not parse: {current_str}\",\n            segment=unmatched_segments[0],\n         )\n     return True\n "
  },
  {
    "instruction": "Rename BaseCrawler class as BaseRule to be clearer, avoid confusion with analysis helper classes, e.g. SelectCrawler\nDiscussed here:\r\nhttps://github.com/sqlfluff/sqlfluff/pull/779#pullrequestreview-604167034\r\n\n",
    "input": " \n from sqlfluff.core.plugin import hookimpl\n from sqlfluff.core.rules.base import (\n    BaseCrawler,\n     LintResult,\n )\n from sqlfluff.core.rules.doc_decorators import (\n \n \n @hookimpl\ndef get_rules() -> List[BaseCrawler]:\n     \"\"\"Get plugin rules.\"\"\"\n     return [Rule_Example_L001]\n \n # to be displayed in the sqlfluff docs\n @document_fix_compatible\n @document_configuration\nclass Rule_Example_L001(BaseCrawler):\n     \"\"\"ORDER BY on these columns is forbidden!\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/linter.py b/src/sqlfluff/core/linter.py\n from sqlfluff.core.parser.segments.base import BaseSegment, FixPatch\n from sqlfluff.core.parser.segments.meta import MetaSegment\n from sqlfluff.core.parser.segments.raw import RawSegment\nfrom sqlfluff.core.rules.base import BaseCrawler\n \n # Instantiate the linter logger\n linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n         # Store references to user rule classes\n         self.user_rules = user_rules or []\n \n    def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseCrawler]:\n         \"\"\"Get hold of a set of rules.\"\"\"\n         rs = get_ruleset()\n         # Register any user rules\ndiff --git a/src/sqlfluff/core/rules/base.py b/src/sqlfluff/core/rules/base.py\n\"\"\"Implements the base crawler which all the rules are based on.\n \nCrawlers, crawl through the trees returned by the parser and\nevaluate particular rules.\n \n The intent is that it should be possible for the rules to be expressed\n as simply as possible, with as much of the complexity abstracted away.\n \n \n class LintResult:\n    \"\"\"A class to hold the results of a crawl operation.\n \n     Args:\n         anchor (:obj:`BaseSegment`, optional): A segment which represents\n             fixes which would correct this issue. If not present then it's\n             assumed that this issue will have to manually fixed.\n         memory (:obj:`dict`, optional): An object which stores any working\n            memory for the crawler. The `memory` returned in any `LintResult`\n             will be passed as an input to the next segment to be crawled.\n         description (:obj:`str`, optional): A description of the problem\n             identified as part of this result. This will override the\n         return True\n \n \nclass BaseCrawler:\n    \"\"\"The base class for a crawler, of which all rules are derived from.\n \n     Args:\n         code (:obj:`str`): The identifier for this rule, used in inclusion\n     def __init__(self, code, description, **kwargs):\n         self.description = description\n         self.code = code\n        # kwargs represents the config passed to the crawler. Add all kwargs as class attributes\n         # so they can be accessed in rules which inherit from this class\n         for key, value in kwargs.items():\n             self.__dict__[key] = value\n         \"\"\"\n         # parent stack should be a tuple if it exists\n \n        # crawlers, should evaluate on segments FIRST, before evaluating on their\n         # children. They should also return a list of violations.\n \n         parent_stack = parent_stack or ()\n         .. code-block:: python\n \n            @myruleset.register\n           class Rule_L001(BaseCrawler):\n                \"Description of rule.\"\n \n                def eval(self, **kwargs):\n         for configuring the rules given the given config.\n \n         Returns:\n            :obj:`list` of instantiated :obj:`BaseCrawler`.\n \n         \"\"\"\n         # Validate all generic rule configs\ndiff --git a/src/sqlfluff/core/rules/std/L001.py b/src/sqlfluff/core/rules/std/L001.py\n \"\"\"Implementation of Rule L001.\"\"\"\nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L001(BaseCrawler):\n     \"\"\"Unnecessary trailing whitespace.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L002.py b/src/sqlfluff/core/rules/std/L002.py\n \"\"\"Implementation of Rule L002.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_configuration,\n     document_fix_compatible,\n \n @document_configuration\n @document_fix_compatible\nclass Rule_L002(BaseCrawler):\n     \"\"\"Mixed Tabs and Spaces in single whitespace.\n \n     This rule will fail if a single section of whitespace\ndiff --git a/src/sqlfluff/core/rules/std/L003.py b/src/sqlfluff/core/rules/std/L003.py\n \"\"\"Implementation of Rule L003.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L003(BaseCrawler):\n     \"\"\"Indentation not consistent with previous lines.\n \n     Note:\ndiff --git a/src/sqlfluff/core/rules/std/L004.py b/src/sqlfluff/core/rules/std/L004.py\n \"\"\"Implementation of Rule L004.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L004(BaseCrawler):\n     \"\"\"Incorrect indentation type.\n \n     Note 1: spaces are only fixed to tabs if the number of spaces in the\ndiff --git a/src/sqlfluff/core/rules/std/L005.py b/src/sqlfluff/core/rules/std/L005.py\n \"\"\"Implementation of Rule L005.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L005(BaseCrawler):\n     \"\"\"Commas should not have whitespace directly before them.\n \n     Unless it's an indent. Trailing/leading commas are dealt with\ndiff --git a/src/sqlfluff/core/rules/std/L006.py b/src/sqlfluff/core/rules/std/L006.py\n \"\"\"Implementation of Rule L006.\"\"\"\n \n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L006(BaseCrawler):\n     \"\"\"Operators should be surrounded by a single whitespace.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L007.py b/src/sqlfluff/core/rules/std/L007.py\n \"\"\"Implementation of Rule L007.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n \n \nclass Rule_L007(BaseCrawler):\n     \"\"\"Operators near newlines should be after, not before the newline.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L008.py b/src/sqlfluff/core/rules/std/L008.py\n \"\"\"Implementation of Rule L008.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L008(BaseCrawler):\n     \"\"\"Commas should be followed by a single whitespace unless followed by a comment.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L009.py b/src/sqlfluff/core/rules/std/L009.py\n \"\"\"Implementation of Rule L009.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L009(BaseCrawler):\n     \"\"\"Files must end with a trailing newline.\"\"\"\n \n     def _eval(self, segment, siblings_post, parent_stack, **kwargs):\ndiff --git a/src/sqlfluff/core/rules/std/L010.py b/src/sqlfluff/core/rules/std/L010.py\n \"\"\"Implementation of Rule L010.\"\"\"\n \n from typing import Tuple, List\nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L010(BaseCrawler):\n     \"\"\"Inconsistent capitalisation of keywords.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L011.py b/src/sqlfluff/core/rules/std/L011.py\n \"\"\"Implementation of Rule L011.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L011(BaseCrawler):\n     \"\"\"Implicit aliasing of table not allowed. Use explicit `AS` clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L013.py b/src/sqlfluff/core/rules/std/L013.py\n \"\"\"Implementation of Rule L013.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n from sqlfluff.core.rules.doc_decorators import document_configuration\n \n \n @document_configuration\nclass Rule_L013(BaseCrawler):\n     \"\"\"Column expression without alias. Use explicit `AS` clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L015.py b/src/sqlfluff/core/rules/std/L015.py\n \"\"\"Implementation of Rule L015.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L015(BaseCrawler):\n     \"\"\"DISTINCT used with parentheses.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L017.py b/src/sqlfluff/core/rules/std/L017.py\n \"\"\"Implementation of Rule L017.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L017(BaseCrawler):\n     \"\"\"Function name not immediately followed by bracket.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L018.py b/src/sqlfluff/core/rules/std/L018.py\n \"\"\"Implementation of Rule L018.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L018(BaseCrawler):\n     \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L019.py b/src/sqlfluff/core/rules/std/L019.py\n \n from typing import Dict, Any\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L019(BaseCrawler):\n     \"\"\"Leading/Trailing comma enforcement.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L020.py b/src/sqlfluff/core/rules/std/L020.py\n \n import itertools\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n from sqlfluff.core.rules.analysis.select import get_select_statement_info\n \n \nclass Rule_L020(BaseCrawler):\n     \"\"\"Table aliases should be unique within each clause.\"\"\"\n \n     def _lint_references_and_aliases(\ndiff --git a/src/sqlfluff/core/rules/std/L021.py b/src/sqlfluff/core/rules/std/L021.py\n \"\"\"Implementation of Rule L021.\"\"\"\n \n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n \n \nclass Rule_L021(BaseCrawler):\n     \"\"\"Ambiguous use of DISTINCT in select statement with GROUP BY.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L022.py b/src/sqlfluff/core/rules/std/L022.py\n \"\"\"Implementation of Rule L022.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L022(BaseCrawler):\n     \"\"\"Blank line expected but not found after CTE closing bracket.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L023.py b/src/sqlfluff/core/rules/std/L023.py\n \n from typing import Optional, List\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L023(BaseCrawler):\n     \"\"\"Single whitespace expected after AS in WITH clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L029.py b/src/sqlfluff/core/rules/std/L029.py\n \"\"\"Implementation of Rule L029.\"\"\"\n \n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n from sqlfluff.core.rules.doc_decorators import document_configuration\n \n \n @document_configuration\nclass Rule_L029(BaseCrawler):\n     \"\"\"Keywords should not be used as identifiers.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L031.py b/src/sqlfluff/core/rules/std/L031.py\n \"\"\"Implementation of Rule L031.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L031(BaseCrawler):\n     \"\"\"Avoid table aliases in from clauses and join conditions.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L032.py b/src/sqlfluff/core/rules/std/L032.py\n \"\"\"Implementation of Rule L032.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n \n \nclass Rule_L032(BaseCrawler):\n     \"\"\"Prefer specifying join keys instead of using \"USING\".\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L033.py b/src/sqlfluff/core/rules/std/L033.py\n \"\"\"Implementation of Rule L033.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n \n \nclass Rule_L033(BaseCrawler):\n     \"\"\"UNION ALL is preferred over UNION.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L034.py b/src/sqlfluff/core/rules/std/L034.py\n \"\"\"Implementation of Rule L034.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L034(BaseCrawler):\n     \"\"\"Use wildcards then simple select targets before calculations and aggregates.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L035.py b/src/sqlfluff/core/rules/std/L035.py\n \"\"\"Implementation of Rule L035.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L035(BaseCrawler):\n     \"\"\"Do not specify \"else null\" in a case when statement (redundant).\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L036.py b/src/sqlfluff/core/rules/std/L036.py\n from typing import List, NamedTuple\n \n from sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n \n \n @document_fix_compatible\nclass Rule_L036(BaseCrawler):\n     \"\"\"Select targets should be on a new line unless there is only one select target.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L037.py b/src/sqlfluff/core/rules/std/L037.py\n \n from typing import NamedTuple, Optional, List\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.parser import BaseSegment\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n \n @document_fix_compatible\nclass Rule_L037(BaseCrawler):\n     \"\"\"Ambiguous ordering directions for columns in order by clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L038.py b/src/sqlfluff/core/rules/std/L038.py\n \"\"\"Implementation of Rule L038.\"\"\"\n \nfrom ..base import BaseCrawler, LintFix, LintResult\n from ..doc_decorators import document_fix_compatible, document_configuration\n \n \n @document_configuration\n @document_fix_compatible\nclass Rule_L038(BaseCrawler):\n     \"\"\"Trailing commas within select clause.\n \n     For some database backends this is allowed. For some users\ndiff --git a/src/sqlfluff/core/rules/std/L039.py b/src/sqlfluff/core/rules/std/L039.py\n \"\"\"Implementation of Rule L039.\"\"\"\n \nfrom ..base import BaseCrawler, LintFix, LintResult\n from ..doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L039(BaseCrawler):\n     \"\"\"Unnecessary whitespace found.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L041.py b/src/sqlfluff/core/rules/std/L041.py\n \"\"\"Implementation of Rule L040.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L041(BaseCrawler):\n     \"\"\"SELECT clause modifiers such as DISTINCT must be on the same line as SELECT.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L042.py b/src/sqlfluff/core/rules/std/L042.py\n \"\"\"Implementation of Rule L042.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n from sqlfluff.core.rules.doc_decorators import document_configuration\n \n \n @document_configuration\nclass Rule_L042(BaseCrawler):\n     \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\n \n     By default this rule is configured to allow subqueries within `FROM`\ndiff --git a/src/sqlfluff/core/rules/std/L043.py b/src/sqlfluff/core/rules/std/L043.py\n \"\"\"Implementation of Rule L043.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseCrawler, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L043(BaseCrawler):\n     \"\"\"Unnecessary case when statement. Use the \"when\" condition itself.\n \n     If a case when else statement returns booleans, we can reduce it to the\ndiff --git a/src/sqlfluff/core/rules/std/L044.py b/src/sqlfluff/core/rules/std/L044.py\n \n from sqlfluff.core.rules.analysis.select_crawler import SelectCrawler\n from sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n \n \n class RuleFailure(Exception):\n     pass\n \n \nclass Rule_L044(BaseCrawler):\n     \"\"\"Query produces an unknown number of result columns.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L045.py b/src/sqlfluff/core/rules/std/L045.py\n from typing import Dict, List\n \n from sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.rules.base import BaseCrawler, LintResult\n from sqlfluff.core.rules.analysis.select_crawler import SelectCrawler\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L045(BaseCrawler):\n     \"\"\"Query defines a CTE (common-table expression) but does not use it.\n \n     | **Anti-pattern**",
    "output": " \n from sqlfluff.core.plugin import hookimpl\n from sqlfluff.core.rules.base import (\n    BaseRule,\n     LintResult,\n )\n from sqlfluff.core.rules.doc_decorators import (\n \n \n @hookimpl\ndef get_rules() -> List[BaseRule]:\n     \"\"\"Get plugin rules.\"\"\"\n     return [Rule_Example_L001]\n \n # to be displayed in the sqlfluff docs\n @document_fix_compatible\n @document_configuration\nclass Rule_Example_L001(BaseRule):\n     \"\"\"ORDER BY on these columns is forbidden!\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/linter.py b/src/sqlfluff/core/linter.py\n from sqlfluff.core.parser.segments.base import BaseSegment, FixPatch\n from sqlfluff.core.parser.segments.meta import MetaSegment\n from sqlfluff.core.parser.segments.raw import RawSegment\nfrom sqlfluff.core.rules.base import BaseRule\n \n # Instantiate the linter logger\n linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n         # Store references to user rule classes\n         self.user_rules = user_rules or []\n \n    def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n         \"\"\"Get hold of a set of rules.\"\"\"\n         rs = get_ruleset()\n         # Register any user rules\ndiff --git a/src/sqlfluff/core/rules/base.py b/src/sqlfluff/core/rules/base.py\n\"\"\"Implements the base rule class.\n \nRules crawl through the trees returned by the parser and evaluate particular\nrules.\n \n The intent is that it should be possible for the rules to be expressed\n as simply as possible, with as much of the complexity abstracted away.\n \n \n class LintResult:\n    \"\"\"A class to hold the results of a rule evaluation.\n \n     Args:\n         anchor (:obj:`BaseSegment`, optional): A segment which represents\n             fixes which would correct this issue. If not present then it's\n             assumed that this issue will have to manually fixed.\n         memory (:obj:`dict`, optional): An object which stores any working\n            memory for the rule. The `memory` returned in any `LintResult`\n             will be passed as an input to the next segment to be crawled.\n         description (:obj:`str`, optional): A description of the problem\n             identified as part of this result. This will override the\n         return True\n \n \nclass BaseRule:\n    \"\"\"The base class for a rule.\n \n     Args:\n         code (:obj:`str`): The identifier for this rule, used in inclusion\n     def __init__(self, code, description, **kwargs):\n         self.description = description\n         self.code = code\n        # kwargs represents the config passed to the rule. Add all kwargs as class attributes\n         # so they can be accessed in rules which inherit from this class\n         for key, value in kwargs.items():\n             self.__dict__[key] = value\n         \"\"\"\n         # parent stack should be a tuple if it exists\n \n        # Rules should evaluate on segments FIRST, before evaluating on their\n         # children. They should also return a list of violations.\n \n         parent_stack = parent_stack or ()\n         .. code-block:: python\n \n            @myruleset.register\n           class Rule_L001(BaseRule):\n                \"Description of rule.\"\n \n                def eval(self, **kwargs):\n         for configuring the rules given the given config.\n \n         Returns:\n            :obj:`list` of instantiated :obj:`BaseRule`.\n \n         \"\"\"\n         # Validate all generic rule configs\ndiff --git a/src/sqlfluff/core/rules/std/L001.py b/src/sqlfluff/core/rules/std/L001.py\n \"\"\"Implementation of Rule L001.\"\"\"\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L001(BaseRule):\n     \"\"\"Unnecessary trailing whitespace.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L002.py b/src/sqlfluff/core/rules/std/L002.py\n \"\"\"Implementation of Rule L002.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_configuration,\n     document_fix_compatible,\n \n @document_configuration\n @document_fix_compatible\nclass Rule_L002(BaseRule):\n     \"\"\"Mixed Tabs and Spaces in single whitespace.\n \n     This rule will fail if a single section of whitespace\ndiff --git a/src/sqlfluff/core/rules/std/L003.py b/src/sqlfluff/core/rules/std/L003.py\n \"\"\"Implementation of Rule L003.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L003(BaseRule):\n     \"\"\"Indentation not consistent with previous lines.\n \n     Note:\ndiff --git a/src/sqlfluff/core/rules/std/L004.py b/src/sqlfluff/core/rules/std/L004.py\n \"\"\"Implementation of Rule L004.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L004(BaseRule):\n     \"\"\"Incorrect indentation type.\n \n     Note 1: spaces are only fixed to tabs if the number of spaces in the\ndiff --git a/src/sqlfluff/core/rules/std/L005.py b/src/sqlfluff/core/rules/std/L005.py\n \"\"\"Implementation of Rule L005.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L005(BaseRule):\n     \"\"\"Commas should not have whitespace directly before them.\n \n     Unless it's an indent. Trailing/leading commas are dealt with\ndiff --git a/src/sqlfluff/core/rules/std/L006.py b/src/sqlfluff/core/rules/std/L006.py\n \"\"\"Implementation of Rule L006.\"\"\"\n \n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L006(BaseRule):\n     \"\"\"Operators should be surrounded by a single whitespace.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L007.py b/src/sqlfluff/core/rules/std/L007.py\n \"\"\"Implementation of Rule L007.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n \n \nclass Rule_L007(BaseRule):\n     \"\"\"Operators near newlines should be after, not before the newline.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L008.py b/src/sqlfluff/core/rules/std/L008.py\n \"\"\"Implementation of Rule L008.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L008(BaseRule):\n     \"\"\"Commas should be followed by a single whitespace unless followed by a comment.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L009.py b/src/sqlfluff/core/rules/std/L009.py\n \"\"\"Implementation of Rule L009.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L009(BaseRule):\n     \"\"\"Files must end with a trailing newline.\"\"\"\n \n     def _eval(self, segment, siblings_post, parent_stack, **kwargs):\ndiff --git a/src/sqlfluff/core/rules/std/L010.py b/src/sqlfluff/core/rules/std/L010.py\n \"\"\"Implementation of Rule L010.\"\"\"\n \n from typing import Tuple, List\nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L010(BaseRule):\n     \"\"\"Inconsistent capitalisation of keywords.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L011.py b/src/sqlfluff/core/rules/std/L011.py\n \"\"\"Implementation of Rule L011.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult, LintFix\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L011(BaseRule):\n     \"\"\"Implicit aliasing of table not allowed. Use explicit `AS` clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L013.py b/src/sqlfluff/core/rules/std/L013.py\n \"\"\"Implementation of Rule L013.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n from sqlfluff.core.rules.doc_decorators import document_configuration\n \n \n @document_configuration\nclass Rule_L013(BaseRule):\n     \"\"\"Column expression without alias. Use explicit `AS` clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L015.py b/src/sqlfluff/core/rules/std/L015.py\n \"\"\"Implementation of Rule L015.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L015(BaseRule):\n     \"\"\"DISTINCT used with parentheses.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L017.py b/src/sqlfluff/core/rules/std/L017.py\n \"\"\"Implementation of Rule L017.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L017(BaseRule):\n     \"\"\"Function name not immediately followed by bracket.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L018.py b/src/sqlfluff/core/rules/std/L018.py\n \"\"\"Implementation of Rule L018.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L018(BaseRule):\n     \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L019.py b/src/sqlfluff/core/rules/std/L019.py\n \n from typing import Dict, Any\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import (\n     document_fix_compatible,\n     document_configuration,\n \n @document_fix_compatible\n @document_configuration\nclass Rule_L019(BaseRule):\n     \"\"\"Leading/Trailing comma enforcement.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L020.py b/src/sqlfluff/core/rules/std/L020.py\n \n import itertools\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n from sqlfluff.core.rules.analysis.select import get_select_statement_info\n \n \nclass Rule_L020(BaseRule):\n     \"\"\"Table aliases should be unique within each clause.\"\"\"\n \n     def _lint_references_and_aliases(\ndiff --git a/src/sqlfluff/core/rules/std/L021.py b/src/sqlfluff/core/rules/std/L021.py\n \"\"\"Implementation of Rule L021.\"\"\"\n \n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n \n \nclass Rule_L021(BaseRule):\n     \"\"\"Ambiguous use of DISTINCT in select statement with GROUP BY.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L022.py b/src/sqlfluff/core/rules/std/L022.py\n \"\"\"Implementation of Rule L022.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L022(BaseRule):\n     \"\"\"Blank line expected but not found after CTE closing bracket.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L023.py b/src/sqlfluff/core/rules/std/L023.py\n \n from typing import Optional, List\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L023(BaseRule):\n     \"\"\"Single whitespace expected after AS in WITH clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L029.py b/src/sqlfluff/core/rules/std/L029.py\n \"\"\"Implementation of Rule L029.\"\"\"\n \n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n from sqlfluff.core.rules.doc_decorators import document_configuration\n \n \n @document_configuration\nclass Rule_L029(BaseRule):\n     \"\"\"Keywords should not be used as identifiers.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L031.py b/src/sqlfluff/core/rules/std/L031.py\n \"\"\"Implementation of Rule L031.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L031(BaseRule):\n     \"\"\"Avoid table aliases in from clauses and join conditions.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L032.py b/src/sqlfluff/core/rules/std/L032.py\n \"\"\"Implementation of Rule L032.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n \n \nclass Rule_L032(BaseRule):\n     \"\"\"Prefer specifying join keys instead of using \"USING\".\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L033.py b/src/sqlfluff/core/rules/std/L033.py\n \"\"\"Implementation of Rule L033.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n \n \nclass Rule_L033(BaseRule):\n     \"\"\"UNION ALL is preferred over UNION.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L034.py b/src/sqlfluff/core/rules/std/L034.py\n \"\"\"Implementation of Rule L034.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L034(BaseRule):\n     \"\"\"Use wildcards then simple select targets before calculations and aggregates.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L035.py b/src/sqlfluff/core/rules/std/L035.py\n \"\"\"Implementation of Rule L035.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L035(BaseRule):\n     \"\"\"Do not specify \"else null\" in a case when statement (redundant).\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L036.py b/src/sqlfluff/core/rules/std/L036.py\n from typing import List, NamedTuple\n \n from sqlfluff.core.parser import BaseSegment\nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n \n \n @document_fix_compatible\nclass Rule_L036(BaseRule):\n     \"\"\"Select targets should be on a new line unless there is only one select target.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L037.py b/src/sqlfluff/core/rules/std/L037.py\n \n from typing import NamedTuple, Optional, List\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.parser import BaseSegment\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n \n @document_fix_compatible\nclass Rule_L037(BaseRule):\n     \"\"\"Ambiguous ordering directions for columns in order by clause.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L038.py b/src/sqlfluff/core/rules/std/L038.py\n \"\"\"Implementation of Rule L038.\"\"\"\n \nfrom ..base import BaseRule, LintFix, LintResult\n from ..doc_decorators import document_fix_compatible, document_configuration\n \n \n @document_configuration\n @document_fix_compatible\nclass Rule_L038(BaseRule):\n     \"\"\"Trailing commas within select clause.\n \n     For some database backends this is allowed. For some users\ndiff --git a/src/sqlfluff/core/rules/std/L039.py b/src/sqlfluff/core/rules/std/L039.py\n \"\"\"Implementation of Rule L039.\"\"\"\n \nfrom ..base import BaseRule, LintFix, LintResult\n from ..doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L039(BaseRule):\n     \"\"\"Unnecessary whitespace found.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L041.py b/src/sqlfluff/core/rules/std/L041.py\n \"\"\"Implementation of Rule L040.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L041(BaseRule):\n     \"\"\"SELECT clause modifiers such as DISTINCT must be on the same line as SELECT.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L042.py b/src/sqlfluff/core/rules/std/L042.py\n \"\"\"Implementation of Rule L042.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n from sqlfluff.core.rules.doc_decorators import document_configuration\n \n \n @document_configuration\nclass Rule_L042(BaseRule):\n     \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\n \n     By default this rule is configured to allow subqueries within `FROM`\ndiff --git a/src/sqlfluff/core/rules/std/L043.py b/src/sqlfluff/core/rules/std/L043.py\n \"\"\"Implementation of Rule L043.\"\"\"\n \nfrom sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L043(BaseRule):\n     \"\"\"Unnecessary case when statement. Use the \"when\" condition itself.\n \n     If a case when else statement returns booleans, we can reduce it to the\ndiff --git a/src/sqlfluff/core/rules/std/L044.py b/src/sqlfluff/core/rules/std/L044.py\n \n from sqlfluff.core.rules.analysis.select_crawler import SelectCrawler\n from sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n \n \n class RuleFailure(Exception):\n     pass\n \n \nclass Rule_L044(BaseRule):\n     \"\"\"Query produces an unknown number of result columns.\n \n     | **Anti-pattern**\ndiff --git a/src/sqlfluff/core/rules/std/L045.py b/src/sqlfluff/core/rules/std/L045.py\n from typing import Dict, List\n \n from sqlfluff.core.dialects.base import Dialect\nfrom sqlfluff.core.rules.base import BaseRule, LintResult\n from sqlfluff.core.rules.analysis.select_crawler import SelectCrawler\n from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n \n \n @document_fix_compatible\nclass Rule_L045(BaseRule):\n     \"\"\"Query defines a CTE (common-table expression) but does not use it.\n \n     | **Anti-pattern**"
  },
  {
    "instruction": "Deduplicate violations in the same position\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nWhen linting jinja files with loops we get multiple output violations for each time around the loop. e.g.\r\n\r\n```sql\r\nselect\r\n    a,\r\n    {% for val in [1, 2, 3, 4, 5, 6] %}\r\n        d+ {{ val }},\r\n    {% endfor %}\r\n    b\r\n```\r\n\r\nwe get\r\n\r\n```\r\n== [test.sql] FAIL\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   4 | P:  10 | L006 | Missing whitespace before +\r\nL:   7 | P:   1 | L001 | Unnecessary trailing whitespace.\r\n```\r\n\r\nThe duplicated `Missing whitespace` isn't helpful for the user. Regardless of whether we keep them in the background (perhaps we should), they shouldn't be shown to the user here because we're showing the same issue multiple times.\n\n### Use case\n\nCLI linting\n\n### Dialect\n\nall\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": " \"\"\"Errors - these are closely linked to what used to be called violations.\"\"\"\nfrom typing import Optional, Tuple, List\n \n CheckTuple = Tuple[str, int, int]\n \n             \"description\": self.desc(),\n         }\n \n     def ignore_if_in(self, ignore_iterable: List[str]):\n         \"\"\"Ignore this violation if it matches the iterable.\"\"\"\n         if self._identifier in ignore_iterable:\n             return True\n         return False\n \n    def check_tuple(self) -> CheckTuple:\n        \"\"\"Get a tuple representing this error. Mostly for testing.\"\"\"\n        return (\n            self.rule.code,\n            self.line_no,\n            self.line_pos,\n         )\n \n     def __repr__(self):\n         return \"<SQLLintError: rule {} pos:{!r}, #fixes: {}, description: {}>\".format(\ndiff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n         \"\"\"Make a list of check_tuples.\n \n         This assumes that all the violations found are\n        linting violations (and therefore implement `check_tuple()`).\n        If they don't then this function raises that error.\n         \"\"\"\n         vs: List[CheckTuple] = []\n         v: SQLLintError\n         for v in self.get_violations():\n            if hasattr(v, \"check_tuple\"):\n                 vs.append(v.check_tuple())\n             elif raise_on_non_linting_violations:\n                 raise v\n         return vs\n \n     def get_violations(\n         self,\n         rules: Optional[Union[str, Tuple[str, ...]]] = None,\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n \n         linted_file = LintedFile(\n             parsed.fname,\n            violations,\n             time_dict,\n             tree,\n             ignore_mask=ignore_buff,",
    "output": " \"\"\"Errors - these are closely linked to what used to be called violations.\"\"\"\nfrom typing import Optional, Tuple, Any, List\n \n CheckTuple = Tuple[str, int, int]\n \n             \"description\": self.desc(),\n         }\n \n    def check_tuple(self) -> CheckTuple:\n        \"\"\"Get a tuple representing this error. Mostly for testing.\"\"\"\n        return (\n            self.rule_code(),\n            self.line_no,\n            self.line_pos,\n        )\n\n    def source_signature(self) -> Tuple[Any, ...]:\n        \"\"\"Return hashable source signature for deduplication.\"\"\"\n        return (self.check_tuple(), self.desc())\n\n     def ignore_if_in(self, ignore_iterable: List[str]):\n         \"\"\"Ignore this violation if it matches the iterable.\"\"\"\n         if self._identifier in ignore_iterable:\n             return True\n         return False\n \n    def source_signature(self) -> Tuple[Any, ...]:\n        \"\"\"Return hashable source signature for deduplication.\n\n        For linting errors we need to dedupe on more than just location and\n        description, we also need to check the edits potentially made, both\n        in the templated file but also in the source.\n        \"\"\"\n        fix_raws = tuple(\n            tuple(e.raw for e in f.edit) if f.edit else None for f in self.fixes\n        )\n        source_fixes = tuple(\n            tuple(tuple(e.source_fixes) for e in f.edit) if f.edit else None\n            for f in self.fixes\n         )\n        return (self.check_tuple(), self.description, fix_raws, source_fixes)\n \n     def __repr__(self):\n         return \"<SQLLintError: rule {} pos:{!r}, #fixes: {}, description: {}>\".format(\ndiff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py\n         \"\"\"Make a list of check_tuples.\n \n         This assumes that all the violations found are\n        linting violations. If they don't then this function\n        raises that error.\n         \"\"\"\n         vs: List[CheckTuple] = []\n         v: SQLLintError\n         for v in self.get_violations():\n            if isinstance(v, SQLLintError):\n                 vs.append(v.check_tuple())\n             elif raise_on_non_linting_violations:\n                 raise v\n         return vs\n \n    @staticmethod\n    def deduplicate_in_source_space(\n        violations: List[SQLBaseError],\n    ) -> List[SQLBaseError]:\n        \"\"\"Removes duplicates in the source space.\n\n        This is useful for templated files with loops, where we'll\n        get a violation for each pass around the loop, but the user\n        only cares about it once and we're only going to fix it once.\n\n        By filtering them early we get a more a more helpful CLI\n        output *and* and more efficient fixing routine (by handling\n        fewer fixes).\n        \"\"\"\n        new_violations = []\n        dedupe_buffer = set()\n        for v in violations:\n            signature = v.source_signature()\n            if signature not in dedupe_buffer:\n                new_violations.append(v)\n                dedupe_buffer.add(signature)\n            else:\n                linter_logger.debug(\"Removing duplicate source violation: %s\", v)\n        return new_violations\n\n     def get_violations(\n         self,\n         rules: Optional[Union[str, Tuple[str, ...]]] = None,\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n \n         linted_file = LintedFile(\n             parsed.fname,\n            # Deduplicate violations\n            LintedFile.deduplicate_in_source_space(violations),\n             time_dict,\n             tree,\n             ignore_mask=ignore_buff,"
  },
  {
    "instruction": "Running `lint` on an empty file fails with critical Exception\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nThis is a bit of an odd one. When running `sqlfluff lint` on an empty file it fails with (Python) exception.\r\n\r\nWhile trying to lint empty file is probably not the main use-case for SQLFluff I still consider this somewhat relevant, when applying SQLFluff in a dynamic code base. \n\n### Expected Behaviour\n\nI'm not entirely sure what the correct result is. Feasible option are\r\n\r\n- Passing\r\n- Raise some kind of lint error (but not a critical exception)\r\n\r\nMy personal take is that lint should pass, which (I think) is similar behaviour to other linters.\n\n### Observed Behaviour\n\n`LT01` and `LT12` with an critical Exception\r\n\r\n```\r\nCRITICAL   [LT01] Applying rule LT01 to 'stdin' threw an Exception: ReflowSequence has empty elements.\r\nCRITICAL   [LT12] Applying rule LT12 to 'stdin' threw an Exception: tuple index out of range\r\n```\r\n\n\n### How to reproduce\n\n```sh\r\ncat /dev/null | sqlfluff lint --dialect ansi -\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\nlatest main branch\r\n\r\n```\r\ngit rev-parse HEAD\r\nd19de0ecd16d298f9e3bfb91da122734c40c01e5\r\n```\n\n### Configuration\n\ndefault\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\nRunning `lint` on an empty file fails with critical Exception\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nThis is a bit of an odd one. When running `sqlfluff lint` on an empty file it fails with (Python) exception.\r\n\r\nWhile trying to lint empty file is probably not the main use-case for SQLFluff I still consider this somewhat relevant, when applying SQLFluff in a dynamic code base. \n\n### Expected Behaviour\n\nI'm not entirely sure what the correct result is. Feasible option are\r\n\r\n- Passing\r\n- Raise some kind of lint error (but not a critical exception)\r\n\r\nMy personal take is that lint should pass, which (I think) is similar behaviour to other linters.\n\n### Observed Behaviour\n\n`LT01` and `LT12` with an critical Exception\r\n\r\n```\r\nCRITICAL   [LT01] Applying rule LT01 to 'stdin' threw an Exception: ReflowSequence has empty elements.\r\nCRITICAL   [LT12] Applying rule LT12 to 'stdin' threw an Exception: tuple index out of range\r\n```\r\n\n\n### How to reproduce\n\n```sh\r\ncat /dev/null | sqlfluff lint --dialect ansi -\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\nlatest main branch\r\n\r\n```\r\ngit rev-parse HEAD\r\nd19de0ecd16d298f9e3bfb91da122734c40c01e5\r\n```\n\n### Configuration\n\ndefault\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "         # We only care about the final segment of the parse tree.\n         parent_stack, segment = get_last_segment(FunctionalContext(context).segment)\n         self.logger.debug(\"Found last segment as: %s\", segment)\n \n         trailing_newlines = Segments(*get_trailing_newlines(context.segment))\n         trailing_literal_newlines = trailing_newlines\ndiff --git a/src/sqlfluff/utils/reflow/sequence.py b/src/sqlfluff/utils/reflow/sequence.py\n \n     @staticmethod\n     def _validate_reflow_sequence(elements: ReflowSequenceType):\n        assert elements, \"ReflowSequence has empty elements.\"\n         # Check odds and evens\n         OddType = elements[0].__class__\n         EvenType = ReflowPoint if OddType is ReflowBlock else ReflowBlock",
    "output": "         # We only care about the final segment of the parse tree.\n         parent_stack, segment = get_last_segment(FunctionalContext(context).segment)\n         self.logger.debug(\"Found last segment as: %s\", segment)\n        if not segment:\n            # NOTE: Edge case. If the file is totally empty, we won't find a final\n            # segment. In this case return without error.\n            return None\n \n         trailing_newlines = Segments(*get_trailing_newlines(context.segment))\n         trailing_literal_newlines = trailing_newlines\ndiff --git a/src/sqlfluff/utils/reflow/sequence.py b/src/sqlfluff/utils/reflow/sequence.py\n \n     @staticmethod\n     def _validate_reflow_sequence(elements: ReflowSequenceType):\n        # An empty set of elements _is_ allowed as an edge case.\n        if not elements:\n            # Return early if so\n            return None\n         # Check odds and evens\n         OddType = elements[0].__class__\n         EvenType = ReflowPoint if OddType is ReflowBlock else ReflowBlock"
  },
  {
    "instruction": "[EXPERIMENT]: Rethink Matching routines\nThis is another experiment, and also a biggie. It's a rethink of matching as part of #5124.\r\n\r\nThis will need some tidying to get it into a state that it's reviewable, but given the scale of it - I think I shouldn't take it much further without getting some of it merged.\r\n\r\nIt's mostly additions for now, so I now need to strip out the things that we can get rid of as a result. Opening PR for testing and in particular for coverage.\n",
    "input": " \"\"\"Base grammar, Ref, Anything and Nothing.\"\"\"\n \n import copy\nfrom dataclasses import dataclass\n from typing import (\n     TYPE_CHECKING,\n     Any,\n     Tuple,\n     TypeVar,\n     Union,\n    cast,\n )\n from uuid import UUID, uuid4\n \nfrom sqlfluff.core.errors import SQLParseError\n from sqlfluff.core.parser.context import ParseContext\n from sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.match_logging import (\n    LateBoundJoinSegmentsCurtailed,\n    parse_match_logging,\n)\n from sqlfluff.core.parser.match_result import MatchResult\n from sqlfluff.core.parser.match_wrapper import match_wrapper\n from sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment, BracketedSegment, allow_ephemeral\n from sqlfluff.core.parser.types import MatchableType, SimpleHintType\n from sqlfluff.core.string_helpers import curtail_string\n \n     from sqlfluff.core.dialects.base import Dialect\n \n \ndef first_trimmed_raw(seg: BaseSegment) -> str:\n    \"\"\"Trim whitespace off a whole element raw.\n\n    Used as a helper function in BaseGrammar._look_ahead_match.\n\n    For existing compound segments, we should assume that within\n    that segment, things are internally consistent, that means\n    rather than enumerating all the individual segments of a longer\n    one we just dump out the whole segment, but splitting off the\n    first element separated by whitespace. This is a) faster and\n    also b) prevents some really horrible bugs with bracket matching.\n    See https://github.com/sqlfluff/sqlfluff/issues/433\n\n    This fetches the _whole_ raw of a potentially compound segment\n    to match against, trimming off any whitespace. This is the\n    most efficient way to get at the first element of a potentially\n    longer segment.\n    \"\"\"\n    s = seg.raw_upper.split(maxsplit=1)\n    return s[0] if s else \"\"\n\n\n@dataclass\nclass BracketInfo:\n    \"\"\"BracketInfo tuple for keeping track of brackets during matching.\n\n    This is used in BaseGrammar._bracket_sensitive_look_ahead_match but\n    defined here for type checking.\n    \"\"\"\n\n    bracket: BaseSegment\n    segments: Tuple[BaseSegment, ...]\n\n    def to_segment(self, end_bracket: Tuple[BaseSegment, ...]) -> BracketedSegment:\n        \"\"\"Turn the contained segments into a bracketed segment.\"\"\"\n        assert len(end_bracket) == 1\n        return BracketedSegment(\n            segments=self.segments,\n            start_bracket=(self.bracket,),\n            end_bracket=cast(Tuple[BaseSegment], end_bracket),\n        )\n\n\n def cached_method_for_parse_context(\n     func: Callable[[Any, ParseContext, Optional[Tuple[str]]], SimpleHintType]\n ) -> Callable[..., SimpleHintType]:\n         # If no match at all, return nothing\n         return MatchResult.from_unmatched(segments), None\n \n    @classmethod\n    def _look_ahead_match(\n        cls,\n        segments: Tuple[BaseSegment, ...],\n        matchers: List[MatchableType],\n        parse_context: ParseContext,\n    ) -> Tuple[Tuple[BaseSegment, ...], MatchResult, Optional[MatchableType]]:\n        \"\"\"Look ahead for matches beyond the first element of the segments list.\n\n        This function also contains the performance improved hash-matching approach to\n        searching for matches, which should significantly improve performance.\n\n        Prioritise the first match, and if multiple match at the same point the longest.\n        If two matches of the same length match at the same time, then it's the first in\n        the iterable of matchers.\n\n        Returns:\n            `tuple` of (unmatched_segments, match_object, matcher).\n\n        \"\"\"\n        parse_match_logging(\n            cls.__name__,\n            \"_look_ahead_match\",\n            \"IN\",\n            parse_context=parse_context,\n            v_level=4,\n            ls=len(segments),\n            seg=LateBoundJoinSegmentsCurtailed(segments),\n        )\n\n        # Have we been passed an empty tuple?\n        if not segments:  # pragma: no cover TODO?\n            return ((), MatchResult.from_empty(), None)\n\n        # Here we enable a performance optimisation. Most of the time in this cycle\n        # happens in loops looking for simple matchers which we should\n        # be able to find a shortcut for.\n\n        parse_match_logging(\n            cls.__name__,\n            \"_look_ahead_match\",\n            \"SI\",\n            parse_context=parse_context,\n            v_level=4,\n        )\n\n        best_simple_match = None\n        simple_match = None\n        for idx, seg in enumerate(segments):\n            trimmed_seg = first_trimmed_raw(seg)\n            for matcher in matchers:\n                simple_match = None\n                simple = matcher.simple(parse_context=parse_context)\n                if not simple:  # pragma: no cover\n                    # NOTE: For all bundled dialects, this clause is true, but until\n                    # the RegexMatcher is completely deprecated (and therefore that\n                    # `.simple()` must provide a result), it is still _possible_\n                    # to end up here.\n                    raise NotImplementedError(\n                        \"All matchers passed to `._look_ahead_match()` are \"\n                        \"assumed to have a functioning `.simple()` option. \"\n                        \"In a future release it will be compulsory for _all_ \"\n                        \"matchables to implement `.simple()`. Please report \"\n                        \"this as a bug on GitHub along with your current query \"\n                        f\"and dialect.\\nProblematic matcher: {matcher}\"\n                    )\n                simple_raws, simple_types = simple\n\n                assert simple_raws or simple_types\n                if simple_raws:\n                    if trimmed_seg in simple_raws:\n                        simple_match = matcher\n\n                if simple_types and not simple_match:\n                    intersection = simple_types.intersection(seg.class_types)\n                    if intersection:\n                        simple_match = matcher\n\n                # If we couldn't achieve a simple match, move on to the next option.\n                if not simple_match:\n                    continue\n\n                # If there is, check the full version matches. If it doesn't\n                # then discount it and move on.\n                match = simple_match.match(segments[idx:], parse_context)\n                if not match:\n                    continue\n\n                best_simple_match = (\n                    segments[:idx],\n                    match,\n                    simple_match,\n                )\n                # Stop looking through matchers\n                break\n\n            # If we have a valid match, stop looking through segments\n            if best_simple_match:\n                break\n\n        # There are no other matchers, we can just shortcut now. Either with\n        # no match, or the best one we found (if we found one).\n        parse_match_logging(\n            cls.__name__,\n            \"_look_ahead_match\",\n            \"SC\",\n            parse_context=parse_context,\n            v_level=4,\n            bsm=None\n            if not best_simple_match\n            else (\n                len(best_simple_match[0]),\n                len(best_simple_match[1]),\n                best_simple_match[2],\n            ),\n        )\n\n        if best_simple_match:\n            return best_simple_match\n        else:\n            return ((), MatchResult.from_unmatched(segments), None)\n\n    @classmethod\n    def _bracket_sensitive_look_ahead_match(\n        cls,\n        segments: Tuple[BaseSegment, ...],\n        matchers: List[MatchableType],\n        parse_context: ParseContext,\n        start_bracket: Optional[MatchableType] = None,\n        end_bracket: Optional[MatchableType] = None,\n        bracket_pairs_set: str = \"bracket_pairs\",\n    ) -> Tuple[Tuple[BaseSegment, ...], MatchResult, Optional[MatchableType]]:\n        \"\"\"Same as `_look_ahead_match` but with bracket counting.\n\n        NB: Given we depend on `_look_ahead_match` we can also utilise\n        the same performance optimisations which are implemented there.\n\n        bracket_pairs_set: Allows specific segments to override the available\n            bracket pairs. See the definition of \"angle_bracket_pairs\" in the\n            BigQuery dialect for additional context on why this exists.\n\n        Returns:\n            `tuple` of (unmatched_segments, match_object, matcher).\n\n        \"\"\"\n        # Have we been passed an empty tuple?\n        if not segments:\n            return ((), MatchResult.from_unmatched(segments), None)\n\n        # Get hold of the bracket matchers from the dialect, and append them\n        # to the list of matchers. We get them from the relevant set on the\n        # dialect. We use zip twice to \"unzip\" them. We ignore the first\n        # argument because that's just the name.\n        _, start_bracket_refs, end_bracket_refs, persists = zip(\n            *parse_context.dialect.bracket_sets(bracket_pairs_set)\n        )\n        # These are matchables, probably StringParsers.\n        start_brackets = [\n            parse_context.dialect.ref(seg_ref) for seg_ref in start_bracket_refs\n        ]\n        end_brackets = [\n            parse_context.dialect.ref(seg_ref) for seg_ref in end_bracket_refs\n        ]\n        # Add any bracket-like things passed as arguments\n        if start_bracket:\n            start_brackets += [start_bracket]\n        if end_bracket:\n            end_brackets += [end_bracket]\n        bracket_matchers = start_brackets + end_brackets\n\n        # Make some buffers\n        seg_buff: Tuple[BaseSegment, ...] = segments\n        pre_seg_buff: Tuple[BaseSegment, ...] = ()\n        bracket_stack: List[BracketInfo] = []\n\n        # Iterate\n        while True:\n            # Do we have anything left to match on?\n            if seg_buff:\n                # Yes we have buffer left to work with.\n                # Are we already in a bracket stack?\n                if bracket_stack:\n                    # Yes, we're just looking for the closing bracket, or\n                    # another opening bracket.\n                    pre, match, matcher = cls._look_ahead_match(\n                        seg_buff,\n                        bracket_matchers,\n                        parse_context=parse_context,\n                    )\n\n                    if match:\n                        # NB: We can only consider this as a nested bracket if the start\n                        # and end tokens are not the same. If a matcher is both a start\n                        # and end token we cannot deepen the bracket stack. In general,\n                        # quoted strings are a typical example where the start and end\n                        # tokens are the same. Currently, though, quoted strings are\n                        # handled elsewhere in the parser, and there are no cases where\n                        # *this* code has to handle identical start and end brackets.\n                        # For now, consider this a small, speculative investment in a\n                        # possible future requirement.\n                        if matcher in start_brackets and matcher not in end_brackets:\n                            # Add any segments leading up to this to the previous\n                            # bracket.\n                            bracket_stack[-1].segments += pre\n                            # Add a bracket to the stack and add the matches from the\n                            # segment.\n                            bracket_stack.append(\n                                BracketInfo(\n                                    bracket=match.matched_segments[0],\n                                    segments=match.matched_segments,\n                                )\n                            )\n                            seg_buff = match.unmatched_segments\n                            continue\n                        elif matcher in end_brackets:\n                            # Found an end bracket. Does its type match that of\n                            # the innermost start bracket? E.g. \")\" matches \"(\",\n                            # \"]\" matches \"[\".\n                            # For the start bracket we don't have the matcher\n                            # but we can work out the type, so we use that for\n                            # the lookup.\n                            start_index = [\n                                bracket.type for bracket in start_brackets\n                            ].index(bracket_stack[-1].bracket.get_type())\n                            # For the end index, we can just look for the matcher\n                            end_index = end_brackets.index(matcher)\n                            bracket_types_match = start_index == end_index\n                            if bracket_types_match:\n                                # Yes, the types match. So we've found a\n                                # matching end bracket. Pop the stack, construct\n                                # a bracketed segment and carry\n                                # on.\n\n                                # Complete the bracketed info\n                                bracket_stack[-1].segments += (\n                                    pre + match.matched_segments\n                                )\n                                # Construct a bracketed segment (as a tuple) if allowed.\n                                persist_bracket = persists[end_brackets.index(matcher)]\n                                if persist_bracket:\n                                    new_segments: Tuple[BaseSegment, ...] = (\n                                        bracket_stack[-1].to_segment(\n                                            end_bracket=match.matched_segments\n                                        ),\n                                    )\n                                else:\n                                    new_segments = bracket_stack[-1].segments\n                                # Remove the bracket set from the stack\n                                bracket_stack.pop()\n                                # If we're still in a bracket, add the new segments to\n                                # that bracket, otherwise add them to the buffer\n                                if bracket_stack:\n                                    bracket_stack[-1].segments += new_segments\n                                else:\n                                    pre_seg_buff += new_segments\n                                seg_buff = match.unmatched_segments\n                                continue\n                            else:\n                                # The types don't match. Error.\n                                raise SQLParseError(\n                                    f\"Found unexpected end bracket!, \"\n                                    f\"was expecting \"\n                                    f\"{end_brackets[start_index]}, \"\n                                    f\"but got {matcher}\",\n                                    segment=match.matched_segments[0],\n                                )\n\n                        else:  # pragma: no cover\n                            raise RuntimeError(\"I don't know how we get here?!\")\n                    else:  # pragma: no cover\n                        # No match, we're in a bracket stack. Error.\n                        raise SQLParseError(\n                            \"Couldn't find closing bracket for opening bracket.\",\n                            segment=bracket_stack[-1].bracket,\n                        )\n                else:\n                    # No, we're open to more opening brackets or the thing(s)\n                    # that we're otherwise looking for.\n                    pre, match, matcher = cls._look_ahead_match(\n                        seg_buff,\n                        matchers + bracket_matchers,\n                        parse_context=parse_context,\n                    )\n\n                    if match:\n                        if matcher in matchers:\n                            # It's one of the things we were looking for!\n                            # Return.\n                            return (pre_seg_buff + pre, match, matcher)\n                        elif matcher in start_brackets:\n                            # We've found the start of a bracket segment.\n                            # NB: It might not *actually* be the bracket itself,\n                            # but could be some non-code element preceding it.\n                            # That's actually ok.\n\n                            # Add the bracket to the stack.\n                            bracket_stack.append(\n                                BracketInfo(\n                                    bracket=match.matched_segments[0],\n                                    segments=match.matched_segments,\n                                )\n                            )\n                            # The matched element has already been added to the bracket.\n                            # Add anything before it to the pre segment buffer.\n                            # Reset the working buffer.\n                            pre_seg_buff += pre\n                            seg_buff = match.unmatched_segments\n                            continue\n                        elif matcher in end_brackets:\n                            # We've found an unexpected end bracket! This is likely\n                            # because we're matching a section which should have ended.\n                            # If we had a match, it would have matched by now, so this\n                            # means no match.\n                            parse_match_logging(\n                                cls.__name__,\n                                \"_bracket_sensitive_look_ahead_match\",\n                                \"UEXB\",\n                                parse_context=parse_context,\n                                v_level=3,\n                                got=matcher,\n                            )\n                            # From here we'll drop out to the happy unmatched exit.\n                        else:  # pragma: no cover\n                            # This shouldn't happen!?\n                            raise NotImplementedError(\n                                \"This shouldn't happen. Panic in \"\n                                \"_bracket_sensitive_look_ahead_match.\"\n                            )\n                    # Not in a bracket stack, but no match.\n                    # From here we'll drop out to the happy unmatched exit.\n            else:\n                # No we're at the end:\n                # Now check have we closed all our brackets?\n                if bracket_stack:  # pragma: no cover\n                    # No we haven't.\n                    raise SQLParseError(\n                        \"Couldn't find closing bracket for opened brackets: \"\n                        f\"`{bracket_stack}`.\",\n                        segment=bracket_stack[-1].bracket,\n                    )\n\n            # This is the happy unmatched path. This occurs when:\n            # - We reached the end with no open brackets.\n            # - No match while outside a bracket stack.\n            # - We found an unexpected end bracket before matching something\n            # interesting. We return with the mutated segments so we can reuse any\n            # bracket matching.\n            return ((), MatchResult.from_unmatched(pre_seg_buff + seg_buff), None)\n\n     def __str__(self) -> str:  # pragma: no cover TODO?\n         return repr(self)\n \n \n     def __repr__(self) -> str:\n         return \"<Ref: {}{}>\".format(\n            str(self._ref), \" [opt]\" if self.is_optional() else \"\"\n         )\n \n     @match_wrapper(v_level=4)  # Log less for Ref\ndiff --git a/src/sqlfluff/core/parser/grammar/greedy.py b/src/sqlfluff/core/parser/grammar/greedy.py\n     BaseSegment,\n     cached_method_for_parse_context,\n )\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments\n from sqlfluff.core.parser.match_result import MatchResult\n from sqlfluff.core.parser.match_wrapper import match_wrapper\n from sqlfluff.core.parser.segments import allow_ephemeral\n         self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n     ) -> MatchResult:\n         \"\"\"Matching for GreedyUntil works just how you'd expect.\"\"\"\n        return self.greedy_match(\n             segments,\n             parse_context,\n             matchers=self._elements,\n             include_terminator=False,\n         )\n \n    @classmethod\n    def greedy_match(\n        cls,\n        segments: Tuple[BaseSegment, ...],\n        parse_context: ParseContext,\n        matchers: Sequence[MatchableType],\n        include_terminator: bool = False,\n    ) -> MatchResult:\n        \"\"\"Matching for GreedyUntil works just how you'd expect.\"\"\"\n        seg_buff = segments\n        seg_bank: Tuple[BaseSegment, ...] = ()  # Empty tuple\n\n        while True:\n            with parse_context.deeper_match(name=\"GreedyUntil\") as ctx:\n                pre, mat, matcher = cls._bracket_sensitive_look_ahead_match(\n                    seg_buff, list(matchers), parse_context=ctx\n                )\n\n            if not mat:\n                # No terminator match? Return everything\n                return MatchResult.from_matched(segments)\n\n            # NOTE: For some terminators we only count them if they're preceded\n            # by whitespace, and others we don't. In principle, we aim that for\n            # _keywords_ we require whitespace, and for symbols we don't.\n            # We do this by looking at the `simple` method of the returned\n            # matcher, and if it's entirely alphabetical (as defined by\n            # str.isalpha()) then we infer that it's a keyword, and therefore\n            # _does_ require whitespace before it.\n            assert matcher, f\"Match without matcher: {mat}\"\n            _simple = matcher.simple(parse_context)\n            assert _simple, f\"Terminators require a simple method: {matcher}\"\n            _strings, _types = _simple\n            # NOTE: Typed matchers aren't common here, but we assume that they\n            # _don't_ require preceding whitespace.\n            # Do we need to enforce whitespace preceding?\n            if all(_s.isalpha() for _s in _strings) and not _types:\n                # Does the match include some whitespace already?\n                # Work forward\n                idx = 0\n                while True:\n                    elem = mat.matched_segments[idx]\n                    if elem.is_meta:  # pragma: no cover TODO?\n                        idx += 1\n                        continue\n                    elif elem.is_type(\n                        \"whitespace\", \"newline\"\n                    ):  # pragma: no cover TODO?\n                        allowable_match = True\n                        break\n                    else:\n                        # No whitespace before. Not allowed.\n                        allowable_match = False\n                        break\n\n                # If we're not ok yet, work backward to the preceding sections.\n                if not allowable_match:\n                    idx = -1\n                    while True:\n                        if len(pre) < abs(idx):  # pragma: no cover TODO?\n                            # If we're at the start, it's ok\n                            allowable_match = True\n                            break\n                        if pre[idx].is_meta:  # pragma: no cover TODO?\n                            idx -= 1\n                            continue\n                        elif pre[idx].is_type(\"whitespace\", \"newline\"):\n                            allowable_match = True\n                            break\n                        else:\n                            # No whitespace before. Not allowed.\n                            allowable_match = False\n                            break\n\n                # If this match isn't preceded by whitespace and that is\n                # a requirement, then we can't use it. Carry on...\n                if not allowable_match:\n                    # Update our buffers and continue onward\n                    seg_bank = seg_bank + pre + mat.matched_segments\n                    seg_buff = mat.unmatched_segments\n                    # Loop around, don't return yet\n                    continue\n\n            # Return everything up to the match unless it's a gap matcher.\n            if include_terminator:\n                return MatchResult(\n                    seg_bank + pre + mat.matched_segments,\n                    mat.unmatched_segments,\n                )\n\n            # We can't claim any non-code segments, so we trim them off the end.\n            leading_nc, pre_seg_mid, trailing_nc = trim_non_code_segments(\n                seg_bank + pre\n            )\n            return MatchResult(\n                leading_nc + pre_seg_mid,\n                trailing_nc + mat.all_segments(),\n            )\n\n \n T = TypeVar(\"T\", bound=\"StartsWith\")\n \n         # with, then we can still used the unmatched parts on the end.\n         # We still need to deal with any non-code segments at the start.\n         assert self.terminators\n        greedy_match = self.greedy_match(\n             match.unmatched_segments,\n             parse_context,\n             # We match up to the terminators for this segment, but _also_\n \n         # NB: If all we matched in the greedy match was non-code then we can't\n         # claim it.\n        if not any(seg.is_code for seg in greedy_match.matched_segments):\n             # So just return the original match.\n             return match\n \n         # Otherwise Combine the results.\n         return MatchResult(\n            match.matched_segments + greedy_match.matched_segments,\n            greedy_match.unmatched_segments,\n         )\ndiff --git a/src/sqlfluff/core/parser/grammar/sequence.py b/src/sqlfluff/core/parser/grammar/sequence.py\n )\n from sqlfluff.core.parser.grammar.conditional import Conditional\n from sqlfluff.core.parser.helpers import check_still_complete, trim_non_code_segments\n from sqlfluff.core.parser.match_result import MatchResult\n from sqlfluff.core.parser.match_wrapper import match_wrapper\n from sqlfluff.core.parser.matchable import Matchable\n             with parse_context.deeper_match(\n                 name=\"Bracketed-End\", clear_terminators=True\n             ) as ctx:\n                content_segs, end_match, _ = self._bracket_sensitive_look_ahead_match(\n                     segments=seg_buff,\n                     matchers=[end_bracket],\n                     parse_context=ctx,\ndiff --git a/src/sqlfluff/core/parser/match_algorithms.py b/src/sqlfluff/core/parser/match_algorithms.py\nnew file mode 100644\ndiff --git a/src/sqlfluff/core/parser/match_logging.py b/src/sqlfluff/core/parser/match_logging.py\n \"\"\"Classes to help with match logging.\"\"\"\n \n import logging\nfrom typing import TYPE_CHECKING, Any, Tuple\n\nfrom sqlfluff.core.parser.helpers import join_segments_raw_curtailed\n \n if TYPE_CHECKING:  # pragma: no cover\n    from sqlfluff.core.parser import BaseSegment\n     from sqlfluff.core.parser.context import ParseContext\n \n \n     ParseMatchLogObject(\n         parse_context, grammar, func, msg, v_level=v_level, **kwargs\n     ).log()\n\n\nclass LateBoundJoinSegmentsCurtailed:\n    \"\"\"Object to delay `join_segments_raw_curtailed` until later.\n\n    This allows us to defer the string manipulation involved\n    until actually required by the logger.\n    \"\"\"\n\n    def __init__(self, segments: Tuple[\"BaseSegment\", ...]) -> None:\n        self.segments = segments\n\n    def __str__(self) -> str:\n        return repr(join_segments_raw_curtailed(self.segments))",
    "output": " \"\"\"Base grammar, Ref, Anything and Nothing.\"\"\"\n \n import copy\n from typing import (\n     TYPE_CHECKING,\n     Any,\n     Tuple,\n     TypeVar,\n     Union,\n )\n from uuid import UUID, uuid4\n \n from sqlfluff.core.parser.context import ParseContext\n from sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.match_logging import parse_match_logging\n from sqlfluff.core.parser.match_result import MatchResult\n from sqlfluff.core.parser.match_wrapper import match_wrapper\n from sqlfluff.core.parser.matchable import Matchable\nfrom sqlfluff.core.parser.segments import BaseSegment, allow_ephemeral\n from sqlfluff.core.parser.types import MatchableType, SimpleHintType\n from sqlfluff.core.string_helpers import curtail_string\n \n     from sqlfluff.core.dialects.base import Dialect\n \n \n def cached_method_for_parse_context(\n     func: Callable[[Any, ParseContext, Optional[Tuple[str]]], SimpleHintType]\n ) -> Callable[..., SimpleHintType]:\n         # If no match at all, return nothing\n         return MatchResult.from_unmatched(segments), None\n \n     def __str__(self) -> str:  # pragma: no cover TODO?\n         return repr(self)\n \n \n     def __repr__(self) -> str:\n         return \"<Ref: {}{}>\".format(\n            repr(self._ref), \" [opt]\" if self.is_optional() else \"\"\n         )\n \n     @match_wrapper(v_level=4)  # Log less for Ref\ndiff --git a/src/sqlfluff/core/parser/grammar/greedy.py b/src/sqlfluff/core/parser/grammar/greedy.py\n     BaseSegment,\n     cached_method_for_parse_context,\n )\nfrom sqlfluff.core.parser.match_algorithms import greedy_match\n from sqlfluff.core.parser.match_result import MatchResult\n from sqlfluff.core.parser.match_wrapper import match_wrapper\n from sqlfluff.core.parser.segments import allow_ephemeral\n         self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n     ) -> MatchResult:\n         \"\"\"Matching for GreedyUntil works just how you'd expect.\"\"\"\n        return greedy_match(\n             segments,\n             parse_context,\n             matchers=self._elements,\n             include_terminator=False,\n         )\n \n \n T = TypeVar(\"T\", bound=\"StartsWith\")\n \n         # with, then we can still used the unmatched parts on the end.\n         # We still need to deal with any non-code segments at the start.\n         assert self.terminators\n        greedy_matched = greedy_match(\n             match.unmatched_segments,\n             parse_context,\n             # We match up to the terminators for this segment, but _also_\n \n         # NB: If all we matched in the greedy match was non-code then we can't\n         # claim it.\n        if not any(seg.is_code for seg in greedy_matched.matched_segments):\n             # So just return the original match.\n             return match\n \n         # Otherwise Combine the results.\n         return MatchResult(\n            match.matched_segments + greedy_matched.matched_segments,\n            greedy_matched.unmatched_segments,\n         )\ndiff --git a/src/sqlfluff/core/parser/grammar/sequence.py b/src/sqlfluff/core/parser/grammar/sequence.py\n )\n from sqlfluff.core.parser.grammar.conditional import Conditional\n from sqlfluff.core.parser.helpers import check_still_complete, trim_non_code_segments\nfrom sqlfluff.core.parser.match_algorithms import bracket_sensitive_look_ahead_match\n from sqlfluff.core.parser.match_result import MatchResult\n from sqlfluff.core.parser.match_wrapper import match_wrapper\n from sqlfluff.core.parser.matchable import Matchable\n             with parse_context.deeper_match(\n                 name=\"Bracketed-End\", clear_terminators=True\n             ) as ctx:\n                content_segs, end_match, _ = bracket_sensitive_look_ahead_match(\n                     segments=seg_buff,\n                     matchers=[end_bracket],\n                     parse_context=ctx,\ndiff --git a/src/sqlfluff/core/parser/match_algorithms.py b/src/sqlfluff/core/parser/match_algorithms.py\nnew file mode 100644\n\"\"\"Matching algorithms.\n\nThese are mostly extracted from the body of either BaseSegment\nor BaseGrammar to un-bloat those classes.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Sequence, Tuple, cast\n\nfrom sqlfluff.core.errors import SQLParseError\nfrom sqlfluff.core.parser.context import ParseContext\nfrom sqlfluff.core.parser.helpers import trim_non_code_segments\nfrom sqlfluff.core.parser.match_result import MatchResult\nfrom sqlfluff.core.parser.segments import BaseSegment, BracketedSegment\nfrom sqlfluff.core.parser.types import MatchableType\n\n\ndef first_trimmed_raw(seg: BaseSegment) -> str:\n    \"\"\"Trim whitespace off a whole element raw.\n\n    Used as a helper function in BaseGrammar._look_ahead_match.\n\n    For existing compound segments, we should assume that within\n    that segment, things are internally consistent, that means\n    rather than enumerating all the individual segments of a longer\n    one we just dump out the whole segment, but splitting off the\n    first element separated by whitespace. This is a) faster and\n    also b) prevents some really horrible bugs with bracket matching.\n    See https://github.com/sqlfluff/sqlfluff/issues/433\n\n    This fetches the _whole_ raw of a potentially compound segment\n    to match against, trimming off any whitespace. This is the\n    most efficient way to get at the first element of a potentially\n    longer segment.\n    \"\"\"\n    s = seg.raw_upper.split(maxsplit=1)\n    return s[0] if s else \"\"\n\n\n@dataclass\nclass BracketInfo:\n    \"\"\"BracketInfo tuple for keeping track of brackets during matching.\n\n    This is used in BaseGrammar._bracket_sensitive_look_ahead_match but\n    defined here for type checking.\n    \"\"\"\n\n    bracket: BaseSegment\n    segments: Tuple[BaseSegment, ...]\n\n    def to_segment(self, end_bracket: Tuple[BaseSegment, ...]) -> BracketedSegment:\n        \"\"\"Turn the contained segments into a bracketed segment.\"\"\"\n        assert len(end_bracket) == 1\n        return BracketedSegment(\n            segments=self.segments,\n            start_bracket=(self.bracket,),\n            end_bracket=cast(Tuple[BaseSegment], end_bracket),\n        )\n\n\ndef look_ahead_match(\n    segments: Tuple[BaseSegment, ...],\n    matchers: List[MatchableType],\n    parse_context: ParseContext,\n) -> Tuple[Tuple[BaseSegment, ...], MatchResult, Optional[MatchableType]]:\n    \"\"\"Look ahead for matches beyond the first element of the segments list.\n\n    This function also contains the performance improved hash-matching approach to\n    searching for matches, which should significantly improve performance.\n\n    Prioritise the first match, and if multiple match at the same point the longest.\n    If two matches of the same length match at the same time, then it's the first in\n    the iterable of matchers.\n\n    Returns:\n        `tuple` of (unmatched_segments, match_object, matcher).\n\n    \"\"\"\n    # Have we been passed an empty tuple?\n    if not segments:  # pragma: no cover TODO?\n        return ((), MatchResult.from_empty(), None)\n\n    # Here we enable a performance optimisation. Most of the time in this cycle\n    # happens in loops looking for simple matchers which we should\n    # be able to find a shortcut for.\n    best_simple_match = None\n    simple_match = None\n    for idx, seg in enumerate(segments):\n        trimmed_seg = first_trimmed_raw(seg)\n        for matcher in matchers:\n            simple_match = None\n            simple = matcher.simple(parse_context=parse_context)\n            if not simple:  # pragma: no cover\n                # NOTE: For all bundled dialects, this clause is true, but until\n                # the RegexMatcher is completely deprecated (and therefore that\n                # `.simple()` must provide a result), it is still _possible_\n                # to end up here.\n                raise NotImplementedError(\n                    \"All matchers passed to `.look_ahead_match()` are \"\n                    \"assumed to have a functioning `.simple()` option. \"\n                    \"In a future release it will be compulsory for _all_ \"\n                    \"matchables to implement `.simple()`. Please report \"\n                    \"this as a bug on GitHub along with your current query \"\n                    f\"and dialect.\\nProblematic matcher: {matcher}\"\n                )\n            simple_raws, simple_types = simple\n\n            assert simple_raws or simple_types\n            if simple_raws:\n                if trimmed_seg in simple_raws:\n                    simple_match = matcher\n\n            if simple_types and not simple_match:\n                intersection = simple_types.intersection(seg.class_types)\n                if intersection:\n                    simple_match = matcher\n\n            # If we couldn't achieve a simple match, move on to the next option.\n            if not simple_match:\n                continue\n\n            # If there is, check the full version matches. If it doesn't\n            # then discount it and move on.\n            match = simple_match.match(segments[idx:], parse_context)\n            if not match:\n                continue\n\n            best_simple_match = (\n                segments[:idx],\n                match,\n                simple_match,\n            )\n            # Stop looking through matchers\n            break\n\n        # If we have a valid match, stop looking through segments\n        if best_simple_match:\n            break\n\n    if best_simple_match:\n        return best_simple_match\n    else:\n        return ((), MatchResult.from_unmatched(segments), None)\n\n\ndef bracket_sensitive_look_ahead_match(\n    segments: Tuple[BaseSegment, ...],\n    matchers: List[MatchableType],\n    parse_context: ParseContext,\n    start_bracket: Optional[MatchableType] = None,\n    end_bracket: Optional[MatchableType] = None,\n    bracket_pairs_set: str = \"bracket_pairs\",\n) -> Tuple[Tuple[BaseSegment, ...], MatchResult, Optional[MatchableType]]:\n    \"\"\"Same as `look_ahead_match` but with bracket counting.\n\n    NB: Given we depend on `look_ahead_match` we can also utilise\n    the same performance optimisations which are implemented there.\n\n    bracket_pairs_set: Allows specific segments to override the available\n        bracket pairs. See the definition of \"angle_bracket_pairs\" in the\n        BigQuery dialect for additional context on why this exists.\n\n    Returns:\n        `tuple` of (unmatched_segments, match_object, matcher).\n\n    \"\"\"\n    # Have we been passed an empty tuple?\n    if not segments:\n        return ((), MatchResult.from_unmatched(segments), None)\n\n    # Get hold of the bracket matchers from the dialect, and append them\n    # to the list of matchers. We get them from the relevant set on the\n    # dialect. We use zip twice to \"unzip\" them. We ignore the first\n    # argument because that's just the name.\n    _, start_bracket_refs, end_bracket_refs, persists = zip(\n        *parse_context.dialect.bracket_sets(bracket_pairs_set)\n    )\n    # These are matchables, probably StringParsers.\n    start_brackets = [\n        parse_context.dialect.ref(seg_ref) for seg_ref in start_bracket_refs\n    ]\n    end_brackets = [parse_context.dialect.ref(seg_ref) for seg_ref in end_bracket_refs]\n    # Add any bracket-like things passed as arguments\n    if start_bracket:\n        start_brackets += [start_bracket]\n    if end_bracket:\n        end_brackets += [end_bracket]\n    bracket_matchers = start_brackets + end_brackets\n\n    # Make some buffers\n    seg_buff: Tuple[BaseSegment, ...] = segments\n    pre_seg_buff: Tuple[BaseSegment, ...] = ()\n    bracket_stack: List[BracketInfo] = []\n\n    # Iterate\n    while True:\n        # Do we have anything left to match on?\n        if seg_buff:\n            # Yes we have buffer left to work with.\n            # Are we already in a bracket stack?\n            if bracket_stack:\n                # Yes, we're just looking for the closing bracket, or\n                # another opening bracket.\n                pre, match, matcher = look_ahead_match(\n                    seg_buff,\n                    bracket_matchers,\n                    parse_context=parse_context,\n                )\n\n                if match:\n                    # NB: We can only consider this as a nested bracket if the start\n                    # and end tokens are not the same. If a matcher is both a start\n                    # and end token we cannot deepen the bracket stack. In general,\n                    # quoted strings are a typical example where the start and end\n                    # tokens are the same. Currently, though, quoted strings are\n                    # handled elsewhere in the parser, and there are no cases where\n                    # *this* code has to handle identical start and end brackets.\n                    # For now, consider this a small, speculative investment in a\n                    # possible future requirement.\n                    if matcher in start_brackets and matcher not in end_brackets:\n                        # Add any segments leading up to this to the previous\n                        # bracket.\n                        bracket_stack[-1].segments += pre\n                        # Add a bracket to the stack and add the matches from the\n                        # segment.\n                        bracket_stack.append(\n                            BracketInfo(\n                                bracket=match.matched_segments[0],\n                                segments=match.matched_segments,\n                            )\n                        )\n                        seg_buff = match.unmatched_segments\n                        continue\n                    elif matcher in end_brackets:\n                        # Found an end bracket. Does its type match that of\n                        # the innermost start bracket? E.g. \")\" matches \"(\",\n                        # \"]\" matches \"[\".\n                        # For the start bracket we don't have the matcher\n                        # but we can work out the type, so we use that for\n                        # the lookup.\n                        start_index = [\n                            bracket.type for bracket in start_brackets\n                        ].index(bracket_stack[-1].bracket.get_type())\n                        # For the end index, we can just look for the matcher\n                        end_index = end_brackets.index(matcher)\n                        bracket_types_match = start_index == end_index\n                        if bracket_types_match:\n                            # Yes, the types match. So we've found a\n                            # matching end bracket. Pop the stack, construct\n                            # a bracketed segment and carry\n                            # on.\n\n                            # Complete the bracketed info\n                            bracket_stack[-1].segments += pre + match.matched_segments\n                            # Construct a bracketed segment (as a tuple) if allowed.\n                            persist_bracket = persists[end_brackets.index(matcher)]\n                            if persist_bracket:\n                                new_segments: Tuple[BaseSegment, ...] = (\n                                    bracket_stack[-1].to_segment(\n                                        end_bracket=match.matched_segments\n                                    ),\n                                )\n                            else:\n                                new_segments = bracket_stack[-1].segments\n                            # Remove the bracket set from the stack\n                            bracket_stack.pop()\n                            # If we're still in a bracket, add the new segments to\n                            # that bracket, otherwise add them to the buffer\n                            if bracket_stack:\n                                bracket_stack[-1].segments += new_segments\n                            else:\n                                pre_seg_buff += new_segments\n                            seg_buff = match.unmatched_segments\n                            continue\n                        else:\n                            # The types don't match. Error.\n                            raise SQLParseError(\n                                f\"Found unexpected end bracket!, \"\n                                f\"was expecting \"\n                                f\"{end_brackets[start_index]}, \"\n                                f\"but got {matcher}\",\n                                segment=match.matched_segments[0],\n                            )\n\n                    else:  # pragma: no cover\n                        raise RuntimeError(\"I don't know how we get here?!\")\n                else:  # pragma: no cover\n                    # No match, we're in a bracket stack. Error.\n                    raise SQLParseError(\n                        \"Couldn't find closing bracket for opening bracket.\",\n                        segment=bracket_stack[-1].bracket,\n                    )\n            else:\n                # No, we're open to more opening brackets or the thing(s)\n                # that we're otherwise looking for.\n                pre, match, matcher = look_ahead_match(\n                    seg_buff,\n                    matchers + bracket_matchers,\n                    parse_context=parse_context,\n                )\n\n                if match:\n                    if matcher in matchers:\n                        # It's one of the things we were looking for!\n                        # Return.\n                        return (pre_seg_buff + pre, match, matcher)\n                    elif matcher in start_brackets:\n                        # We've found the start of a bracket segment.\n                        # NB: It might not *actually* be the bracket itself,\n                        # but could be some non-code element preceding it.\n                        # That's actually ok.\n\n                        # Add the bracket to the stack.\n                        bracket_stack.append(\n                            BracketInfo(\n                                bracket=match.matched_segments[0],\n                                segments=match.matched_segments,\n                            )\n                        )\n                        # The matched element has already been added to the bracket.\n                        # Add anything before it to the pre segment buffer.\n                        # Reset the working buffer.\n                        pre_seg_buff += pre\n                        seg_buff = match.unmatched_segments\n                        continue\n                    elif matcher in end_brackets:\n                        # We've found an unexpected end bracket! This is likely\n                        # because we're matching a section which should have ended.\n                        # If we had a match, it would have matched by now, so this\n                        # means no match.\n                        pass\n                        # From here we'll drop out to the happy unmatched exit.\n                    else:  # pragma: no cover\n                        # This shouldn't happen!?\n                        raise NotImplementedError(\n                            \"This shouldn't happen. Panic in \"\n                            \"_bracket_sensitive_look_ahead_match.\"\n                        )\n                # Not in a bracket stack, but no match.\n                # From here we'll drop out to the happy unmatched exit.\n        else:\n            # No we're at the end:\n            # Now check have we closed all our brackets?\n            if bracket_stack:  # pragma: no cover\n                # No we haven't.\n                raise SQLParseError(\n                    \"Couldn't find closing bracket for opened brackets: \"\n                    f\"`{bracket_stack}`.\",\n                    segment=bracket_stack[-1].bracket,\n                )\n\n        # This is the happy unmatched path. This occurs when:\n        # - We reached the end with no open brackets.\n        # - No match while outside a bracket stack.\n        # - We found an unexpected end bracket before matching something\n        # interesting. We return with the mutated segments so we can reuse any\n        # bracket matching.\n        return ((), MatchResult.from_unmatched(pre_seg_buff + seg_buff), None)\n\n\ndef greedy_match(\n    segments: Tuple[BaseSegment, ...],\n    parse_context: ParseContext,\n    matchers: Sequence[MatchableType],\n    include_terminator: bool = False,\n) -> MatchResult:\n    \"\"\"Looks ahead to claim everything up to some future terminators.\"\"\"\n    seg_buff = segments\n    seg_bank: Tuple[BaseSegment, ...] = ()  # Empty tuple\n\n    while True:\n        with parse_context.deeper_match(name=\"GreedyUntil\") as ctx:\n            pre, mat, matcher = bracket_sensitive_look_ahead_match(\n                seg_buff, list(matchers), parse_context=ctx\n            )\n\n        if not mat:\n            # No terminator match? Return everything\n            return MatchResult.from_matched(segments)\n\n        # NOTE: For some terminators we only count them if they're preceded\n        # by whitespace, and others we don't. In principle, we aim that for\n        # _keywords_ we require whitespace, and for symbols we don't.\n        # We do this by looking at the `simple` method of the returned\n        # matcher, and if it's entirely alphabetical (as defined by\n        # str.isalpha()) then we infer that it's a keyword, and therefore\n        # _does_ require whitespace before it.\n        assert matcher, f\"Match without matcher: {mat}\"\n        _simple = matcher.simple(parse_context)\n        assert _simple, f\"Terminators require a simple method: {matcher}\"\n        _strings, _types = _simple\n        # NOTE: Typed matchers aren't common here, but we assume that they\n        # _don't_ require preceding whitespace.\n        # Do we need to enforce whitespace preceding?\n        if all(_s.isalpha() for _s in _strings) and not _types:\n            # Does the match include some whitespace already?\n            # Work forward\n            idx = 0\n            while True:\n                elem = mat.matched_segments[idx]\n                if elem.is_meta:  # pragma: no cover TODO?\n                    idx += 1\n                    continue\n                elif elem.is_type(\"whitespace\", \"newline\"):  # pragma: no cover TODO?\n                    allowable_match = True\n                    break\n                else:\n                    # No whitespace before. Not allowed.\n                    allowable_match = False\n                    break\n\n            # If we're not ok yet, work backward to the preceding sections.\n            if not allowable_match:\n                idx = -1\n                while True:\n                    if len(pre) < abs(idx):  # pragma: no cover TODO?\n                        # If we're at the start, it's ok\n                        allowable_match = True\n                        break\n                    if pre[idx].is_meta:  # pragma: no cover TODO?\n                        idx -= 1\n                        continue\n                    elif pre[idx].is_type(\"whitespace\", \"newline\"):\n                        allowable_match = True\n                        break\n                    else:\n                        # No whitespace before. Not allowed.\n                        allowable_match = False\n                        break\n\n            # If this match isn't preceded by whitespace and that is\n            # a requirement, then we can't use it. Carry on...\n            if not allowable_match:\n                # Update our buffers and continue onward\n                seg_bank = seg_bank + pre + mat.matched_segments\n                seg_buff = mat.unmatched_segments\n                # Loop around, don't return yet\n                continue\n\n        # Return everything up to the match unless it's a gap matcher.\n        if include_terminator:\n            return MatchResult(\n                seg_bank + pre + mat.matched_segments,\n                mat.unmatched_segments,\n            )\n\n        # We can't claim any non-code segments, so we trim them off the end.\n        leading_nc, pre_seg_mid, trailing_nc = trim_non_code_segments(seg_bank + pre)\n        return MatchResult(\n            leading_nc + pre_seg_mid,\n            trailing_nc + mat.all_segments(),\n        )\ndiff --git a/src/sqlfluff/core/parser/match_logging.py b/src/sqlfluff/core/parser/match_logging.py\n \"\"\"Classes to help with match logging.\"\"\"\n \n import logging\nfrom typing import TYPE_CHECKING, Any\n \n if TYPE_CHECKING:  # pragma: no cover\n     from sqlfluff.core.parser.context import ParseContext\n \n \n     ParseMatchLogObject(\n         parse_context, grammar, func, msg, v_level=v_level, **kwargs\n     ).log()"
  },
  {
    "instruction": "Jinja: sqlfluff fails in the presence of assignments with multiple targets\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\nI did search, and I think this _may_ be related, but since no more information was provided I cannot confirm it: https://github.com/sqlfluff/sqlfluff/issues/2947. For this reason, I opened a new issue.\r\n\r\n### What Happened\r\n\r\nJinja templates support multiple targets in [assignments](https://jinja.palletsprojects.com/en/3.0.x/templates/#assignments). However, `sqlfluff` fails to lint a file in the presence of an assignment with multiple targets.\r\n\r\nI traced this back to the `update_inside_set_or_macro` function, specifically [this line](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L244=).\r\n\r\nThe way `sqlfluff` is determining whether we are inside a [block assignment](https://jinja.palletsprojects.com/en/3.0.x/templates/#block-assignments) is by checking for the presence of an equals in the second index of the trimmed parts of the current raw slice:\r\n\r\n```python\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\r\n```\r\n\r\nThis condition is false for single target assignments:\r\n\r\n```sql\r\n{% set a = 1 %}\r\n```\r\n\r\nWhich produce the expected trimmed parts (with spaces removed as in [line 243](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L243=)):\r\n\r\n```python\r\n['set', 'a', '=', '1']\r\n#             2    \r\n```\r\n\r\nHowever, with multiple targets:\r\n\r\n```sql\r\n{% set a, b = 1, 2 %}\r\n```\r\n\r\n```python\r\n['set', 'a', ',', 'b', '=', '1', '2']\r\n#                       4    \r\n```\r\n\r\nEquals is no longer in the index 2, but has been bumped to index 4, yet we are not in the expanded block form of set assignments. This causes the `inside_set_or_macro` flag to be incorrectly set to `True`, as if we were using a block assignment, which causes the entire template to be ignored (or something like that), and leads to the eventual `ValueError` raised.\r\n\r\nI played around a bit with potential solutions: first, I tried incrementing the index of the equals by the number of commas:\r\n\r\n```python\r\nequals_index = 2 + sum((c == ',' for c in  filtered_trimmed_parts))\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[equals_index] != \"=\":\r\n```\r\n\r\nHowever, this would bring issues if using the expanded form of set assignments with any commas in it, or in the presence of an uneven number of commas on both sides of the assignment.\r\n\r\nAnother simpler option would be to check for the presence of a single equals:\r\n\r\n```python\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts.count(\"=\") != 1:\r\n```\r\n\r\nThis one seems more promising, specially considering that multiple targets appear not to be supported with block assignments (at least, that's what I think, as the docs don't mention it, and trying it locally raises a too many values to unpack error). Thus, the first condition will always be true for block assignments (so, even the presence of an equals in the body of the assignment would not cause issues).\r\n\r\n### Expected Behaviour\r\n\r\nsqlfluff should lint files properly, even in the presence of assignments with multiple targets.\r\n\r\n### Observed Behaviour\r\n\r\nLinting fails when an exception is raised:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10dbt/bin/sqlfluff\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/cli/commands.py\", line 541, in lint\r\n    result = lnt.lint_paths(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1098, in lint_paths\r\n    self.lint_path(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1050, in lint_path\r\n    for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 101, in run\r\n    for fname, partial in self.iter_partials(fnames, fix=fix):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 54, in iter_partials\r\n    for fname, rendered in self.iter_rendered(fnames):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 43, in iter_rendered\r\n    yield fname, self.linter.render_file(fname, self.config)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 771, in render_file\r\n    return self.render_string(raw_file, fname, config, encoding)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 742, in render_string\r\n    templated_file, templater_violations = self.templater.process(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/jinja.py\", line 394, in process\r\n    TemplatedFile(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/base.py\", line 94, in __init__\r\n    raise ValueError(\"Cannot instantiate a templated file unsliced!\")\r\nValueError: Cannot instantiate a templated file unsliced!\r\n```\r\n\r\n### How to reproduce\r\n\r\n1. Save the following template to `model.sql` in an empty directory:\r\n```sql\r\n{% set a, b = 1, 2 %}\r\n\r\nSELECT {{ a }}\r\n```\r\n2. Run `sqlfluff lint model.sql --dialect 'postgres'`\r\n\r\n\r\n### Dialect\r\n\r\nTried with postgres and redshift dialects, however I think others may be affected as long as they use jinja templates.\r\n\r\n### Version\r\n\r\nv0.12.0\r\n\r\n### Configuration\r\n\r\nNothing, ran from an empty directory.\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n",
    "input": " \n from jinja2 import Environment\n from jinja2.environment import Template\n \n from sqlfluff.core.templaters.base import (\n     RawFileSlice,\n             # as other code inside these regions require special handling.\n             # (Generally speaking, JinjaTracer ignores the contents of these\n             # blocks, treating them like opaque templated regions.)\n            filtered_trimmed_parts = [p for p in trimmed_parts if not p.isspace()]\n            if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\n                # Entering a set/macro block.\n                self.inside_set_or_macro = True\n         elif block_type == \"block_end\" and (trimmed_parts[0] in (\"endmacro\", \"endset\")):\n             # Exiting a set/macro block.\n             self.inside_set_or_macro = False",
    "output": " \n from jinja2 import Environment\n from jinja2.environment import Template\nfrom jinja2.exceptions import TemplateSyntaxError\n \n from sqlfluff.core.templaters.base import (\n     RawFileSlice,\n             # as other code inside these regions require special handling.\n             # (Generally speaking, JinjaTracer ignores the contents of these\n             # blocks, treating them like opaque templated regions.)\n            try:\n                # Entering a set/macro block. Build a source string consisting\n                # of just this one Jinja command and see if it parses. If so,\n                # it's a standalone command. OTOH, if it fails with \"Unexpected\n                # end of template\", it was the opening command for a block.\n                self.env.from_string(\n                    f\"{self.env.block_start_string} {' '.join(trimmed_parts)} \"\n                    f\"{self.env.block_end_string}\"\n                )\n            except TemplateSyntaxError as e:\n                if \"Unexpected end of template\" in e.message:\n                    # It was opening a block, thus we're inside a set or macro.\n                    self.inside_set_or_macro = True\n                else:\n                    raise  # pragma: no cover\n         elif block_type == \"block_end\" and (trimmed_parts[0] in (\"endmacro\", \"endset\")):\n             # Exiting a set/macro block.\n             self.inside_set_or_macro = False"
  },
  {
    "instruction": "Suppress dbt logs and warnings when using --format github-annotation\nSometimes, running:\r\n```\r\nsqlfluff lint --format github-annotation --annotation-level failure --nofail \r\n```\r\n\r\nCan result in the first couple of output lines being logs which break the annotations, for example:\r\n```\r\n14:21:42  Partial parse save file not found. Starting full parse.\r\nWarning:  [WARNING]: Did not find matching node for patch with name 'xxxx' in the 'models' section of file 'models/production/xxxxx/xxxxx.yml'\r\n```\r\n\r\n## Version\r\ndbt 1.0.0, SQLFLuff 0.9.0\r\n\n",
    "input": " import logging\n import time\n from logging import LogRecord\nfrom typing import (\n    Callable,\n    Tuple,\n    NoReturn,\n    Optional,\n    List,\n)\n \n import yaml\n \n     click.echo(format_dialects(dialect_readout), color=c.get(\"color\"))\n \n \n @cli.command()\n @common_options\n @core_options\n     type=click.Choice([ft.value for ft in FormatType], case_sensitive=False),\n     help=\"What format to return the lint result in (default=human).\",\n )\n @click.option(\n     \"--annotation-level\",\n     default=\"notice\",\n     paths: Tuple[str],\n     processes: int,\n     format: str,\n     annotation_level: str,\n     nofail: bool,\n     disregard_sqlfluffignores: bool,\n \n     \"\"\"\n     config = get_config(extra_config_path, ignore_local_config, **kwargs)\n    non_human_output = format != FormatType.human.value\n     lnt, formatter = get_linter_and_formatter(config, silent=non_human_output)\n \n     verbose = config.get(\"verbose\")\n             click.echo(format_linting_stats(result, verbose=verbose))\n \n     if format == FormatType.json.value:\n        click.echo(json.dumps(result.as_records()))\n     elif format == FormatType.yaml.value:\n        click.echo(yaml.dump(result.as_records(), sort_keys=False))\n     elif format == FormatType.github_annotation.value:\n         github_result = []\n         for record in result.as_records():\n                         \"annotation_level\": annotation_level,\n                     }\n                 )\n        click.echo(json.dumps(github_result))\n \n     if bench:\n         click.echo(\"==== overall timings ====\")\n     ),\n     help=\"What format to return the parse result in.\",\n )\n @click.option(\n     \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n )\n     code_only: bool,\n     include_meta: bool,\n     format: str,\n     profiler: bool,\n     bench: bool,\n     nofail: bool,\n     \"\"\"\n     c = get_config(extra_config_path, ignore_local_config, **kwargs)\n     # We don't want anything else to be logged if we want json or yaml output\n    non_human_output = format in (FormatType.json.value, FormatType.yaml.value)\n     lnt, formatter = get_linter_and_formatter(c, silent=non_human_output)\n     verbose = c.get(\"verbose\")\n     recurse = c.get(\"recurse\")\n                 # For yaml dumping always dump double quoted strings if they contain\n                 # tabs or newlines.\n                 yaml.add_representer(str, quoted_presenter)\n                click.echo(yaml.dump(parsed_strings_dict, sort_keys=False))\n             elif format == FormatType.json.value:\n                click.echo(json.dumps(parsed_strings_dict))\n \n     except OSError:  # pragma: no cover\n         click.echo(",
    "output": " import logging\n import time\n from logging import LogRecord\nfrom typing import Callable, Tuple, NoReturn, Optional, List, cast\n \n import yaml\n \n     click.echo(format_dialects(dialect_readout), color=c.get(\"color\"))\n \n \ndef dump_file_payload(filename: Optional[str], payload: str):\n    \"\"\"Write the output file content to stdout or file.\"\"\"\n    # If there's a file specified to write to, write to it.\n    if filename:\n        with open(filename, \"w\") as out_file:\n            out_file.write(payload)\n    # Otherwise write to stdout\n    else:\n        click.echo(payload)\n\n\n @cli.command()\n @common_options\n @core_options\n     type=click.Choice([ft.value for ft in FormatType], case_sensitive=False),\n     help=\"What format to return the lint result in (default=human).\",\n )\n@click.option(\n    \"--write-output\",\n    help=(\n        \"Optionally provide a filename to write the results to, mostly used in \"\n        \"tandem with --format. NB: Setting an output file re-enables normal \"\n        \"stdout logging.\"\n    ),\n)\n @click.option(\n     \"--annotation-level\",\n     default=\"notice\",\n     paths: Tuple[str],\n     processes: int,\n     format: str,\n    write_output: Optional[str],\n     annotation_level: str,\n     nofail: bool,\n     disregard_sqlfluffignores: bool,\n \n     \"\"\"\n     config = get_config(extra_config_path, ignore_local_config, **kwargs)\n    non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    file_output = None\n     lnt, formatter = get_linter_and_formatter(config, silent=non_human_output)\n \n     verbose = config.get(\"verbose\")\n             click.echo(format_linting_stats(result, verbose=verbose))\n \n     if format == FormatType.json.value:\n        file_output = json.dumps(result.as_records())\n     elif format == FormatType.yaml.value:\n        file_output = yaml.dump(result.as_records(), sort_keys=False)\n     elif format == FormatType.github_annotation.value:\n         github_result = []\n         for record in result.as_records():\n                         \"annotation_level\": annotation_level,\n                     }\n                 )\n        file_output = json.dumps(github_result)\n\n    if file_output:\n        dump_file_payload(write_output, cast(str, file_output))\n \n     if bench:\n         click.echo(\"==== overall timings ====\")\n     ),\n     help=\"What format to return the parse result in.\",\n )\n@click.option(\n    \"--write-output\",\n    help=(\n        \"Optionally provide a filename to write the results to, mostly used in \"\n        \"tandem with --format. NB: Setting an output file re-enables normal \"\n        \"stdout logging.\"\n    ),\n)\n @click.option(\n     \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n )\n     code_only: bool,\n     include_meta: bool,\n     format: str,\n    write_output: Optional[str],\n     profiler: bool,\n     bench: bool,\n     nofail: bool,\n     \"\"\"\n     c = get_config(extra_config_path, ignore_local_config, **kwargs)\n     # We don't want anything else to be logged if we want json or yaml output\n    # unless we're writing to a file.\n    non_human_output = (format != FormatType.human.value) or (write_output is not None)\n     lnt, formatter = get_linter_and_formatter(c, silent=non_human_output)\n     verbose = c.get(\"verbose\")\n     recurse = c.get(\"recurse\")\n                 # For yaml dumping always dump double quoted strings if they contain\n                 # tabs or newlines.\n                 yaml.add_representer(str, quoted_presenter)\n                file_output = yaml.dump(parsed_strings_dict, sort_keys=False)\n             elif format == FormatType.json.value:\n                file_output = json.dumps(parsed_strings_dict)\n\n            # Dump the output to stdout or to file as appropriate.\n            dump_file_payload(write_output, file_output)\n \n     except OSError:  # pragma: no cover\n         click.echo("
  },
  {
    "instruction": "Add \"enable\" and \"disable\" syntax to noqa to allow rules disabling across multiple lines\nSee the `pylint` docs for an example: https://docs.pylint.org/en/1.6.0/faq.html#is-it-possible-to-locally-disable-a-particular-message\n",
    "input": "         \"\"\"Get the position marker of the violation.\n \n         Returns:\n            The :obj:`PosMarker` of the segments if the violation has a segment,\n            the :obj:`PosMarker` directly stored in a `pos` attribute or None\n            if neither a present.\n \n         \"\"\"\n         if hasattr(self, \"segment\"):\ndiff --git a/src/sqlfluff/core/linter.py b/src/sqlfluff/core/linter.py\n     description: str\n \n \n class ProtoFile(NamedTuple):\n     \"\"\"Proto object to be inherited by LintedFile.\"\"\"\n \n     violations: list\n     time_dict: dict\n     tree: Any\n    ignore_mask: list\n \n \n class ParsedString(NamedTuple):\n     \"\"\"A class to store the idea of a linted file.\"\"\"\n \n     path: str\n    violations: list\n     time_dict: dict\n     tree: Optional[BaseSegment]\n    ignore_mask: list\n     templated_file: TemplatedFile\n \n     def check_tuples(self) -> List[CheckTuple]:\n             violations = [v for v in violations if not v.ignore]\n             # Ignore any rules in the ignore mask\n             if self.ignore_mask:\n                for line_no, rules in self.ignore_mask:\n                    violations = [\n                        v\n                        for v in violations\n                        if not (\n                            v.line_no() == line_no\n                            and (rules is None or v.rule_code() in rules)\n                        )\n                    ]\n         return violations\n \n     def num_violations(self, **kwargs) -> int:\n         bencher(\"Finish parsing {0!r}\".format(short_fname))\n         return ParsedString(parsed, violations, time_dict, templated_file, config)\n \n    @staticmethod\n    def extract_ignore_from_comment(comment: RawSegment):\n        \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n         # Also trim any whitespace afterward\n        comment_content = comment.raw_trimmed().strip()\n        if comment_content.startswith(\"noqa\"):\n             # This is an ignore identifier\n            comment_remainder = comment_content[4:]\n             if comment_remainder:\n                 if not comment_remainder.startswith(\":\"):\n                     return SQLParseError(\n                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n                        segment=comment,\n                     )\n                comment_remainder = comment_remainder[1:]\n                rules = [r.strip() for r in comment_remainder.split(\",\")]\n                return (comment.pos_marker.line_no, tuple(rules))\n            else:\n                return (comment.pos_marker.line_no, None)\n         return None\n \n     @staticmethod\n     def _warn_unfixable(code: str):\n         linter_logger.warning(",
    "output": "         \"\"\"Get the position marker of the violation.\n \n         Returns:\n            The :obj:`FilePositionMarker` of the segments if the violation has a segment,\n            the :obj:`FilePositionMarker` directly stored in a `pos` attribute or None\n            if neither is present.\n \n         \"\"\"\n         if hasattr(self, \"segment\"):\ndiff --git a/src/sqlfluff/core/linter.py b/src/sqlfluff/core/linter.py\n     description: str\n \n \nclass NoQaDirective(NamedTuple):\n    \"\"\"Parsed version of a 'noqa' comment.\"\"\"\n\n    line_no: int  # Source line number\n    rules: Optional[Tuple[str, ...]]  # Affected rule names\n    action: Optional[str]  # \"enable\", \"disable\", or \"None\"\n\n\n class ProtoFile(NamedTuple):\n     \"\"\"Proto object to be inherited by LintedFile.\"\"\"\n \n     violations: list\n     time_dict: dict\n     tree: Any\n    ignore_mask: List[NoQaDirective]\n \n \n class ParsedString(NamedTuple):\n     \"\"\"A class to store the idea of a linted file.\"\"\"\n \n     path: str\n    violations: List[SQLBaseError]\n     time_dict: dict\n     tree: Optional[BaseSegment]\n    ignore_mask: List[NoQaDirective]\n     templated_file: TemplatedFile\n \n     def check_tuples(self) -> List[CheckTuple]:\n             violations = [v for v in violations if not v.ignore]\n             # Ignore any rules in the ignore mask\n             if self.ignore_mask:\n                violations = self._ignore_masked_violations(violations)\n        return violations\n\n    @staticmethod\n    def _ignore_masked_violations_single_line(\n        violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-specific directives.\n\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives with\n        action=None.\n        \"\"\"\n        for ignore in ignore_mask:\n            violations = [\n                v\n                for v in violations\n                if not (\n                    v.line_no() == ignore.line_no\n                    and (ignore.rules is None or v.rule_code() in ignore.rules)\n                )\n            ]\n        return violations\n\n    @staticmethod\n    def _should_ignore_violation_line_range(\n        line_no: int, ignore_rule: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore a violation at line_no.\"\"\"\n        # Loop through the NoQaDirectives to find the state of things at\n        # line_no. Assumptions about \"ignore_rule\":\n        # - Contains directives for only ONE RULE, i.e. the rule that was\n        #   violated at line_no\n        # - Sorted in ascending order by line number\n        disable = False\n        for ignore in ignore_rule:\n            if ignore.line_no > line_no:\n                break\n            disable = ignore.action == \"disable\"\n        return disable\n\n    @classmethod\n    def _ignore_masked_violations_line_range(\n        cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-range directives.\n\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives where\n        action is \"enable\" or \"disable\".\n        \"\"\"\n        result = []\n        for v in violations:\n            # Find the directives that affect the violated rule \"v\", either\n            # because they specifically reference it or because they don't\n            # specify a list of rules, thus affecting ALL rules.\n            ignore_rule = sorted(\n                [\n                    ignore\n                    for ignore in ignore_mask\n                    if not ignore.rules\n                    or (v.rule_code() in cast(Tuple[str, ...], ignore.rules))\n                ],\n                key=lambda ignore: ignore.line_no,\n            )\n            # Determine whether to ignore the violation, based on the relevant\n            # enable/disable directives.\n            if not cls._should_ignore_violation_line_range(v.line_no(), ignore_rule):\n                result.append(v)\n        return result\n\n    def _ignore_masked_violations(\n        self, violations: List[SQLBaseError]\n    ) -> List[SQLBaseError]:\n        \"\"\"Remove any violations specified by ignore_mask.\n\n        This involves two steps:\n        1. Filter out violations affected by single-line \"noqa\" directives.\n        2. Filter out violations affected by disable/enable \"noqa\" directives.\n        \"\"\"\n        ignore_specific = [ignore for ignore in self.ignore_mask if not ignore.action]\n        ignore_range = [ignore for ignore in self.ignore_mask if ignore.action]\n        violations = self._ignore_masked_violations_single_line(\n            violations, ignore_specific\n        )\n        violations = self._ignore_masked_violations_line_range(violations, ignore_range)\n         return violations\n \n     def num_violations(self, **kwargs) -> int:\n         bencher(\"Finish parsing {0!r}\".format(short_fname))\n         return ParsedString(parsed, violations, time_dict, templated_file, config)\n \n    @classmethod\n    def parse_noqa(cls, comment: str, line_no: int):\n        \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n         # Also trim any whitespace afterward\n        if comment.startswith(\"noqa\"):\n             # This is an ignore identifier\n            comment_remainder = comment[4:]\n             if comment_remainder:\n                 if not comment_remainder.startswith(\":\"):\n                     return SQLParseError(\n                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n                     )\n                comment_remainder = comment_remainder[1:].strip()\n                if comment_remainder:\n                    action: Optional[str]\n                    if \"=\" in comment_remainder:\n                        action, rule_part = comment_remainder.split(\"=\", 1)\n                        if action not in {\"disable\", \"enable\"}:\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. Expected 'noqa: enable=<rule>[,...] | all' or 'noqa: disable=<rule>[,...] | all\",\n                            )\n                    else:\n                        action = None\n                        rule_part = comment_remainder\n                        if rule_part in {\"disable\", \"enable\"}:\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. Expected 'noqa: enable=<rule>[,...] | all' or 'noqa: disable=<rule>[,...] | all\",\n                            )\n                    rules: Optional[Tuple[str, ...]]\n                    if rule_part != \"all\":\n                        rules = tuple(r.strip() for r in rule_part.split(\",\"))\n                    else:\n                        rules = None\n                    return NoQaDirective(line_no, rules, action)\n            return NoQaDirective(line_no, None, None)\n         return None\n \n    @classmethod\n    def extract_ignore_from_comment(cls, comment: RawSegment):\n        \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n        # Also trim any whitespace afterward\n        comment_content = comment.raw_trimmed().strip()\n        result = cls.parse_noqa(comment_content, comment.pos_marker.line_no)\n        if isinstance(result, SQLParseError):\n            result.segment = comment\n        return result\n\n     @staticmethod\n     def _warn_unfixable(code: str):\n         linter_logger.warning("
  },
  {
    "instruction": "Configuration from current working path not being loaded when path provided.\nI have the following directory structure.\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  tree -a\r\n.\r\n\u251c\u2500\u2500 .sqlfluffignore\r\n\u251c\u2500\u2500 ignore_me_1.sql\r\n\u251c\u2500\u2500 path_a\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ignore_me_2.sql\r\n\u2514\u2500\u2500 path_b\r\n    \u251c\u2500\u2500 ignore_me_3.sql\r\n    \u2514\u2500\u2500 lint_me_1.sql\r\n\r\n2 directories, 5 files\r\n```\r\nAnd the following ignore file\r\n\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  cat .sqlfluffignore\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  cat .sqlfluffignore\r\nignore_me_1.sql\r\npath_a/\r\npath_b/ignore_me_3.sql%\r\n```\r\n\r\nWhen I run the following I get the expected result. Sqlfluff only lints the one file that is not ignored.\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint .\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint .\r\n== [path_b/lint_me_1.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n```\r\n\r\nHowever when I run the lint explicitly on one of the two directories then ignored files are also linted.\r\n\r\n```\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_a\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_a\r\n== [path_a/ignore_me_2.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_b\r\n\r\n~/GitHub/sqlfluff-bug\r\n\u279c  sqlfluff lint path_b\r\n== [path_b/ignore_me_3.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n== [path_b/lint_me_1.sql] FAIL\r\nL:   2 | P:   1 | L003 | Indent expected and not found compared to line #1\r\nL:   2 | P:  10 | L010 | Inconsistent capitalisation of keywords.\r\n```\r\n\r\nIf this is the expected behaviour then it might be worthwhile to add an example to the [docs](https://docs.sqlfluff.com/en/latest/configuration.html#sqlfluffignore).\r\n\r\nEdit: I've replicated this issue on sqlfluff version 0.3.2 to 0.3.6.\n",
    "input": " \n     @classmethod\n     def find_ignore_config_files(\n        cls, path, working_path=os.getcwd(), ignore_file_name=\".sqlfluffignore\"\n     ):\n         \"\"\"Finds sqlfluff ignore files from both the path and its parent paths.\"\"\"\n         return set(\n         The lowest priority is the user appdir, then home dir, then increasingly\n         the configs closest to the file being directly linted.\n         \"\"\"\n        given_path = Path(path).resolve()\n        working_path = Path(working_path).resolve()\n \n         # If we've been passed a file and not a directory,\n         # then go straight to the directory.\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n         # matched, but we warn the users when that happens\n         is_exact_file = os.path.isfile(path)\n \n         if is_exact_file:\n             # When the exact file to lint is passed, we\n             # fill path_walk with an input that follows\n             #   (root, directories, files)\n             dirpath = os.path.dirname(path)\n             files = [os.path.basename(path)]\n            ignore_file_paths = ConfigLoader.find_ignore_config_files(\n                path=path, working_path=working_path, ignore_file_name=ignore_file_name\n            )\n            # Add paths that could contain \"ignore files\"\n            # to the path_walk list\n            path_walk_ignore_file = [\n                (\n                    os.path.dirname(ignore_file_path),\n                    None,\n                    # Only one possible file, since we only\n                    # have one \"ignore file name\"\n                    [os.path.basename(ignore_file_path)],\n                )\n                for ignore_file_path in ignore_file_paths\n            ]\n            path_walk: WalkableType = [(dirpath, None, files)] + path_walk_ignore_file\n         else:\n            path_walk = os.walk(path)\n \n         # If it's a directory then expand the path!\n         buffer = []",
    "output": " \n     @classmethod\n     def find_ignore_config_files(\n        cls, path, working_path=Path.cwd(), ignore_file_name=\".sqlfluffignore\"\n     ):\n         \"\"\"Finds sqlfluff ignore files from both the path and its parent paths.\"\"\"\n         return set(\n         The lowest priority is the user appdir, then home dir, then increasingly\n         the configs closest to the file being directly linted.\n         \"\"\"\n        given_path = Path(path).absolute()\n        working_path = Path(working_path).absolute()\n \n         # If we've been passed a file and not a directory,\n         # then go straight to the directory.\ndiff --git a/src/sqlfluff/core/linter/linter.py b/src/sqlfluff/core/linter/linter.py\n         # matched, but we warn the users when that happens\n         is_exact_file = os.path.isfile(path)\n \n        path_walk: WalkableType\n         if is_exact_file:\n             # When the exact file to lint is passed, we\n             # fill path_walk with an input that follows\n             #   (root, directories, files)\n             dirpath = os.path.dirname(path)\n             files = [os.path.basename(path)]\n            path_walk = [(dirpath, None, files)]\n         else:\n            path_walk = list(os.walk(path))\n\n        ignore_file_paths = ConfigLoader.find_ignore_config_files(\n            path=path, working_path=working_path, ignore_file_name=ignore_file_name\n        )\n        # Add paths that could contain \"ignore files\"\n        # to the path_walk list\n        path_walk_ignore_file = [\n            (\n                os.path.dirname(ignore_file_path),\n                None,\n                # Only one possible file, since we only\n                # have one \"ignore file name\"\n                [os.path.basename(ignore_file_path)],\n            )\n            for ignore_file_path in ignore_file_paths\n        ]\n        path_walk += path_walk_ignore_file\n \n         # If it's a directory then expand the path!\n         buffer = []"
  },
  {
    "instruction": "Write-output human format does not produce result\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen running SQLFluff using the following statement:\r\n`python -m sqlfluff lint --write-output test.txt --config=config/sql-lint.cfg`\r\nno result was produced.\n\n### Expected Behaviour\n\nI expect a file to appear, in this case called test,txt, containing all violations found.\n\n### Observed Behaviour\n\nLooking through the code I saw human was the default format so expected adding --format=human would not make a difference. To be sure, I also ran the statement using the flag and it still produced nothing.\r\n\r\nTo make sure it was just the human format which was having problems, I also executed the statement using --format=json,yaml,github-annotations, all of which did produce the expected result which leads me to believe there is something wrong with the human format.\n\n### How to reproduce\n\nI imagine simply executing `sqlfluff lint --write-output test.txt example.sql`\n\n### Dialect\n\nT-SQL\n\n### Version\n\n0.11.2\n\n### Configuration\n\n[sqlfluff]\r\ndialect = tsql\r\nexclude_rules = L014,\r\n                L016,\r\n                L031,\r\n                L035,\r\n                L059\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": " \"\"\"Contains the CLI.\"\"\"\n \n from itertools import chain\n import sys\n import json\n import logging\n     colorize,\n     format_dialect_warning,\n     format_dialects,\n    CallbackFormatter,\n )\n from sqlfluff.cli.helpers import cli_table, get_package_version\n \n # Import from sqlfluff core.\n from sqlfluff.core import (\n         sys.exit(66)\n \n \ndef _callback_handler(cfg: FluffConfig) -> Callable:\n    \"\"\"Returns function which will be bound as a callback for printing passed message.\n\n    Called in `get_linter_and_formatter`.\n    \"\"\"\n\n    def _echo_with_tqdm_lock(message: str) -> None:\n        \"\"\"Makes sure that message printing (echoing) will be not in conflict with tqdm.\n\n        It may happen that progressbar conflicts with extra printing. Nothing very\n        serious happens then, except that there is printed (not removed) progressbar\n        line. The `external_write_mode` allows to disable tqdm for writing time.\n        \"\"\"\n        with tqdm.external_write_mode():\n            click.echo(message=message, color=cfg.get(\"color\"))\n\n    return _echo_with_tqdm_lock\n\n\n def get_linter_and_formatter(\n    cfg: FluffConfig, silent: bool = False\n) -> Tuple[Linter, CallbackFormatter]:\n     \"\"\"Get a linter object given a config.\"\"\"\n     try:\n         # We're just making sure it exists at this stage.\n     except KeyError:  # pragma: no cover\n         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n         sys.exit(66)\n\n    if not silent:\n        # Instantiate the linter and return it (with an output function)\n        formatter = CallbackFormatter(\n            callback=_callback_handler(cfg=cfg),\n            verbosity=cfg.get(\"verbose\"),\n            output_line_length=cfg.get(\"output_line_length\"),\n        )\n        return Linter(config=cfg, formatter=formatter), formatter\n    else:\n        # Instantiate the linter and return. NB: No formatter\n        # in the Linter and a black formatter otherwise.\n        formatter = CallbackFormatter(callback=lambda m: None, verbosity=0)\n        return Linter(config=cfg), formatter\n \n \n @click.group(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n     )\n     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n     file_output = None\n    lnt, formatter = get_linter_and_formatter(config, silent=non_human_output)\n \n     verbose = config.get(\"verbose\")\n     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n     if file_output:\n         dump_file_payload(write_output, cast(str, file_output))\n \n     if bench:\n         click.echo(\"==== overall timings ====\")\n         click.echo(cli_table([(\"Clock time\", result.total_time)]))\n         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n     )\n     fix_even_unparsable = config.get(\"fix_even_unparsable\")\n    lnt, formatter = get_linter_and_formatter(config, silent=fixing_stdin)\n \n     verbose = config.get(\"verbose\")\n     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n     # We don't want anything else to be logged if we want json or yaml output\n     # unless we're writing to a file.\n     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    lnt, formatter = get_linter_and_formatter(c, silent=non_human_output)\n     verbose = c.get(\"verbose\")\n     recurse = c.get(\"recurse\")\n \n         # iterative print for human readout\n         if format == FormatType.human.value:\n             violations_count = _print_out_violations_and_timing(\n                bench, code_only, total_time, verbose, parsed_strings\n             )\n         else:\n             parsed_strings_dict = [\n \n \n def _print_out_violations_and_timing(\n     bench: bool,\n     code_only: bool,\n     total_time: float,\n         timing.add(parsed_string.time_dict)\n \n         if parsed_string.tree:\n            click.echo(parsed_string.tree.stringify(code_only=code_only))\n         else:\n             # TODO: Make this prettier\n            click.echo(\"...Failed to Parse...\")  # pragma: no cover\n \n         violations_count += len(parsed_string.violations)\n         if parsed_string.violations:\n            click.echo(\"==== parsing violations ====\")  # pragma: no cover\n         for v in parsed_string.violations:\n            click.echo(format_violation(v))  # pragma: no cover\n         if parsed_string.violations and parsed_string.config.get(\"dialect\") == \"ansi\":\n            click.echo(format_dialect_warning())  # pragma: no cover\n \n         if verbose >= 2:\n            click.echo(\"==== timings ====\")\n            click.echo(cli_table(parsed_string.time_dict.items()))\n \n     if verbose >= 2 or bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", total_time)]))\n         timing_summary = timing.summary()\n         for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n \n     return violations_count\n \ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n \n \n from io import StringIO\nfrom typing import Callable, List, Union\n \n from sqlfluff.cli.helpers import (\n     colorize,\n     get_python_implementation,\n     pad_line,\n )\n from sqlfluff.core import SQLBaseError, FluffConfig, Linter\n from sqlfluff.core.enums import Color\n from sqlfluff.core.linter import LintedFile\n     )\n \n \nclass CallbackFormatter:\n    \"\"\"Formatter which uses a callback to output information.\n \n     On instantiation, this formatter accepts a function to\n     dispatch messages. Each public method accepts an object\n \n \n     Args:\n        callback (:obj:`callable`): A callable which can be\n            be called with a string to be output.\n        verbosity (:obj:`int`): An integer specifying how\n            verbose the output should be.\n        filter_empty (:obj:`bool`): If True, empty messages\n            will not be dispatched.\n\n     \"\"\"\n \n     def __init__(\n         self,\n        callback: Callable,\n         verbosity: int = 0,\n         filter_empty: bool = True,\n         output_line_length: int = 80,\n     ):\n        self._callback = callback\n         self._verbosity = verbosity\n         self._filter_empty = filter_empty\n         self.output_line_length = output_line_length\n         \"\"\"\n         # The strip here is to filter out any empty messages\n         if (not self._filter_empty) or s.strip(\" \\n\\t\"):\n            self._callback(s)\n \n     def _format_config(self, linter: Linter) -> str:\n         \"\"\"Format the config of a `Linter`.\"\"\"\ndiff --git a/src/sqlfluff/cli/outputstream.py b/src/sqlfluff/cli/outputstream.py\nnew file mode 100644",
    "output": " \"\"\"Contains the CLI.\"\"\"\n \n from itertools import chain\nimport os\n import sys\n import json\n import logging\n     colorize,\n     format_dialect_warning,\n     format_dialects,\n    OutputStreamFormatter,\n )\n from sqlfluff.cli.helpers import cli_table, get_package_version\nfrom sqlfluff.cli.outputstream import make_output_stream, OutputStream\n \n # Import from sqlfluff core.\n from sqlfluff.core import (\n         sys.exit(66)\n \n \n def get_linter_and_formatter(\n    cfg: FluffConfig, output_stream: Optional[OutputStream] = None\n) -> Tuple[Linter, OutputStreamFormatter]:\n     \"\"\"Get a linter object given a config.\"\"\"\n     try:\n         # We're just making sure it exists at this stage.\n     except KeyError:  # pragma: no cover\n         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n         sys.exit(66)\n    formatter = OutputStreamFormatter(\n        output_stream=output_stream or make_output_stream(cfg),\n        verbosity=cfg.get(\"verbose\"),\n        output_line_length=cfg.get(\"output_line_length\"),\n    )\n    return Linter(config=cfg, formatter=formatter), formatter\n \n \n @click.group(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n     )\n     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n     file_output = None\n    output_stream = make_output_stream(config, format, write_output)\n    lnt, formatter = get_linter_and_formatter(config, output_stream)\n \n     verbose = config.get(\"verbose\")\n     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n     if file_output:\n         dump_file_payload(write_output, cast(str, file_output))\n \n    output_stream.close()\n     if bench:\n         click.echo(\"==== overall timings ====\")\n         click.echo(cli_table([(\"Clock time\", result.total_time)]))\n         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n     )\n     fix_even_unparsable = config.get(\"fix_even_unparsable\")\n    output_stream = make_output_stream(\n        config, None, os.devnull if fixing_stdin else None\n    )\n    lnt, formatter = get_linter_and_formatter(config, output_stream)\n \n     verbose = config.get(\"verbose\")\n     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n     # We don't want anything else to be logged if we want json or yaml output\n     # unless we're writing to a file.\n     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n    output_stream = make_output_stream(c, format, write_output)\n    lnt, formatter = get_linter_and_formatter(c, output_stream)\n     verbose = c.get(\"verbose\")\n     recurse = c.get(\"recurse\")\n \n         # iterative print for human readout\n         if format == FormatType.human.value:\n             violations_count = _print_out_violations_and_timing(\n                output_stream, bench, code_only, total_time, verbose, parsed_strings\n             )\n         else:\n             parsed_strings_dict = [\n \n \n def _print_out_violations_and_timing(\n    output_stream: OutputStream,\n     bench: bool,\n     code_only: bool,\n     total_time: float,\n         timing.add(parsed_string.time_dict)\n \n         if parsed_string.tree:\n            output_stream.write(parsed_string.tree.stringify(code_only=code_only))\n         else:\n             # TODO: Make this prettier\n            output_stream.write(\"...Failed to Parse...\")  # pragma: no cover\n \n         violations_count += len(parsed_string.violations)\n         if parsed_string.violations:\n            output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n         for v in parsed_string.violations:\n            output_stream.write(format_violation(v))  # pragma: no cover\n         if parsed_string.violations and parsed_string.config.get(\"dialect\") == \"ansi\":\n            output_stream.write(format_dialect_warning())  # pragma: no cover\n \n         if verbose >= 2:\n            output_stream.write(\"==== timings ====\")\n            output_stream.write(cli_table(parsed_string.time_dict.items()))\n \n     if verbose >= 2 or bench:\n        output_stream.write(\"==== overall timings ====\")\n        output_stream.write(cli_table([(\"Clock time\", total_time)]))\n         timing_summary = timing.summary()\n         for step in timing_summary:\n            output_stream.write(f\"=== {step} ===\")\n            output_stream.write(cli_table(timing_summary[step].items()))\n \n     return violations_count\n \ndiff --git a/src/sqlfluff/cli/formatters.py b/src/sqlfluff/cli/formatters.py\n \n \n from io import StringIO\nfrom typing import List, Union\n \n from sqlfluff.cli.helpers import (\n     colorize,\n     get_python_implementation,\n     pad_line,\n )\nfrom sqlfluff.cli.outputstream import OutputStream\n from sqlfluff.core import SQLBaseError, FluffConfig, Linter\n from sqlfluff.core.enums import Color\n from sqlfluff.core.linter import LintedFile\n     )\n \n \nclass OutputStreamFormatter:\n    \"\"\"Formatter which writes to an OutputStream.\n \n     On instantiation, this formatter accepts a function to\n     dispatch messages. Each public method accepts an object\n \n \n     Args:\n        output_stream: Output is sent here\n        verbosity: Specifies how verbose output should be\n        filter_empty: If True, empty messages will not be dispatched\n        output_line_length: Maximum line length\n     \"\"\"\n \n     def __init__(\n         self,\n        output_stream: OutputStream,\n         verbosity: int = 0,\n         filter_empty: bool = True,\n         output_line_length: int = 80,\n     ):\n        self._output_stream = output_stream\n         self._verbosity = verbosity\n         self._filter_empty = filter_empty\n         self.output_line_length = output_line_length\n         \"\"\"\n         # The strip here is to filter out any empty messages\n         if (not self._filter_empty) or s.strip(\" \\n\\t\"):\n            self._output_stream.write(s)\n \n     def _format_config(self, linter: Linter) -> str:\n         \"\"\"Format the config of a `Linter`.\"\"\"\ndiff --git a/src/sqlfluff/cli/outputstream.py b/src/sqlfluff/cli/outputstream.py\nnew file mode 100644\n\"\"\"Classes for managing linter output, used with OutputStreamFormatter.\"\"\"\nimport abc\nimport os\nfrom typing import Any, Optional\n\nimport click\nfrom tqdm import tqdm\n\nfrom sqlfluff.core import FluffConfig\nfrom sqlfluff.core.enums import FormatType\n\n\nclass OutputStream(abc.ABC):\n    \"\"\"Base class for linter output stream.\"\"\"\n\n    def __init__(self, config: FluffConfig, context: Any = None):\n        self.config = config\n\n    def write(self, message: str) -> None:\n        \"\"\"Write message to output.\"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    def close(self):\n        \"\"\"Close output stream.\"\"\"\n        pass\n\n\nclass TqdmOutput(OutputStream):\n    \"\"\"Outputs to stdout, coordinates to avoid conflict with tqdm.\n\n    It may happen that progressbar conflicts with extra printing. Nothing very\n    serious happens then, except that there is printed (not removed) progressbar\n    line. The `external_write_mode` allows to disable tqdm for writing time.\n    \"\"\"\n\n    def __init__(self, config: FluffConfig):\n        super().__init__(config)\n\n    def write(self, message: str) -> None:\n        \"\"\"Write message to stdout.\"\"\"\n        with tqdm.external_write_mode():\n            click.echo(message=message, color=self.config.get(\"color\"))\n\n\nclass FileOutput(OutputStream):\n    \"\"\"Outputs to a specified file.\"\"\"\n\n    def __init__(self, config: FluffConfig, output_path: str):\n        super().__init__(config)\n        self.file = open(output_path, \"w\")\n\n    def write(self, message: str) -> None:\n        \"\"\"Write message to output_path.\"\"\"\n        print(message, file=self.file)\n\n    def close(self):\n        \"\"\"Close output file.\"\"\"\n        self.file.close()\n\n\ndef make_output_stream(\n    config: FluffConfig,\n    format: Optional[str] = None,\n    output_path: Optional[str] = None,\n) -> OutputStream:\n    \"\"\"Create and return appropriate OutputStream instance.\"\"\"\n    if format is None or format == FormatType.human.value:\n        if not output_path:\n            # Human-format output to stdout.\n            return TqdmOutput(config)\n        else:\n            # Human-format output to a file.\n            return FileOutput(config, output_path)\n    else:\n        # Discard human output as not required\n        return FileOutput(config, os.devnull)"
  },
  {
    "instruction": "dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file\n_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._\r\n\r\n## Expected Behaviour\r\nViolation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.\r\n\r\n## Observed Behaviour\r\nReported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\\u2192' in position 120: character maps to <undefined>`\r\n\r\n## Steps to Reproduce\r\nSQL file:\r\n```sql\r\nSELECT\r\n    reacted_table_name_right.descendant_id AS category_id,\r\n    string_agg(redacted_table_name_left.name, ' \u2192 ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa\r\nFROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left\r\nINNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right\r\n    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id\r\nGROUP BY reacted_table_name_right.descendant_id\r\n```\r\nRunning `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.\r\n\r\n## Dialect\r\n`postgres`, with `dbt` templater\r\n\r\n## Version\r\n`python 3.7.12`\r\n`sqlfluff 0.7.0`\r\n`sqlfluff-templater-dbt 0.7.0`\r\n\r\n## Configuration\r\nI've tried a few, here's one:\r\n```\r\n[sqlfluff]\r\nverbose = 2\r\ndialect = postgres\r\ntemplater = dbt\r\nexclude_rules = None\r\noutput_line_length = 80\r\nrunaway_limit = 10\r\nignore_templated_areas = True\r\nprocesses = 3\r\n# Comma separated list of file extensions to lint.\r\n\r\n# NB: This config will only apply in the root folder.\r\nsql_file_exts = .sql\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Common config across rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nindent_unit = space\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# L001 - Remove trailing whitespace (fix)\r\n# L002 - Single section of whitespace should not contain both tabs and spaces (fix)\r\n# L003 - Keep consistent indentation (fix)\r\n# L004 - We use 4 spaces for indentation just for completeness (fix)\r\n# L005 - Remove space before commas (fix)\r\n# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)\r\n\r\n# L007 - Operators should not be at the end of a line\r\n[sqlfluff:rules:L007]  # Keywords\r\noperator_new_lines = after\r\n\r\n# L008 - Always use a single whitespace after a comma (fix)\r\n# L009 - Files will always end with a trailing newline\r\n\r\n# L010 - All keywords will use full upper case (fix)\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n# L011 - Always explicitly alias tables (fix)\r\n[sqlfluff:rules:L011]  # Aliasing\r\naliasing = explicit\r\n\r\n# L012 - Do not have to explicitly alias all columns\r\n[sqlfluff:rules:L012]  # Aliasing\r\naliasing = explicit\r\n\r\n# L013 - Always explicitly alias a column with an expression in it (fix)\r\n[sqlfluff:rules:L013]  # Aliasing\r\nallow_scalar = False\r\n\r\n# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)\r\n[sqlfluff:rules:L014]  # Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n\r\n# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)\r\n\r\n# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)\r\n[sqlfluff:rules:L016]\r\nignore_comment_lines = False\r\nmax_line_length = 120\r\n\r\n# L017 - There should not be whitespace between function name and brackets (fix)\r\n# L018 - Always align closing bracket of WITH to the WITH keyword (fix)\r\n\r\n# L019 - Always use trailing commas / commas at the end of the line (fix)\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n# L020 - Table aliases will always be unique per statement\r\n# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.\r\n# L022 - Add blank lines after common table expressions (CTE) / WITH.\r\n# L023 - Always add a single whitespace after AS in a WITH clause (fix)\r\n\r\n[sqlfluff:rules:L026]\r\nforce_enable = False\r\n\r\n# L027 - Always add references if more than one referenced table or view is used\r\n\r\n[sqlfluff:rules:L028]\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]  # Keyword identifiers\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L030]  # Function names\r\ncapitalisation_policy = upper\r\n\r\n# L032 - We prefer use of join keys rather than USING\r\n# L034 - We prefer ordering of columns in select statements as (fix):\r\n# 1. wildcards\r\n# 2. single identifiers\r\n# 3. calculations and aggregates\r\n\r\n# L035 - Omit 'else NULL'; it is redundant (fix)\r\n# L036 - Move select targets / identifiers onto new lines each (fix)\r\n# L037 - When using ORDER BY, make the direction explicit (fix)\r\n\r\n# L038 - Never use trailing commas at the end of the SELECT clause\r\n[sqlfluff:rules:L038]\r\nselect_clause_trailing_comma = forbid\r\n\r\n# L039 - Remove unnecessary whitespace (fix)\r\n\r\n[sqlfluff:rules:L040]  # Null & Boolean Literals\r\ncapitalisation_policy = upper\r\n\r\n# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses.\r\nforbid_subquery_in = join\r\n\r\n# L043 - Reduce CASE WHEN conditions to COALESCE (fix)\r\n# L044 - Prefer a known number of columns along the path to the source data\r\n# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)\r\n# L046 - Jinja tags should have a single whitespace on both sides\r\n\r\n# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)\r\n[sqlfluff:rules:L047]  # Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n# L048 - Quoted literals should be surrounded by a single whitespace (fix)\r\n# L049 - Always use IS or IS NOT for comparisons with NULL (fix)\r\n```\r\n\n",
    "input": " \n import os\n import logging\n from typing import (\n     Any,\n     Iterable,\n             if suffix:\n                 root, ext = os.path.splitext(fname)\n                 fname = root + suffix + ext\n            # Actually write the file.\n            with open(fname, \"w\", encoding=self.encoding) as f:\n                f.write(write_buff)\n         return success",
    "output": " \n import os\n import logging\nimport shutil\nimport tempfile\n from typing import (\n     Any,\n     Iterable,\n             if suffix:\n                 root, ext = os.path.splitext(fname)\n                 fname = root + suffix + ext\n            self._safe_create_replace_file(fname, write_buff, self.encoding)\n         return success\n\n    @staticmethod\n    def _safe_create_replace_file(fname, write_buff, encoding):\n        # Write to a temporary file first, so in case of encoding or other\n        # issues, we don't delete or corrupt the user's existing file.\n        dirname, basename = os.path.split(fname)\n        with tempfile.NamedTemporaryFile(\n            mode=\"w\",\n            encoding=encoding,\n            prefix=basename,\n            dir=dirname,\n            suffix=os.path.splitext(fname)[1],\n            delete=False,\n        ) as tmp:\n            tmp.file.write(write_buff)\n            tmp.flush()\n            os.fsync(tmp.fileno())\n        # Once the temp file is safely written, replace the existing file.\n        shutil.move(tmp.name, fname)"
  },
  {
    "instruction": "Multiple processes not used when list of explicit filenames is passed\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen providing a long list of file names to `sqlfluff lint -p -1`, only a single CPU is used. This seems to stem from the fact that https://github.com/sqlfluff/sqlfluff/blob/a006378af8b670f9235653694dbcddd4c62d1ab9/src/sqlfluff/core/linter/linter.py#L1190 is iterating over the list of files. For each listed path there, it would run the found files in parallel. As we are inputting whole filenames here, a path equals a single file and thus `sqlfluff` would only process one file at a time.\r\n\r\nThe context here is the execution of `sqlfluff lint` inside a `pre-commit` hook.\n\n### Expected Behaviour\n\nAll CPU cores are used as `-p -1` is passed on the commandline.\n\n### Observed Behaviour\n\nOnly a single CPU core is used.\n\n### How to reproduce\n\nRun `sqlfluff lint -p -1` with a long list of files.\n\n### Dialect\n\nAffects all. \n\n### Version\n\n1.4.2\n\n### Configuration\n\nNone.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n",
    "input": "         processes: Optional[int] = None,\n     ) -> LintedDir:\n         \"\"\"Lint a path.\"\"\"\n        linted_path = LintedDir(path)\n        if self.formatter:\n            self.formatter.dispatch_path(path)\n        fnames = list(\n            self.paths_from_path(\n                 path,\n                 ignore_non_existent_files=ignore_non_existent_files,\n                 ignore_files=ignore_files,\n            )\n        )\n \n         if processes is None:\n             processes = self.config.get(\"processes\", default=1)\n \n             self.formatter.dispatch_processing_header(effective_processes)\n \n         # Show files progress bar only when there is more than one.\n        files_count = len(fnames)\n         progress_bar_files = tqdm(\n             total=files_count,\n            desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n             leave=False,\n             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n         )\n \n        for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n            linted_path.add(linted_file)\n             # If any fatal errors, then stop iteration.\n             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                 break\n \n             # Progress bar for files is rendered only when there is more than one file.\n            # Additionally as it's updated after each loop, we need to get file name\n             # from the next loop. This is why `enumerate` starts with `1` and there\n             # is `i < len` to not exceed files list length.\n             progress_bar_files.update(n=1)\n            if i < len(fnames):\n                progress_bar_files.set_description(\n                    f\"file {os.path.basename(fnames[i])}\"\n                )\n\n        return linted_path\n\n    def lint_paths(\n        self,\n        paths: Tuple[str, ...],\n        fix: bool = False,\n        ignore_non_existent_files: bool = False,\n        ignore_files: bool = True,\n        processes: Optional[int] = None,\n    ) -> LintingResult:\n        \"\"\"Lint an iterable of paths.\"\"\"\n        paths_count = len(paths)\n\n        # If no paths specified - assume local\n        if not paths_count:  # pragma: no cover\n            paths = (os.getcwd(),)\n        # Set up the result to hold what we get back\n        result = LintingResult()\n\n        progress_bar_paths = tqdm(\n            total=paths_count,\n            desc=\"path\",\n            leave=False,\n            disable=paths_count <= 1 or progress_bar_configuration.disable_progress_bar,\n        )\n        for path in paths:\n            progress_bar_paths.set_description(f\"path {path}\")\n\n            # Iterate through files recursively in the specified directory (if it's a\n            # directory) or read the file directly if it's not\n            result.add(\n                self.lint_path(\n                    path,\n                    fix=fix,\n                    ignore_non_existent_files=ignore_non_existent_files,\n                    ignore_files=ignore_files,\n                    processes=processes,\n                )\n            )\n\n            progress_bar_paths.update(1)\n \n         result.stop_timer()\n         return result",
    "output": "         processes: Optional[int] = None,\n     ) -> LintedDir:\n         \"\"\"Lint a path.\"\"\"\n        return self.lint_paths(\n            (path,), fix, ignore_non_existent_files, ignore_files, processes\n        ).paths[0]\n\n    def lint_paths(\n        self,\n        paths: Tuple[str, ...],\n        fix: bool = False,\n        ignore_non_existent_files: bool = False,\n        ignore_files: bool = True,\n        processes: Optional[int] = None,\n    ) -> LintingResult:\n        \"\"\"Lint an iterable of paths.\"\"\"\n        # If no paths specified - assume local\n        if not paths:  # pragma: no cover\n            paths = (os.getcwd(),)\n        # Set up the result to hold what we get back\n        result = LintingResult()\n\n        expanded_paths: List[str] = []\n        expanded_path_to_linted_dir = {}\n        for path in paths:\n            linted_dir = LintedDir(path)\n            result.add(linted_dir)\n            for fname in self.paths_from_path(\n                 path,\n                 ignore_non_existent_files=ignore_non_existent_files,\n                 ignore_files=ignore_files,\n            ):\n                expanded_paths.append(fname)\n                expanded_path_to_linted_dir[fname] = linted_dir\n \n        files_count = len(expanded_paths)\n         if processes is None:\n             processes = self.config.get(\"processes\", default=1)\n \n             self.formatter.dispatch_processing_header(effective_processes)\n \n         # Show files progress bar only when there is more than one.\n        first_path = expanded_paths[0] if expanded_paths else \"\"\n         progress_bar_files = tqdm(\n             total=files_count,\n            desc=f\"file {first_path}\",\n             leave=False,\n             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n         )\n \n        for i, linted_file in enumerate(runner.run(expanded_paths, fix), start=1):\n            linted_dir = expanded_path_to_linted_dir[linted_file.path]\n            linted_dir.add(linted_file)\n             # If any fatal errors, then stop iteration.\n             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                 break\n \n             # Progress bar for files is rendered only when there is more than one file.\n            # Additionally, as it's updated after each loop, we need to get file name\n             # from the next loop. This is why `enumerate` starts with `1` and there\n             # is `i < len` to not exceed files list length.\n             progress_bar_files.update(n=1)\n            if i < len(expanded_paths):\n                progress_bar_files.set_description(f\"file {expanded_paths[i]}\")\n \n         result.stop_timer()\n         return result"
  },
  {
    "instruction": "2.x: Nested(many=True) eats first element from generator value when dumping\nAs reproduced in Python 3.6.8:\r\n\r\n```py\r\nfrom marshmallow import Schema, fields\r\n\r\nclass O(Schema):\r\n    i = fields.Int()\r\n\r\nclass P(Schema):\r\n    os = fields.Nested(O, many=True)\r\n\r\ndef gen():\r\n    yield {'i': 1}\r\n    yield {'i': 0}\r\n\r\np = P()\r\np.dump({'os': gen()})\r\n# MarshalResult(data={'os': [{'i': 0}]}, errors={})\r\n```\r\n\r\nProblematic code is here:\r\n\r\nhttps://github.com/marshmallow-code/marshmallow/blob/2.x-line/src/marshmallow/fields.py#L447\r\n\r\nAnd here:\r\n\r\nhttps://github.com/marshmallow-code/marshmallow/blob/2.x-line/src/marshmallow/schema.py#L832\r\n\r\nThe easiest solution would be to cast `nested_obj` to list before calling `schema._update_fields`, just like a normal Schema with `many=True` does.\n",
    "input": "         schema = self.schema\n         if nested_obj is None:\n             return None\n         if not self.__updated_fields:\n             schema._update_fields(obj=nested_obj, many=self.many)\n             self.__updated_fields = True\ndiff --git a/src/marshmallow/schema.py b/src/marshmallow/schema.py\n \n         :param set field_names: Field names to include in the final\n             return dictionary.\n         :returns: An dict of field_name:field_obj pairs.\n         \"\"\"\n         if obj and many:\n            try:  # Homogeneous collection\n                # Prefer getitem over iter to prevent breaking serialization\n                # of objects for which iter will modify position in the collection\n                # e.g. Pymongo cursors\n                if hasattr(obj, '__getitem__') and callable(getattr(obj, '__getitem__')):\n                    try:\n                        obj_prototype = obj[0]\n                    except KeyError:\n                        obj_prototype = next(iter(obj))\n                else:\n                    obj_prototype = next(iter(obj))\n            except (StopIteration, IndexError):  # Nothing to serialize\n                 return dict((k, v) for k, v in self.declared_fields.items() if k in field_names)\n            obj = obj_prototype\n         ret = self.dict_class()\n         for key in field_names:\n             if key in self.declared_fields:",
    "output": "         schema = self.schema\n         if nested_obj is None:\n             return None\n        if self.many and utils.is_iterable_but_not_string(nested_obj):\n            nested_obj = list(nested_obj)\n         if not self.__updated_fields:\n             schema._update_fields(obj=nested_obj, many=self.many)\n             self.__updated_fields = True\ndiff --git a/src/marshmallow/schema.py b/src/marshmallow/schema.py\n \n         :param set field_names: Field names to include in the final\n             return dictionary.\n        :param object|Mapping|list obj The object to base filtered fields on.\n         :returns: An dict of field_name:field_obj pairs.\n         \"\"\"\n         if obj and many:\n            try:  # list\n                obj = obj[0]\n            except IndexError:  # Nothing to serialize\n                 return dict((k, v) for k, v in self.declared_fields.items() if k in field_names)\n         ret = self.dict_class()\n         for key in field_names:\n             if key in self.declared_fields:"
  },
  {
    "instruction": "ISO8601 DateTimes ending with Z considered not valid in 2.19.4\nProbably related to #1247 and #1234 - in marshmallow `2.19.4`, with `python-dateutil` _not_ installed, it seems that loading a datetime in ISO8601 that ends in `Z` (UTC time) results in an error:\r\n\r\n```python\r\nclass Foo(Schema):\r\n    date = DateTime(required=True)\r\n\r\n\r\nfoo_schema = Foo(strict=True)\r\n\r\na_date_with_z = '2019-06-17T00:57:41.000Z'\r\nfoo_schema.load({'date': a_date_with_z})\r\n```\r\n\r\n```\r\nmarshmallow.exceptions.ValidationError: {'date': ['Not a valid datetime.']}\r\n```\r\n\r\nDigging a bit deeper, it seems [`from_iso_datetime`](https://github.com/marshmallow-code/marshmallow/blob/dev/src/marshmallow/utils.py#L213-L215) is failing with a `unconverted data remains: Z` - my understanding of the spec is rather limited, but it seems that they are indeed valid ISO8601 dates (and in `marshmallow==2.19.3` and earlier, the previous snippet seems to work without raising validation errors).\r\n\n",
    "input": "         # Strip off timezone info.\n         if '.' in datestring:\n             # datestring contains microseconds\n             return datetime.datetime.strptime(datestring[:26], '%Y-%m-%dT%H:%M:%S.%f')\n         return datetime.datetime.strptime(datestring[:19], '%Y-%m-%dT%H:%M:%S')\n ",
    "output": "         # Strip off timezone info.\n         if '.' in datestring:\n             # datestring contains microseconds\n            (dt_nomstz, mstz) = datestring.split('.')\n            ms_notz = mstz[:len(mstz) - len(mstz.lstrip('0123456789'))]\n            datestring = '.'.join((dt_nomstz, ms_notz))\n             return datetime.datetime.strptime(datestring[:26], '%Y-%m-%dT%H:%M:%S.%f')\n         return datetime.datetime.strptime(datestring[:19], '%Y-%m-%dT%H:%M:%S')\n "
  },
  {
    "instruction": "3.0: DateTime fields cannot be used as inner field for List or Tuple fields\nBetween releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):\r\n\r\n```python\r\nfrom marshmallow import fields, Schema\r\n\r\nclass MySchema(Schema):\r\n    times = fields.List(fields.DateTime())\r\n\r\ns = MySchema()\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-mm.py\", line 8, in <module>\r\n    s = MySchema()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 383, in __init__\r\n    self.fields = self._init_fields()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 913, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 969, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 636, in _bind_to_schema\r\n    self.inner._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 1117, in _bind_to_schema\r\n    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'List' object has no attribute 'opts'\r\n```\r\n\r\nIt seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.\n",
    "input": "         super()._bind_to_schema(field_name, schema)\n         self.format = (\n             self.format\n            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\n             or self.DEFAULT_FORMAT\n         )\n ",
    "output": "         super()._bind_to_schema(field_name, schema)\n         self.format = (\n             self.format\n            or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\n             or self.DEFAULT_FORMAT\n         )\n "
  },
  {
    "instruction": "[version 2.20.0] TypeError: 'NoneType' object is not subscriptable\nAfter update from version 2.19.5 to 2.20.0 I got error for code like:\r\n\r\n```python\r\nfrom marshmallow import Schema, fields, validates\r\n\r\n\r\nclass Bar(Schema):\r\n    value = fields.String()\r\n\r\n    @validates('value')  # <- issue here\r\n    def validate_value(self, value):\r\n        pass\r\n\r\n\r\nclass Foo(Schema):\r\n    bar = fields.Nested(Bar)\r\n\r\n\r\nsch = Foo()\r\n\r\nsch.validate({\r\n    'bar': 'invalid',\r\n})\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/_/bug_mschema.py\", line 19, in <module>\r\n    'bar': 'invalid',\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 628, in validate\r\n    _, errors = self._do_load(data, many, partial=partial, postprocess=False)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 670, in _do_load\r\n    index_errors=self.opts.index_errors,\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 292, in deserialize\r\n    index=(index if index_errors else None)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 65, in call_and_store\r\n    value = getter_func(data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 285, in <lambda>\r\n    data\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 265, in deserialize\r\n    output = self._deserialize(value, attr, data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 465, in _deserialize\r\n    data, errors = self.schema.load(value)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 588, in load\r\n    result, errors = self._do_load(data, many, partial=partial, postprocess=True)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 674, in _do_load\r\n    self._invoke_field_validators(unmarshal, data=result, many=many)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 894, in _invoke_field_validators\r\n    value = data[field_obj.attribute or field_name]\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\n",
    "input": "                 for idx, item in enumerate(data):\n                     try:\n                         value = item[field_obj.attribute or field_name]\n                    except KeyError:\n                         pass\n                     else:\n                         validated_value = unmarshal.call_and_store(\n             else:\n                 try:\n                     value = data[field_obj.attribute or field_name]\n                except KeyError:\n                     pass\n                 else:\n                     validated_value = unmarshal.call_and_store(",
    "output": "                 for idx, item in enumerate(data):\n                     try:\n                         value = item[field_obj.attribute or field_name]\n                    except (KeyError, TypeError):\n                         pass\n                     else:\n                         validated_value = unmarshal.call_and_store(\n             else:\n                 try:\n                     value = data[field_obj.attribute or field_name]\n                except (KeyError, TypeError):\n                     pass\n                 else:\n                     validated_value = unmarshal.call_and_store("
  },
  {
    "instruction": "fields.URL should allow relative-only validation\nRelative URLs may be used to redirect the user within the site, such as to sign in, and allowing absolute URLs without extra validation opens up a possibility of nefarious redirects.\r\n\r\nCurrent `fields.URL(relative = True)` allows relative URLs _in addition_ to absolute URLs, so one must set up extra validation to catch either all absolute URLs or just those that don't have a valid domain names.\r\n\r\nIt would be helpful if there was a way to set up URL validation to allow only relative URLs. \r\n\r\n~One quick and dirty way to do this would be if there was a `validate.Not` operator, then at the expense of matching the value twice, it would be possible to use something like this:~\r\n\r\n~`fields.URL(relative = True, validate=validate.Not(validate.URL()))`~\r\n\r\nEDIT: Never mind the crossed out thought above - failed validations are handled only via exceptions and while failing the inner validator works in general, it requires suppressing exception handlers and is just not a good way to go about it. \n",
    "input": "         self,\n         *,\n         relative: bool = False,\n         schemes: types.StrSequenceOrSet | None = None,\n         require_tld: bool = True,\n         **kwargs,\n         super().__init__(**kwargs)\n \n         self.relative = relative\n         self.require_tld = require_tld\n         # Insert validation into self.validators so that multiple errors can be stored.\n         validator = validate.URL(\n             relative=self.relative,\n             schemes=schemes,\n             require_tld=self.require_tld,\n             error=self.error_messages[\"invalid\"],\ndiff --git a/src/marshmallow/validate.py b/src/marshmallow/validate.py\n     \"\"\"Validate a URL.\n \n     :param relative: Whether to allow relative URLs.\n     :param error: Error message to raise in case of a validation error.\n         Can be interpolated with `{input}`.\n     :param schemes: Valid schemes. By default, ``http``, ``https``,\n         def __init__(self):\n             self._memoized = {}\n \n        def _regex_generator(self, relative: bool, require_tld: bool) -> typing.Pattern:\n            return re.compile(\n                r\"\".join(\n                    (\n                        r\"^\",\n                        r\"(\" if relative else r\"\",\n                        r\"(?:[a-z0-9\\.\\-\\+]*)://\",  # scheme is validated separately\n                        r\"(?:[^:@]+?(:[^:@]*?)?@|)\",  # basic auth\n                        r\"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+\",\n                        r\"(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|\",  # domain...\n                        r\"localhost|\",  # localhost...\n                        (\n                            r\"(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.?)|\"\n                            if not require_tld\n                            else r\"\"\n                        ),  # allow dotless hostnames\n                        r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|\",  # ...or ipv4\n                        r\"\\[[A-F0-9]*:[A-F0-9:]+\\])\",  # ...or ipv6\n                        r\"(?::\\d+)?\",  # optional port\n                        r\")?\"\n                        if relative\n                        else r\"\",  # host is optional, allow for relative URLs\n                        r\"(?:/?|[/?]\\S+)\\Z\",\n                    )\n                 ),\n                re.IGNORECASE,\n             )\n \n        def __call__(self, relative: bool, require_tld: bool) -> typing.Pattern:\n            key = (relative, require_tld)\n             if key not in self._memoized:\n                self._memoized[key] = self._regex_generator(relative, require_tld)\n \n             return self._memoized[key]\n \n         self,\n         *,\n         relative: bool = False,\n         schemes: types.StrSequenceOrSet | None = None,\n         require_tld: bool = True,\n         error: str | None = None,\n     ):\n         self.relative = relative\n         self.error = error or self.default_message  # type: str\n         self.schemes = schemes or self.default_schemes\n         self.require_tld = require_tld\n \n     def _repr_args(self) -> str:\n        return f\"relative={self.relative!r}\"\n \n     def _format_error(self, value) -> str:\n         return self.error.format(input=value)\n             if scheme not in self.schemes:\n                 raise ValidationError(message)\n \n        regex = self._regex(self.relative, self.require_tld)\n \n         if not regex.search(value):\n             raise ValidationError(message)",
    "output": "         self,\n         *,\n         relative: bool = False,\n        absolute: bool = True,\n         schemes: types.StrSequenceOrSet | None = None,\n         require_tld: bool = True,\n         **kwargs,\n         super().__init__(**kwargs)\n \n         self.relative = relative\n        self.absolute = absolute\n         self.require_tld = require_tld\n         # Insert validation into self.validators so that multiple errors can be stored.\n         validator = validate.URL(\n             relative=self.relative,\n            absolute=self.absolute,\n             schemes=schemes,\n             require_tld=self.require_tld,\n             error=self.error_messages[\"invalid\"],\ndiff --git a/src/marshmallow/validate.py b/src/marshmallow/validate.py\n     \"\"\"Validate a URL.\n \n     :param relative: Whether to allow relative URLs.\n    :param absolute: Whether to allow absolute URLs.\n     :param error: Error message to raise in case of a validation error.\n         Can be interpolated with `{input}`.\n     :param schemes: Valid schemes. By default, ``http``, ``https``,\n         def __init__(self):\n             self._memoized = {}\n \n        def _regex_generator(\n            self, relative: bool, absolute: bool, require_tld: bool\n        ) -> typing.Pattern:\n            hostname_variants = [\n                # a normal domain name, expressed in [A-Z0-9] chars with hyphens allowed only in the middle\n                # note that the regex will be compiled with IGNORECASE, so these are upper and lowercase chars\n                (\n                    r\"(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+\"\n                    r\"(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)\"\n                 ),\n                # or the special string 'localhost'\n                r\"localhost\",\n                # or IPv4\n                r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\n                # or IPv6\n                r\"\\[[A-F0-9]*:[A-F0-9:]+\\]\",\n            ]\n            if not require_tld:\n                # allow dotless hostnames\n                hostname_variants.append(r\"(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.?)\")\n\n            absolute_part = \"\".join(\n                (\n                    # scheme (e.g. 'https://', 'ftp://', etc)\n                    # this is validated separately against allowed schemes, so in the regex\n                    # we simply want to capture its existence\n                    r\"(?:[a-z0-9\\.\\-\\+]*)://\",\n                    # basic_auth, for URLs encoding a username:password\n                    # e.g. 'ftp://foo:bar@ftp.example.org/'\n                    r\"(?:[^:@]+?(:[^:@]*?)?@|)\",\n                    # netloc, the hostname/domain part of the URL plus the optional port\n                    r\"(?:\",\n                    \"|\".join(hostname_variants),\n                    r\")\",\n                    r\"(?::\\d+)?\",\n                )\n             )\n            relative_part = r\"(?:/?|[/?]\\S+)\\Z\"\n\n            if relative:\n                if absolute:\n                    parts: tuple[str, ...] = (\n                        r\"^(\",\n                        absolute_part,\n                        r\")?\",\n                        relative_part,\n                    )\n                else:\n                    parts = (r\"^\", relative_part)\n            else:\n                parts = (r\"^\", absolute_part, relative_part)\n \n            return re.compile(\"\".join(parts), re.IGNORECASE)\n\n        def __call__(\n            self, relative: bool, absolute: bool, require_tld: bool\n        ) -> typing.Pattern:\n            key = (relative, absolute, require_tld)\n             if key not in self._memoized:\n                self._memoized[key] = self._regex_generator(\n                    relative, absolute, require_tld\n                )\n \n             return self._memoized[key]\n \n         self,\n         *,\n         relative: bool = False,\n        absolute: bool = True,\n         schemes: types.StrSequenceOrSet | None = None,\n         require_tld: bool = True,\n         error: str | None = None,\n     ):\n        if not relative and not absolute:\n            raise ValueError(\n                \"URL validation cannot set both relative and absolute to False.\"\n            )\n         self.relative = relative\n        self.absolute = absolute\n         self.error = error or self.default_message  # type: str\n         self.schemes = schemes or self.default_schemes\n         self.require_tld = require_tld\n \n     def _repr_args(self) -> str:\n        return f\"relative={self.relative!r}, absolute={self.absolute!r}\"\n \n     def _format_error(self, value) -> str:\n         return self.error.format(input=value)\n             if scheme not in self.schemes:\n                 raise ValidationError(message)\n \n        regex = self._regex(self.relative, self.absolute, self.require_tld)\n \n         if not regex.search(value):\n             raise ValidationError(message)"
  },
  {
    "instruction": "`only` argument inconsistent between Nested(S, many=True) and List(Nested(S))\n```python\r\nfrom pprint import pprint\r\n\r\nfrom marshmallow import Schema\r\nfrom marshmallow.fields import Integer, List, Nested, String\r\n\r\n\r\nclass Child(Schema):\r\n    name = String()\r\n    age = Integer()\r\n\r\n\r\nclass Family(Schema):\r\n    children = List(Nested(Child))\r\n\r\n\r\nclass Family2(Schema):\r\n    children = Nested(Child, many=True)\r\n\r\nfamily = {'children':[\r\n    {'name': 'Tommy', 'age': 12},\r\n    {'name': 'Lily', 'age': 15},\r\n]}\r\n\r\npprint(Family( only=['children.name']).dump(family).data)\r\npprint(Family2( only=['children.name']).dump(family).data)\r\n```\r\nreturns\r\n```\r\n{'children': [{'age': 12, 'name': 'Tommy'}, {'age': 15, 'name': 'Lily'}]}\r\n{'children': [{'name': 'Tommy'}, {'name': 'Lily'}]}\r\n```\r\n\r\ntested with marshmallow 2.15.4\r\n\r\nThe same applies to `exclude` argument.\n",
    "input": "                 'The list elements must be a subclass or instance of '\n                 'marshmallow.base.FieldABC.',\n             )\n \n     def _bind_to_schema(self, field_name, schema):\n         super()._bind_to_schema(field_name, schema)\n         self.container = copy.deepcopy(self.container)\n         self.container.parent = self\n         self.container.name = field_name\n \n     def _serialize(self, value, attr, obj, **kwargs):\n         if value is None:\n         super()._bind_to_schema(field_name, schema)\n         new_tuple_fields = []\n         for container in self.tuple_fields:\n            new_container = copy.deepcopy(container)\n            new_container.parent = self\n            new_container.name = field_name\n            new_tuple_fields.append(new_container)\n         self.tuple_fields = new_tuple_fields\n \n     def _serialize(self, value, attr, obj, **kwargs):\n                     '\"values\" must be a subclass or instance of '\n                     'marshmallow.base.FieldABC.',\n                 )\n \n     def _bind_to_schema(self, field_name, schema):\n         super()._bind_to_schema(field_name, schema)\n             self.value_container = copy.deepcopy(self.value_container)\n             self.value_container.parent = self\n             self.value_container.name = field_name\n         if self.key_container:\n             self.key_container = copy.deepcopy(self.key_container)\n             self.key_container.parent = self",
    "output": "                 'The list elements must be a subclass or instance of '\n                 'marshmallow.base.FieldABC.',\n             )\n        if isinstance(self.container, Nested):\n            self.only = self.container.only\n            self.exclude = self.container.exclude\n \n     def _bind_to_schema(self, field_name, schema):\n         super()._bind_to_schema(field_name, schema)\n         self.container = copy.deepcopy(self.container)\n         self.container.parent = self\n         self.container.name = field_name\n        if isinstance(self.container, Nested):\n            self.container.only = self.only\n            self.container.exclude = self.exclude\n \n     def _serialize(self, value, attr, obj, **kwargs):\n         if value is None:\n         super()._bind_to_schema(field_name, schema)\n         new_tuple_fields = []\n         for container in self.tuple_fields:\n            container = copy.deepcopy(container)\n            container.parent = self\n            container.name = field_name\n            new_tuple_fields.append(container)\n\n         self.tuple_fields = new_tuple_fields\n \n     def _serialize(self, value, attr, obj, **kwargs):\n                     '\"values\" must be a subclass or instance of '\n                     'marshmallow.base.FieldABC.',\n                 )\n            if isinstance(self.value_container, Nested):\n                self.only = self.value_container.only\n                self.exclude = self.value_container.exclude\n \n     def _bind_to_schema(self, field_name, schema):\n         super()._bind_to_schema(field_name, schema)\n             self.value_container = copy.deepcopy(self.value_container)\n             self.value_container.parent = self\n             self.value_container.name = field_name\n        if isinstance(self.value_container, Nested):\n            self.value_container.only = self.only\n            self.value_container.exclude = self.exclude\n         if self.key_container:\n             self.key_container = copy.deepcopy(self.key_container)\n             self.key_container.parent = self"
  },
  {
    "instruction": "3.12 no longer supports fields named `parent`\nPretty sure that #1631 broke it. Reproducible example:\r\n\r\n```py\r\nfrom marshmallow import INCLUDE\r\nfrom marshmallow.fields import Nested\r\nfrom sqlalchemy import Column, DATE, create_engine, ForeignKey\r\nfrom sqlalchemy.dialects.postgresql import UUID\r\nfrom sqlalchemy.orm import declarative_base, relationship\r\nfrom marshmallow_sqlalchemy import SQLAlchemyAutoSchema\r\nfrom testing.postgresql import Postgresql\r\n\r\n\r\nBase = declarative_base()\r\n\r\n\r\nclass Author(Base):\r\n    __tablename__ = 'author'\r\n    id = Column(UUID(as_uuid=True), primary_key=True)\r\n    docs = relationship('Document', back_populates='parent')\r\n\r\n\r\nclass Document(Base):\r\n    __tablename__ = 'document'\r\n    id = Column(UUID(as_uuid=True), primary_key=True)\r\n    parent_id = Column(UUID(as_uuid=True), ForeignKey('author.id'))\r\n    parent = relationship(Author, back_populates='docs')\r\n    last_updated = Column(DATE)\r\n\r\n\r\nclass AuthorSchema(SQLAlchemyAutoSchema):\r\n    class Meta(SQLAlchemyAutoSchema.Meta):\r\n        model = Author\r\n\r\n\r\nclass DocumentSchema(SQLAlchemyAutoSchema):\r\n    parent = Nested(AuthorSchema)\r\n\r\n    class Meta(SQLAlchemyAutoSchema.Meta):\r\n        model = Document\r\n\r\n\r\nwith Postgresql() as postgresql:\r\n    url = postgresql.url(drivername='postgresql+psycopg2')\r\n    engine = create_engine(url, echo=True)\r\n    Base.metadata.create_all(engine)\r\n\r\n    DocumentSchema(unknown=INCLUDE)\r\n```\r\n\r\nResults in:\r\n\r\n```pytb\r\nTraceback (most recent call last):\r\n  File \"/home/phil/.config/JetBrains/PyCharm2021.1/scratches/sqlalchemy-marshmallow-reprex.py\", line 44, in <module>\r\n    DocumentSchema(unknown=INCLUDE)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow_sqlalchemy/schema/load_instance_mixin.py\", line 43, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 392, in __init__\r\n    self._init_fields()\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 971, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/schema.py\", line 1030, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/home/phil/Dev/Python/venvs/cellarity/lib/python3.9/site-packages/marshmallow/fields.py\", line 1201, in _bind_to_schema\r\n    or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'NoneType' object has no attribute 'opts'\r\n```\r\n\r\nHere, `self.root` resolves to `None` for the `last_updated` field:\r\n\r\nhttps://github.com/marshmallow-code/marshmallow/blob/69270215ab9275dc566b010ecdb8777c186aa776/src/marshmallow/fields.py#L411-L420\r\n\r\nThis happens since that field\u2019s `.parent` is the `DocumentSchema` class, which *does* have a `.parent` attribute. However that attribute is a `Nested` instance, not another schema as expected\nReturn a field\u2019s root schema as soon as it is found\nThis prevents accessing a schema\u2019s `.parent` attribute if it has one (e.g. a field called `parent`)\r\n\r\nFixes #1808, I think.\n",
    "input": " \n     parent = None\n     name = None\n \n     def serialize(self, attr, obj, accessor=None):\n         raise NotImplementedError\ndiff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py\n         :meth:`Schema._bind_field <marshmallow.Schema._bind_field>`.\n \n         :param str field_name: Field name set in schema.\n        :param Schema schema: Parent schema.\n         \"\"\"\n         self.parent = self.parent or schema\n         self.name = self.name or field_name\n \n     def _serialize(self, value: typing.Any, attr: str, obj: typing.Any, **kwargs):\n         \"\"\"Serializes ``value`` to a basic Python datatype. Noop by default.\n         \"\"\"The context dictionary for the parent :class:`Schema`.\"\"\"\n         return self.parent.context\n \n    @property\n    def root(self):\n        \"\"\"Reference to the `Schema` that this field belongs to even if it is buried in a\n        container field (e.g. `List`).\n        Return `None` for unbound fields.\n        \"\"\"\n        ret = self\n        while hasattr(ret, \"parent\"):\n            ret = ret.parent\n        return ret if isinstance(ret, SchemaABC) else None\n\n \n class Raw(Field):\n     \"\"\"Field that applies no formatting.\"\"\"",
    "output": " \n     parent = None\n     name = None\n    root = None\n \n     def serialize(self, attr, obj, accessor=None):\n         raise NotImplementedError\ndiff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py\n         :meth:`Schema._bind_field <marshmallow.Schema._bind_field>`.\n \n         :param str field_name: Field name set in schema.\n        :param Schema|Field schema: Parent object.\n         \"\"\"\n         self.parent = self.parent or schema\n         self.name = self.name or field_name\n        self.root = self.root or (\n            self.parent.root if isinstance(self.parent, FieldABC) else self.parent\n        )\n \n     def _serialize(self, value: typing.Any, attr: str, obj: typing.Any, **kwargs):\n         \"\"\"Serializes ``value`` to a basic Python datatype. Noop by default.\n         \"\"\"The context dictionary for the parent :class:`Schema`.\"\"\"\n         return self.parent.context\n \n \n class Raw(Field):\n     \"\"\"Field that applies no formatting.\"\"\""
  },
  {
    "instruction": "RFC: Change the way we store metadata?\nUsers are often bit by the fact that fields store arbitrary keyword arguments as metadata. See https://github.com/marshmallow-code/marshmallow/issues/683.\r\n\r\n> ...The reasons we use **kwargs instead of e.g. `metadata=` are mostly historical. The original decision was that storing kwargs 1) was more concise and 2) saved us from having to come up with an appropriate name... \"metadata\" didn't seem right because there are use cases where the things your storing aren't really metadata. At this point, it's not worth breaking the API.\r\n\r\n> Not the best reasons, but I think it's not terrible. We've discussed adding a [whitelist of metadata keys](https://github.com/marshmallow-code/marshmallow/issues/683#issuecomment-385113845) in the past, but we decided it wasn't worth the added API surface.\r\n\r\n_Originally posted by @sloria in https://github.com/marshmallow-code/marshmallow/issues/779#issuecomment-522283135_\r\n\r\nPossible solutions:\r\n\r\n1. Use `metadata=`.\r\n2. Specify a whitelist of allowed metadata arguments.\r\n\r\nFeedback welcome!\n",
    "input": "         its value will be present in the deserialized object. In the context of an\n         HTTP API, this effectively marks the field as \"read-only\".\n     :param dict error_messages: Overrides for `Field.default_error_messages`.\n    :param metadata: Extra arguments to be stored as metadata.\n \n     .. versionchanged:: 2.0.0\n         Removed `error` parameter. Use ``error_messages`` instead.\n         load_only: bool = False,\n         dump_only: bool = False,\n         error_messages: typing.Optional[typing.Dict[str, str]] = None,\n        **metadata\n     ) -> None:\n         self.default = default\n         self.attribute = attribute\n             raise ValueError(\"'missing' must not be set for required fields.\")\n         self.required = required\n         self.missing = missing\n        self.metadata = metadata\n         self._creation_index = Field._creation_index\n         Field._creation_index += 1\n ",
    "output": "         its value will be present in the deserialized object. In the context of an\n         HTTP API, this effectively marks the field as \"read-only\".\n     :param dict error_messages: Overrides for `Field.default_error_messages`.\n    :param metadata: Extra information to be stored as field metadata.\n \n     .. versionchanged:: 2.0.0\n         Removed `error` parameter. Use ``error_messages`` instead.\n         load_only: bool = False,\n         dump_only: bool = False,\n         error_messages: typing.Optional[typing.Dict[str, str]] = None,\n        metadata: typing.Optional[typing.Mapping[str, typing.Any]] = None,\n        **additional_metadata\n     ) -> None:\n         self.default = default\n         self.attribute = attribute\n             raise ValueError(\"'missing' must not be set for required fields.\")\n         self.required = required\n         self.missing = missing\n\n        metadata = metadata or {}\n        self.metadata = {**metadata, **additional_metadata}\n        if additional_metadata:\n            warnings.warn(\n                \"Passing field metadata as a keyword arg is deprecated. Use the \"\n                \"explicit `metadata=...` argument instead.\",\n                RemovedInMarshmallow4Warning,\n            )\n\n         self._creation_index = Field._creation_index\n         Field._creation_index += 1\n "
  },
  {
    "instruction": "Incorrect Email Validation\nhttps://github.com/marshmallow-code/marshmallow/blob/fbe22eb47db5df64b2c4133f9a5cb6c6920e8dd2/src/marshmallow/validate.py#L136-L151\r\n\r\nThe email validation regex will match `email@domain.com\\n`, `email\\n@domain.com`, and `email\\n@domain.com\\n`.\r\n\r\nThe issue is that `$` is used to match until the end of a string. Instead, `\\Z` should be used. - https://stackoverflow.com/a/48730645\r\n\r\nIt is possible that other validators might suffer from the same bug, so it would be good if other regexes were also checked.\r\n\r\nIt is unclear, but this may lead to a security vulnerability in some projects that use marshmallow (depending on how the validator is used), so a quick fix here might be helpful. In my quick look around I didn't notice anything critical, however, so I figured it would be fine to open this issue.\n",
    "input": "                 r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)',  # ...or ipv6\n                 r'(?::\\d+)?',  # optional port\n                 r')?' if relative else r'',  # host is optional, allow for relative URLs\n                r'(?:/?|[/?]\\S+)$',\n             )), re.IGNORECASE)\n \n         def __call__(self, relative, require_tld):\n     \"\"\"\n \n     USER_REGEX = re.compile(\n        r\"(^[-!#$%&'*+/=?^`{}|~\\w]+(\\.[-!#$%&'*+/=?^`{}|~\\w]+)*$\"  # dot-atom\n         # quoted-string\n         r'|^\"([\\001-\\010\\013\\014\\016-\\037!#-\\[\\]-\\177]'\n        r'|\\\\[\\001-\\011\\013\\014\\016-\\177])*\"$)', re.IGNORECASE | re.UNICODE)\n \n     DOMAIN_REGEX = re.compile(\n         # domain\n         r'(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+'\n        r'(?:[A-Z]{2,6}|[A-Z0-9-]{2,})$'\n         # literal form, ipv4 address (SMTP 4.1.3)\n         r'|^\\[(25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)'\n        r'(\\.(25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)){3}\\]$', re.IGNORECASE | re.UNICODE)\n \n     DOMAIN_WHITELIST = ('localhost',)\n ",
    "output": "                 r'\\[?[A-F0-9]*:[A-F0-9:]+\\]?)',  # ...or ipv6\n                 r'(?::\\d+)?',  # optional port\n                 r')?' if relative else r'',  # host is optional, allow for relative URLs\n                r'(?:/?|[/?]\\S+)\\Z',\n             )), re.IGNORECASE)\n \n         def __call__(self, relative, require_tld):\n     \"\"\"\n \n     USER_REGEX = re.compile(\n        r\"(^[-!#$%&'*+/=?^`{}|~\\w]+(\\.[-!#$%&'*+/=?^`{}|~\\w]+)*\\Z\"  # dot-atom\n         # quoted-string\n         r'|^\"([\\001-\\010\\013\\014\\016-\\037!#-\\[\\]-\\177]'\n        r'|\\\\[\\001-\\011\\013\\014\\016-\\177])*\"\\Z)', re.IGNORECASE | re.UNICODE)\n \n     DOMAIN_REGEX = re.compile(\n         # domain\n         r'(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+'\n        r'(?:[A-Z]{2,6}|[A-Z0-9-]{2,})\\Z'\n         # literal form, ipv4 address (SMTP 4.1.3)\n         r'|^\\[(25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)'\n        r'(\\.(25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)){3}\\]\\Z', re.IGNORECASE | re.UNICODE)\n \n     DOMAIN_WHITELIST = ('localhost',)\n "
  },
  {
    "instruction": "ValueError: SingleAxisTracker, Array, and running the model on a tuple/list of weather\n**Describe the bug**\r\nI know a refactoring of the Array with single axis tracking is in the works #1146. In the meantime, a `ValueError` is raised when trying to run a SingleAxisTracker defined with an array and supplying (ghi, dni, dhi) weather as a tuple/list. I would expect calling `run_model([weather])` would work similarly to a modelchain for a fixed system with an array singleton. The error stems from `pvlib.tracking.SingleAxisTracker.get_irradiance`  because most inputs are `pandas.Series`, but ghi, dhi, dni are `Tuple[Series]`.\r\n\r\n**To Reproduce**\r\n```python\r\nimport pandas as pd\r\nfrom pvlib.location import Location\r\nfrom pvlib.pvsystem import Array\r\nfrom pvlib.tracking import SingleAxisTracker\r\nfrom pvlib.modelchain import ModelChain\r\n\r\n\r\narray_params = {\r\n    \"surface_tilt\": None,\r\n    \"surface_azimuth\": None,\r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\r\n    \"albedo\": 0.2,\r\n    \"temperature_model_parameters\": {\r\n        \"u_c\": 29.0,\r\n        \"u_v\": 0.0,\r\n        \"eta_m\": 0.1,\r\n        \"alpha_absorption\": 0.9,\r\n    },\r\n    \"strings\": 5,\r\n    \"modules_per_string\": 7,\r\n    \"module_parameters\": {\r\n        \"alpha_sc\": 0.004539,\r\n        \"gamma_ref\": 1.2,\r\n        \"mu_gamma\": -0.003,\r\n        \"I_L_ref\": 5.11426,\r\n        \"I_o_ref\": 8.10251e-10,\r\n        \"R_sh_ref\": 381.254,\r\n        \"R_sh_0\": 400.0,\r\n        \"R_s\": 1.06602,\r\n        \"cells_in_series\": 96,\r\n        \"R_sh_exp\": 5.5,\r\n        \"EgRef\": 1.121,\r\n    },\r\n}\r\ninverter_parameters = {\r\n    \"Paco\": 250.0,\r\n    \"Pdco\": 259.589,\r\n    \"Vdco\": 40.0,\r\n    \"Pso\": 2.08961,\r\n    \"C0\": -4.1e-05,\r\n    \"C1\": -9.1e-05,\r\n    \"C2\": 0.000494,\r\n    \"C3\": -0.013171,\r\n    \"Pnt\": 0.075,\r\n}\r\n\r\n\r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)\r\n\r\n\r\ntracking = SingleAxisTracker(\r\n    arrays=[Array(**array_params, name=0)],\r\n    axis_tilt=0,\r\n    axis_azimuth=180,\r\n    gcr=0.1,\r\n    backtrack=True,\r\n    inverter_parameters=inverter_parameters,\r\n)\r\n\r\nweather = pd.DataFrame(\r\n    {\r\n        \"ghi\": [1100.0, 1101.0],\r\n        \"dni\": [1000.0, 1001],\r\n        \"dhi\": [100.0, 100],\r\n        \"module_temperature\": [25.0, 25],\r\n    },\r\n    index=pd.DatetimeIndex(\r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\r\n    ),\r\n)\r\nmc = ModelChain(\r\n    tracking,\r\n    location,\r\n    aoi_model=\"no_loss\",\r\n    spectral_model=\"no_loss\",\r\n)\r\nmc.run_model(weather)  # OK\r\nmc.run_model([weather])  # ValueError\r\n\r\n```\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.0-alpha.2+2.g47654a0\r\n\n",
    "input": " import pandas as pd\n \n from pvlib.tools import cosd, sind, tand\nfrom pvlib.pvsystem import PVSystem\n from pvlib import irradiance, atmosphere\n \n \n                              solar_zenith, solar_azimuth)\n         return aoi\n \n     def get_irradiance(self, surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n                        dni_extra=None, airmass=None, model='haydavies',\n         if airmass is None:\n             airmass = atmosphere.get_relative_airmass(solar_zenith)\n \n        return irradiance.get_total_irradiance(surface_tilt,\n                                               surface_azimuth,\n                                               solar_zenith,\n                                               solar_azimuth,\n                                               dni, ghi, dhi,\n                                               dni_extra=dni_extra,\n                                               airmass=airmass,\n                                               model=model,\n                                               albedo=self.albedo,\n                                               **kwargs)\n \n \n def singleaxis(apparent_zenith, apparent_azimuth,",
    "output": " import pandas as pd\n \n from pvlib.tools import cosd, sind, tand\nfrom pvlib.pvsystem import PVSystem, _unwrap_single_value\n from pvlib import irradiance, atmosphere\n \n \n                              solar_zenith, solar_azimuth)\n         return aoi\n \n    @_unwrap_single_value\n     def get_irradiance(self, surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n                        dni_extra=None, airmass=None, model='haydavies',\n         if airmass is None:\n             airmass = atmosphere.get_relative_airmass(solar_zenith)\n \n        # SingleAxisTracker only supports a single Array, but we need the\n        # validate/iterate machinery so that single length tuple input/output\n        # is handled the same as PVSystem.get_irradiance. GH 1159\n        dni = self._validate_per_array(dni, system_wide=True)\n        ghi = self._validate_per_array(ghi, system_wide=True)\n        dhi = self._validate_per_array(dhi, system_wide=True)\n\n        return tuple(\n            irradiance.get_total_irradiance(\n                surface_tilt,\n                surface_azimuth,\n                solar_zenith,\n                solar_azimuth,\n                dni, ghi, dhi,\n                dni_extra=dni_extra,\n                airmass=airmass,\n                model=model,\n                albedo=self.albedo,\n                **kwargs)\n            for array, dni, ghi, dhi in zip(\n                self.arrays, dni, ghi, dhi\n            )\n        )\n \n \n def singleaxis(apparent_zenith, apparent_azimuth,"
  },
  {
    "instruction": "`pvlib.soiling.hsu` takes `tilt` instead of `surface_tilt`\n`pvlib.soiling.hsu` takes a `tilt` parameter representing the same thing we normally call `surface_tilt`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L13-L14\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L33-L34\r\n\r\nI don't see any good reason for this naming inconsistency (I suspect `tilt` just got copied from the matlab implementation) and suggest we rename the parameter to `surface_tilt` with a deprecation.\r\n\r\nAlso, the docstring parameter type description says it must be `float`, but the model's reference explicitly says time series tilt is allowed: \r\n\r\n> The angle is variable for tracking systems and is taken as the average angle over the time step.\r\n\r\n\n",
    "input": " from pvlib.tools import cosd\n \n \ndef hsu(rainfall, cleaning_threshold, tilt, pm2_5, pm10,\n         depo_veloc=None, rain_accum_period=pd.Timedelta('1h')):\n     \"\"\"\n     Calculates soiling ratio given particulate and rain data using the\n         Amount of rain in an accumulation period needed to clean the PV\n         modules. [mm]\n \n    tilt : float\n         Tilt of the PV panels from horizontal. [degree]\n \n     pm2_5 : numeric\n     horiz_mass_rate = (\n         pm2_5 * depo_veloc['2_5'] + np.maximum(pm10 - pm2_5, 0.)\n         * depo_veloc['10']) * dt_sec\n    tilted_mass_rate = horiz_mass_rate * cosd(tilt)  # assuming no rain\n \n     # tms -> tilt_mass_rate\n     tms_cumsum = np.cumsum(tilted_mass_rate * np.ones(rainfall.shape))",
    "output": " from pvlib.tools import cosd\n \n \ndef hsu(rainfall, cleaning_threshold, surface_tilt, pm2_5, pm10,\n         depo_veloc=None, rain_accum_period=pd.Timedelta('1h')):\n     \"\"\"\n     Calculates soiling ratio given particulate and rain data using the\n         Amount of rain in an accumulation period needed to clean the PV\n         modules. [mm]\n \n    surface_tilt : numeric\n         Tilt of the PV panels from horizontal. [degree]\n \n     pm2_5 : numeric\n     horiz_mass_rate = (\n         pm2_5 * depo_veloc['2_5'] + np.maximum(pm10 - pm2_5, 0.)\n         * depo_veloc['10']) * dt_sec\n    tilted_mass_rate = horiz_mass_rate * cosd(surface_tilt)  # assuming no rain\n \n     # tms -> tilt_mass_rate\n     tms_cumsum = np.cumsum(tilted_mass_rate * np.ones(rainfall.shape))"
  },
  {
    "instruction": "_golden_sect_DataFrame changes in 0.9.4\n**Describe the bug**\r\n\r\n`0.9.4` introduced the following changes in the `_golden_sect_DataFrame`: We are checking `upper` and `lower` parameters and raise an error if `lower > upper`.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/81598e4fa8a9bd8fadaa7544136579c44885b3d1/pvlib/tools.py#L344-L345\r\n\r\n`_golden_sect_DataFrame` is used by `_lambertw`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/81598e4fa8a9bd8fadaa7544136579c44885b3d1/pvlib/singlediode.py#L644-L649\r\n\r\nI often have slightly negative `v_oc` values (really close to 0) when running simulations (second number in the array below):\r\n```\r\narray([ 9.46949758e-16, -8.43546518e-15,  2.61042547e-15,  3.82769773e-15,\r\n        1.01292315e-15,  4.81308106e+01,  5.12484772e+01,  5.22675087e+01,\r\n        5.20708941e+01,  5.16481028e+01,  5.12364071e+01,  5.09209060e+01,\r\n        5.09076598e+01,  5.10187680e+01,  5.11328118e+01,  5.13997628e+01,\r\n        5.15121386e+01,  5.05621451e+01,  4.80488068e+01,  7.18224446e-15,\r\n        1.21386700e-14,  6.40136698e-16,  4.36081007e-16,  6.51236255e-15])\r\n```\r\n\r\nIf we have one negative number in a large timeseries, the simulation will crash which seems too strict.\r\n\r\n**Expected behavior**\r\n\r\nThat would be great to either:\r\n* Have this data check be less strict and allow for slightly negative numbers, which are not going to affect the quality of the results.\r\n* On `_lambertw`: Do not allow negative `v_oc` and set negative values to `np.nan`, so that the error is not triggered. It will be up to the upstream code (user) to manage those `np.nan`.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: >= 0.9.4\r\n - ``pandas.__version__``: 1.5.3\r\n - python: 3.10.11\r\n\nsinglediode error with very low effective_irradiance\n**Describe the bug**\r\n\r\nSince pvlib 0.9.4 release (https://github.com/pvlib/pvlib-python/pull/1606) I get an error while running the single-diode model with some very low effective irradiance values.\r\n\r\n**To Reproduce**\r\n\r\n```python\r\nfrom pvlib import pvsystem\r\n\r\neffective_irradiance=1.341083e-17\r\ntemp_cell=13.7 \r\n\r\ncec_modules = pvsystem.retrieve_sam('CECMod')\r\ncec_module = cec_modules['Trina_Solar_TSM_300DEG5C_07_II_']\r\n\r\nmount = pvsystem.FixedMount()\r\narray = pvsystem.Array(mount=mount,\r\n                       module_parameters=cec_module)\r\n\r\nsystem = pvsystem.PVSystem(arrays=[array])\r\n\r\nparams = system.calcparams_cec(effective_irradiance, \r\n                               temp_cell)\r\n\r\nsystem.singlediode(*params)\r\n```\r\n\r\n```in _golden_sect_DataFrame(params, lower, upper, func, atol)\r\n    303 \"\"\"\r\n    304 Vectorized golden section search for finding maximum of a function of a\r\n    305 single variable.\r\n   (...)\r\n    342 pvlib.singlediode._pwr_optfcn\r\n    343 \"\"\"\r\n    344 if np.any(upper - lower < 0.):\r\n--> 345     raise ValueError('upper >= lower is required')\r\n    347 phim1 = (np.sqrt(5) - 1) / 2\r\n    349 df = params\r\n\r\nValueError: upper >= lower is required\r\n```\r\n\r\n**Expected behavior**\r\nThis complicates the bifacial modeling procedure as `run_model_from_effective_irradiance` can be called with very low irradiance values estimated by pvfactors (at sunrise or sunset for instance). \r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  0.9.4\r\n - ``pandas.__version__``: 1.5.3\r\n - python: 3.10\r\n\r\n**Additional context**\r\n\r\nv_oc is negative in this case which causes the error. \r\n\r\n```python\r\nfrom pvlib.singlediode import _lambertw_v_from_i\r\nphotocurrent = params[0]\r\nsaturation_current = params[1]\r\nresistance_series = params[2]\r\nresistance_shunt = params[3]\r\nnNsVth = params[4]\r\nv_oc = _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, 0.,\r\n                              saturation_current, photocurrent)\r\n```\r\n\n",
    "input": "     # Compute open circuit voltage\n     v_oc = _lambertw_v_from_i(0., **params)\n \n     # Find the voltage, v_mp, where the power is maximized.\n     # Start the golden section search at v_oc * 1.14\n     p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14, _pwr_optfcn)",
    "output": "     # Compute open circuit voltage\n     v_oc = _lambertw_v_from_i(0., **params)\n \n    # Set small elements <0 in v_oc to 0\n    if isinstance(v_oc, np.ndarray):\n        v_oc[(v_oc < 0) & (v_oc > -1e-12)] = 0.\n    elif isinstance(v_oc, (float, int)):\n        if v_oc < 0 and v_oc > -1e-12:\n            v_oc = 0.\n\n     # Find the voltage, v_mp, where the power is maximized.\n     # Start the golden section search at v_oc * 1.14\n     p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14, _pwr_optfcn)"
  },
  {
    "instruction": "Match `pvsystem.i_from_v`, `v_from_i` single diode parameters with `singlediode` order.\n**Is your feature request related to a problem? Please describe.**\r\nThe single diode model parameters for `i_from_v`, `v_from_i` in `pvsystem` are expected in a different order than `pvsystem.singlediode`.\r\nThis makes it difficult to pass the parameters to all of these functions using `*args`.\r\n\r\n**Describe the solution you'd like**\r\nGroup and reorder the single diode parameters of `i_from_v`, `v_from_i` to match the order of `singlediode`.\r\n\n",
    "input": " \n     while maxerr > eps and k < niter:\n         # Predict Voc\n        pvoc = v_from_i(rsh, rs, nnsvth, 0., tio, iph)\n \n         # Difference in Voc\n         dvoc = pvoc - voc\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n                            resistance_series, resistance_shunt, nNsVth,\n                            ivcurve_pnts=ivcurve_pnts)\n \n    def i_from_v(self, resistance_shunt, resistance_series, nNsVth, voltage,\n                 saturation_current, photocurrent):\n         \"\"\"Wrapper around the :py:func:`pvlib.pvsystem.i_from_v` function.\n \n        See :py:func:`pvsystem.i_from_v` for details\n         \"\"\"\n        return i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n                        saturation_current, photocurrent)\n \n     def get_ac(self, model, p_dc, v_dc=None):\n         r\"\"\"Calculates AC power from p_dc using the inverter model indicated\n     return out\n \n \ndef v_from_i(resistance_shunt, resistance_series, nNsVth, current,\n             saturation_current, photocurrent, method='lambertw'):\n     '''\n     Device voltage at the given device current for the single diode model.\n \n     the caller's responsibility to ensure that the arguments are all float64\n     and within the proper ranges.\n \n     Parameters\n     ----------\n    resistance_shunt : numeric\n        Shunt resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rsh``.\n        0 < resistance_shunt <= numpy.inf\n \n     resistance_series : numeric\n         Series resistance in ohms under desired IV curve conditions.\n         Often abbreviated ``Rs``.\n         0 <= resistance_series < numpy.inf\n \n     nNsVth : numeric\n         The product of three components. 1) The usual diode ideal factor\n         (n), 2) the number of cells in series (Ns), and 3) the cell\n         q is the charge of an electron (coulombs).\n         0 < nNsVth\n \n    current : numeric\n        The current in amperes under desired IV curve conditions.\n\n    saturation_current : numeric\n        Diode saturation current in amperes under desired IV curve\n        conditions. Often abbreviated ``I_0``.\n        0 < saturation_current\n\n    photocurrent : numeric\n        Light-generated current (photocurrent) in amperes under desired\n        IV curve conditions. Often abbreviated ``I_L``.\n        0 <= photocurrent\n\n     method : str\n         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:\n         ``'brentq'`` is limited to 1st quadrant only.\n     '''\n     if method.lower() == 'lambertw':\n         return _singlediode._lambertw_v_from_i(\n            resistance_shunt, resistance_series, nNsVth, current,\n            saturation_current, photocurrent\n         )\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\n         return V\n \n \ndef i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n             saturation_current, photocurrent, method='lambertw'):\n     '''\n     Device current at the given device voltage for the single diode model.\n \n     Uses the single diode model (SDM) as described in, e.g.,\n     Jain and Kapoor 2004 [1]_.\n     The solution is per Eq 2 of [1] except when resistance_series=0,\n     in which case the explict solution for current is used.\n     Ideal device parameters are specified by resistance_shunt=np.inf and\n     resistance_series=0.\n     Inputs to this function can include scalars and pandas.Series, but it is\n     the caller's responsibility to ensure that the arguments are all float64\n     and within the proper ranges.\n \n     Parameters\n     ----------\n    resistance_shunt : numeric\n        Shunt resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rsh``.\n        0 < resistance_shunt <= numpy.inf\n \n     resistance_series : numeric\n         Series resistance in ohms under desired IV curve conditions.\n         Often abbreviated ``Rs``.\n         0 <= resistance_series < numpy.inf\n \n     nNsVth : numeric\n         The product of three components. 1) The usual diode ideal factor\n         (n), 2) the number of cells in series (Ns), and 3) the cell\n         q is the charge of an electron (coulombs).\n         0 < nNsVth\n \n    voltage : numeric\n        The voltage in Volts under desired IV curve conditions.\n\n    saturation_current : numeric\n        Diode saturation current in amperes under desired IV curve\n        conditions. Often abbreviated ``I_0``.\n        0 < saturation_current\n\n    photocurrent : numeric\n        Light-generated current (photocurrent) in amperes under desired\n        IV curve conditions. Often abbreviated ``I_L``.\n        0 <= photocurrent\n\n     method : str\n         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:\n         ``'brentq'`` is limited to 1st quadrant only.\n     '''\n     if method.lower() == 'lambertw':\n         return _singlediode._lambertw_i_from_v(\n            resistance_shunt, resistance_series, nNsVth, voltage,\n            saturation_current, photocurrent\n         )\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\ndiff --git a/pvlib/singlediode.py b/pvlib/singlediode.py\n     return args, v0\n \n \ndef _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, current,\n                       saturation_current, photocurrent):\n     # Record if inputs were all scalar\n     output_is_scalar = all(map(np.isscalar,\n                               [resistance_shunt, resistance_series, nNsVth,\n                                current, saturation_current, photocurrent]))\n \n     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n     #  is generally more numerically stable\n     # Ensure that we are working with read-only views of numpy arrays\n     # Turns Series into arrays so that we don't have to worry about\n     #  multidimensional broadcasting failing\n    Gsh, Rs, a, I, I0, IL = \\\n        np.broadcast_arrays(conductance_shunt, resistance_series, nNsVth,\n                            current, saturation_current, photocurrent)\n \n     # Intitalize output V (I might not be float64)\n     V = np.full_like(I, np.nan, dtype=np.float64)\n         return V\n \n \ndef _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n                       saturation_current, photocurrent):\n     # Record if inputs were all scalar\n     output_is_scalar = all(map(np.isscalar,\n                               [resistance_shunt, resistance_series, nNsVth,\n                                voltage, saturation_current, photocurrent]))\n \n     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n     #  is generally more numerically stable\n     # Ensure that we are working with read-only views of numpy arrays\n     # Turns Series into arrays so that we don't have to worry about\n     #  multidimensional broadcasting failing\n    Gsh, Rs, a, V, I0, IL = \\\n        np.broadcast_arrays(conductance_shunt, resistance_series, nNsVth,\n                            voltage, saturation_current, photocurrent)\n \n     # Intitalize output I (V might not be float64)\n     I = np.full_like(V, np.nan, dtype=np.float64)           # noqa: E741, N806\n \n def _lambertw(photocurrent, saturation_current, resistance_series,\n               resistance_shunt, nNsVth, ivcurve_pnts=None):\n     # Compute short circuit current\n    i_sc = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth, 0.,\n                              saturation_current, photocurrent)\n \n     # Compute open circuit voltage\n    v_oc = _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, 0.,\n                              saturation_current, photocurrent)\n\n    params = {'r_sh': resistance_shunt,\n              'r_s': resistance_series,\n              'nNsVth': nNsVth,\n              'i_0': saturation_current,\n              'i_l': photocurrent}\n \n     # Find the voltage, v_mp, where the power is maximized.\n     # Start the golden section search at v_oc * 1.14\n    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,\n                                        _pwr_optfcn)\n \n     # Find Imp using Lambert W\n    i_mp = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth,\n                              v_mp, saturation_current, photocurrent)\n \n     # Find Ix and Ixx using Lambert W\n    i_x = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth,\n                             0.5 * v_oc, saturation_current, photocurrent)\n \n    i_xx = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth,\n                              0.5 * (v_oc + v_mp), saturation_current,\n                              photocurrent)\n \n     out = (i_sc, v_oc, i_mp, v_mp, p_mp, i_x, i_xx)\n \n         ivcurve_v = (np.asarray(v_oc)[..., np.newaxis] *\n                      np.linspace(0, 1, ivcurve_pnts))\n \n        ivcurve_i = _lambertw_i_from_v(resistance_shunt, resistance_series,\n                                       nNsVth, ivcurve_v.T, saturation_current,\n                                       photocurrent).T\n \n         out += (ivcurve_i, ivcurve_v)\n \n     Function to find power from ``i_from_v``.\n     '''\n \n    I = _lambertw_i_from_v(df['r_sh'], df['r_s'],           # noqa: E741, N806\n                           df['nNsVth'], df[loc], df['i_0'], df['i_l'])\n \n    return I * df[loc]\ndiff --git a/pvlib/tools.py b/pvlib/tools.py\n \n     phim1 = (np.sqrt(5) - 1) / 2\n \n    df = params\n     df['VH'] = upper\n     df['VL'] = lower\n ",
    "output": " \n     while maxerr > eps and k < niter:\n         # Predict Voc\n        pvoc = v_from_i(0., iph, tio, rs, rsh, nnsvth)\n \n         # Difference in Voc\n         dvoc = pvoc - voc\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n                            resistance_series, resistance_shunt, nNsVth,\n                            ivcurve_pnts=ivcurve_pnts)\n \n    def i_from_v(self, voltage, photocurrent, saturation_current,\n                 resistance_series, resistance_shunt, nNsVth):\n         \"\"\"Wrapper around the :py:func:`pvlib.pvsystem.i_from_v` function.\n \n        See :py:func:`pvlib.pvsystem.i_from_v` for details.\n\n        .. versionchanged:: 0.10.0\n           The function's arguments have been reordered.\n         \"\"\"\n        return i_from_v(voltage, photocurrent, saturation_current,\n                        resistance_series, resistance_shunt, nNsVth)\n \n     def get_ac(self, model, p_dc, v_dc=None):\n         r\"\"\"Calculates AC power from p_dc using the inverter model indicated\n     return out\n \n \ndef v_from_i(current, photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, method='lambertw'):\n     '''\n     Device voltage at the given device current for the single diode model.\n \n     the caller's responsibility to ensure that the arguments are all float64\n     and within the proper ranges.\n \n    .. versionchanged:: 0.10.0\n       The function's arguments have been reordered.\n\n     Parameters\n     ----------\n    current : numeric\n        The current in amperes under desired IV curve conditions.\n\n    photocurrent : numeric\n        Light-generated current (photocurrent) in amperes under desired\n        IV curve conditions. Often abbreviated ``I_L``.\n        0 <= photocurrent\n\n    saturation_current : numeric\n        Diode saturation current in amperes under desired IV curve\n        conditions. Often abbreviated ``I_0``.\n        0 < saturation_current\n \n     resistance_series : numeric\n         Series resistance in ohms under desired IV curve conditions.\n         Often abbreviated ``Rs``.\n         0 <= resistance_series < numpy.inf\n \n    resistance_shunt : numeric\n        Shunt resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rsh``.\n        0 < resistance_shunt <= numpy.inf\n\n     nNsVth : numeric\n         The product of three components. 1) The usual diode ideal factor\n         (n), 2) the number of cells in series (Ns), and 3) the cell\n         q is the charge of an electron (coulombs).\n         0 < nNsVth\n \n     method : str\n         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:\n         ``'brentq'`` is limited to 1st quadrant only.\n     '''\n     if method.lower() == 'lambertw':\n         return _singlediode._lambertw_v_from_i(\n            current, photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth\n         )\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\n         return V\n \n \ndef i_from_v(voltage, photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, method='lambertw'):\n     '''\n     Device current at the given device voltage for the single diode model.\n \n     Uses the single diode model (SDM) as described in, e.g.,\n    Jain and Kapoor 2004 [1]_.\n     The solution is per Eq 2 of [1] except when resistance_series=0,\n    in which case the explict solution for current is used.\n     Ideal device parameters are specified by resistance_shunt=np.inf and\n    resistance_series=0.\n     Inputs to this function can include scalars and pandas.Series, but it is\n    the caller's responsibility to ensure that the arguments are all float64\n    and within the proper ranges.\n\n    .. versionchanged:: 0.10.0\n       The function's arguments have been reordered.\n \n     Parameters\n     ----------\n    voltage : numeric\n        The voltage in Volts under desired IV curve conditions.\n\n    photocurrent : numeric\n        Light-generated current (photocurrent) in amperes under desired\n        IV curve conditions. Often abbreviated ``I_L``.\n        0 <= photocurrent\n\n    saturation_current : numeric\n        Diode saturation current in amperes under desired IV curve\n        conditions. Often abbreviated ``I_0``.\n        0 < saturation_current\n \n     resistance_series : numeric\n         Series resistance in ohms under desired IV curve conditions.\n         Often abbreviated ``Rs``.\n         0 <= resistance_series < numpy.inf\n \n    resistance_shunt : numeric\n        Shunt resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rsh``.\n        0 < resistance_shunt <= numpy.inf\n\n     nNsVth : numeric\n         The product of three components. 1) The usual diode ideal factor\n         (n), 2) the number of cells in series (Ns), and 3) the cell\n         q is the charge of an electron (coulombs).\n         0 < nNsVth\n \n     method : str\n         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:\n         ``'brentq'`` is limited to 1st quadrant only.\n     '''\n     if method.lower() == 'lambertw':\n         return _singlediode._lambertw_i_from_v(\n            voltage, photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth\n         )\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\ndiff --git a/pvlib/singlediode.py b/pvlib/singlediode.py\n     return args, v0\n \n \ndef _lambertw_v_from_i(current, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth):\n     # Record if inputs were all scalar\n     output_is_scalar = all(map(np.isscalar,\n                               (current, photocurrent, saturation_current,\n                                resistance_series, resistance_shunt, nNsVth)))\n \n     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n     #  is generally more numerically stable\n     # Ensure that we are working with read-only views of numpy arrays\n     # Turns Series into arrays so that we don't have to worry about\n     #  multidimensional broadcasting failing\n    I, IL, I0, Rs, Gsh, a = \\\n        np.broadcast_arrays(current, photocurrent, saturation_current,\n                            resistance_series, conductance_shunt, nNsVth)\n \n     # Intitalize output V (I might not be float64)\n     V = np.full_like(I, np.nan, dtype=np.float64)\n         return V\n \n \ndef _lambertw_i_from_v(voltage, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth):\n     # Record if inputs were all scalar\n     output_is_scalar = all(map(np.isscalar,\n                               (voltage, photocurrent, saturation_current,\n                                resistance_series, resistance_shunt, nNsVth)))\n \n     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n     #  is generally more numerically stable\n     # Ensure that we are working with read-only views of numpy arrays\n     # Turns Series into arrays so that we don't have to worry about\n     #  multidimensional broadcasting failing\n    V, IL, I0, Rs, Gsh, a = \\\n        np.broadcast_arrays(voltage, photocurrent, saturation_current,\n                            resistance_series, conductance_shunt, nNsVth)\n \n     # Intitalize output I (V might not be float64)\n     I = np.full_like(V, np.nan, dtype=np.float64)           # noqa: E741, N806\n \n def _lambertw(photocurrent, saturation_current, resistance_series,\n               resistance_shunt, nNsVth, ivcurve_pnts=None):\n    # collect args\n    params = {'photocurrent': photocurrent,\n              'saturation_current': saturation_current,\n              'resistance_series': resistance_series,\n              'resistance_shunt': resistance_shunt, 'nNsVth': nNsVth}\n\n     # Compute short circuit current\n    i_sc = _lambertw_i_from_v(0., **params)\n \n     # Compute open circuit voltage\n    v_oc = _lambertw_v_from_i(0., **params)\n \n     # Find the voltage, v_mp, where the power is maximized.\n     # Start the golden section search at v_oc * 1.14\n    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14, _pwr_optfcn)\n \n     # Find Imp using Lambert W\n    i_mp = _lambertw_i_from_v(v_mp, **params)\n \n     # Find Ix and Ixx using Lambert W\n    i_x = _lambertw_i_from_v(0.5 * v_oc, **params)\n \n    i_xx = _lambertw_i_from_v(0.5 * (v_oc + v_mp), **params)\n \n     out = (i_sc, v_oc, i_mp, v_mp, p_mp, i_x, i_xx)\n \n         ivcurve_v = (np.asarray(v_oc)[..., np.newaxis] *\n                      np.linspace(0, 1, ivcurve_pnts))\n \n        ivcurve_i = _lambertw_i_from_v(ivcurve_v.T, **params).T\n \n         out += (ivcurve_i, ivcurve_v)\n \n     Function to find power from ``i_from_v``.\n     '''\n \n    current = _lambertw_i_from_v(df[loc], df['photocurrent'],\n                                 df['saturation_current'],\n                                 df['resistance_series'],\n                                 df['resistance_shunt'], df['nNsVth'])\n \n    return current * df[loc]\ndiff --git a/pvlib/tools.py b/pvlib/tools.py\n \n     phim1 = (np.sqrt(5) - 1) / 2\n \n    df = params.copy()  # shallow copy to avoid modifying caller's dict\n     df['VH'] = upper\n     df['VL'] = lower\n "
  },
  {
    "instruction": "deprecate existing code in forecast.py, possibly replace with solarforecastarbiter shim\n`forecast.py` is a burden to maintain. I haven't used it in years, I don't think any of the other pvlib maintainers are interested in it, and I don't see any users stepping up to volunteer to maintain it. The code is not up to my present standards and I don't see how I'd get it there without a complete rewrite. This leads to difficult to track bugs such as the one recently reported on the [google group](https://groups.google.com/g/pvlib-python/c/b9HdgWV6w6g). It also complicates the pvlib dependencies.\r\n\r\n[solarforecastarbiter](https://github.com/SolarArbiter/solarforecastarbiter-core) includes a [reference_forecasts](https://github.com/SolarArbiter/solarforecastarbiter-core/tree/master/solarforecastarbiter/reference_forecasts) package that is much more robust. See [documentation here](https://solarforecastarbiter-core.readthedocs.io/en/latest/reference-forecasts.html) and [example notebook here](https://github.com/SolarArbiter/workshop/blob/master/reference_forecasts.ipynb) (no promises that this works without modification for the latest version).\r\n\r\nThe main reason to prefer `forecast.py` to `solarforecastarbiter` is the data fetch process. `forecast.py` pulls point data from a Unidata THREDDS server. `solarforecastarbiter.reference_forecasts` assumes you already have gridded data stored in a netcdf file. `solarforecastarbiter.io.nwp` provides functions to fetch that gridded data from NCEP. We have very good reasons for that approach in `solarforecastarbiter`, but I doubt that many `forecast.py` users are interested in configuring that two step process for their application.\r\n\r\nI'm very tempted to stop here, remove `forecast.py` after deprecation, and say \"not my problem anymore\", but it seems to attract a fair number of people to pvlib, so I hesitate to remove it without some kind of replacement. Let's explore a few ideas.\r\n\r\n1. Within `forecast.py`, rewrite code to fetch relevant data from Unidata. Make this function compatible with the expectations for the [`load_forecast`](https://github.com/SolarArbiter/solarforecastarbiter-core/blob/6200ec067bf83bc198a3af59da1d924d4124d4ec/solarforecastarbiter/reference_forecasts/models.py#L16-L19) function passed into `solarforecastarbiter.reference_forecasts.models` functions.\r\n2. Same as 1., except put that code somewhere else. Could be a documentation example, could be in solarforecastarbiter, or could be in a gist.\r\n3. Copy/refactor solarforecastarbiter code into `forecast.py`.\r\n4. Do nothing and let the forecast.py bugs and technical debt pile up. \r\n\r\nOther thoughts?\n",
    "input": " from siphon.ncss import NCSS\n \n import warnings\n \n \n warnings.warn(\n     'The API may change, the functionality may be consolidated into an io '\n     'module, or the module may be separated into its own package.')\n \n \n class ForecastModel:\n     \"\"\"\n     An object for querying and holding forecast model information for\n         return wind_speed\n \n \n class GFS(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing GFS\n         return data[self.output_variables]\n \n \n class HRRR_ESRL(ForecastModel):                                 # noqa: N801\n     \"\"\"\n     Subclass of the ForecastModel class representing\n         return data[self.output_variables]\n \n \n class NAM(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing NAM\n         return data[self.output_variables]\n \n \n class HRRR(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing HRRR\n         return data[self.output_variables]\n \n \n class NDFD(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing NDFD forecast\n         return data[self.output_variables]\n \n \n class RAP(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing RAP forecast model.",
    "output": " from siphon.ncss import NCSS\n \n import warnings\nfrom pvlib._deprecation import deprecated\n \n \n warnings.warn(\n     'The API may change, the functionality may be consolidated into an io '\n     'module, or the module may be separated into its own package.')\n \n_forecast_deprecated = deprecated(\n    since='0.9.1',\n    removal='a future release',\n    addendum='For details, see https://pvlib-python.readthedocs.io/en/stable/user_guide/forecasts.html'  # noqa: E501\n)\n \n# don't decorate the base class to prevent the subclasses from showing\n# duplicate warnings:\n# @_forecast_deprecated\n class ForecastModel:\n     \"\"\"\n     An object for querying and holding forecast model information for\n         return wind_speed\n \n \n@_forecast_deprecated\n class GFS(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing GFS\n         return data[self.output_variables]\n \n \n@_forecast_deprecated\n class HRRR_ESRL(ForecastModel):                                 # noqa: N801\n     \"\"\"\n     Subclass of the ForecastModel class representing\n         return data[self.output_variables]\n \n \n@_forecast_deprecated\n class NAM(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing NAM\n         return data[self.output_variables]\n \n \n@_forecast_deprecated\n class HRRR(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing HRRR\n         return data[self.output_variables]\n \n \n@_forecast_deprecated\n class NDFD(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing NDFD forecast\n         return data[self.output_variables]\n \n \n@_forecast_deprecated\n class RAP(ForecastModel):\n     \"\"\"\n     Subclass of the ForecastModel class representing RAP forecast model."
  },
  {
    "instruction": "document or support modules_per_string strings_per_inverter with pvwatts in modelchain\nHi, \r\n\r\nI am trying to run Modelchain with pvwatt model but it seems that the `modules_per_string` and `strings_per inverter ` doesn't have any affect on the total output. \r\n\r\nI am not sure why is it so. \r\nMay be ModelChain isn't supporting so. If that's the case how can I achieve the desired result?\r\n\r\nHere is my code: \r\n\r\nThanks in advance\r\n```\r\n# built-in python modules\r\nimport os\r\nimport inspect\r\n\r\n# scientific python add-ons\r\nimport numpy as np\r\nimport pandas as pd\r\n\r\n# plotting stuff\r\n# first line makes the plots appear in the notebook\r\n%matplotlib inline \r\nimport matplotlib.pyplot as plt\r\nimport matplotlib as mpl\r\n# seaborn makes your plots look better\r\ntry:\r\n    import seaborn as sns\r\n    sns.set(rc={\"figure.figsize\": (12, 6)})\r\n    sns.set_color_codes()\r\nexcept ImportError:\r\n    print('We suggest you install seaborn using conda or pip and rerun this cell')\r\n\r\n# finally, we import the pvlib library\r\nimport pvlib\r\n\r\ntmy = pd.read_csv(\"http://re.jrc.ec.europa.eu/pvgis5/tmy.php?lat=29.74&lon=40.10\")\r\n\r\ntmy.Date = pd.to_datetime(tmy.Date, format='%Y-%d-%m %H:%M:%S')\r\n\r\ntmy.rename(columns={' Ghor':'ghi','Dhor':'dhi','DNI':'dni','Tair':'temp_air',\r\n                        'Ws':'wind_speed'},inplace=True)\t\t\t\t\t\r\n\t\t\t\t\t\t\r\ntmy.set_index(tmy['Date'],inplace=True)\r\n    #Drop unnecessary column\r\n\r\ntmy = tmy.drop('Date', 1)\r\ntmy = tmy.drop('RH', 1)\r\ntmy = tmy.drop('IR', 1)\r\ntmy = tmy.drop(' Wd', 1)\r\ntmy = tmy.drop('Pres', 1)\r\n\r\n#module =Jinko_Solar_JKM320P_72_V\r\n#inverter = ABB__PVS980_58_2000kVA_K__N_A_V__CEC_2018_\r\n\r\nlat = 29.74\r\nlon = 40.10\r\naltitude = 676\r\ntz = 'Etc/GMT+3'  \r\n\r\nloc = pvlib.location.Location(latitude=lat,longitude= lon,tz=tz)\r\n\r\n#model = pvwatts \r\npvwatts_system = pvlib.pvsystem.PVSystem(module_parameters={'pdc0': 320, 'gamma_pdc': -0.0041},inverter_parameters={'pdc' : 3200000, 'pdc0' : 2024292, 'eta_inv_nom':0.988, 'eta_inv_ref':0.986},surface_tilt = 20, surface_azimuth=0,\r\n                    modules_per_string=30,strings_per_inverter=267, albedo = 0.2)\r\n\t\t\t\t\t\r\nmc = pvlib.modelchain.ModelChain(pvwatts_system, loc, transposition_model =\"perez\",aoi_model = 'ashrae',spectral_model='no_loss')\r\nprint(mc)\r\nmc.run_model(times=tmy.index,weather=tmy)\r\na = mc.ac\r\na = pd.Series.to_frame(a)\r\na = a * 530  # 530 = number of inverters in the system \r\n\r\na['month'] = a.index\r\na.month = a.month.dt.month\r\nmonthly = a.groupby('month').sum()\r\n```\r\n\r\n\n",
    "input": "         return self._singlediode(self.system.calcparams_pvsyst)\n \n     def pvwatts_dc(self):\n         self.results.dc = self.system.pvwatts_dc(\n             self.results.effective_irradiance, self.results.cell_temperature)\n         return self\n \n     @property\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         Parameters\n         ----------\n         data: DataFrame or tuple of DataFrame\n            Must contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n             'i_sc', 'p_mp'`.\n \n         Returns\n \n def scale_voltage_current_power(data, voltage=1, current=1):\n     \"\"\"\n    Scales the voltage, current, and power of the DataFrames\n    returned by :py:func:`singlediode` and :py:func:`sapm`.\n \n     Parameters\n     ----------\n     data: DataFrame\n        Must contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n         'i_sc', 'p_mp'`.\n     voltage: numeric, default 1\n         The amount by which to multiply the voltages.\n \n     # as written, only works with a DataFrame\n     # could make it work with a dict, but it would be more verbose\n    data = data.copy()\n    voltages = ['v_mp', 'v_oc']\n    currents = ['i_mp', 'i_x', 'i_xx', 'i_sc']\n    data[voltages] *= voltage\n    data[currents] *= current\n    data['p_mp'] *= voltage * current\n\n    return data\n \n \n def pvwatts_dc(g_poa_effective, temp_cell, pdc0, gamma_pdc, temp_ref=25.):\n     Parameters\n     ----------\n     g_poa_effective: numeric\n        Irradiance transmitted to the PV cells in units of W/m**2. To be\n         fully consistent with PVWatts, the user must have already\n         applied angle of incidence losses, but not soiling, spectral,\n        etc.\n     temp_cell: numeric\n        Cell temperature in degrees C.\n     pdc0: numeric\n        Power of the modules at 1000 W/m2 and cell reference temperature.\n     gamma_pdc: numeric\n        The temperature coefficient in units of 1/C. Typically -0.002 to\n        -0.005 per degree C.\n     temp_ref: numeric, default 25.0\n         Cell reference temperature. PVWatts defines it to be 25 C and\n        is included here for flexibility.\n \n     Returns\n     -------",
    "output": "         return self._singlediode(self.system.calcparams_pvsyst)\n \n     def pvwatts_dc(self):\n        \"\"\"Calculate DC power using the PVWatts model.\n\n        Results are stored in ModelChain.results.dc. DC power is computed\n        from PVSystem.module_parameters['pdc0'] and then scaled by\n        PVSystem.modules_per_string and PVSystem.strings_per_inverter.\n\n        Returns\n        -------\n        self\n\n        See also\n        --------\n        pvlib.pvsystem.PVSystem.pvwatts_dc\n        pvlib.pvsystem.PVSystem.scale_voltage_current_power\n        \"\"\"\n         self.results.dc = self.system.pvwatts_dc(\n             self.results.effective_irradiance, self.results.cell_temperature)\n        if isinstance(self.results.dc, tuple):\n            temp = tuple(\n                pd.DataFrame(s, columns=['p_mp']) for s in self.results.dc)\n        else:\n            temp = pd.DataFrame(self.results.dc, columns=['p_mp'])\n        scaled = self.system.scale_voltage_current_power(temp)\n        if isinstance(scaled, tuple):\n            self.results.dc = tuple(s['p_mp'] for s in scaled)\n        else:\n            self.results.dc = scaled['p_mp']\n         return self\n \n     @property\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         Parameters\n         ----------\n         data: DataFrame or tuple of DataFrame\n            May contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n             'i_sc', 'p_mp'`.\n \n         Returns\n \n def scale_voltage_current_power(data, voltage=1, current=1):\n     \"\"\"\n    Scales the voltage, current, and power in data by the voltage\n    and current factors.\n \n     Parameters\n     ----------\n     data: DataFrame\n        May contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n         'i_sc', 'p_mp'`.\n     voltage: numeric, default 1\n         The amount by which to multiply the voltages.\n \n     # as written, only works with a DataFrame\n     # could make it work with a dict, but it would be more verbose\n    voltage_keys = ['v_mp', 'v_oc']\n    current_keys = ['i_mp', 'i_x', 'i_xx', 'i_sc']\n    power_keys = ['p_mp']\n    voltage_df = data.filter(voltage_keys, axis=1) * voltage\n    current_df = data.filter(current_keys, axis=1) * current\n    power_df = data.filter(power_keys, axis=1) * voltage * current\n    df = pd.concat([voltage_df, current_df, power_df], axis=1)\n    df_sorted = df[data.columns]  # retain original column order\n    return df_sorted\n \n \n def pvwatts_dc(g_poa_effective, temp_cell, pdc0, gamma_pdc, temp_ref=25.):\n     Parameters\n     ----------\n     g_poa_effective: numeric\n        Irradiance transmitted to the PV cells. To be\n         fully consistent with PVWatts, the user must have already\n         applied angle of incidence losses, but not soiling, spectral,\n        etc. [W/m^2]\n     temp_cell: numeric\n        Cell temperature [C].\n     pdc0: numeric\n        Power of the modules at 1000 W/m^2 and cell reference temperature. [W]\n     gamma_pdc: numeric\n        The temperature coefficient of power. Typically -0.002 to\n        -0.005 per degree C. [1/C]\n     temp_ref: numeric, default 25.0\n         Cell reference temperature. PVWatts defines it to be 25 C and\n        is included here for flexibility. [C]\n \n     Returns\n     -------"
  },
  {
    "instruction": "Bug within scaling.py wavelet calculation methodology\n**Describe the bug**\r\nMathematical error within the wavelet computation for the scaling.py WVM implementation. Error arises from the methodology, as opposed to just a software bug. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```\r\nimport numpy as np\r\nfrom pvlib import scaling\r\ncs = np.random.rand(2**14)\r\nw, ts = scaling._compute_wavelet(cs,1)\r\nprint(np.all( (sum(w)-cs) < 1e-8 ))  # Returns False, expect True\r\n```\r\n\r\n**Expected behavior**\r\nFor a discrete wavelet transform (DWT) the sum of all wavelet modes should equate to the original data. \r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.7.2\r\n - ``pandas.__version__``: 1.2.3\r\n - python: 3.8.8\r\n\r\n**Additional context**\r\nThis bug is also present in the [PV_LIB](https://pvpmc.sandia.gov/applications/wavelet-variability-model/) Matlab version that was used as the basis for this code (I did reach out to them using the PVLIB MATLAB email form, but don't know who actually wrote that code). Essentially, the existing code throws away the highest level of Detail Coefficient in the transform and keeps an extra level of Approximation coefficient. The impact on the calculation is small, but leads to an incorrect DWT and reconstruction. I have a fix that makes the code pass the theoretical test about the DWT proposed under 'To Reproduce' but there may be some question as to whether this should be corrected or left alone to match the MATLAB code it was based on. \r\n\n",
    "input": " def wvm(clearsky_index, positions, cloud_speed, dt=None):\n     \"\"\"\n     Compute spatial aggregation time series smoothing on clear sky index based\n    on the Wavelet Variability model of Lave et al [1-2]. Implementation is\n    basically a port of the Matlab version of the code [3].\n \n     Parameters\n     ----------\n \n     References\n     ----------\n    [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n    Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n    Energy, vol. 4, no. 2, pp. 501-509, 2013.\n \n    [2] M. Lave and J. Kleissl. Cloud speed impact on solar variability\n    scaling - Application to the wavelet variability model. Solar Energy,\n    vol. 91, pp. 11-21, 2013.\n \n    [3] Wavelet Variability Model - Matlab Code:\n    https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n     \"\"\"\n \n     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n \n     References\n     ----------\n    [1] H. Moritz. Geodetic Reference System 1980, Journal of Geodesy, vol. 74,\n    no. 1, pp 128\u2013133, 2000.\n \n    [2] https://pypi.org/project/pyproj/\n \n    [3] Wavelet Variability Model - Matlab Code:\n    https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n     \"\"\"\n \n     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n \n def _compute_wavelet(clearsky_index, dt=None):\n     \"\"\"\n    Compute the wavelet transform on the input clear_sky time series.\n \n     Parameters\n     ----------\n     Returns\n     -------\n     wavelet: numeric\n        The individual wavelets for the time series\n \n     tmscales: numeric\n         The timescales associated with the wavelets in seconds [s]\n \n     References\n     ----------\n    [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n    Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n    Energy, vol. 4, no. 2, pp. 501-509, 2013.\n \n    [3] Wavelet Variability Model - Matlab Code:\n    https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n     \"\"\"\n \n     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n \n     # Compute wavelet time scales\n     min_tmscale = np.ceil(np.log(dt)/np.log(2))  # Minimum wavelet timescale\n    max_tmscale = int(12 - min_tmscale)  # maximum wavelet timescale\n \n     tmscales = np.zeros(max_tmscale)\n     csi_mean = np.zeros([max_tmscale, len(cs_long)])\n     # Loop for all time scales we will consider\n    for i in np.arange(0, max_tmscale):\n        j = i+1\n        tmscales[i] = 2**j * dt  # Wavelet integration time scale\n        intvlen = 2**j  # Wavelet integration time series interval\n         # Rolling average, retains only lower frequencies than interval\n         df = cs_long.rolling(window=intvlen, center=True, min_periods=1).mean()\n         # Fill nan's in both directions\n         df = df.fillna(method='bfill').fillna(method='ffill')\n         # Pop values back out of the dataframe and store\n         csi_mean[i, :] = df.values.flatten()\n \n    # Calculate the wavelets by isolating the rolling mean frequency ranges\n     wavelet_long = np.zeros(csi_mean.shape)\n     for i in np.arange(0, max_tmscale-1):\n         wavelet_long[i, :] = csi_mean[i, :] - csi_mean[i+1, :]\n    wavelet_long[max_tmscale-1, :] = csi_mean[max_tmscale-1, :]  # Lowest freq\n \n     # Clip off the padding and just return the original time window\n     wavelet = np.zeros([max_tmscale, len(vals)])\n     for i in np.arange(0, max_tmscale):\n        wavelet[i, :] = wavelet_long[i, len(vals)+1: 2*len(vals)+1]\n \n     return wavelet, tmscales",
    "output": " def wvm(clearsky_index, positions, cloud_speed, dt=None):\n     \"\"\"\n     Compute spatial aggregation time series smoothing on clear sky index based\n    on the Wavelet Variability model of Lave et al. [1]_, [2]_. Implementation\n    is basically a port of the Matlab version of the code [3]_.\n \n     Parameters\n     ----------\n \n     References\n     ----------\n    .. [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n       Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n       Energy, vol. 4, no. 2, pp. 501-509, 2013.\n \n    .. [2] M. Lave and J. Kleissl. Cloud speed impact on solar variability\n       scaling - Application to the wavelet variability model. Solar Energy,\n       vol. 91, pp. 11-21, 2013.\n \n    .. [3] Wavelet Variability Model - Matlab Code:\n       https://github.com/sandialabs/wvm\n     \"\"\"\n \n     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n \n     References\n     ----------\n    .. [1] H. Moritz. Geodetic Reference System 1980, Journal of Geodesy, vol.\n       74, no. 1, pp 128\u2013133, 2000.\n \n    .. [2] https://pypi.org/project/pyproj/\n \n    .. [3] Wavelet Variability Model - Matlab Code:\n       https://github.com/sandialabs/wvm\n     \"\"\"\n \n     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n \n def _compute_wavelet(clearsky_index, dt=None):\n     \"\"\"\n    Compute the wavelet transform on the input clear_sky time series. Uses a\n    top hat wavelet [-1,1,1,-1] shape, based on the difference of successive\n    centered moving averages. Smallest scale (filter size of 2) is a degenerate\n    case that resembles a Haar wavelet. Returns one level of approximation\n    coefficient (CAn) and n levels of detail coefficients (CD1, CD2, ...,\n    CDn-1, CDn).\n \n     Parameters\n     ----------\n     Returns\n     -------\n     wavelet: numeric\n        The individual wavelets for the time series. Format follows increasing\n        scale (decreasing frequency): [CD1, CD2, ..., CDn, CAn]\n \n     tmscales: numeric\n         The timescales associated with the wavelets in seconds [s]\n \n     References\n     ----------\n    .. [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n       Model (WVM) for Solar PV Power Plants. IEEE Transactions on\n       Sustainable Energy, vol. 4, no. 2, pp. 501-509, 2013.\n \n    .. [2] Wavelet Variability Model - Matlab Code:\n       https://github.com/sandialabs/wvm\n     \"\"\"\n \n     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n \n     # Compute wavelet time scales\n     min_tmscale = np.ceil(np.log(dt)/np.log(2))  # Minimum wavelet timescale\n    max_tmscale = int(13 - min_tmscale)  # maximum wavelet timescale\n \n     tmscales = np.zeros(max_tmscale)\n     csi_mean = np.zeros([max_tmscale, len(cs_long)])\n    # Skip averaging for the 0th scale\n    csi_mean[0, :] = cs_long.values.flatten()\n    tmscales[0] = 1\n     # Loop for all time scales we will consider\n    for i in np.arange(1, max_tmscale):\n        tmscales[i] = 2**i * dt  # Wavelet integration time scale\n        intvlen = 2**i  # Wavelet integration time series interval\n         # Rolling average, retains only lower frequencies than interval\n        # Produces slightly different end effects than the MATLAB version\n         df = cs_long.rolling(window=intvlen, center=True, min_periods=1).mean()\n         # Fill nan's in both directions\n         df = df.fillna(method='bfill').fillna(method='ffill')\n         # Pop values back out of the dataframe and store\n         csi_mean[i, :] = df.values.flatten()\n        # Shift to account for different indexing in MATLAB moving average\n        csi_mean[i, :] = np.roll(csi_mean[i, :], -1)\n        csi_mean[i, -1] = csi_mean[i, -2]\n \n    # Calculate detail coefficients by difference between successive averages\n     wavelet_long = np.zeros(csi_mean.shape)\n     for i in np.arange(0, max_tmscale-1):\n         wavelet_long[i, :] = csi_mean[i, :] - csi_mean[i+1, :]\n    wavelet_long[-1, :] = csi_mean[-1, :]  # Lowest freq (CAn)\n \n     # Clip off the padding and just return the original time window\n     wavelet = np.zeros([max_tmscale, len(vals)])\n     for i in np.arange(0, max_tmscale):\n        wavelet[i, :] = wavelet_long[i, len(vals): 2*len(vals)]\n \n     return wavelet, tmscales"
  },
  {
    "instruction": "regression: iam.physical returns nan for aoi > 90\u00b0 when n = 1\n**Describe the bug**\r\nFor pvlib==0.9.5, when n = 1 (no reflection) and aoi > 90\u00b0, we get nan as result.\r\n\r\n**To Reproduce**\r\n```python\r\nimport pvlib\r\npvlib.iam.physical(aoi=100, n=1)\r\n```\r\nreturns `nan`.\r\n\r\n**Expected behavior**\r\nThe result should be `0`, as it was for pvlib <= 0.9.4.\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: '0.9.5'\r\n - ``pandas.__version__``:  '1.5.3'\r\n - python: 3.10.4\r\n\n",
    "input": "     n2costheta2 = n2 * costheta\n \n     # reflectance of s-, p-polarized, and normal light by the first interface\n    rho12_s = ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2\n    rho12_p = ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2\n     rho12_0 = ((n1 - n2) / (n1 + n2)) ** 2\n \n     # transmittance through the first interface\n         tau_0 *= (1 - rho23_0) / (1 - rho23_0 * rho12_0)\n \n     # transmittance after absorption in the glass\n    tau_s *= np.exp(-K * L / costheta)\n    tau_p *= np.exp(-K * L / costheta)\n     tau_0 *= np.exp(-K * L)\n \n     # incidence angle modifier\n     iam = (tau_s + tau_p) / 2 / tau_0\n \n     return iam\n \n ",
    "output": "     n2costheta2 = n2 * costheta\n \n     # reflectance of s-, p-polarized, and normal light by the first interface\n    with np.errstate(divide='ignore', invalid='ignore'):\n        rho12_s = \\\n            ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2\n        rho12_p = \\\n            ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2\n\n     rho12_0 = ((n1 - n2) / (n1 + n2)) ** 2\n \n     # transmittance through the first interface\n         tau_0 *= (1 - rho23_0) / (1 - rho23_0 * rho12_0)\n \n     # transmittance after absorption in the glass\n    with np.errstate(divide='ignore', invalid='ignore'):\n        tau_s *= np.exp(-K * L / costheta)\n        tau_p *= np.exp(-K * L / costheta)\n\n     tau_0 *= np.exp(-K * L)\n \n     # incidence angle modifier\n     iam = (tau_s + tau_p) / 2 / tau_0\n \n    # for light coming from behind the plane, none can enter the module\n    # when n2 > 1, this is already the case\n    if np.isclose(n2, 1).any():\n        iam = np.where(aoi >= 90, 0, iam)\n        if isinstance(aoi, pd.Series):\n            iam = pd.Series(iam, index=aoi.index)\n\n     return iam\n \n "
  },
  {
    "instruction": "Add retrieval function for horizon profile from MINES Paris Tech\n<!-- Thank you for your contribution! The following items must be addressed before the code can be merged. Please don't hesitate to ask for help if you're unsure of how to accomplish any of the items. Feel free to remove checklist items that are not relevant to your change. -->\r\n\r\n - [x] I am familiar with the [contributing guidelines](https://pvlib-python.readthedocs.io/en/latest/contributing.html)\r\n - [x] Tests added\r\n - [x] Updates entries to [`docs/sphinx/source/api.rst`](https://github.com/pvlib/pvlib-python/blob/master/docs/sphinx/source/api.rst) for API changes.\r\n - [x] Adds description and name entries in the appropriate \"what's new\" file in [`docs/sphinx/source/whatsnew`](https://github.com/pvlib/pvlib-python/tree/master/docs/sphinx/source/whatsnew) for all changes. Includes link to the GitHub Issue with `` :issue:`num` `` or this Pull Request with `` :pull:`num` ``. Includes contributor name and/or GitHub username (link with `` :ghuser:`user` ``).\r\n - [x] New code is fully documented. Includes [numpydoc](https://numpydoc.readthedocs.io/en/latest/format.html) compliant docstrings, examples, and comments where necessary.\r\n - [x] Pull request is nearly complete and ready for detailed review.\r\n - [x] Maintainer: Appropriate GitHub Labels and Milestone are assigned to the Pull Request and linked Issue.\r\n\r\n<!-- Brief description of the problem and proposed solution (if not already fully described in the issue linked to above): -->\r\n\r\nThe proposed function retrieves the local horizon profile for a specific location (latitude, longitude, and elevation). The returned horizon profile has a resolution of 1 degree in the azimuth direction. The service is provided by MINES ParisTech though I cannot find any official documentation for it.\r\n\r\nThe function added in this PR (``pvlib.iotools.get_mines_horizon``) is very similar to the function added in #1395 (``pvlib.iotools.get_pvgis_horizon``).\n",
    "input": " from pvlib.iotools.pvgis import get_pvgis_tmy, read_pvgis_tmy  # noqa: F401\n from pvlib.iotools.pvgis import read_pvgis_hourly  # noqa: F401\n from pvlib.iotools.pvgis import get_pvgis_hourly  # noqa: F401\n from pvlib.iotools.bsrn import get_bsrn  # noqa: F401\n from pvlib.iotools.bsrn import read_bsrn  # noqa: F401\n from pvlib.iotools.bsrn import parse_bsrn  # noqa: F401\ndiff --git a/pvlib/iotools/pvgis.py b/pvlib/iotools/pvgis.py\n         data = data.rename(columns=VARIABLE_MAP)\n \n     return data, months_selected, inputs, meta",
    "output": " from pvlib.iotools.pvgis import get_pvgis_tmy, read_pvgis_tmy  # noqa: F401\n from pvlib.iotools.pvgis import read_pvgis_hourly  # noqa: F401\n from pvlib.iotools.pvgis import get_pvgis_hourly  # noqa: F401\nfrom pvlib.iotools.pvgis import get_pvgis_horizon  # noqa: F401\n from pvlib.iotools.bsrn import get_bsrn  # noqa: F401\n from pvlib.iotools.bsrn import read_bsrn  # noqa: F401\n from pvlib.iotools.bsrn import parse_bsrn  # noqa: F401\ndiff --git a/pvlib/iotools/pvgis.py b/pvlib/iotools/pvgis.py\n         data = data.rename(columns=VARIABLE_MAP)\n \n     return data, months_selected, inputs, meta\n\n\ndef get_pvgis_horizon(latitude, longitude, url=URL, **kwargs):\n    \"\"\"Get horizon data from PVGIS.\n\n    Parameters\n    ----------\n    latitude : float\n        Latitude in degrees north\n    longitude : float\n        Longitude in degrees east\n    url: str, default: :const:`pvlib.iotools.pvgis.URL`\n        Base URL for PVGIS\n    kwargs:\n        Passed to requests.get\n\n    Returns\n    -------\n    data : pd.Series\n        Pandas Series of the retrived horizon elevation angles. Index is the\n        corresponding horizon azimuth angles.\n    metadata : dict\n        Metadata returned by PVGIS.\n\n    Notes\n    -----\n    The horizon azimuths are specified clockwise from north, e.g., south=180.\n    This is the standard pvlib convention, although the PVGIS website specifies\n    south=0.\n\n    References\n    ----------\n    .. [1] `PVGIS horizon profile tool\n       <https://ec.europa.eu/jrc/en/PVGIS/tools/horizon>`_\n    \"\"\"\n    params = {'lat': latitude, 'lon': longitude, 'outputformat': 'json'}\n    res = requests.get(url + 'printhorizon', params=params, **kwargs)\n    if not res.ok:\n        try:\n            err_msg = res.json()\n        except Exception:\n            res.raise_for_status()\n        else:\n            raise requests.HTTPError(err_msg['message'])\n    json_output = res.json()\n    metadata = json_output['meta']\n    data = pd.DataFrame(json_output['outputs']['horizon_profile'])\n    data.columns = ['horizon_azimuth', 'horizon_elevation']\n    # Convert azimuth to pvlib convention (north=0, south=180)\n    data['horizon_azimuth'] += 180\n    data.set_index('horizon_azimuth', inplace=True)\n    data = data['horizon_elevation']  # convert to pd.Series\n    data = data[data.index < 360]  # remove duplicate north point (0 and 360)\n    return data, metadata"
  },
  {
    "instruction": "`pvsystem.calcparams_cec()` does not propagate parameters\n**Describe the bug**\r\n\r\nThe function calls `calcparams_desoto` with hardcoded reference values.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/40ba4bd5c8b91754aa73e638ed984ab9657847cd/pvlib/pvsystem.py#L1850-L1855\r\n\r\nThis means the function is silently ignoring its inputs, yielding incorrect results that may go unnoticed.\r\n\r\n\r\n**Expected behavior**\r\n\r\nThe function parameters are propagated into the `calcparams_desoto` call. In particular: `EgRef`, `dEgdT`, `irrad_ref`, `temp_ref`\n",
    "input": "                              alpha_sc*(1.0 - Adjust/100),\n                              a_ref, I_L_ref, I_o_ref,\n                              R_sh_ref, R_s,\n                             EgRef=1.121, dEgdT=-0.0002677,\n                             irrad_ref=1000, temp_ref=25)\n \n \n def calcparams_pvsyst(effective_irradiance, temp_cell,",
    "output": "                              alpha_sc*(1.0 - Adjust/100),\n                              a_ref, I_L_ref, I_o_ref,\n                              R_sh_ref, R_s,\n                             EgRef=EgRef, dEgdT=dEgdT,\n                             irrad_ref=irrad_ref, temp_ref=temp_ref)\n \n \n def calcparams_pvsyst(effective_irradiance, temp_cell,"
  },
  {
    "instruction": "irradiance.aoi can return NaN when module orientation is perfectly aligned with solar position\n**Describe the bug**\r\nI was playing with a dual-axis tracking mount with #1176 and found that when the modules are perfectly aligned with the sun (i.e. AOI should be exactly zero), floating point round-off can result in aoi projection values slightly greater than one, resulting in NaN aoi.  This only happens for some perfectly-aligned inputs (for example tilt=zenith=20, azimuth=180 returns aoi=0 as expected).\r\n\r\n**To Reproduce**\r\n```python\r\nimport pvlib\r\nzenith = 89.26778228223463\r\nazimuth = 60.932028605997004\r\nprint(pvlib.irradiance.aoi_projection(zenith, azimuth, zenith, azimuth))\r\nprint(pvlib.irradiance.aoi(zenith, azimuth, zenith, azimuth))\r\n\r\n# output:\r\n1.0000000000000002\r\nRuntimeWarning: invalid value encountered in arccos:  aoi_value = np.rad2deg(np.arccos(projection))\r\nnan\r\n```\r\n\r\n**Expected behavior**\r\nI expect aoi=0 whenever module orientation and solar position angles are identical.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: `0.9.0-alpha.4+14.g61650e9`\r\n - ``pandas.__version__``: `0.25.1`\r\n - ``numpy.__version__``: `1.17.0`\r\n - python: `3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]`\r\n\r\n**Additional context**\r\nSome ideas for fixes:\r\n1) In `irradiance.aoi_projection`, return a hard-coded `1.0` for inputs within some small tolerance\r\n2) In `irradiance.aoi_projection`, clamp return value to `[-1, +1]`\r\n3) In `irradiance.aoi`, clamp aoi_projection values to `[-1, +1`] before calling `arccos`\r\n4) Rework the `irradiance.aoi_projection` trig equations to not generate impossible values?\n",
    "input": "         tools.sind(surface_tilt) * tools.sind(solar_zenith) *\n         tools.cosd(solar_azimuth - surface_azimuth))\n \n     try:\n         projection.name = 'aoi_projection'\n     except AttributeError:",
    "output": "         tools.sind(surface_tilt) * tools.sind(solar_zenith) *\n         tools.cosd(solar_azimuth - surface_azimuth))\n \n    # GH 1185\n    projection = np.clip(projection, -1, 1)\n\n     try:\n         projection.name = 'aoi_projection'\n     except AttributeError:"
  },
  {
    "instruction": "backtracking for rare case when sun below tracker improvement\n**Describe the bug**\r\n- related to #656\r\n- in the rare case when the sun rays are below the tracker, then the top of the next row is shaded\r\n- currently tracker backtracks away from sun, back is facing sun instead of front\r\n- this only happens for tilted trackers and very low sun angles, either early morning or late evening when the sun rays are furthest north or south\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. create a tilted tracker\r\n```python\r\n# in Brazil so facing north\r\naxis_azimuth = 0.0 \r\naxis_tilt = 20\r\nmax_angle = 75.0\r\ngcr = 0.35\r\n```\r\n2. pick the earliest morning (or latest evening) timestamp\r\n```python\r\nimport pvlib\r\nimport pandas as pd\r\n\r\n# Brazil, timezone is UTC-3[hrs]\r\nstarttime = '2017-01-01T00:30:00-0300'\r\nstoptime = '2017-12-31T23:59:59-0300'\r\nlat, lon = -27.597300, -48.549610\r\ntimes = pd.DatetimeIndex(pd.date_range(\r\n    starttime, stoptime, freq='H'))\r\nsolpos = pvlib.solarposition.get_solarposition(\r\n    times, lat, lon)\r\n# get the early times\r\nts0 = '2017-01-01 05:30:00-03:00'\r\nts1 = '2017-01-01 12:30:00-03:00'\r\napparent_zenith = solpos['apparent_zenith'][ts0:ts1]\r\nazimuth = solpos['azimuth'][ts0:ts1]\r\nsat = pvlib.tracking.singleaxis(\r\n    apparent_zenith, azimuth, axis_tilt, axis_azimuth, max_angle, True, gcr)\r\n```\r\n3. notice that the tracker suddenly jumps from one side facing east to west\r\n```\r\n                           tracker_theta        aoi  surface_azimuth  surface_tilt\r\n2017-01-01 05:30:00-03:00     -21.964540  62.721237       310.299287     29.368272\r\n2017-01-01 06:30:00-03:00      16.231156  69.264752        40.403367     25.546154\r\n2017-01-01 07:30:00-03:00      69.073645  20.433849        82.548858     70.389280\r\n2017-01-01 08:30:00-03:00      54.554616  18.683626        76.316479     56.978562\r\n2017-01-01 09:30:00-03:00      40.131687  17.224233        67.917292     44.072837\r\n2017-01-01 10:30:00-03:00      25.769332  16.144347        54.683567     32.194782\r\n2017-01-01 11:30:00-03:00      11.439675  15.509532        30.610665     22.923644\r\n2017-01-01 12:30:00-03:00      -2.877428  15.358209       351.639727     20.197537\r\n```\r\n\r\n4. AOI is also wrong\r\n\r\n**Expected behavior**\r\nThe tracker should avoid shade. It should not jump from one direction to the other. If the sun ray is below the tracker then it will need to track to it's max rotation or backtrack. If there is shading at it's max rotation then it should track backtrack to zero, or perhaps parallel to the sun rays. Perhaps if bifacial, then it could go backwards, 180 from the correct backtrack position to show it's backside to the sun.\r\n\r\nproposed algorithm (_updated after [this comment](#issuecomment-559154895)_):\r\n```python\r\nif backtracking:\r\n    # cos(R) = L / Lx, R is rotation, L is surface length,\r\n    # Lx is shadow on ground, tracker shades when Lx > x\r\n    # x is row spacing related to GCR, x = L/GCR\r\n    lrot = np.cos(tr_rot_no_lim)  # tracker rotation not limited by max angle\r\n\r\n    # Note: if tr_rot > 90[deg] then lrot < 0 \r\n    # which *can* happen at low angles if axis tilt > 0\r\n    # tracker should never backtrack more than 90[deg], when lrot = 0\r\n    cos_rot = np.minimum(np.abs(lrot) / self.gcr, 1)\r\n\r\n    # so if lrot<0 tracker should backtrack forward\r\n    # backtrack_rot = np.sign(lrot) * np.arccos(cos_rot)\r\n\r\n    # NOTE: updated after comment from @kevinsa5 at Nov 27, 2019, 8:16 AM PST\r\n    # to remove sign()\r\n    backtrack_rot = np.arccos(cos_rot)\r\n```\r\n\r\nalso remove abs from aoi calculation\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/c699575cb6857674f0a96348b77e10c805e741c7/pvlib/tracking.py#L461\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  0.6.3\r\n - ``pandas.__version__``:  0.24\r\n - python: 3.7\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n[STY] remove old comments from single axis tracking\n**Describe the bug**\r\nAfter #823 is merged there may be stale comments in `pvlib.tracking.singleaxis` and commented code that can be removed. This might make the code more readable. It would also resolve some stickler complaints about long lines.\r\n\r\n**To Reproduce**\r\nComments to remove:\r\n1. [L375-L379](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L375-L379) - the tracking algorithm now follows [1] that uses clockwise rotation around z-axis from north\r\n2. [L393-L395](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L393-L395) - ditto\r\n3. [L400-L410](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L400-L410) - ditto\r\n4. [L441-L452](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L441-L452) - pvlib has been using arctan2(x,z) in `pvlib.tracking.singleaxis` for 6 years since 1fb82cc262e43e1d2b55e4b5510a1a5e7e340667, so I believe these comments are unnecessary now\r\n5. [L471-L472](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L471-L472) - this commented code was updated in #823, should we leave it or delete it?\r\n3. [L553-L555](../blob/e210b8253458a60c95fc21939e9817271cf51934/pvlib/tracking.py#L553-L555)\r\n\r\netc.\r\n\r\n[1] https://www.nrel.gov/docs/fy20osti/76626.pdf\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Screenshots**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: \r\n - ``pandas.__version__``: \r\n - python:\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n\n",
    "input": " import numpy as np\n import pandas as pd\n \nfrom pvlib.tools import cosd, sind\n from pvlib.pvsystem import _combine_localized_attributes\n from pvlib.pvsystem import PVSystem\n from pvlib.location import Location\n \n class SingleAxisTracker(PVSystem):\n     \"\"\"\n    Inherits the PV modeling methods from :py:class:`~pvlib.pvsystem.PVSystem`.\n\n \n     Parameters\n     ----------\n \n     axis_azimuth : float, default 0\n         A value denoting the compass direction along which the axis of\n        rotation lies. Measured in decimal degrees East of North.\n \n     max_angle : float, default 90\n         A value denoting the maximum rotation angle, in decimal degrees,\n         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n         provided, a gcr of 2/7 is default. gcr must be <=1.\n \n     \"\"\"\n \n    def __init__(self, axis_tilt=0, axis_azimuth=0,\n                 max_angle=90, backtrack=True, gcr=2.0/7.0, **kwargs):\n \n         self.axis_tilt = axis_tilt\n         self.axis_azimuth = axis_azimuth\n         self.max_angle = max_angle\n         self.backtrack = backtrack\n         self.gcr = gcr\n \n         kwargs['surface_tilt'] = None\n         kwargs['surface_azimuth'] = None\n         super(SingleAxisTracker, self).__init__(**kwargs)\n \n     def __repr__(self):\n        attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr']\n         sat_repr = ('SingleAxisTracker:\\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n         # get the parent PVSystem info\n         \"\"\"\n         tracking_data = singleaxis(apparent_zenith, apparent_azimuth,\n                                    self.axis_tilt, self.axis_azimuth,\n                                   self.max_angle,\n                                   self.backtrack, self.gcr)\n \n         return tracking_data\n \n \n class LocalizedSingleAxisTracker(SingleAxisTracker, Location):\n     \"\"\"\n    The LocalizedSingleAxisTracker class defines a standard set of\n    installed PV system attributes and modeling functions. This class\n    combines the attributes and methods of the SingleAxisTracker (a\n    subclass of PVSystem) and Location classes.\n\n    The LocalizedSingleAxisTracker may have bugs due to the difficulty\n    of robustly implementing multiple inheritance. See\n     :py:class:`~pvlib.modelchain.ModelChain` for an alternative paradigm\n     for modeling PV systems at specific locations.\n     \"\"\"\n \n def singleaxis(apparent_zenith, apparent_azimuth,\n                axis_tilt=0, axis_azimuth=0, max_angle=90,\n               backtrack=True, gcr=2.0/7.0):\n     \"\"\"\n    Determine the rotation angle of a single axis tracker when given a\n    particular sun zenith and azimuth angle. See [1]_ for details about\n    the equations.\n    Backtracking may be specified, and if so, a ground coverage\n    ratio is required.\n\n    Rotation angle is determined in a panel-oriented coordinate system.\n    The tracker azimuth axis_azimuth defines the positive y-axis; the\n    positive x-axis is 90 degress clockwise from the y-axis and parallel\n    to the earth surface, and the positive z-axis is normal and oriented\n    towards the sun. Rotation angle tracker_theta indicates tracker\n    position relative to horizontal: tracker_theta = 0 is horizontal,\n    and positive tracker_theta is a clockwise rotation around the y axis\n    in the x, y, z coordinate system. For example, if tracker azimuth\n    axis_azimuth is 180 (oriented south), tracker_theta = 30 is a\n    rotation of 30 degrees towards the west, and tracker_theta = -90 is\n    a rotation to the vertical plane facing east.\n \n     Parameters\n     ----------\n \n     axis_azimuth : float, default 0\n         A value denoting the compass direction along which the axis of\n        rotation lies. Measured in decimal degrees East of North.\n \n     max_angle : float, default 90\n         A value denoting the maximum rotation angle, in decimal degrees,\n         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n         provided, a gcr of 2/7 is default. gcr must be <=1.\n \n     Returns\n     -------\n     dict or DataFrame with the following columns:\n         * `tracker_theta`: The rotation angle of the tracker.\n           tracker_theta = 0 is horizontal, and positive rotation angles are\n          clockwise.\n         * `aoi`: The angle-of-incidence of direct irradiance onto the\n          rotated panel surface.\n         * `surface_tilt`: The angle between the panel surface and the earth\n          surface, accounting for panel rotation.\n         * `surface_azimuth`: The azimuth of the rotated panel, determined by\n           projecting the vector normal to the panel's surface to the earth's\n          surface.\n \n     References\n     ----------\n    .. [1] Lorenzo, E et al., 2011, \"Tracking and back-tracking\", Prog. in\n       Photovoltaics: Research and Applications, v. 19, pp. 747-753.\n     \"\"\"\n \n     # MATLAB to Python conversion by\n     if apparent_azimuth.ndim > 1 or apparent_zenith.ndim > 1:\n         raise ValueError('Input dimensions must not exceed 1')\n \n    # Calculate sun position x, y, z using coordinate system as in [1], Eq 2.\n\n    # Positive y axis is oriented parallel to earth surface along tracking axis\n    # (for the purpose of illustration, assume y is oriented to the south);\n    # positive x axis is orthogonal, 90 deg clockwise from y-axis, and parallel\n    # to the earth's surface (if y axis is south, x axis is west);\n    # positive z axis is normal to x, y axes, pointed upward.\n\n    # Equations in [1] assume solar azimuth is relative to reference vector\n    # pointed south, with clockwise positive.\n    # Here, the input solar azimuth is degrees East of North,\n    # i.e., relative to a reference vector pointed\n    # north with clockwise positive.\n    # Rotate sun azimuth to coordinate system as in [1]\n    # to calculate sun position.\n\n    az = apparent_azimuth - 180\n    apparent_elevation = 90 - apparent_zenith\n    x = cosd(apparent_elevation) * sind(az)\n    y = cosd(apparent_elevation) * cosd(az)\n    z = sind(apparent_elevation)\n\n    # translate array azimuth from compass bearing to [1] coord system\n    # wholmgren: strange to see axis_azimuth calculated differently from az,\n    # (not that it matters, or at least it shouldn't...).\n    axis_azimuth_south = axis_azimuth - 180\n\n    # translate input array tilt angle axis_tilt to [1] coordinate system.\n\n    # In [1] coordinates, axis_tilt is a rotation about the x-axis.\n    # For a system with array azimuth (y-axis) oriented south,\n    # the x-axis is oriented west, and a positive axis_tilt is a\n    # counterclockwise rotation, i.e, lifting the north edge of the panel.\n    # Thus, in [1] coordinate system, in the northern hemisphere a positive\n    # axis_tilt indicates a rotation toward the equator,\n    # whereas in the southern hemisphere rotation toward the equator is\n    # indicated by axis_tilt<0.  Here, the input axis_tilt is\n    # always positive and is a rotation toward the equator.\n\n    # Calculate sun position (xp, yp, zp) in panel-oriented coordinate system:\n    # positive y-axis is oriented along tracking axis at panel tilt;\n    # positive x-axis is orthogonal, clockwise, parallel to earth surface;\n    # positive z-axis is normal to x-y axes, pointed upward.\n    # Calculate sun position (xp,yp,zp) in panel coordinates using [1] Eq 11\n    # note that equation for yp (y' in Eq. 11 of Lorenzo et al 2011) is\n    # corrected, after conversation with paper's authors.\n\n    xp = x*cosd(axis_azimuth_south) - y*sind(axis_azimuth_south)\n    yp = (x*cosd(axis_tilt)*sind(axis_azimuth_south) +\n          y*cosd(axis_tilt)*cosd(axis_azimuth_south) -\n          z*sind(axis_tilt))\n    zp = (x*sind(axis_tilt)*sind(axis_azimuth_south) +\n          y*sind(axis_tilt)*cosd(axis_azimuth_south) +\n          z*cosd(axis_tilt))\n \n     # The ideal tracking angle wid is the rotation to place the sun position\n    # vector (xp, yp, zp) in the (y, z) plane; i.e., normal to the panel and\n    # containing the axis of rotation.  wid = 0 indicates that the panel is\n    # horizontal.  Here, our convention is that a clockwise rotation is\n     # positive, to view rotation angles in the same frame of reference as\n    # azimuth.  For example, for a system with tracking axis oriented south,\n    # a rotation toward the east is negative, and a rotation to the west is\n    # positive.\n\n    # Use arctan2 and avoid the tmp corrections.\n\n    # angle from x-y plane to projection of sun vector onto x-z plane\n#     tmp = np.degrees(np.arctan(zp/xp))\n\n    # Obtain wid by translating tmp to convention for rotation angles.\n    # Have to account for which quadrant of the x-z plane in which the sun\n    # vector lies.  Complete solution here but probably not necessary to\n    # consider QIII and QIV.\n#     wid = pd.Series(index=times)\n#     wid[(xp>=0) & (zp>=0)] =  90 - tmp[(xp>=0) & (zp>=0)]  # QI\n#     wid[(xp<0)  & (zp>=0)] = -90 - tmp[(xp<0)  & (zp>=0)]  # QII\n#     wid[(xp<0)  & (zp<0)]  = -90 - tmp[(xp<0)  & (zp<0)]   # QIII\n#     wid[(xp>=0) & (zp<0)]  =  90 - tmp[(xp>=0) & (zp<0)]   # QIV\n \n     # Calculate angle from x-y plane to projection of sun vector onto x-z plane\n    # and then obtain wid by translating tmp to convention for rotation angles.\n    wid = 90 - np.degrees(np.arctan2(zp, xp))\n \n     # filter for sun above panel horizon\n     zen_gt_90 = apparent_zenith > 90\n     wid[zen_gt_90] = np.nan\n \n    # Account for backtracking; modified from [1] to account for rotation\n    # angle convention being used here.\n     if backtrack:\n        axes_distance = 1/gcr\n        # clip needed for low angles. GH 656\n        temp = np.clip(axes_distance*cosd(wid), -1, 1)\n \n        # backtrack angle\n        # (always positive b/c acosd returns values between 0 and 180)\n        wc = np.degrees(np.arccos(temp))\n \n        # Eq 4 applied when wid in QIV (wid < 0 evalulates True), QI\n         with np.errstate(invalid='ignore'):\n            # errstate for GH 622\n            tracker_theta = np.where(wid < 0, wid + wc, wid - wc)\n     else:\n         tracker_theta = wid\n \n    tracker_theta = np.minimum(tracker_theta, max_angle)\n    tracker_theta = np.maximum(tracker_theta, -max_angle)\n \n    # calculate panel normal vector in panel-oriented x, y, z coordinates.\n    # y-axis is axis of tracker rotation.  tracker_theta is a compass angle\n     # (clockwise is positive) rather than a trigonometric angle.\n    # the *0 is a trick to preserve NaN values.\n     panel_norm = np.array([sind(tracker_theta),\n                            tracker_theta*0,\n                            cosd(tracker_theta)])\n     # calculate angle-of-incidence on panel\n     aoi = np.degrees(np.arccos(np.abs(np.sum(sun_vec*panel_norm, axis=0))))\n \n    # calculate panel tilt and azimuth\n    # in a coordinate system where the panel tilt is the\n    # angle from horizontal, and the panel azimuth is\n    # the compass angle (clockwise from north) to the projection\n    # of the panel's normal to the earth's surface.\n    # These outputs are provided for convenience and comparison\n    # with other PV software which use these angle conventions.\n \n    # project normal vector to earth surface.\n    # First rotate about x-axis by angle -axis_tilt so that y-axis is\n    # also parallel to earth surface, then project.\n \n     # Calculate standard rotation matrix\n     rot_x = np.array([[1, 0, 0],\n                       [0, cosd(-axis_tilt), -sind(-axis_tilt)],\n                       [0, sind(-axis_tilt), cosd(-axis_tilt)]])\n \n    # panel_norm_earth contains the normal vector\n    # expressed in earth-surface coordinates\n    # (z normal to surface, y aligned with tracker axis parallel to earth)\n     panel_norm_earth = np.dot(rot_x, panel_norm).T\n \n    # projection to plane tangent to earth surface,\n    # in earth surface coordinates\n     projected_normal = np.array([panel_norm_earth[:, 0],\n                                  panel_norm_earth[:, 1],\n                                  panel_norm_earth[:, 2]*0]).T\n     # calculate vector magnitudes\n     projected_normal_mag = np.sqrt(np.nansum(projected_normal**2, axis=1))\n \n    # renormalize the projected vector\n    # avoid creating nan values.\n     non_zeros = projected_normal_mag != 0\n     projected_normal[non_zeros] = (projected_normal[non_zeros].T /\n                                    projected_normal_mag[non_zeros]).T\n \n     # calculation of surface_azimuth\n    # 1. Find the angle.\n#     surface_azimuth = pd.Series(\n#         np.degrees(np.arctan(projected_normal[:,1]/projected_normal[:,0])),\n#                                 index=times)\n     surface_azimuth = \\\n         np.degrees(np.arctan2(projected_normal[:, 1], projected_normal[:, 0]))\n \n    # 2. Clean up atan when x-coord or y-coord is zero\n#     surface_azimuth[(projected_normal[:,0]==0) & (projected_normal[:,1]>0)] =  90\n#     surface_azimuth[(projected_normal[:,0]==0) & (projected_normal[:,1]<0)] =  -90\n#     surface_azimuth[(projected_normal[:,1]==0) & (projected_normal[:,0]>0)] =  0\n#     surface_azimuth[(projected_normal[:,1]==0) & (projected_normal[:,0]<0)] = 180\n\n    # 3. Correct atan for QII and QIII\n#     surface_azimuth[(projected_normal[:,0]<0) & (projected_normal[:,1]>0)] += 180 # QII\n#     surface_azimuth[(projected_normal[:,0]<0) & (projected_normal[:,1]<0)] += 180 # QIII\n\n    # 4. Skip to below\n\n    # at this point surface_azimuth contains angles between -90 and +270,\n    # where 0 is along the positive x-axis,\n    # the y-axis is in the direction of the tracker azimuth,\n    # and positive angles are rotations from the positive x axis towards\n    # the positive y-axis.\n    # Adjust to compass angles\n    # (clockwise rotation from 0 along the positive y-axis)\n#    surface_azimuth[surface_azimuth<=90] = 90 - surface_azimuth[surface_azimuth<=90]\n#    surface_azimuth[surface_azimuth>90] = 450 - surface_azimuth[surface_azimuth>90]\n\n    # finally rotate to align y-axis with true north\n    # PVLIB_MATLAB has this latitude correction,\n    # but I don't think it's latitude dependent if you always\n    # specify axis_azimuth with respect to North.\n#     if latitude > 0 or True:\n#         surface_azimuth = surface_azimuth - axis_azimuth\n#     else:\n#         surface_azimuth = surface_azimuth - axis_azimuth - 180\n#     surface_azimuth[surface_azimuth<0] = 360 + surface_azimuth[surface_azimuth<0]\n\n    # the commented code above is mostly part of PVLIB_MATLAB.\n    # My (wholmgren) take is that it can be done more simply.\n    # Say that we're pointing along the postive x axis (likely west).\n    # We just need to rotate 90 degrees to get from the x axis\n    # to the y axis (likely south),\n    # and then add the axis_azimuth to get back to North.\n    # Anything left over is the azimuth that we want,\n    # and we can map it into the [0,360) domain.\n\n    # 4. Rotate 0 reference from panel's x axis to it's y axis and\n    #    then back to North.\n     surface_azimuth = 90 - surface_azimuth + axis_azimuth\n \n    # 5. Map azimuth into [0,360) domain.\n    # surface_azimuth[surface_azimuth < 0] += 360\n    # surface_azimuth[surface_azimuth >= 360] -= 360\n    surface_azimuth = surface_azimuth % 360\n \n     # Calculate surface_tilt\n     dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n \n     # Bundle DataFrame for return values and filter for sun below horizon.\n     out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n           'surface_azimuth': surface_azimuth, 'surface_tilt': surface_tilt}\n     if index is not None:\n         out = pd.DataFrame(out, index=index)\n         out = out[['tracker_theta', 'aoi', 'surface_azimuth', 'surface_tilt']]\n         out = {k: np.where(zen_gt_90, np.nan, v) for k, v in out.items()}\n \n     return out",
    "output": " import numpy as np\n import pandas as pd\n \nfrom pvlib.tools import cosd, sind, tand\n from pvlib.pvsystem import _combine_localized_attributes\n from pvlib.pvsystem import PVSystem\n from pvlib.location import Location\n \n class SingleAxisTracker(PVSystem):\n     \"\"\"\n    A class for single-axis trackers that inherits the PV modeling methods from\n    :py:class:`~pvlib.pvsystem.PVSystem`. For details on calculating tracker\n    rotation see :py:func:`pvlib.tracking.singleaxis`.\n \n     Parameters\n     ----------\n \n     axis_azimuth : float, default 0\n         A value denoting the compass direction along which the axis of\n        rotation lies. Measured in decimal degrees east of north.\n \n     max_angle : float, default 90\n         A value denoting the maximum rotation angle, in decimal degrees,\n         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n         provided, a gcr of 2/7 is default. gcr must be <=1.\n \n    cross_axis_tilt : float, default 0.0\n        The angle, relative to horizontal, of the line formed by the\n        intersection between the slope containing the tracker axes and a plane\n        perpendicular to the tracker axes. Cross-axis tilt should be specified\n        using a right-handed convention. For example, trackers with axis\n        azimuth of 180 degrees (heading south) will have a negative cross-axis\n        tilt if the tracker axes plane slopes down to the east and positive\n        cross-axis tilt if the tracker axes plane slopes up to the east. Use\n        :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n        `cross_axis_tilt`. [degrees]\n\n    See also\n    --------\n    pvlib.tracking.singleaxis\n    pvlib.tracking.calc_axis_tilt\n    pvlib.tracking.calc_cross_axis_tilt\n     \"\"\"\n \n    def __init__(self, axis_tilt=0, axis_azimuth=0, max_angle=90,\n                 backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0.0, **kwargs):\n \n         self.axis_tilt = axis_tilt\n         self.axis_azimuth = axis_azimuth\n         self.max_angle = max_angle\n         self.backtrack = backtrack\n         self.gcr = gcr\n        self.cross_axis_tilt = cross_axis_tilt\n \n         kwargs['surface_tilt'] = None\n         kwargs['surface_azimuth'] = None\n         super(SingleAxisTracker, self).__init__(**kwargs)\n \n     def __repr__(self):\n        attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr',\n                 'cross_axis_tilt']\n         sat_repr = ('SingleAxisTracker:\\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n         # get the parent PVSystem info\n         \"\"\"\n         tracking_data = singleaxis(apparent_zenith, apparent_azimuth,\n                                    self.axis_tilt, self.axis_azimuth,\n                                   self.max_angle, self.backtrack,\n                                   self.gcr, self.cross_axis_tilt)\n \n         return tracking_data\n \n \n class LocalizedSingleAxisTracker(SingleAxisTracker, Location):\n     \"\"\"\n    The :py:class:`~pvlib.tracking.LocalizedSingleAxisTracker` class defines a\n    standard set of installed PV system attributes and modeling functions. This\n    class combines the attributes and methods of the\n    :py:class:`~pvlib.tracking.SingleAxisTracker` (a subclass of\n    :py:class:`~pvlib.pvsystem.PVSystem`) and\n    :py:class:`~pvlib.location.Location` classes.\n\n    The :py:class:`~pvlib.tracking.LocalizedSingleAxisTracker` may have bugs\n    due to the difficulty of robustly implementing multiple inheritance. See\n     :py:class:`~pvlib.modelchain.ModelChain` for an alternative paradigm\n     for modeling PV systems at specific locations.\n     \"\"\"\n \n def singleaxis(apparent_zenith, apparent_azimuth,\n                axis_tilt=0, axis_azimuth=0, max_angle=90,\n               backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0):\n     \"\"\"\n    Determine the rotation angle of a single-axis tracker when given particular\n    solar zenith and azimuth angles.\n\n    See [1]_ for details about the equations. Backtracking may be specified,\n    and if so, a ground coverage ratio is required.\n\n    Rotation angle is determined in a right-handed coordinate system. The\n    tracker `axis_azimuth` defines the positive y-axis, the positive x-axis is\n    90 degrees clockwise from the y-axis and parallel to the Earth's surface,\n    and the positive z-axis is normal to both x & y-axes and oriented skyward.\n    Rotation angle `tracker_theta` is a right-handed rotation around the y-axis\n    in the x, y, z coordinate system and indicates tracker position relative to\n    horizontal. For example, if tracker `axis_azimuth` is 180 (oriented south)\n    and `axis_tilt` is zero, then a `tracker_theta` of zero is horizontal, a\n    `tracker_theta` of 30 degrees is a rotation of 30 degrees towards the west,\n    and a `tracker_theta` of -90 degrees is a rotation to the vertical plane\n    facing east.\n \n     Parameters\n     ----------\n \n     axis_azimuth : float, default 0\n         A value denoting the compass direction along which the axis of\n        rotation lies. Measured in decimal degrees east of north.\n \n     max_angle : float, default 90\n         A value denoting the maximum rotation angle, in decimal degrees,\n         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n         provided, a gcr of 2/7 is default. gcr must be <=1.\n \n    cross_axis_tilt : float, default 0.0\n        The angle, relative to horizontal, of the line formed by the\n        intersection between the slope containing the tracker axes and a plane\n        perpendicular to the tracker axes. Cross-axis tilt should be specified\n        using a right-handed convention. For example, trackers with axis\n        azimuth of 180 degrees (heading south) will have a negative cross-axis\n        tilt if the tracker axes plane slopes down to the east and positive\n        cross-axis tilt if the tracker axes plane slopes up to the east. Use\n        :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n        `cross_axis_tilt`. [degrees]\n\n     Returns\n     -------\n     dict or DataFrame with the following columns:\n         * `tracker_theta`: The rotation angle of the tracker.\n           tracker_theta = 0 is horizontal, and positive rotation angles are\n          clockwise. [degrees]\n         * `aoi`: The angle-of-incidence of direct irradiance onto the\n          rotated panel surface. [degrees]\n         * `surface_tilt`: The angle between the panel surface and the earth\n          surface, accounting for panel rotation. [degrees]\n         * `surface_azimuth`: The azimuth of the rotated panel, determined by\n           projecting the vector normal to the panel's surface to the earth's\n          surface. [degrees]\n\n    See also\n    --------\n    pvlib.tracking.calc_axis_tilt\n    pvlib.tracking.calc_cross_axis_tilt\n \n     References\n     ----------\n    .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n       Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n       https://www.nrel.gov/docs/fy20osti/76626.pdf\n     \"\"\"\n \n     # MATLAB to Python conversion by\n     if apparent_azimuth.ndim > 1 or apparent_zenith.ndim > 1:\n         raise ValueError('Input dimensions must not exceed 1')\n \n    # Calculate sun position x, y, z using coordinate system as in [1], Eq 1.\n\n    # NOTE: solar elevation = 90 - solar zenith, then use trig identities:\n    # sin(90-x) = cos(x) & cos(90-x) = sin(x)\n    sin_zenith = sind(apparent_zenith)\n    x = sin_zenith * sind(apparent_azimuth)\n    y = sin_zenith * cosd(apparent_azimuth)\n    z = cosd(apparent_zenith)\n\n    # Assume the tracker reference frame is right-handed. Positive y-axis is\n    # oriented along tracking axis; from north, the y-axis is rotated clockwise\n    # by the axis azimuth and tilted from horizontal by the axis tilt. The\n    # positive x-axis is 90 deg clockwise from the y-axis and parallel to\n    # horizontal (e.g., if the y-axis is south, the x-axis is west); the\n    # positive z-axis is normal to the x and y axes, pointed upward.\n\n    # Calculate sun position (xp, yp, zp) in tracker coordinate system using\n    # [1] Eq 4.\n\n    cos_axis_azimuth = cosd(axis_azimuth)\n    sin_axis_azimuth = sind(axis_azimuth)\n    cos_axis_tilt = cosd(axis_tilt)\n    sin_axis_tilt = sind(axis_tilt)\n    xp = x*cos_axis_azimuth - y*sin_axis_azimuth\n    yp = (x*cos_axis_tilt*sin_axis_azimuth\n          + y*cos_axis_tilt*cos_axis_azimuth\n          - z*sin_axis_tilt)\n    zp = (x*sin_axis_tilt*sin_axis_azimuth\n          + y*sin_axis_tilt*cos_axis_azimuth\n          + z*cos_axis_tilt)\n \n     # The ideal tracking angle wid is the rotation to place the sun position\n    # vector (xp, yp, zp) in the (y, z) plane, which is normal to the panel and\n    # contains the axis of rotation.  wid = 0 indicates that the panel is\n    # horizontal. Here, our convention is that a clockwise rotation is\n     # positive, to view rotation angles in the same frame of reference as\n    # azimuth. For example, for a system with tracking axis oriented south, a\n    # rotation toward the east is negative, and a rotation to the west is\n    # positive. This is a right-handed rotation around the tracker y-axis.\n \n     # Calculate angle from x-y plane to projection of sun vector onto x-z plane\n    # using [1] Eq. 5.\n\n    wid = np.degrees(np.arctan2(xp, zp))\n \n     # filter for sun above panel horizon\n     zen_gt_90 = apparent_zenith > 90\n     wid[zen_gt_90] = np.nan\n \n    # Account for backtracking\n     if backtrack:\n        # distance between rows in terms of rack lengths relative to cross-axis\n        # tilt\n        axes_distance = 1/(gcr * cosd(cross_axis_tilt))\n \n        # NOTE: account for rare angles below array, see GH 824\n        temp = np.abs(axes_distance * cosd(wid - cross_axis_tilt))\n \n        # backtrack angle using [1], Eq. 14\n         with np.errstate(invalid='ignore'):\n            wc = np.degrees(-np.sign(wid)*np.arccos(temp))\n\n        # NOTE: in the middle of the day, arccos(temp) is out of range because\n        # there's no row-to-row shade to avoid, & backtracking is unnecessary\n        # [1], Eqs. 15-16\n        with np.errstate(invalid='ignore'):\n            tracker_theta = wid + np.where(temp < 1, wc, 0)\n     else:\n         tracker_theta = wid\n \n    # NOTE: max_angle defined relative to zero-point rotation, not the\n    # system-plane normal\n    tracker_theta = np.clip(tracker_theta, -max_angle, max_angle)\n \n    # Calculate panel normal vector in panel-oriented x, y, z coordinates.\n    # y-axis is axis of tracker rotation. tracker_theta is a compass angle\n     # (clockwise is positive) rather than a trigonometric angle.\n    # NOTE: the *0 is a trick to preserve NaN values.\n     panel_norm = np.array([sind(tracker_theta),\n                            tracker_theta*0,\n                            cosd(tracker_theta)])\n     # calculate angle-of-incidence on panel\n     aoi = np.degrees(np.arccos(np.abs(np.sum(sun_vec*panel_norm, axis=0))))\n \n    # Calculate panel tilt and azimuth in a coordinate system where the panel\n    # tilt is the angle from horizontal, and the panel azimuth is the compass\n    # angle (clockwise from north) to the projection of the panel's normal to\n    # the earth's surface. These outputs are provided for convenience and\n    # comparison with other PV software which use these angle conventions.\n \n    # Project normal vector to earth surface. First rotate about x-axis by\n    # angle -axis_tilt so that y-axis is also parallel to earth surface, then\n    # project.\n \n     # Calculate standard rotation matrix\n     rot_x = np.array([[1, 0, 0],\n                       [0, cosd(-axis_tilt), -sind(-axis_tilt)],\n                       [0, sind(-axis_tilt), cosd(-axis_tilt)]])\n \n    # panel_norm_earth contains the normal vector expressed in earth-surface\n    # coordinates (z normal to surface, y aligned with tracker axis parallel to\n    # earth)\n     panel_norm_earth = np.dot(rot_x, panel_norm).T\n \n    # projection to plane tangent to earth surface, in earth surface\n    # coordinates\n     projected_normal = np.array([panel_norm_earth[:, 0],\n                                  panel_norm_earth[:, 1],\n                                  panel_norm_earth[:, 2]*0]).T\n     # calculate vector magnitudes\n     projected_normal_mag = np.sqrt(np.nansum(projected_normal**2, axis=1))\n \n    # renormalize the projected vector, avoid creating nan values.\n     non_zeros = projected_normal_mag != 0\n     projected_normal[non_zeros] = (projected_normal[non_zeros].T /\n                                    projected_normal_mag[non_zeros]).T\n \n     # calculation of surface_azimuth\n     surface_azimuth = \\\n         np.degrees(np.arctan2(projected_normal[:, 1], projected_normal[:, 0]))\n \n    # Rotate 0 reference from panel's x-axis to its y-axis and then back to\n    # north.\n     surface_azimuth = 90 - surface_azimuth + axis_azimuth\n \n    # Map azimuth into [0,360) domain.\n    with np.errstate(invalid='ignore'):\n        surface_azimuth = surface_azimuth % 360\n \n     # Calculate surface_tilt\n     dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n \n     # Bundle DataFrame for return values and filter for sun below horizon.\n     out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n           'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\n     if index is not None:\n         out = pd.DataFrame(out, index=index)\n         out = out[['tracker_theta', 'aoi', 'surface_azimuth', 'surface_tilt']]\n         out = {k: np.where(zen_gt_90, np.nan, v) for k, v in out.items()}\n \n     return out\n\n\ndef calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth):\n    \"\"\"\n    Calculate tracker axis tilt in the global reference frame when on a sloped\n    plane.\n\n    Parameters\n    ----------\n    slope_azimuth : float\n        direction of normal to slope on horizontal [degrees]\n    slope_tilt : float\n        tilt of normal to slope relative to vertical [degrees]\n    axis_azimuth : float\n        direction of tracker axes on horizontal [degrees]\n\n    Returns\n    -------\n    axis_tilt : float\n        tilt of tracker [degrees]\n\n    See also\n    --------\n    pvlib.tracking.singleaxis\n    pvlib.tracking.calc_cross_axis_tilt\n\n    Notes\n    -----\n    See [1]_ for derivation of equations.\n\n    References\n    ----------\n    .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n       Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n       https://www.nrel.gov/docs/fy20osti/76626.pdf\n    \"\"\"\n    delta_gamma = axis_azimuth - slope_azimuth\n    # equations 18-19\n    tan_axis_tilt = cosd(delta_gamma) * tand(slope_tilt)\n    return np.degrees(np.arctan(tan_axis_tilt))\n\n\ndef _calc_tracker_norm(ba, bg, dg):\n    \"\"\"\n    Calculate tracker normal, v, cross product of tracker axis and unit normal,\n    N, to the system slope plane.\n\n    Parameters\n    ----------\n    ba : float\n        axis tilt [degrees]\n    bg : float\n        ground tilt [degrees]\n    dg : float\n        delta gamma, difference between axis and ground azimuths [degrees]\n\n    Returns\n    -------\n    vector : tuple\n        vx, vy, vz\n    \"\"\"\n    cos_ba = cosd(ba)\n    cos_bg = cosd(bg)\n    sin_bg = sind(bg)\n    sin_dg = sind(dg)\n    vx = sin_dg * cos_ba * cos_bg\n    vy = sind(ba)*sin_bg + cosd(dg)*cos_ba*cos_bg\n    vz = -sin_dg*sin_bg*cos_ba\n    return vx, vy, vz\n\n\ndef _calc_beta_c(v, dg, ba):\n    \"\"\"\n    Calculate the cross-axis tilt angle.\n\n    Parameters\n    ----------\n    v : tuple\n        tracker normal\n    dg : float\n        delta gamma, difference between axis and ground azimuths [degrees]\n    ba : float\n        axis tilt [degrees]\n\n    Returns\n    -------\n    beta_c : float\n        cross-axis tilt angle [radians]\n    \"\"\"\n    vnorm = np.sqrt(np.dot(v, v))\n    beta_c = np.arcsin(\n        ((v[0]*cosd(dg) - v[1]*sind(dg)) * sind(ba) + v[2]*cosd(ba)) / vnorm)\n    return beta_c\n\n\ndef calc_cross_axis_tilt(\n        slope_azimuth, slope_tilt, axis_azimuth, axis_tilt):\n    \"\"\"\n    Calculate the angle, relative to horizontal, of the line formed by the\n    intersection between the slope containing the tracker axes and a plane\n    perpendicular to the tracker axes.\n\n    Use the cross-axis tilt to avoid row-to-row shade when backtracking on a\n    slope not parallel with the axis azimuth. Cross-axis tilt should be\n    specified using a right-handed convention. For example, trackers with axis\n    azimuth of 180 degrees (heading south) will have a negative cross-axis tilt\n    if the tracker axes plane slopes down to the east and positive cross-axis\n    tilt if the tracker axes plane slopes up to the east.\n\n    Parameters\n    ----------\n    slope_azimuth : float\n        direction of the normal to the slope containing the tracker axes, when\n        projected on the horizontal [degrees]\n    slope_tilt : float\n        angle of the slope containing the tracker axes, relative to horizontal\n        [degrees]\n    axis_azimuth : float\n        direction of tracker axes projected on the horizontal [degrees]\n    axis_tilt : float\n        tilt of trackers relative to horizontal [degrees]\n\n    Returns\n    -------\n    cross_axis_tilt : float\n        angle, relative to horizontal, of the line formed by the intersection\n        between the slope containing the tracker axes and a plane perpendicular\n        to the tracker axes [degrees]\n\n    See also\n    --------\n    pvlib.tracking.singleaxis\n    pvlib.tracking.calc_axis_tilt\n\n    Notes\n    -----\n    See [1]_ for derivation of equations.\n\n    References\n    ----------\n    .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n       Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n       https://www.nrel.gov/docs/fy20osti/76626.pdf\n    \"\"\"\n    # delta-gamma, difference between axis and slope azimuths\n    delta_gamma = axis_azimuth - slope_azimuth\n    # equation 22\n    v = _calc_tracker_norm(axis_tilt, slope_tilt, delta_gamma)\n    # equation 26\n    beta_c = _calc_beta_c(v, delta_gamma, axis_tilt)\n    return np.degrees(beta_c)"
  },
  {
    "instruction": "`read_crn` returns -99999 instead of `NaN`\n**Describe the bug**\r\n`read_crn` fails to map -99999 to `NaN`\r\n\r\n**To Reproduce**\r\n```python\r\nfrom pvlib.iotools import read_crn\r\ncrn = read_crn('https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/2021/CRNS0101-05-2021-NY_Millbrook_3_W.txt')\r\ncrn.loc['2021-12-14 0930':'2021-12-14 1130', 'ghi']\r\n2021-12-14 09:30:00+00:00        0.0\r\n2021-12-14 09:35:00+00:00        0.0\r\n2021-12-14 09:40:00+00:00        0.0\r\n2021-12-14 09:45:00+00:00        0.0\r\n2021-12-14 09:50:00+00:00        0.0\r\n2021-12-14 09:55:00+00:00        0.0\r\n2021-12-14 10:00:00+00:00        0.0\r\n2021-12-14 10:05:00+00:00   -99999.0\r\n2021-12-14 10:10:00+00:00   -99999.0\r\n2021-12-14 10:15:00+00:00   -99999.0\r\n2021-12-14 10:20:00+00:00   -99999.0\r\n2021-12-14 10:25:00+00:00   -99999.0\r\n2021-12-14 10:30:00+00:00   -99999.0\r\n2021-12-14 10:35:00+00:00   -99999.0\r\n2021-12-14 10:40:00+00:00   -99999.0\r\n2021-12-14 10:45:00+00:00   -99999.0\r\n2021-12-14 10:50:00+00:00   -99999.0\r\n2021-12-14 10:55:00+00:00   -99999.0\r\n2021-12-14 11:00:00+00:00   -99999.0\r\n2021-12-14 11:05:00+00:00        0.0\r\n2021-12-14 11:10:00+00:00        0.0\r\n2021-12-14 11:15:00+00:00        0.0\r\n2021-12-14 11:20:00+00:00        0.0\r\n2021-12-14 11:25:00+00:00        0.0\r\n2021-12-14 11:30:00+00:00        0.0\r\nName: ghi, dtype: float64\r\n```\r\n\r\n**Expected behavior**\r\nShould return `NaN` instead of -99999\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.0\r\n - ``pandas.__version__``: 1.0.3 (doesn't matter)\r\n - python: 3.7\r\n\r\n**Additional context**\r\n\r\nDocumentation [here](https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/) says\r\n\r\n>          C.  Missing data are indicated by the lowest possible integer for a \r\n>             given column format, such as -9999.0 for 7-character fields with \r\n>             one decimal place or -99.000 for 7-character fields with three\r\n>             decimal places.\r\n\r\nSo we should change \r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/1ab0eb20f9cd9fb9f7a0ddf35f81283f2648e34a/pvlib/iotools/crn.py#L112-L117\r\n\r\nto include -99999 and perhaps -999999. Or do the smarter thing as discussed in the comment.\r\n\r\nalso https://github.com/SolarArbiter/solarforecastarbiter-core/issues/773\n",
    "input": " \"\"\"\n \n import pandas as pd\nimport numpy as np\n \n \nHEADERS = (\n    'WBANNO UTC_DATE UTC_TIME LST_DATE LST_TIME CRX_VN LONGITUDE LATITUDE '\n    'AIR_TEMPERATURE PRECIPITATION SOLAR_RADIATION SR_FLAG '\n    'SURFACE_TEMPERATURE ST_TYPE ST_FLAG RELATIVE_HUMIDITY RH_FLAG '\n    'SOIL_MOISTURE_5 SOIL_TEMPERATURE_5 WETNESS WET_FLAG WIND_1_5 WIND_FLAG'\n)\n \n VARIABLE_MAP = {\n     'LONGITUDE': 'longitude',\n     'WIND_FLAG': 'wind_speed_flag'\n }\n \n # as specified in CRN README.txt file. excludes 1 space between columns\n WIDTHS = [5, 8, 4, 8, 4, 6, 7, 7, 7, 7, 6, 1, 7, 1, 1, 5, 1, 7, 7, 5, 1, 6, 1]\n # add 1 to make fields contiguous (required by pandas.read_fwf)\n ]\n \n \ndef read_crn(filename):\n    \"\"\"\n    Read a NOAA USCRN fixed-width file into pandas dataframe.  The CRN is\n    described in [1]_ and [2]_.\n \n     Parameters\n     ----------\n     filename: str, path object, or file-like\n         filepath or url to read for the fixed-width file.\n \n     Returns\n     -------\n     -----\n     CRN files contain 5 minute averages labeled by the interval ending\n     time. Here, missing data is flagged as NaN, rather than the lowest\n    possible integer for a field (e.g. -999 or -99). Air temperature in\n    deg C. Wind speed in m/s at a height of 1.5 m above ground level.\n \n    Variables corresponding to standard pvlib variables are renamed,\n     e.g. `SOLAR_RADIATION` becomes `ghi`. See the\n    `pvlib.iotools.crn.VARIABLE_MAP` dict for the complete mapping.\n \n     CRN files occasionally have a set of null characters on a line\n     instead of valid data. This function drops those lines. Sometimes\n        Amer. Meteor. Soc., 94, 489-498. :doi:`10.1175/BAMS-D-12-00170.1`\n     \"\"\"\n \n    # read in data. set fields with NUL characters to NaN\n    data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n                       widths=WIDTHS, na_values=['\\x00\\x00\\x00\\x00\\x00\\x00'])\n    # at this point we only have NaNs from NUL characters, not -999 etc.\n    # these bad rows need to be removed so that dtypes can be set.\n    # NaNs require float dtype so we run into errors if we don't do this.\n    data = data.dropna(axis=0)\n    # loop here because dtype kwarg not supported in read_fwf until 0.20\n    for (col, _dtype) in zip(data.columns, DTYPES):\n        data[col] = data[col].astype(_dtype)\n \n     # set index\n     # UTC_TIME does not have leading 0s, so must zfill(4) to comply\n     dtindex = pd.to_datetime(dts['UTC_DATE'] + dts['UTC_TIME'].str.zfill(4),\n                              format='%Y%m%d%H%M', utc=True)\n     data = data.set_index(dtindex)\n    try:\n        # to_datetime(utc=True) does not work in older versions of pandas\n        data = data.tz_localize('UTC')\n    except TypeError:\n        pass\n\n    # Now we can set nans. This could be done a per column basis to be\n    # safer, since in principle a real -99 value could occur in a -9999\n    # column. Very unlikely to see that in the real world.\n    for val in [-99, -999, -9999]:\n        # consider replacing with .replace([-99, -999, -9999])\n        data = data.where(data != val, np.nan)\n\n    data = data.rename(columns=VARIABLE_MAP)\n \n     return data",
    "output": " \"\"\"\n \n import pandas as pd\n \n \nHEADERS = [\n    'WBANNO', 'UTC_DATE', 'UTC_TIME', 'LST_DATE', 'LST_TIME', 'CRX_VN',\n    'LONGITUDE', 'LATITUDE', 'AIR_TEMPERATURE', 'PRECIPITATION',\n    'SOLAR_RADIATION', 'SR_FLAG', 'SURFACE_TEMPERATURE', 'ST_TYPE', 'ST_FLAG',\n    'RELATIVE_HUMIDITY', 'RH_FLAG', 'SOIL_MOISTURE_5', 'SOIL_TEMPERATURE_5',\n    'WETNESS', 'WET_FLAG', 'WIND_1_5', 'WIND_FLAG']\n \n VARIABLE_MAP = {\n     'LONGITUDE': 'longitude',\n     'WIND_FLAG': 'wind_speed_flag'\n }\n \nNAN_DICT = {\n    'CRX_VN': -99999,\n    'AIR_TEMPERATURE': -9999,\n    'PRECIPITATION': -9999,\n    'SOLAR_RADIATION': -99999,\n    'SURFACE_TEMPERATURE': -9999,\n    'RELATIVE_HUMIDITY': -9999,\n    'SOIL_MOISTURE_5': -99,\n    'SOIL_TEMPERATURE_5': -9999,\n    'WETNESS': -9999,\n    'WIND_1_5': -99}\n\n# Add NUL characters to possible NaN values for all columns\nNAN_DICT = {k: [v, '\\x00\\x00\\x00\\x00\\x00\\x00'] for k, v in NAN_DICT.items()}\n\n # as specified in CRN README.txt file. excludes 1 space between columns\n WIDTHS = [5, 8, 4, 8, 4, 6, 7, 7, 7, 7, 6, 1, 7, 1, 1, 5, 1, 7, 7, 5, 1, 6, 1]\n # add 1 to make fields contiguous (required by pandas.read_fwf)\n ]\n \n \ndef read_crn(filename, map_variables=True):\n    \"\"\"Read a NOAA USCRN fixed-width file into a pandas dataframe.\n\n    The CRN network consists of over 100 meteorological stations covering the\n    U.S. and is described in [1]_ and [2]_. The primary goal of CRN is to\n    provide long-term measurements of temperature, precipitation, and soil\n    moisture and temperature. Additionally, global horizontal irradiance (GHI)\n    is measured at each site using a photodiode pyranometer.\n \n     Parameters\n     ----------\n     filename: str, path object, or file-like\n         filepath or url to read for the fixed-width file.\n    map_variables: boolean, default: True\n        When true, renames columns of the Dataframe to pvlib variable names\n        where applicable. See variable :const:`VARIABLE_MAP`.\n \n     Returns\n     -------\n     -----\n     CRN files contain 5 minute averages labeled by the interval ending\n     time. Here, missing data is flagged as NaN, rather than the lowest\n    possible integer for a field (e.g. -999 or -99). Air temperature is in\n    deg C and wind speed is in m/s at a height of 1.5 m above ground level.\n \n    Variables corresponding to standard pvlib variables are by default renamed,\n     e.g. `SOLAR_RADIATION` becomes `ghi`. See the\n    :const:`pvlib.iotools.crn.VARIABLE_MAP` dict for the complete mapping.\n \n     CRN files occasionally have a set of null characters on a line\n     instead of valid data. This function drops those lines. Sometimes\n        Amer. Meteor. Soc., 94, 489-498. :doi:`10.1175/BAMS-D-12-00170.1`\n     \"\"\"\n \n    # read in data\n    data = pd.read_fwf(filename, header=None, names=HEADERS, widths=WIDTHS,\n                       na_values=NAN_DICT)\n    # Remove rows with all nans\n    data = data.dropna(axis=0, how='all')\n    # set dtypes here because dtype kwarg not supported in read_fwf until 0.20\n    data = data.astype(dict(zip(HEADERS, DTYPES)))\n \n     # set index\n     # UTC_TIME does not have leading 0s, so must zfill(4) to comply\n     dtindex = pd.to_datetime(dts['UTC_DATE'] + dts['UTC_TIME'].str.zfill(4),\n                              format='%Y%m%d%H%M', utc=True)\n     data = data.set_index(dtindex)\n\n    if map_variables:\n        data = data.rename(columns=VARIABLE_MAP)\n \n     return data"
  },
  {
    "instruction": "Output of solarposition.sun_rise_set_transit_ephem depends on installed ephem version\n**Describe the bug**\r\n`pvlib.solarposition.sun_rise_set_transit_ephem` returns a different answer depending on what version of `ephem` is installed. I think the problem is that our wrapper assumes that ephem doesn't pay attention to timezone localization, so it converts the timestamp components to UTC but doesn't bother to change the timestamp's tzinfo:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/8d0f863da92739669e01ac4da4145e4547638b50/pvlib/solarposition.py#L577-L579\r\n\r\nHowever, starting in `ephem==4.1.1` the timezone is no longer ignored ([ref](https://rhodesmill.org/pyephem/CHANGELOG.html#version-4-1-1-2021-november-27)), so the UTC offset is applied twice.  This can shift the timestamp into the next solar period and return the rise/set/transit for the wrong day. \r\n\r\n\r\n**To Reproduce**\r\nSee how the returned sunrise differs by ~24 hours (2019-01-01 vs 2019-01-02) here:\r\n\r\n```python\r\nimport pandas as pd\r\nimport pvlib\r\ntimes = pd.date_range('2019-01-01', freq='h', periods=1, tz='Etc/GMT+8')\r\nout = pvlib.solarposition.sun_rise_set_transit_ephem(times, 40, -120)\r\nprint(out.T)\r\n```\r\n\r\n#### `ephem==4.0.0.1`:\r\n```\r\n\r\n               2019-01-01 00:00:00-08:00\r\nsunrise 2019-01-01 07:21:28.793036-08:00\r\nsunset  2019-01-01 16:45:50.959086-08:00\r\ntransit 2019-01-01 12:03:35.730674-08:00\r\n```\r\n\r\n#### `ephem==4.1.2`:\r\n```\r\n               2019-01-01 00:00:00-08:00\r\nsunrise 2019-01-02 07:21:35.237404-08:00\r\nsunset  2019-01-01 16:45:50.947472-08:00\r\ntransit 2019-01-01 12:03:35.728413-08:00\r\n```\r\n\r\n**Expected behavior**\r\npvlib should give ephem timestamps consistent with its input requirements.  Replacing the above manual utc offset subtraction (which leaves the original tzinfo in place) with `thetime.astimezone(pytz.UTC)` may be suitable for both old and new versions of ephem.  I don't ever use pytz and python datetimes so maybe there's a better alternative.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.1\r\n - python: 3.7\r\n\r\n**Additional context**\r\nThis difference would have popped up back in November when ephem 4.1.1 was released had it not been for #1447.  Here's an example failure: https://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=6027&view=logs&j=e1592cb8-2816-5754-b393-3839a187d454&t=377c4fd6-97bd-5996-bc02-4d072a8786ea&l=2267\r\n\n",
    "input": " \"\"\"\n \n import pandas as pd\n \n \n HEADERS = [\n     \"\"\"\n \n     # read in data\n     data = pd.read_fwf(filename, header=None, names=HEADERS, widths=WIDTHS,\n                       na_values=NAN_DICT)\n    # Remove rows with all nans\n     data = data.dropna(axis=0, how='all')\n    # set dtypes here because dtype kwarg not supported in read_fwf until 0.20\n     data = data.astype(dict(zip(HEADERS, DTYPES)))\n \n     # set index\n     # UTC_TIME does not have leading 0s, so must zfill(4) to comply\n     # with %H%M format\ndiff --git a/pvlib/solarposition.py b/pvlib/solarposition.py\n import pandas as pd\n import scipy.optimize as so\n import warnings\n \n from pvlib import atmosphere\n from pvlib.tools import datetime_to_djd, djd_to_datetime\n     trans = []\n     for thetime in times:\n         thetime = thetime.to_pydatetime()\n        # pyephem drops timezone when converting to its internal datetime\n        # format, so handle timezone explicitly here\n        obs.date = ephem.Date(thetime - thetime.utcoffset())\n         sunrise.append(_ephem_to_timezone(rising(sun), tzinfo))\n         sunset.append(_ephem_to_timezone(setting(sun), tzinfo))\n         trans.append(_ephem_to_timezone(transit(sun), tzinfo))\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n \n     # Calculate surface_tilt\n     dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n     surface_tilt = 90 - np.degrees(np.arccos(dotproduct))\n \n     # Bundle DataFrame for return values and filter for sun below horizon.\ndiff --git a/setup.py b/setup.py\n URL = 'https://github.com/pvlib/pvlib-python'\n \n INSTALL_REQUIRES = ['numpy >= 1.16.0',\n                    'pandas >= 0.22.0',\n                     'pytz',\n                     'requests',\n                     'scipy >= 1.2.0',",
    "output": " \"\"\"\n \n import pandas as pd\nimport numpy as np\n \n \n HEADERS = [\n     \"\"\"\n \n     # read in data\n    # TODO: instead of parsing as strings and then post-processing, switch to\n    # pd.read_fwf(..., dtype=dict(zip(HEADERS, DTYPES)), skip_blank_lines=True)\n    # when our minimum pandas >= 1.2.0 (skip_blank_lines bug for <1.2.0).\n    # As a workaround, parse all values as strings, then drop NaN, then cast\n    # to the appropriate dtypes, and mask \"sentinal\" NaN (e.g. -9999.0)\n     data = pd.read_fwf(filename, header=None, names=HEADERS, widths=WIDTHS,\n                       dtype=str)\n\n    # drop empty (bad) lines\n     data = data.dropna(axis=0, how='all')\n\n    # can't set dtypes in read_fwf because int cols can't contain NaN, so\n    # do it here instead\n     data = data.astype(dict(zip(HEADERS, DTYPES)))\n \n    # finally, replace -999 values with NaN\n    data = data.replace(NAN_DICT, value=np.nan)\n\n     # set index\n     # UTC_TIME does not have leading 0s, so must zfill(4) to comply\n     # with %H%M format\ndiff --git a/pvlib/solarposition.py b/pvlib/solarposition.py\n import pandas as pd\n import scipy.optimize as so\n import warnings\nimport datetime\n \n from pvlib import atmosphere\n from pvlib.tools import datetime_to_djd, djd_to_datetime\n     trans = []\n     for thetime in times:\n         thetime = thetime.to_pydatetime()\n        # older versions of pyephem ignore timezone when converting to its\n        # internal datetime format, so convert to UTC here to support\n        # all versions.  GH #1449\n        obs.date = ephem.Date(thetime.astimezone(datetime.timezone.utc))\n         sunrise.append(_ephem_to_timezone(rising(sun), tzinfo))\n         sunset.append(_ephem_to_timezone(setting(sun), tzinfo))\n         trans.append(_ephem_to_timezone(transit(sun), tzinfo))\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n \n     # Calculate surface_tilt\n     dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n    # for edge cases like axis_tilt=90, numpy's SIMD can produce values like\n    # dotproduct = (1 + 2e-16). Clip off the excess so that arccos works:\n    dotproduct = np.clip(dotproduct, -1, 1)\n     surface_tilt = 90 - np.degrees(np.arccos(dotproduct))\n \n     # Bundle DataFrame for return values and filter for sun below horizon.\ndiff --git a/setup.py b/setup.py\n URL = 'https://github.com/pvlib/pvlib-python'\n \n INSTALL_REQUIRES = ['numpy >= 1.16.0',\n                    'pandas >= 0.25.0',\n                     'pytz',\n                     'requests',\n                     'scipy >= 1.2.0',"
  },
  {
    "instruction": "ZeroDivisionError when gcr is zero\n**Describe the bug**\r\n\r\nThough maybe not intuitive, setting ground coverage ratio to zero is useful when a plant consists of a single shed, e.g. calculating the irradiance on the backside of the panels. However, e.g., `bifacial.infinite_sheds.get_irradiance_poa` fails with `ZeroDivisionError` whenever `gcr=0`.\r\n\r\n**To Reproduce**\r\n\r\n```python\r\nfrom pvlib.bifacial.infinite_sheds import get_irradiance_poa\r\n\r\nget_irradiance_poa(surface_tilt=160, surface_azimuth=180, solar_zenith=20, solar_azimuth=180, gcr=0, height=1, pitch=1000, ghi=200, dhi=200, dni=0, albedo=0.2)\r\n```\r\nreturns:\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-7-0cb583b2b311>\", line 3, in <cell line: 3>\r\n    get_irradiance_poa(surface_tilt=160, surface_azimuth=180, solar_zenith=20, solar_azimuth=180, gcr=0, height=1, pitch=1, ghi=200, dhi=200, dni=0, albedo=0.2)\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\bifacial\\infinite_sheds.py\", line 522, in get_irradiance_poa\r\n    vf_shade_sky, vf_noshade_sky = _vf_row_sky_integ(\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\bifacial\\infinite_sheds.py\", line 145, in _vf_row_sky_integ\r\n    psi_t_shaded = masking_angle(surface_tilt, gcr, x)\r\n  File \"C:\\Python\\Python310\\lib\\site-packages\\pvlib\\shading.py\", line 56, in masking_angle\r\n    denominator = 1/gcr - (1 - slant_height) * cosd(surface_tilt)\r\nZeroDivisionError: division by zero\r\n```\r\n\r\n**Expected behavior**\r\n\r\nOne can easily solve this `ZeroDivisionError` by multiplying both numerator and denominator with `gcr` inside `shading.masking_angle` and the same inside `bifacial.infinite_sheds._ground_angle`.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: '0.9.3'\r\n - ``pandas.__version__``: '1.4.4'\r\n - python: '3.10.4'\r\n\n",
    "input": "     #  :         \\  v      *-.\\\n     #  :          \\<-----P---->\\\n \n    x1 = x * sind(surface_tilt)\n    x2 = (x * cosd(surface_tilt) + 1 / gcr)\n     psi = np.arctan2(x1, x2)  # do this first because it handles 0 / 0\n     return np.rad2deg(psi)\n \ndiff --git a/pvlib/shading.py b/pvlib/shading.py\n     # The original equation (8 in [1]) requires pitch and collector width,\n     # but it's easy to non-dimensionalize it to make it a function of GCR\n     # by factoring out B from the argument to arctan.\n    numerator = (1 - slant_height) * sind(surface_tilt)\n    denominator = 1/gcr - (1 - slant_height) * cosd(surface_tilt)\n     phi = np.arctan(numerator / denominator)\n     return np.degrees(phi)\n ",
    "output": "     #  :         \\  v      *-.\\\n     #  :          \\<-----P---->\\\n \n    x1 = gcr * x * sind(surface_tilt)\n    x2 = gcr * x * cosd(surface_tilt) + 1\n     psi = np.arctan2(x1, x2)  # do this first because it handles 0 / 0\n     return np.rad2deg(psi)\n \ndiff --git a/pvlib/shading.py b/pvlib/shading.py\n     # The original equation (8 in [1]) requires pitch and collector width,\n     # but it's easy to non-dimensionalize it to make it a function of GCR\n     # by factoring out B from the argument to arctan.\n    numerator = gcr * (1 - slant_height) * sind(surface_tilt)\n    denominator = 1 - gcr * (1 - slant_height) * cosd(surface_tilt)\n     phi = np.arctan(numerator / denominator)\n     return np.degrees(phi)\n "
  },
  {
    "instruction": "expand PVSystem repr\nThe PVSystem repr is\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L239-L243\r\n\r\nThe main issue that I have is that the repr doesn't give me enough information about the temperature model settings. It's relatively important because `temperature_model_params` (not printed) may be inferred from `module_type` (not printed) and `racking_model` (printed). So I'd like to add both `temperature_model_params` and `module_type`.\r\n\r\nWe also don't include `module_parameters`, `inverter_parameters`, and `losses_parameters` in the repr. If I recall correctly, we decided against including these because they can be relatively long. I still think that's reasonable. We could add something like `if len(module_parameters): 'Set. See PVSystem.module_parameters'; else: {}`, but I don't know if that's worth the effort.\n",
    "input": " \n     def __repr__(self):\n         attrs = ['name', 'surface_tilt', 'surface_azimuth', 'module',\n                 'inverter', 'albedo', 'racking_model']\n        return ('PVSystem: \\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n \n     def get_aoi(self, solar_zenith, solar_azimuth):\n     def __repr__(self):\n         attrs = ['name', 'latitude', 'longitude', 'altitude', 'tz',\n                  'surface_tilt', 'surface_azimuth', 'module', 'inverter',\n                 'albedo', 'racking_model']\n        return ('LocalizedPVSystem: \\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n \n \ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n \n     def __repr__(self):\n         attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr']\n        sat_repr = ('SingleAxisTracker: \\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n         # get the parent PVSystem info\n         pvsystem_repr = super(SingleAxisTracker, self).__repr__()",
    "output": " \n     def __repr__(self):\n         attrs = ['name', 'surface_tilt', 'surface_azimuth', 'module',\n                 'inverter', 'albedo', 'racking_model', 'module_type',\n                 'temperature_model_parameters']\n        return ('PVSystem:\\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n \n     def get_aoi(self, solar_zenith, solar_azimuth):\n     def __repr__(self):\n         attrs = ['name', 'latitude', 'longitude', 'altitude', 'tz',\n                  'surface_tilt', 'surface_azimuth', 'module', 'inverter',\n                 'albedo', 'racking_model', 'module_type',\n                 'temperature_model_parameters']\n        return ('LocalizedPVSystem:\\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n \n \ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n \n     def __repr__(self):\n         attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr']\n        sat_repr = ('SingleAxisTracker:\\n  ' + '\\n  '.join(\n             ('{}: {}'.format(attr, getattr(self, attr)) for attr in attrs)))\n         # get the parent PVSystem info\n         pvsystem_repr = super(SingleAxisTracker, self).__repr__()"
  },
  {
    "instruction": "`spectrum.spectrl2` calculates negative irradiance for angle of incidence outside +/- 90\u00b0\nWhen using pvlib (but also the spectrl2 implementation provided by NREL), I obtain negative Irradiance for a north-facing panel.\r\nFrom @kevinsa5 's [reply on StackOverflow](https://stackoverflow.com/questions/70172766/pvlib-bird1984-north-facing-element-shows-negative-irradiance/70174010#70174010) I take that this is in fact not intended.\r\n\r\nIn the example code below, the angle of incidence is calculated as values around 115\u00b0, so exceeding a possible (implicitly assumed) +/- 90\u00b0 bound (sun behind panel).\r\n\r\nThis seems to be left open in the original report ([Bird & Riordan, 1984](https://www.nrel.gov/docs/legosti/old/2436.pdf)).\r\n\r\nThe direct irradiance `I_d` (*of a horizontal panel*, Eq 2-1) is obtained by multiplying by cosine of the sun zenith angle. I'd guess that setting that value strictly to zero for angles when cosZ is negative would not be too much of a stretch.\r\n\r\nThen, the direct irradiance `I_d` goes into (Eq 3-18):\r\n\r\n```\r\nI_T(t) = I_d*cos(aoi) + I_s * ( (I_d*cos(aoi) / (H_0*D*cos(Z)) ) + 0.5*(1+cos(t)) * (1 - I_d/(H_0*D)) + 0.5 * I_T0 * r_g * (1-cos(t))\r\n```\r\n\r\nAs such, when you view the angle of incidence `aoi` as the analogue of the sun zenith angle in the prior example, the two first terms of the diffuse irradiation (Eq 3-18) would become zero, which - again - for the direct irradiance would kind of make sense. What remains of (Eq 3-18) would be\r\n\r\n```\r\nI_T(t) = 0 + 0 + 0.5*(1+cos(t))*(1 - 0) + 0.5*I_T0*r_g*(1-cos(t))\r\n```\r\n\r\nI'm not from the field, so I'm very, very wary about the implications of such a work-around suggestion. Can anyone with a proper background comment on this? (Maybe it's the future of air conditioning :-D)\r\n\r\n\r\n**MWE based on the tutorial below**\r\n\r\n```python\r\n## Using PV Lib\r\n\r\nfrom pvlib import spectrum, solarposition, irradiance, atmosphere\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n# assumptions from the technical report:\r\nlat = 49.88\r\nlon = 8.63\r\ntilt = 45\r\nazimuth = 0 # North = 0\r\npressure = 101300  # sea level, roughly\r\nwater_vapor_content = 0.5  # cm\r\ntau500 = 0.1\r\nozone = 0.31  # atm-cm\r\nalbedo = 0.2\r\n\r\ntimes = pd.date_range('2021-11-30 8:00', freq='h', periods=6, tz=\"Europe/Berlin\") # , tz='Etc/GMT+9'\r\nsolpos = solarposition.get_solarposition(times, lat, lon)\r\naoi = irradiance.aoi(tilt, azimuth, solpos.apparent_zenith, solpos.azimuth)\r\n\r\n# The technical report uses the 'kasten1966' airmass model, but later\r\n# versions of SPECTRL2 use 'kastenyoung1989'.  Here we use 'kasten1966'\r\n# for consistency with the technical report.\r\nrelative_airmass = atmosphere.get_relative_airmass(solpos.apparent_zenith,\r\n                                                   model='kasten1966')\r\n\r\nspectra = spectrum.spectrl2(\r\n    apparent_zenith=solpos.apparent_zenith,\r\n    aoi=aoi,\r\n    surface_tilt=tilt,\r\n    ground_albedo=albedo,\r\n    surface_pressure=pressure,\r\n    relative_airmass=relative_airmass,\r\n    precipitable_water=water_vapor_content,\r\n    ozone=ozone,\r\n    aerosol_turbidity_500nm=tau500,\r\n)\r\n\r\nplt.figure()\r\nplt.plot(spectra['wavelength'], spectra['poa_global'])\r\nplt.xlim(200, 2700)\r\n# plt.ylim(0, 1.8)\r\nplt.title(r\"2021-11-30, Darmstadt, $\\tau=0.1$, Wv=0.5 cm\")\r\nplt.ylabel(r\"Irradiance ($W m^{-2} nm^{-1}$)\")\r\nplt.xlabel(r\"Wavelength ($nm$)\")\r\ntime_labels = times.strftime(\"%H:%M %p\")\r\nlabels = [\r\n    \"AM {:0.02f}, Z{:0.02f}, {}\".format(*vals)\r\n    for vals in zip(relative_airmass, solpos.apparent_zenith, time_labels)\r\n]\r\nplt.legend(labels)\r\nplt.show()\r\n```\r\n\r\n![Figure_ne](https://user-images.githubusercontent.com/15192310/144224709-dea899e4-435e-4ff2-a3de-9e9524b28eb8.png)\r\n\r\n\n",
    "input": "     2-5                 kasten1966 kasten1966 kastenyoung1989\n     =================== ========== ========== ===============\n \n     References\n     ----------\n     .. [1] Bird, R, and Riordan, C., 1984, \"Simple solar spectral model for\n     Is = (Ir + Ia + Ig) * Cs  # Eq 3-1\n \n     # calculate spectral irradiance on a tilted surface, Eq 3-18\n    Ibeam = Id * cosd(aoi)\n\n    # don't need surface_azimuth if we provide projection_ratio\n    projection_ratio = cosd(aoi) / cosZ\n     Isky = pvlib.irradiance.haydavies(surface_tilt=surface_tilt,\n                                       surface_azimuth=None,\n                                       dhi=Is,",
    "output": "     2-5                 kasten1966 kasten1966 kastenyoung1989\n     =================== ========== ========== ===============\n \n    This implementation also deviates from the reference by including a\n    check for angles of incidence greater than 90 degrees; without this,\n    the model might return negative spectral irradiance values when the\n    sun is behind the plane of array.\n\n     References\n     ----------\n     .. [1] Bird, R, and Riordan, C., 1984, \"Simple solar spectral model for\n     Is = (Ir + Ia + Ig) * Cs  # Eq 3-1\n \n     # calculate spectral irradiance on a tilted surface, Eq 3-18\n    # Note: clipping cosd(aoi) to >=0 is not in the reference, but is necessary\n    # to prevent nonsense values when the sun is behind the plane of array.\n    # The same constraint is applied in irradiance.haydavies when not\n    # supplying `projection_ratio`.\n    aoi_projection_nn = np.maximum(cosd(aoi), 0)  # GH 1348\n    Ibeam = Id * aoi_projection_nn\n\n    # don't need surface_azimuth if we provide projection_ratio.\n    # Also constrain cos zenith to avoid blowup, as in irradiance.haydavies\n    projection_ratio = aoi_projection_nn / np.maximum(cosZ, 0.01745)\n     Isky = pvlib.irradiance.haydavies(surface_tilt=surface_tilt,\n                                       surface_azimuth=None,\n                                       dhi=Is,"
  },
  {
    "instruction": "pvlib.soiling.hsu model implementation errors\n**Describe the bug**\r\nI ran an example run using the Matlab version of the HSU soiling function and found that the python version did not give anywhere near the same results.  The Matlab results matched the results in the original JPV paper.  As a result of this test, I found two errors in the python implementation, which are listed below:\r\n\r\n1.  depo_veloc = {'2_5': 0.004, '10': 0.0009} has the wrong default values.  They are reversed.\r\nThe proper dictionary should be: {'2_5': 0.0009, '10': 0.004}.  This is confirmed in the JPV paper and the Matlab version of the function.\r\n\r\n2. The horiz_mass_rate is in g/(m^2*hr) but should be in g/(m^2*s).  The line needs to be multiplied by 60x60 or 3600.\r\nThe proper line of code should be: \r\nhoriz_mass_rate = (pm2_5 * depo_veloc['2_5']+ np.maximum(pm10 - pm2_5, 0.) * depo_veloc['10'])*3600\r\n\r\nWhen I made these changes I was able to match the validation dataset from the JPV paper, as shown below.\r\n![image](https://user-images.githubusercontent.com/5392756/82380831-61c43d80-99e6-11ea-9ee3-2368fa71e580.png)\r\n\r\n\n",
    "input": " def hsu(rainfall, cleaning_threshold, tilt, pm2_5, pm10,\n         depo_veloc=None, rain_accum_period=pd.Timedelta('1h')):\n     \"\"\"\n    Calculates soiling ratio given particulate and rain data using the model\n    from Humboldt State University (HSU).\n \n     The HSU soiling model [1]_ returns the soiling ratio, a value between zero\n     and one which is equivalent to (1 - transmission loss). Therefore a soiling\n     # cleaning is True for intervals with rainfall greater than threshold\n     cleaning_times = accum_rain.index[accum_rain >= cleaning_threshold]\n \n    horiz_mass_rate = pm2_5 * depo_veloc['2_5']\\\n        + np.maximum(pm10 - pm2_5, 0.) * depo_veloc['10'] * 3600\n     tilted_mass_rate = horiz_mass_rate * cosd(tilt)  # assuming no rain\n \n     # tms -> tilt_mass_rate",
    "output": " def hsu(rainfall, cleaning_threshold, tilt, pm2_5, pm10,\n         depo_veloc=None, rain_accum_period=pd.Timedelta('1h')):\n     \"\"\"\n    Calculates soiling ratio given particulate and rain data using the\n    Fixed Velocity model from Humboldt State University (HSU).\n \n     The HSU soiling model [1]_ returns the soiling ratio, a value between zero\n     and one which is equivalent to (1 - transmission loss). Therefore a soiling\n     # cleaning is True for intervals with rainfall greater than threshold\n     cleaning_times = accum_rain.index[accum_rain >= cleaning_threshold]\n \n    # determine the time intervals in seconds (dt_sec)\n    dt = rainfall.index\n    # subtract shifted values from original and convert to seconds\n    dt_diff = (dt[1:] - dt[:-1]).total_seconds()\n    # ensure same number of elements in the array, assuming that the interval\n    # prior to the first value is equal in length to the first interval\n    dt_sec = np.append(dt_diff[0], dt_diff).astype('float64')\n\n    horiz_mass_rate = (\n        pm2_5 * depo_veloc['2_5'] + np.maximum(pm10 - pm2_5, 0.)\n        * depo_veloc['10']) * dt_sec\n     tilted_mass_rate = horiz_mass_rate * cosd(tilt)  # assuming no rain\n \n     # tms -> tilt_mass_rate"
  },
  {
    "instruction": "warnings in test_sdm\nhttps://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=209\r\n\r\nthrough\r\n\r\nhttps://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=295\r\n\r\nSo almost 100 lines of warnings.\n",
    "input": " \n \"\"\"\n \nfrom pvlib.ivtools import sde, sdm, utility  # noqa: F401\ndiff --git a/pvlib/ivtools/sde.py b/pvlib/ivtools/sde.py\n \n import numpy as np\n \nfrom pvlib.ivtools.utility import _schumaker_qspline\n \n \n # set constant for numpy.linalg.lstsq parameter rcond\ndiff --git a/pvlib/ivtools/sdm.py b/pvlib/ivtools/sdm.py\n \n from pvlib.pvsystem import singlediode, v_from_i\n \nfrom pvlib.ivtools.utility import constants, rectify_iv_curve, _numdiff\n from pvlib.ivtools.sde import _fit_sandia_cocontent\n \n \n     return y\n \n \ndef fit_pvsyst_sandia(ivcurves, specs, const=constants, maxiter=5, eps1=1.e-3):\n     \"\"\"\n     Estimate parameters for the PVsyst module performance model.\n \n     .. [7] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n     \"\"\"\n \n     ee = ivcurves['ee']\n     tc = ivcurves['tc']\n     tck = tc + 273.15\n     return pvsyst\n \n \ndef fit_desoto_sandia(ivcurves, specs, const=constants, maxiter=5, eps1=1.e-3):\n     \"\"\"\n     Estimate parameters for the De Soto module performance model.\n \n     .. [4] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n     \"\"\"\n \n     ee = ivcurves['ee']\n     tc = ivcurves['tc']\n     tck = tc + 273.15\n         dvoc = pvoc - voc\n \n         # Update Io\n        new_io = tio * (1. + (2. * dvoc) / (2. * nnsvth - dvoc))\n \n        # Calculate Maximum Percent Difference\n        maxerr = np.max(np.abs(new_io - tio) / tio) * 100.\n         tio = new_io\n         k += 1.\n \n \n     for i in range(niter):\n         _, z = _calc_theta_phi_exact(vmp, imp, iph, io, rs, x1, nnsvth)\n        next_x1 = (1 + z) / z * ((iph + io) * x1 / imp - nnsvth * z / imp - 2 *\n                                 vmp / imp)\n         x1 = next_x1\n \n     return x1\n \n     # Argument for Lambert W function involved in V = V(I) [2] Eq. 12; [3]\n     # Eq. 3\n    with np.errstate(over=\"ignore\"):\n         argw = np.where(\n             nnsvth == 0,\n             np.nan,\n             rsh * io / nnsvth * np.exp(rsh * (iph + io - imp) / nnsvth))\n    phi = np.where(argw > 0, lambertw(argw).real, np.nan)\n \n     # NaN where argw overflows. Switch to log space to evaluate\n     u = np.isinf(argw)\n \n     # Argument for Lambert W function involved in I = I(V) [2] Eq. 11; [3]\n     # E1. 2\n    with np.errstate(over=\"ignore\"):\n         argw = np.where(\n             nnsvth == 0,\n             np.nan,\n             rsh / (rsh + rs) * rs * io / nnsvth * np.exp(\n                 rsh / (rsh + rs) * (rs * (iph + io) + vmp) / nnsvth))\n    theta = np.where(argw > 0, lambertw(argw).real, np.nan)\n \n     # NaN where argw overflows. Switch to log space to evaluate\n     u = np.isinf(argw)\n     if np.any(u):\n        logargw = (\n            np.log(rsh[u]) / (rsh[u] + rs[u]) + np.log(rs[u]) + np.log(io[u])\n            - np.log(nnsvth[u]) + (rsh[u] / (rsh[u] + rs[u]))\n            * (rs[u] * (iph[u] + io[u]) + vmp[u]) / nnsvth[u])\n         # Three iterations of Newton-Raphson method to solve w+log(w)=logargW.\n         # The initial guess is w=logargW. Where direct evaluation (above)\n         # results in NaN from overflow, 3 iterations of Newton's method gives\ndiff --git a/pvlib/ivtools/utility.py b/pvlib/ivtools/utils.py\nsimilarity index 98%\nrename from pvlib/ivtools/utility.py\nrename to pvlib/ivtools/utils.py\n \"\"\"\nThe ``pvlib.ivtools.utility.py`` module contains utility functions related to\n working with IV curves, or fitting equations to IV curve data.\n \n \"\"\"\n EPS = np.finfo('float').eps**(1/3)\n \n \nconstants = {'E0': 1000.0, 'T0': 25.0, 'k': 1.38066e-23, 'q': 1.60218e-19}\n\n\n def _numdiff(x, f):\n     \"\"\"\n     Compute first and second order derivative using possibly unequally",
    "output": " \n \"\"\"\n \nfrom pvlib.ivtools import sde, sdm, utils  # noqa: F401\ndiff --git a/pvlib/ivtools/sde.py b/pvlib/ivtools/sde.py\n \n import numpy as np\n \nfrom pvlib.ivtools.utils import _schumaker_qspline\n \n \n # set constant for numpy.linalg.lstsq parameter rcond\ndiff --git a/pvlib/ivtools/sdm.py b/pvlib/ivtools/sdm.py\n \n from pvlib.pvsystem import singlediode, v_from_i\n \nfrom pvlib.ivtools.utils import rectify_iv_curve, _numdiff\n from pvlib.ivtools.sde import _fit_sandia_cocontent\n \n \n     return y\n \n \ndef fit_pvsyst_sandia(ivcurves, specs, const=None, maxiter=5, eps1=1.e-3):\n     \"\"\"\n     Estimate parameters for the PVsyst module performance model.\n \n     .. [7] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n     \"\"\"\n \n    if const is None:\n        const = {'E0': 1000.0, 'T0': 25.0, 'k': 1.38066e-23, 'q': 1.60218e-19}\n\n     ee = ivcurves['ee']\n     tc = ivcurves['tc']\n     tck = tc + 273.15\n     return pvsyst\n \n \ndef fit_desoto_sandia(ivcurves, specs, const=None, maxiter=5, eps1=1.e-3):\n     \"\"\"\n     Estimate parameters for the De Soto module performance model.\n \n     .. [4] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n     \"\"\"\n \n    if const is None:\n        const = {'E0': 1000.0, 'T0': 25.0, 'k': 1.38066e-23, 'q': 1.60218e-19}\n\n     ee = ivcurves['ee']\n     tc = ivcurves['tc']\n     tck = tc + 273.15\n         dvoc = pvoc - voc\n \n         # Update Io\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n            new_io = tio * (1. + (2. * dvoc) / (2. * nnsvth - dvoc))\n            # Calculate Maximum Percent Difference\n            maxerr = np.max(np.abs(new_io - tio) / tio) * 100.\n \n         tio = new_io\n         k += 1.\n \n \n     for i in range(niter):\n         _, z = _calc_theta_phi_exact(vmp, imp, iph, io, rs, x1, nnsvth)\n        with np.errstate(divide=\"ignore\"):\n            next_x1 = (1 + z) / z * ((iph + io) * x1 / imp - nnsvth * z / imp\n                                     - 2 * vmp / imp)\n         x1 = next_x1\n \n     return x1\n \n     # Argument for Lambert W function involved in V = V(I) [2] Eq. 12; [3]\n     # Eq. 3\n    with np.errstate(over=\"ignore\", divide=\"ignore\", invalid=\"ignore\"):\n         argw = np.where(\n             nnsvth == 0,\n             np.nan,\n             rsh * io / nnsvth * np.exp(rsh * (iph + io - imp) / nnsvth))\n        phi = np.where(argw > 0, lambertw(argw).real, np.nan)\n \n     # NaN where argw overflows. Switch to log space to evaluate\n     u = np.isinf(argw)\n \n     # Argument for Lambert W function involved in I = I(V) [2] Eq. 11; [3]\n     # E1. 2\n    with np.errstate(over=\"ignore\", divide=\"ignore\", invalid=\"ignore\"):\n         argw = np.where(\n             nnsvth == 0,\n             np.nan,\n             rsh / (rsh + rs) * rs * io / nnsvth * np.exp(\n                 rsh / (rsh + rs) * (rs * (iph + io) + vmp) / nnsvth))\n        theta = np.where(argw > 0, lambertw(argw).real, np.nan)\n \n     # NaN where argw overflows. Switch to log space to evaluate\n     u = np.isinf(argw)\n     if np.any(u):\n        with np.errstate(divide=\"ignore\"):\n            logargw = (\n                np.log(rsh[u]) - np.log(rsh[u] + rs[u]) + np.log(rs[u])\n                + np.log(io[u]) - np.log(nnsvth[u])\n                + (rsh[u] / (rsh[u] + rs[u]))\n                * (rs[u] * (iph[u] + io[u]) + vmp[u]) / nnsvth[u])\n         # Three iterations of Newton-Raphson method to solve w+log(w)=logargW.\n         # The initial guess is w=logargW. Where direct evaluation (above)\n         # results in NaN from overflow, 3 iterations of Newton's method gives\ndiff --git a/pvlib/ivtools/utility.py b/pvlib/ivtools/utils.py\nsimilarity index 98%\nrename from pvlib/ivtools/utility.py\nrename to pvlib/ivtools/utils.py\n \"\"\"\nThe ``pvlib.ivtools.utils.py`` module contains utility functions related to\n working with IV curves, or fitting equations to IV curve data.\n \n \"\"\"\n EPS = np.finfo('float').eps**(1/3)\n \n \n def _numdiff(x, f):\n     \"\"\"\n     Compute first and second order derivative using possibly unequally"
  },
  {
    "instruction": "Add variable mapping of read_tmy3\n**Is your feature request related to a problem? Please describe.**\r\nThis PR proposes that a `map_variables` parameter be added to the `read_tmy3` function. Additionally, the current `rename_columns` parameter (which removes the units from the column names) should be deprecated. See #714 for a discussion on the topic.\r\n\r\n**Describe the solution you'd like**\r\nA `map_variables` parameter should be added (defaulting to None), and if specified as True then it should override the `rename_columns` parameter and map the column names to standard pvlib names. A deperecation warning should be added stating that the `rename_columns` parameter will be retired starting in pvlib 0.11.0 - the deprecation warning should be silenced if `map_variables` is specified as either True or False.\r\n\n",
    "input": " PVLIB_DIR = pvlib.__path__[0]\n DATA_FILE = os.path.join(PVLIB_DIR, 'data', '723170TYA.CSV')\n \ntmy, metadata = iotools.read_tmy3(DATA_FILE, coerce_year=1990)\n \ndf = pd.DataFrame({'ghi': tmy['GHI'], 'dhi': tmy['DHI'], 'dni': tmy['DNI'],\n                   'temp_air': tmy['DryBulb'], 'wind_speed': tmy['Wspd'],\n                    })\n \n # %%\ndiff --git a/docs/examples/irradiance-decomposition/plot_diffuse_fraction.py b/docs/examples/irradiance-decomposition/plot_diffuse_fraction.py\n # of data measured from 1990 to 2010. Therefore we change the timestamps to a\n # common year, 1990.\n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\ngreensboro, metadata = read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990)\n \n # Many of the diffuse fraction estimation methods require the \"true\" zenith, so\n # we calculate the solar positions for the 1990 at Greensboro, NC.\n solpos = get_solarposition(\n     greensboro.index.shift(freq=\"-30T\"), latitude=metadata['latitude'],\n     longitude=metadata['longitude'], altitude=metadata['altitude'],\n    pressure=greensboro.Pressure*100,  # convert from millibar to Pa\n    temperature=greensboro.DryBulb)\n solpos.index = greensboro.index  # reset index to end of the hour\n \n # %%\n # an exponential relation with airmass.\n \n out_disc = irradiance.disc(\n    greensboro.GHI, solpos.zenith, greensboro.index, greensboro.Pressure*100)\n # use \"complete sum\" AKA \"closure\" equations: DHI = GHI - DNI * cos(zenith)\n df_disc = irradiance.complete_irradiance(\n    solar_zenith=solpos.apparent_zenith, ghi=greensboro.GHI, dni=out_disc.dni,\n     dhi=None)\n out_disc = out_disc.rename(columns={'dni': 'dni_disc'})\n out_disc['dhi_disc'] = df_disc.dhi\n # developed by Richard Perez and Pierre Ineichen in 1992.\n \n dni_dirint = irradiance.dirint(\n    greensboro.GHI, solpos.zenith, greensboro.index, greensboro.Pressure*100,\n    temp_dew=greensboro.DewPoint)\n # use \"complete sum\" AKA \"closure\" equation: DHI = GHI - DNI * cos(zenith)\n df_dirint = irradiance.complete_irradiance(\n    solar_zenith=solpos.apparent_zenith, ghi=greensboro.GHI, dni=dni_dirint,\n     dhi=None)\n out_dirint = pd.DataFrame(\n     {'dni_dirint': dni_dirint, 'dhi_dirint': df_dirint.dhi},\n # splits kt into 3 regions: linear for kt <= 0.22, a 4th order polynomial\n # between 0.22 < kt <= 0.8, and a horizontal line for kt > 0.8.\n \nout_erbs = irradiance.erbs(greensboro.GHI, solpos.zenith, greensboro.index)\n out_erbs = out_erbs.rename(columns={'dni': 'dni_erbs', 'dhi': 'dhi_erbs'})\n \n # %%\n # exponential correlation that is continuously differentiable and bounded\n # between zero and one.\n \nout_boland = irradiance.boland(greensboro.GHI, solpos.zenith, greensboro.index)\n out_boland = out_boland.rename(\n     columns={'dni': 'dni_boland', 'dhi': 'dhi_boland'})\n \n # file together to make plotting easier.\n \n dni_renames = {\n    'DNI': 'TMY3', 'dni_disc': 'DISC', 'dni_dirint': 'DIRINT',\n     'dni_erbs': 'Erbs', 'dni_boland': 'Boland'}\n dni = [\n    greensboro.DNI, out_disc.dni_disc, out_dirint.dni_dirint,\n     out_erbs.dni_erbs, out_boland.dni_boland]\n dni = pd.concat(dni, axis=1).rename(columns=dni_renames)\n dhi_renames = {\n    'DHI': 'TMY3', 'dhi_disc': 'DISC', 'dhi_dirint': 'DIRINT',\n     'dhi_erbs': 'Erbs', 'dhi_boland': 'Boland'}\n dhi = [\n    greensboro.DHI, out_disc.dhi_disc, out_dirint.dhi_dirint,\n     out_erbs.dhi_erbs, out_boland.dhi_boland]\n dhi = pd.concat(dhi, axis=1).rename(columns=dhi_renames)\nghi_kt = pd.concat([greensboro.GHI/1000.0, out_erbs.kt], axis=1)\n \n # %%\n # Winter\ndiff --git a/docs/examples/irradiance-transposition/plot_seasonal_tilt.py b/docs/examples/irradiance-transposition/plot_seasonal_tilt.py\n # like we expect:\n \n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\ntmy, metadata = iotools.read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990)\n # shift from TMY3 right-labeled index to left-labeled index:\n tmy.index = tmy.index - pd.Timedelta(hours=1)\n weather = pd.DataFrame({\n    'ghi': tmy['GHI'], 'dhi': tmy['DHI'], 'dni': tmy['DNI'],\n    'temp_air': tmy['DryBulb'], 'wind_speed': tmy['Wspd'],\n })\n loc = location.Location.from_tmy(metadata)\n solpos = loc.get_solarposition(weather.index)\ndiff --git a/docs/examples/irradiance-transposition/plot_transposition_gain.py b/docs/examples/irradiance-transposition/plot_transposition_gain.py\n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\n \n # get TMY3 dataset\ntmy, metadata = read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990)\n # TMY3 datasets are right-labeled (AKA \"end of interval\") which means the last\n # interval of Dec 31, 23:00 to Jan 1 00:00 is labeled Jan 1 00:00. When rolling\n # up hourly irradiance to monthly insolation, a spurious January value is\n     poa = irradiance.get_total_irradiance(\n         surface_tilt=surface_tilt,\n         surface_azimuth=surface_azimuth,\n        dni=tmy['DNI'],\n        ghi=tmy['GHI'],\n        dhi=tmy['DHI'],\n         solar_zenith=solar_position['apparent_zenith'],\n         solar_azimuth=solar_position['azimuth'],\n         model='isotropic')\n df_monthly['SAT-0.4'] = poa_irradiance.resample('m').sum()\n \n # calculate the percent difference from GHI\nghi_monthly = tmy['GHI'].resample('m').sum()\n df_monthly = 100 * (df_monthly.divide(ghi_monthly, axis=0) - 1)\n \n df_monthly.plot()\ndiff --git a/docs/examples/soiling/plot_greensboro_kimber_soiling.py b/docs/examples/soiling/plot_greensboro_kimber_soiling.py\n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\n \n # get TMY3 data with rain\ngreensboro, _ = read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990)\n # get the rain data\ngreensboro_rain = greensboro.Lprecipdepth\n # calculate soiling with no wash dates and cleaning threshold of 25-mm of rain\n THRESHOLD = 25.0\n soiling_no_wash = kimber(greensboro_rain, cleaning_threshold=THRESHOLD)\ndiff --git a/pvlib/iotools/tmy.py b/pvlib/iotools/tmy.py\n import datetime\n import re\n import pandas as pd\n\n\ndef read_tmy3(filename, coerce_year=None, recolumn=True):\n     \"\"\"Read a TMY3 file into a pandas dataframe.\n \n     Note that values contained in the metadata dictionary are unchanged\n         If supplied, the year of the index will be set to `coerce_year`, except\n         for the last index value which will be set to the *next* year so that\n         the index increases monotonically.\n    recolumn : bool, default True\n         If ``True``, apply standard names to TMY3 columns. Typically this\n         results in stripping the units from the column name.\n \n     Returns\n     -------\n     USAF              Int     USAF identifier\n     ===============   ======  ===================\n \n    =====================       ======================================================================================================================================================\n    field                       description\n    =====================       ======================================================================================================================================================\n    Index                       A pandas datetime index. NOTE, the index is timezone aware, and times are set to local standard time (daylight savings is not included)\n    ETR                         Extraterrestrial horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    ETRN                        Extraterrestrial normal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    GHI                         Direct and diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    GHISource                   See [1]_, Table 1-4\n    GHIUncertainty              Uncertainty based on random and bias error estimates see [2]_\n    DNI                         Amount of direct normal radiation (modeled) recv'd during 60 mintues prior to timestamp, Wh/m^2\n    DNISource                   See [1]_, Table 1-4\n    DNIUncertainty              Uncertainty based on random and bias error estimates see [2]_\n    DHI                         Amount of diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    DHISource                   See [1]_, Table 1-4\n    DHIUncertainty              Uncertainty based on random and bias error estimates see [2]_\n    GHillum                     Avg. total horizontal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    GHillumSource               See [1]_, Table 1-4\n    GHillumUncertainty          Uncertainty based on random and bias error estimates see [2]_\n    DNillum                     Avg. direct normal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    DNillumSource               See [1]_, Table 1-4\n    DNillumUncertainty          Uncertainty based on random and bias error estimates see [2]_\n    DHillum                     Avg. horizontal diffuse illuminance recv'd during the 60 minutes prior to timestamp, lx\n    DHillumSource               See [1]_, Table 1-4\n    DHillumUncertainty          Uncertainty based on random and bias error estimates see [2]_\n    Zenithlum                   Avg. luminance at the sky's zenith during the 60 minutes prior to timestamp, cd/m^2\n    ZenithlumSource             See [1]_, Table 1-4\n    ZenithlumUncertainty        Uncertainty based on random and bias error estimates see [1]_ section 2.10\n    TotCld                      Amount of sky dome covered by clouds or obscuring phenonema at time stamp, tenths of sky\n    TotCldSource                See [1]_, Table 1-5\n    TotCldUncertainty           See [1]_, Table 1-6\n    OpqCld                      Amount of sky dome covered by clouds or obscuring phenonema that prevent observing the sky at time stamp, tenths of sky\n    OpqCldSource                See [1]_, Table 1-5\n    OpqCldUncertainty           See [1]_, Table 1-6\n    DryBulb                     Dry bulb temperature at the time indicated, deg C\n    DryBulbSource               See [1]_, Table 1-5\n    DryBulbUncertainty          See [1]_, Table 1-6\n    DewPoint                    Dew-point temperature at the time indicated, deg C\n    DewPointSource              See [1]_, Table 1-5\n    DewPointUncertainty         See [1]_, Table 1-6\n    RHum                        Relatitudeive humidity at the time indicated, percent\n    RHumSource                  See [1]_, Table 1-5\n    RHumUncertainty             See [1]_, Table 1-6\n    Pressure                    Station pressure at the time indicated, 1 mbar\n    PressureSource              See [1]_, Table 1-5\n    PressureUncertainty         See [1]_, Table 1-6\n    Wdir                        Wind direction at time indicated, degrees from north (360 = north; 0 = undefined,calm)\n    WdirSource                  See [1]_, Table 1-5\n    WdirUncertainty             See [1]_, Table 1-6\n    Wspd                        Wind speed at the time indicated, meter/second\n    WspdSource                  See [1]_, Table 1-5\n    WspdUncertainty             See [1]_, Table 1-6\n    Hvis                        Distance to discernable remote objects at time indicated (7777=unlimited), meter\n    HvisSource                  See [1]_, Table 1-5\n    HvisUncertainty             See [1]_, Table 1-6\n    CeilHgt                     Height of cloud base above local terrain (7777=unlimited), meter\n    CeilHgtSource               See [1]_, Table 1-5\n    CeilHgtUncertainty          See [1]_, Table 1-6\n    Pwat                        Total precipitable water contained in a column of unit cross section from earth to top of atmosphere, cm\n    PwatSource                  See [1]_, Table 1-5\n    PwatUncertainty             See [1]_, Table 1-6\n    AOD                         The broadband aerosol optical depth per unit of air mass due to extinction by aerosol component of atmosphere, unitless\n    AODSource                   See [1]_, Table 1-5\n    AODUncertainty              See [1]_, Table 1-6\n    Alb                         The ratio of reflected solar irradiance to global horizontal irradiance, unitless\n    AlbSource                   See [1]_, Table 1-5\n    AlbUncertainty              See [1]_, Table 1-6\n    Lprecipdepth                The amount of liquid precipitation observed at indicated time for the period indicated in the liquid precipitation quantity field, millimeter\n    Lprecipquantity             The period of accumulatitudeion for the liquid precipitation depth field, hour\n    LprecipSource               See [1]_, Table 1-5\n    LprecipUncertainty          See [1]_, Table 1-6\n    PresWth                     Present weather code, see [2]_.\n    PresWthSource               Present weather code source, see [2]_.\n    PresWthUncertainty          Present weather code uncertainty, see [2]_.\n    =====================       ======================================================================================================================================================\n \n     .. admonition:: Midnight representation\n \n     ----------\n     .. [1] Wilcox, S and Marion, W. \"Users Manual for TMY3 Data Sets\".\n        NREL/TP-581-43156, Revised May 2008.\n     .. [2] Wilcox, S. (2007). National Solar Radiation Database 1991 2005\n        Update: Users Manual. 472 pp.; NREL Report No. TP-581-41364.\n     .. [3] `SolarAnywhere file formats\n        <https://www.solaranywhere.com/support/historical-data/file-formats/>`_\n     \"\"\"  # noqa: E501\n     # NOTE: as of pvlib-0.6.3, min req is pandas-0.18.1, so pd.to_timedelta\n     # unit must be in (D,h,m,s,ms,us,ns), but pandas>=0.24 allows unit='hour'\n     data.index = data_ymd + pd.to_timedelta(shifted_hour, unit='h')\n\n    if recolumn:\n        data = _recolumn(data)  # rename to standard column names\n \n     data = data.tz_localize(int(meta['TZ'] * 3600))\n ",
    "output": " PVLIB_DIR = pvlib.__path__[0]\n DATA_FILE = os.path.join(PVLIB_DIR, 'data', '723170TYA.CSV')\n \ntmy, metadata = iotools.read_tmy3(DATA_FILE, coerce_year=1990,\n                                  map_variables=True)\n \ndf = pd.DataFrame({'ghi': tmy['ghi'], 'dhi': tmy['dhi'], 'dni': tmy['dni'],\n                   'temp_air': tmy['temp_air'],\n                   'wind_speed': tmy['wind_speed'],\n                    })\n \n # %%\ndiff --git a/docs/examples/irradiance-decomposition/plot_diffuse_fraction.py b/docs/examples/irradiance-decomposition/plot_diffuse_fraction.py\n # of data measured from 1990 to 2010. Therefore we change the timestamps to a\n # common year, 1990.\n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\ngreensboro, metadata = read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990,\n                                 map_variables=True)\n \n # Many of the diffuse fraction estimation methods require the \"true\" zenith, so\n # we calculate the solar positions for the 1990 at Greensboro, NC.\n solpos = get_solarposition(\n     greensboro.index.shift(freq=\"-30T\"), latitude=metadata['latitude'],\n     longitude=metadata['longitude'], altitude=metadata['altitude'],\n    pressure=greensboro.pressure*100,  # convert from millibar to Pa\n    temperature=greensboro.temp_air)\n solpos.index = greensboro.index  # reset index to end of the hour\n \n # %%\n # an exponential relation with airmass.\n \n out_disc = irradiance.disc(\n    greensboro.ghi, solpos.zenith, greensboro.index, greensboro.pressure*100)\n # use \"complete sum\" AKA \"closure\" equations: DHI = GHI - DNI * cos(zenith)\n df_disc = irradiance.complete_irradiance(\n    solar_zenith=solpos.apparent_zenith, ghi=greensboro.ghi, dni=out_disc.dni,\n     dhi=None)\n out_disc = out_disc.rename(columns={'dni': 'dni_disc'})\n out_disc['dhi_disc'] = df_disc.dhi\n # developed by Richard Perez and Pierre Ineichen in 1992.\n \n dni_dirint = irradiance.dirint(\n    greensboro.ghi, solpos.zenith, greensboro.index, greensboro.pressure*100,\n    temp_dew=greensboro.temp_dew)\n # use \"complete sum\" AKA \"closure\" equation: DHI = GHI - DNI * cos(zenith)\n df_dirint = irradiance.complete_irradiance(\n    solar_zenith=solpos.apparent_zenith, ghi=greensboro.ghi, dni=dni_dirint,\n     dhi=None)\n out_dirint = pd.DataFrame(\n     {'dni_dirint': dni_dirint, 'dhi_dirint': df_dirint.dhi},\n # splits kt into 3 regions: linear for kt <= 0.22, a 4th order polynomial\n # between 0.22 < kt <= 0.8, and a horizontal line for kt > 0.8.\n \nout_erbs = irradiance.erbs(greensboro.ghi, solpos.zenith, greensboro.index)\n out_erbs = out_erbs.rename(columns={'dni': 'dni_erbs', 'dhi': 'dhi_erbs'})\n \n # %%\n # exponential correlation that is continuously differentiable and bounded\n # between zero and one.\n \nout_boland = irradiance.boland(greensboro.ghi, solpos.zenith, greensboro.index)\n out_boland = out_boland.rename(\n     columns={'dni': 'dni_boland', 'dhi': 'dhi_boland'})\n \n # file together to make plotting easier.\n \n dni_renames = {\n    'dni': 'TMY3', 'dni_disc': 'DISC', 'dni_dirint': 'DIRINT',\n     'dni_erbs': 'Erbs', 'dni_boland': 'Boland'}\n dni = [\n    greensboro.dni, out_disc.dni_disc, out_dirint.dni_dirint,\n     out_erbs.dni_erbs, out_boland.dni_boland]\n dni = pd.concat(dni, axis=1).rename(columns=dni_renames)\n dhi_renames = {\n    'dhi': 'TMY3', 'dhi_disc': 'DISC', 'dhi_dirint': 'DIRINT',\n     'dhi_erbs': 'Erbs', 'dhi_boland': 'Boland'}\n dhi = [\n    greensboro.dhi, out_disc.dhi_disc, out_dirint.dhi_dirint,\n     out_erbs.dhi_erbs, out_boland.dhi_boland]\n dhi = pd.concat(dhi, axis=1).rename(columns=dhi_renames)\nghi_kt = pd.concat([greensboro.ghi/1000.0, out_erbs.kt], axis=1)\n \n # %%\n # Winter\ndiff --git a/docs/examples/irradiance-transposition/plot_seasonal_tilt.py b/docs/examples/irradiance-transposition/plot_seasonal_tilt.py\n # like we expect:\n \n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\ntmy, metadata = iotools.read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990,\n                                  map_variables=True)\n # shift from TMY3 right-labeled index to left-labeled index:\n tmy.index = tmy.index - pd.Timedelta(hours=1)\n weather = pd.DataFrame({\n    'ghi': tmy['ghi'], 'dhi': tmy['dhi'], 'dni': tmy['dni'],\n    'temp_air': tmy['temp_air'], 'wind_speed': tmy['wind_speed'],\n })\n loc = location.Location.from_tmy(metadata)\n solpos = loc.get_solarposition(weather.index)\ndiff --git a/docs/examples/irradiance-transposition/plot_transposition_gain.py b/docs/examples/irradiance-transposition/plot_transposition_gain.py\n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\n \n # get TMY3 dataset\ntmy, metadata = read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990,\n                          map_variables=True)\n # TMY3 datasets are right-labeled (AKA \"end of interval\") which means the last\n # interval of Dec 31, 23:00 to Jan 1 00:00 is labeled Jan 1 00:00. When rolling\n # up hourly irradiance to monthly insolation, a spurious January value is\n     poa = irradiance.get_total_irradiance(\n         surface_tilt=surface_tilt,\n         surface_azimuth=surface_azimuth,\n        dni=tmy['dni'],\n        ghi=tmy['ghi'],\n        dhi=tmy['dhi'],\n         solar_zenith=solar_position['apparent_zenith'],\n         solar_azimuth=solar_position['azimuth'],\n         model='isotropic')\n df_monthly['SAT-0.4'] = poa_irradiance.resample('m').sum()\n \n # calculate the percent difference from GHI\nghi_monthly = tmy['ghi'].resample('m').sum()\n df_monthly = 100 * (df_monthly.divide(ghi_monthly, axis=0) - 1)\n \n df_monthly.plot()\ndiff --git a/docs/examples/soiling/plot_greensboro_kimber_soiling.py b/docs/examples/soiling/plot_greensboro_kimber_soiling.py\n DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\n \n # get TMY3 data with rain\ngreensboro, _ = read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990,\n                          map_variables=True)\n # get the rain data\ngreensboro_rain = greensboro['Lprecip depth (mm)']\n # calculate soiling with no wash dates and cleaning threshold of 25-mm of rain\n THRESHOLD = 25.0\n soiling_no_wash = kimber(greensboro_rain, cleaning_threshold=THRESHOLD)\ndiff --git a/pvlib/iotools/tmy.py b/pvlib/iotools/tmy.py\n import datetime\n import re\n import pandas as pd\nimport warnings\nfrom pvlib._deprecation import pvlibDeprecationWarning\n\n# Dictionary mapping TMY3 names to pvlib names\nVARIABLE_MAP = {\n    'GHI (W/m^2)': 'ghi',\n    'ETR (W/m^2)': 'ghi_extra',\n    'DNI (W/m^2)': 'dni',\n    'ETRN (W/m^2)': 'dni_extra',\n    'DHI (W/m^2)': 'dhi',\n    'Pressure (mbar)': 'pressure',\n    'Wdir (degrees)': 'wind_direction',\n    'Wspd (m/s)': 'wind_speed',\n    'Dry-bulb (C)': 'temp_air',\n    'Dew-point (C)': 'temp_dew',\n    'RHum (%)': 'relative_humidity',\n    'Alb (unitless)': 'albedo',\n    'Pwat (cm)': 'precipitable_water'\n}\n\n\ndef read_tmy3(filename, coerce_year=None, map_variables=None, recolumn=None):\n     \"\"\"Read a TMY3 file into a pandas dataframe.\n \n     Note that values contained in the metadata dictionary are unchanged\n         If supplied, the year of the index will be set to `coerce_year`, except\n         for the last index value which will be set to the *next* year so that\n         the index increases monotonically.\n    map_variables : bool, default None\n        When True, renames columns of the DataFrame to pvlib variable names\n        where applicable. See variable :const:`VARIABLE_MAP`.\n    recolumn : bool (deprecated, use map_variables instead)\n         If ``True``, apply standard names to TMY3 columns. Typically this\n         results in stripping the units from the column name.\n        Cannot be used in combination with ``map_variables``.\n \n     Returns\n     -------\n     USAF              Int     USAF identifier\n     ===============   ======  ===================\n \n\n    ========================       ======================================================================================================================================================\n    field                          description\n    ========================       ======================================================================================================================================================\n    **\u2020 denotes variables that are mapped when `map_variables` is True**\n    -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Index                          A pandas datetime index. NOTE, the index is timezone aware, and times are set to local standard time (daylight savings is not included)\n    ghi_extra\u2020                     Extraterrestrial horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    dni_extra\u2020                     Extraterrestrial normal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    ghi\u2020                           Direct and diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    GHI source                     See [1]_, Table 1-4\n    GHI uncert (%)                 Uncertainty based on random and bias error estimates see [2]_\n    dni\u2020                           Amount of direct normal radiation (modeled) recv'd during 60 mintues prior to timestamp, Wh/m^2\n    DNI source                     See [1]_, Table 1-4\n    DNI uncert (%)                 Uncertainty based on random and bias error estimates see [2]_\n    dhi\u2020                           Amount of diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    DHI source                     See [1]_, Table 1-4\n    DHI uncert (%)                 Uncertainty based on random and bias error estimates see [2]_\n    GH illum (lx)                  Avg. total horizontal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    GH illum source                See [1]_, Table 1-4\n    GH illum uncert (%)            Uncertainty based on random and bias error estimates see [2]_\n    DN illum (lx)                  Avg. direct normal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    DN illum source                See [1]_, Table 1-4\n    DN illum uncert (%)            Uncertainty based on random and bias error estimates see [2]_\n    DH illum (lx)                  Avg. horizontal diffuse illuminance recv'd during the 60 minutes prior to timestamp, lx\n    DH illum source                See [1]_, Table 1-4\n    DH illum uncert (%)            Uncertainty based on random and bias error estimates see [2]_\n    Zenith lum (cd/m^2)            Avg. luminance at the sky's zenith during the 60 minutes prior to timestamp, cd/m^2\n    Zenith lum source              See [1]_, Table 1-4\n    Zenith lum uncert (%)          Uncertainty based on random and bias error estimates see [1]_ section 2.10\n    TotCld (tenths)                Amount of sky dome covered by clouds or obscuring phenonema at time stamp, tenths of sky\n    TotCld source                  See [1]_, Table 1-5\n    TotCld uncert (code)           See [1]_, Table 1-6\n    OpqCld (tenths)                Amount of sky dome covered by clouds or obscuring phenonema that prevent observing the sky at time stamp, tenths of sky\n    OpqCld source                  See [1]_, Table 1-5\n    OpqCld uncert (code)           See [1]_, Table 1-6\n    temp_air\u2020                      Dry bulb temperature at the time indicated, deg C\n    Dry-bulb source                See [1]_, Table 1-5\n    Dry-bulb uncert (code)         See [1]_, Table 1-6\n    temp_dew\u2020                      Dew-point temperature at the time indicated, deg C\n    Dew-point source               See [1]_, Table 1-5\n    Dew-point uncert (code)        See [1]_, Table 1-6\n    relative_humidity\u2020             Relatitudeive humidity at the time indicated, percent\n    RHum source                    See [1]_, Table 1-5\n    RHum uncert (code)             See [1]_, Table 1-6\n    pressure\u2020                      Station pressure at the time indicated, 1 mbar\n    Pressure source                See [1]_, Table 1-5\n    Pressure uncert (code)         See [1]_, Table 1-6\n    wind_direction\u2020                Wind direction at time indicated, degrees from north (360 = north; 0 = undefined,calm)\n    Wdir source                    See [1]_, Table 1-5\n    Wdir uncert (code)             See [1]_, Table 1-6\n    wind_speed\u2020                    Wind speed at the time indicated, meter/second\n    Wspd source                    See [1]_, Table 1-5\n    Wspd uncert (code)             See [1]_, Table 1-6\n    Hvis (m)                       Distance to discernable remote objects at time indicated (7777=unlimited), meter\n    Hvis source                    See [1]_, Table 1-5\n    Hvis uncert (coe)              See [1]_, Table 1-6\n    CeilHgt (m)                    Height of cloud base above local terrain (7777=unlimited), meter\n    CeilHgt source                 See [1]_, Table 1-5\n    CeilHgt uncert (code)          See [1]_, Table 1-6\n    precipitable_water\u2020            Total precipitable water contained in a column of unit cross section from earth to top of atmosphere, cm\n    Pwat source                    See [1]_, Table 1-5\n    Pwat uncert (code)             See [1]_, Table 1-6\n    AOD                            The broadband aerosol optical depth per unit of air mass due to extinction by aerosol component of atmosphere, unitless\n    AOD source                     See [1]_, Table 1-5\n    AOD uncert (code)              See [1]_, Table 1-6\n    albedo\u2020                        The ratio of reflected solar irradiance to global horizontal irradiance, unitless\n    Alb source                     See [1]_, Table 1-5\n    Alb uncert (code)              See [1]_, Table 1-6\n    Lprecip depth (mm)             The amount of liquid precipitation observed at indicated time for the period indicated in the liquid precipitation quantity field, millimeter\n    Lprecip quantity (hr)          The period of accumulatitudeion for the liquid precipitation depth field, hour\n    Lprecip source                 See [1]_, Table 1-5\n    Lprecip uncert (code)          See [1]_, Table 1-6\n    PresWth (METAR code)           Present weather code, see [2]_.\n    PresWth source                 Present weather code source, see [2]_.\n    PresWth uncert (code)          Present weather code uncertainty, see [2]_.\n    ========================       ======================================================================================================================================================\n \n     .. admonition:: Midnight representation\n \n     ----------\n     .. [1] Wilcox, S and Marion, W. \"Users Manual for TMY3 Data Sets\".\n        NREL/TP-581-43156, Revised May 2008.\n       :doi:`10.2172/928611`\n     .. [2] Wilcox, S. (2007). National Solar Radiation Database 1991 2005\n        Update: Users Manual. 472 pp.; NREL Report No. TP-581-41364.\n       :doi:`10.2172/901864`\n     .. [3] `SolarAnywhere file formats\n        <https://www.solaranywhere.com/support/historical-data/file-formats/>`_\n     \"\"\"  # noqa: E501\n     # NOTE: as of pvlib-0.6.3, min req is pandas-0.18.1, so pd.to_timedelta\n     # unit must be in (D,h,m,s,ms,us,ns), but pandas>=0.24 allows unit='hour'\n     data.index = data_ymd + pd.to_timedelta(shifted_hour, unit='h')\n    # shouldnt' specify both recolumn and map_variables\n    if recolumn is not None and map_variables is not None:\n        msg = \"`map_variables` and `recolumn` cannot both be specified\"\n        raise ValueError(msg)\n    elif map_variables is None and recolumn is not None:\n        warnings.warn(\n            'The recolumn parameter is deprecated and will be removed in '\n            'pvlib 0.11.0. Use `map_variables` instead, although note that '\n            'its behavior is different from `recolumn`.',\n            pvlibDeprecationWarning)\n    elif map_variables is None and recolumn is None:\n        warnings.warn(\n            'TMY3 variable names will be renamed to pvlib conventions by '\n            'default starting in pvlib 0.11.0. Specify map_variables=True '\n            'to enable that behavior now, or specify map_variables=False '\n            'to hide this warning.', pvlibDeprecationWarning)\n    if map_variables:\n        data = data.rename(columns=VARIABLE_MAP)\n    elif recolumn or (recolumn is None and map_variables is None):\n        data = _recolumn(data)\n \n     data = data.tz_localize(int(meta['TZ'] * 3600))\n "
  },
  {
    "instruction": "Incorrect AOI from pvlib.tracking.singleaxis\n`pvlib.tracking.singleaxis` produces an incorrect AOI when the sun is above the earth horizon but behind the module plane.\r\n\r\n**To Reproduce**\r\nModel a fixed tilt system (90 tilt, 180 azimuth) and compare to a vertical single axis tracker with very small rotation limit.\r\n\r\n```\r\n\r\nimport pandas as pd\r\nimport pytz\r\nimport pvlib\r\nfrom matplotlib import pyplot as plt\r\n\r\nloc = pvlib.location.Location(40.1134, -88.3695)\r\n\r\ndr = pd.date_range(start='02-Jun-1998 00:00:00', end='02-Jun-1998 23:55:00',\r\n                   freq='5T')\r\ntz = pytz.timezone('Etc/GMT+6')\r\ndr = dr.tz_localize(tz)\r\nhr = dr.hour + dr.minute/60\r\n\r\nsp = loc.get_solarposition(dr)\r\n\r\ncs = loc.get_clearsky(dr)\r\n\r\ntr = pvlib.tracking.singleaxis(sp['apparent_zenith'], sp['azimuth'],\r\n                               axis_tilt=90, axis_azimuth=180, max_angle=0.01,\r\n                               backtrack=False)\r\n\r\nfixed = pvlib.irradiance.aoi(90, 180, sp['apparent_zenith'], sp['azimuth'])\r\n\r\nplt.plot(hr, fixed)\r\nplt.plot(hr, tr['aoi'])\r\nplt.plot(hr, sp[['apparent_elevation']])\r\nplt.show()\r\n\r\nplt.legend(['aoi - fixed', 'aoi - tracked', 'apparent_elevation'])\r\n```\r\n\r\n**Expected behavior**\r\nThe AOI for the fixed tilt system shows values greater than 90 when the sun is behind the module plane. The AOI from `singleaxis` does not.\r\n\r\nI think the source of the error is the use of `abs` in [this ](https://github.com/pvlib/pvlib-python/blob/ca61503fa83e76631f84fb4237d9e11ae99f3c77/pvlib/tracking.py#L446)line.\r\n\r\n**Screenshots**\r\n![aoi_fixed_vs_tracked](https://user-images.githubusercontent.com/5393711/117505270-01087a80-af41-11eb-9220-10cccf2714e1.png)\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.8.1\r\n\r\nFirst reported by email from Jim Wilzcak (NOAA) for the PVlib Matlab function [pvl_singleaxis.m](https://github.com/sandialabs/MATLAB_PV_LIB/blob/master/pvl_singleaxis.m)\r\n\n",
    "input": "     sun_vec = np.array([xp, yp, zp])\n \n     # calculate angle-of-incidence on panel\n    aoi = np.degrees(np.arccos(np.abs(np.sum(sun_vec*panel_norm, axis=0))))\n \n     # Calculate panel tilt and azimuth in a coordinate system where the panel\n     # tilt is the angle from horizontal, and the panel azimuth is the compass",
    "output": "     sun_vec = np.array([xp, yp, zp])\n \n     # calculate angle-of-incidence on panel\n    # TODO: use irradiance.aoi\n    projection = np.clip(np.sum(sun_vec*panel_norm, axis=0), -1, 1)\n    aoi = np.degrees(np.arccos(projection))\n \n     # Calculate panel tilt and azimuth in a coordinate system where the panel\n     # tilt is the angle from horizontal, and the panel azimuth is the compass"
  },
  {
    "instruction": "getter/parser for PVGIS hourly-radiation\n**Is your feature request related to a problem? Please describe.**\r\nRelated to #845 \r\n\r\n**Describe the solution you'd like**\r\nSimilar to `get_pvgis_tmy` retrieve pvgis hourly radiation data from their api\r\n\r\n**Describe alternatives you've considered**\r\nPvgis is becoming a popular resource more and more people are asking me for it, it is nice because it is a global collection of several different radiation databases including nsrdb and others, and different from cams, the data is complete, ie it has air temperature, wind speed as well as all 3 components of irradiance\r\n\r\n**Additional context**\r\nThis would be part of the `iotool` sub-package. There's already a `pvgis.py` module with a getter for tmy be data\n",
    "input": " from pvlib.iotools.psm3 import read_psm3  # noqa: F401\n from pvlib.iotools.psm3 import parse_psm3  # noqa: F401\n from pvlib.iotools.pvgis import get_pvgis_tmy, read_pvgis_tmy  # noqa: F401\n from pvlib.iotools.bsrn import read_bsrn  # noqa: F401\n from pvlib.iotools.sodapro import get_cams  # noqa: F401\n from pvlib.iotools.sodapro import read_cams  # noqa: F401\ndiff --git a/pvlib/iotools/pvgis.py b/pvlib/iotools/pvgis.py\n \n URL = 'https://re.jrc.ec.europa.eu/api/'\n \n \n def get_pvgis_tmy(lat, lon, outputformat='json', usehorizon=True,\n                   userhorizon=None, startyear=None, endyear=None, url=URL,",
    "output": " from pvlib.iotools.psm3 import read_psm3  # noqa: F401\n from pvlib.iotools.psm3 import parse_psm3  # noqa: F401\n from pvlib.iotools.pvgis import get_pvgis_tmy, read_pvgis_tmy  # noqa: F401\nfrom pvlib.iotools.pvgis import read_pvgis_hourly  # noqa: F401\nfrom pvlib.iotools.pvgis import get_pvgis_hourly  # noqa: F401\n from pvlib.iotools.bsrn import read_bsrn  # noqa: F401\n from pvlib.iotools.sodapro import get_cams  # noqa: F401\n from pvlib.iotools.sodapro import read_cams  # noqa: F401\ndiff --git a/pvlib/iotools/pvgis.py b/pvlib/iotools/pvgis.py\n \n URL = 'https://re.jrc.ec.europa.eu/api/'\n \n# Dictionary mapping PVGIS names to pvlib names\nPVGIS_VARIABLE_MAP = {\n    'G(h)': 'ghi',\n    'Gb(n)': 'dni',\n    'Gd(h)': 'dhi',\n    'G(i)': 'poa_global',\n    'Gb(i)': 'poa_direct',\n    'Gd(i)': 'poa_sky_diffuse',\n    'Gr(i)': 'poa_ground_diffuse',\n    'H_sun': 'solar_elevation',\n    'T2m': 'temp_air',\n    'RH': 'relative_humidity',\n    'SP': 'pressure',\n    'WS10m': 'wind_speed',\n    'WD10m': 'wind_direction',\n}\n\n\ndef get_pvgis_hourly(latitude, longitude, start=None, end=None,\n                     raddatabase=None, components=True,\n                     surface_tilt=0, surface_azimuth=0,\n                     outputformat='json',\n                     usehorizon=True, userhorizon=None,\n                     pvcalculation=False,\n                     peakpower=None, pvtechchoice='crystSi',\n                     mountingplace='free', loss=0, trackingtype=0,\n                     optimal_surface_tilt=False, optimalangles=False,\n                     url=URL, map_variables=True, timeout=30):\n    \"\"\"Get hourly solar irradiation and modeled PV power output from PVGIS.\n\n    PVGIS data is freely available at [1]_.\n\n    Parameters\n    ----------\n    latitude: float\n        In decimal degrees, between -90 and 90, north is positive (ISO 19115)\n    longitude: float\n        In decimal degrees, between -180 and 180, east is positive (ISO 19115)\n    start: int or datetime like, default: None\n        First year of the radiation time series. Defaults to first year\n        available.\n    end: int or datetime like, default: None\n        Last year of the radiation time series. Defaults to last year\n        available.\n    raddatabase: str, default: None\n        Name of radiation database. Options depend on location, see [3]_.\n    components: bool, default: True\n        Output solar radiation components (beam, diffuse, and reflected).\n        Otherwise only global irradiance is returned.\n    surface_tilt: float, default: 0\n        Tilt angle from horizontal plane. Ignored for two-axis tracking.\n    surface_azimuth: float, default: 0\n        Orientation (azimuth angle) of the (fixed) plane. 0=south, 90=west,\n        -90: east. Ignored for tracking systems.\n    usehorizon: bool, default: True\n        Include effects of horizon\n    userhorizon: list of float, default: None\n        Optional user specified elevation of horizon in degrees, at equally\n        spaced azimuth clockwise from north, only valid if `usehorizon` is\n        true, if `usehorizon` is true but `userhorizon` is `None` then PVGIS\n        will calculate the horizon [4]_\n    pvcalculation: bool, default: False\n        Return estimate of hourly PV production.\n    peakpower: float, default: None\n        Nominal power of PV system in kW. Required if pvcalculation=True.\n    pvtechchoice: {'crystSi', 'CIS', 'CdTe', 'Unknown'}, default: 'crystSi'\n        PV technology.\n    mountingplace: {'free', 'building'}, default: free\n        Type of mounting for PV system. Options of 'free' for free-standing\n        and 'building' for building-integrated.\n    loss: float, default: 0\n        Sum of PV system losses in percent. Required if pvcalculation=True\n    trackingtype: {0, 1, 2, 3, 4, 5}, default: 0\n        Type of suntracking. 0=fixed, 1=single horizontal axis aligned\n        north-south, 2=two-axis tracking, 3=vertical axis tracking, 4=single\n        horizontal axis aligned east-west, 5=single inclined axis aligned\n        north-south.\n    optimal_surface_tilt: bool, default: False\n        Calculate the optimum tilt angle. Ignored for two-axis tracking\n    optimalangles: bool, default: False\n        Calculate the optimum tilt and azimuth angles. Ignored for two-axis\n        tracking.\n    outputformat: str, default: 'json'\n        Must be in ``['json', 'csv']``. See PVGIS hourly data\n        documentation [2]_ for more info.\n    url: str, default: const:`pvlib.iotools.pvgis.URL`\n        Base url of PVGIS API. ``seriescalc`` is appended to get hourly data\n        endpoint.\n    map_variables: bool, default: True\n        When true, renames columns of the Dataframe to pvlib variable names\n        where applicable. See variable PVGIS_VARIABLE_MAP.\n    timeout: int, default: 30\n        Time in seconds to wait for server response before timeout\n\n    Returns\n    -------\n    data : pandas.DataFrame\n        Time-series of hourly data, see Notes for fields\n    inputs : dict\n        Dictionary of the request input parameters\n    metadata : dict\n        Dictionary containing metadata\n\n    Raises\n    ------\n    requests.HTTPError\n        If the request response status is ``HTTP/1.1 400 BAD REQUEST``, then\n        the error message in the response will be raised as an exception,\n        otherwise raise whatever ``HTTP/1.1`` error occurred\n\n    Hint\n    ----\n    PVGIS provides access to a number of different solar radiation datasets,\n    including satellite-based (SARAH, CMSAF, and NSRDB PSM3) and re-analysis\n    products (ERA5 and COSMO). Each data source has a different geographical\n    coverage and time stamp convention, e.g., SARAH and CMSAF provide\n    instantaneous values, whereas values from ERA5 are averages for the hour.\n\n    Notes\n    -----\n    data includes the following fields:\n\n    ===========================  ======  ======================================\n    raw, mapped                  Format  Description\n    ===========================  ======  ======================================\n    *Mapped field names are returned when the map_variables argument is True*\n    ---------------------------------------------------------------------------\n    P\u2020                           float   PV system power (W)\n    G(i), poa_global\u2021            float   Global irradiance on inclined plane (W/m^2)\n    Gb(i), poa_direct\u2021           float   Beam (direct) irradiance on inclined plane (W/m^2)\n    Gd(i), poa_sky_diffuse\u2021      float   Diffuse irradiance on inclined plane (W/m^2)\n    Gr(i), poa_ground_diffuse\u2021   float   Reflected irradiance on inclined plane (W/m^2)\n    H_sun, solar_elevation       float   Sun height/elevation (degrees)\n    T2m, temp_air                float   Air temperature at 2 m (degrees Celsius)\n    WS10m, wind_speed            float   Wind speed at 10 m (m/s)\n    Int                          int     Solar radiation reconstructed (1/0)\n    ===========================  ======  ======================================\n\n    \u2020P (PV system power) is only returned when pvcalculation=True.\n\n    \u2021Gb(i), Gd(i), and Gr(i) are returned when components=True, otherwise the\n    sum of the three components, G(i), is returned.\n\n    See Also\n    --------\n    pvlib.iotools.read_pvgis_hourly, pvlib.iotools.get_pvgis_tmy\n\n    References\n    ----------\n    .. [1] `PVGIS <https://ec.europa.eu/jrc/en/pvgis>`_\n    .. [2] `PVGIS Hourly Radiation\n       <https://ec.europa.eu/jrc/en/PVGIS/tools/hourly-radiation>`_\n    .. [3] `PVGIS Non-interactive service\n       <https://ec.europa.eu/jrc/en/PVGIS/docs/noninteractive>`_\n    .. [4] `PVGIS horizon profile tool\n       <https://ec.europa.eu/jrc/en/PVGIS/tools/horizon>`_\n    \"\"\"  # noqa: E501\n    # use requests to format the query string by passing params dictionary\n    params = {'lat': latitude, 'lon': longitude, 'outputformat': outputformat,\n              'angle': surface_tilt, 'aspect': surface_azimuth,\n              'pvcalculation': int(pvcalculation),\n              'pvtechchoice': pvtechchoice, 'mountingplace': mountingplace,\n              'trackingtype': trackingtype, 'components': int(components),\n              'usehorizon': int(usehorizon),\n              'optimalangles': int(optimalangles),\n              'optimalinclination': int(optimalangles), 'loss': loss}\n    # pvgis only takes 0 for False, and 1 for True, not strings\n    if userhorizon is not None:\n        params['userhorizon'] = ','.join(str(x) for x in userhorizon)\n    if raddatabase is not None:\n        params['raddatabase'] = raddatabase\n    if start is not None:\n        params['startyear'] = start if isinstance(start, int) else start.year\n    if end is not None:\n        params['endyear'] = end if isinstance(end, int) else end.year\n    if peakpower is not None:\n        params['peakpower'] = peakpower\n\n    # The url endpoint for hourly radiation is 'seriescalc'\n    res = requests.get(url + 'seriescalc', params=params, timeout=timeout)\n    # PVGIS returns really well formatted error messages in JSON for HTTP/1.1\n    # 400 BAD REQUEST so try to return that if possible, otherwise raise the\n    # HTTP/1.1 error caught by requests\n    if not res.ok:\n        try:\n            err_msg = res.json()\n        except Exception:\n            res.raise_for_status()\n        else:\n            raise requests.HTTPError(err_msg['message'])\n\n    return read_pvgis_hourly(io.StringIO(res.text), pvgis_format=outputformat,\n                             map_variables=map_variables)\n\n\ndef _parse_pvgis_hourly_json(src, map_variables):\n    inputs = src['inputs']\n    metadata = src['meta']\n    data = pd.DataFrame(src['outputs']['hourly'])\n    data.index = pd.to_datetime(data['time'], format='%Y%m%d:%H%M', utc=True)\n    data = data.drop('time', axis=1)\n    data = data.astype(dtype={'Int': 'int'})  # The 'Int' column to be integer\n    if map_variables:\n        data = data.rename(columns=PVGIS_VARIABLE_MAP)\n    return data, inputs, metadata\n\n\ndef _parse_pvgis_hourly_csv(src, map_variables):\n    # The first 4 rows are latitude, longitude, elevation, radiation database\n    inputs = {}\n    # 'Latitude (decimal degrees): 45.000\\r\\n'\n    inputs['latitude'] = float(src.readline().split(':')[1])\n    # 'Longitude (decimal degrees): 8.000\\r\\n'\n    inputs['longitude'] = float(src.readline().split(':')[1])\n    # Elevation (m): 1389.0\\r\\n\n    inputs['elevation'] = float(src.readline().split(':')[1])\n    # 'Radiation database: \\tPVGIS-SARAH\\r\\n'\n    inputs['radiation_database'] = src.readline().split(':')[1].strip()\n    # Parse through the remaining metadata section (the number of lines for\n    # this section depends on the requested parameters)\n    while True:\n        line = src.readline()\n        if line.startswith('time,'):  # The data header starts with 'time,'\n            # The last line of the metadata section contains the column names\n            names = line.strip().split(',')\n            break\n        # Only retrieve metadata from non-empty lines\n        elif line.strip() != '':\n            inputs[line.split(':')[0]] = line.split(':')[1].strip()\n        elif line == '':  # If end of file is reached\n            raise ValueError('No data section was detected. File has probably '\n                             'been modified since being downloaded from PVGIS')\n    # Save the entries from the data section to a list, until an empty line is\n    # reached an empty line. The length of the section depends on the request\n    data_lines = []\n    while True:\n        line = src.readline()\n        if line.strip() == '':\n            break\n        else:\n            data_lines.append(line.strip().split(','))\n    data = pd.DataFrame(data_lines, columns=names)\n    data.index = pd.to_datetime(data['time'], format='%Y%m%d:%H%M', utc=True)\n    data = data.drop('time', axis=1)\n    if map_variables:\n        data = data.rename(columns=PVGIS_VARIABLE_MAP)\n    # All columns should have the dtype=float, except 'Int' which should be\n    # integer. It is necessary to convert to float, before converting to int\n    data = data.astype(float).astype(dtype={'Int': 'int'})\n    # Generate metadata dictionary containing description of parameters\n    metadata = {}\n    for line in src.readlines():\n        if ':' in line:\n            metadata[line.split(':')[0]] = line.split(':')[1].strip()\n    return data, inputs, metadata\n\n\ndef read_pvgis_hourly(filename, pvgis_format=None, map_variables=True):\n    \"\"\"Read a PVGIS hourly file.\n\n    Parameters\n    ----------\n    filename : str, pathlib.Path, or file-like buffer\n        Name, path, or buffer of hourly data file downloaded from PVGIS.\n    pvgis_format : str, default None\n        Format of PVGIS file or buffer. Equivalent to the ``outputformat``\n        parameter in the PVGIS API. If `filename` is a file and\n        `pvgis_format` is ``None`` then the file extension will be used to\n        determine the PVGIS format to parse. If `filename` is a buffer, then\n        `pvgis_format` is required and must be in ``['csv', 'json']``.\n    map_variables: bool, default True\n        When true, renames columns of the DataFrame to pvlib variable names\n        where applicable. See variable PVGIS_VARIABLE_MAP.\n\n    Returns\n    -------\n    data : pandas.DataFrame\n        the time series data\n    inputs : dict\n        the inputs\n    metadata : dict\n        metadata\n\n    Raises\n    ------\n    ValueError\n        if `pvgis_format` is ``None`` and the file extension is neither\n        ``.csv`` nor ``.json`` or if `pvgis_format` is provided as\n        input but isn't in ``['csv', 'json']``\n    TypeError\n        if `pvgis_format` is ``None`` and `filename` is a buffer\n\n    See Also\n    --------\n    get_pvgis_hourly, read_pvgis_tmy\n    \"\"\"\n    # get the PVGIS outputformat\n    if pvgis_format is None:\n        # get the file extension from suffix, but remove the dot and make sure\n        # it's lower case to compare with csv, or json\n        # NOTE: basic format is not supported for PVGIS Hourly as the data\n        # format does not include a header\n        # NOTE: raises TypeError if filename is a buffer\n        outputformat = Path(filename).suffix[1:].lower()\n    else:\n        outputformat = pvgis_format\n\n    # parse the pvgis file based on the output format, either 'json' or 'csv'\n    # NOTE: json and csv output formats have parsers defined as private\n    # functions in this module\n\n    # JSON: use Python built-in json module to convert file contents to a\n    # Python dictionary, and pass the dictionary to the\n    # _parse_pvgis_hourly_json() function from this module\n    if outputformat == 'json':\n        try:\n            src = json.load(filename)\n        except AttributeError:  # str/path has no .read() attribute\n            with open(str(filename), 'r') as fbuf:\n                src = json.load(fbuf)\n        return _parse_pvgis_hourly_json(src, map_variables=map_variables)\n\n    # CSV: use _parse_pvgis_hourly_csv()\n    if outputformat == 'csv':\n        try:\n            pvgis_data = _parse_pvgis_hourly_csv(\n                filename, map_variables=map_variables)\n        except AttributeError:  # str/path has no .read() attribute\n            with open(str(filename), 'r') as fbuf:\n                pvgis_data = _parse_pvgis_hourly_csv(\n                    fbuf, map_variables=map_variables)\n        return pvgis_data\n\n    # raise exception if pvgis format isn't in ['csv', 'json']\n    err_msg = (\n        \"pvgis format '{:s}' was unknown, must be either 'json' or 'csv'\")\\\n        .format(outputformat)\n    raise ValueError(err_msg)\n\n \n def get_pvgis_tmy(lat, lon, outputformat='json', usehorizon=True,\n                   userhorizon=None, startyear=None, endyear=None, url=URL,"
  },
  {
    "instruction": "singlediode: newton solver fails with Series input of length one\nThe vectorized newton solver doesn't work if parameters are Series of length one.\r\n\r\n```\r\n\r\nimport pandas as pd\r\nimport pvlib\r\n\r\n\r\nargs = (0.001, 1.5, 6., 5e-9, 1000., 0.5)\r\nparams = pvlib.pvsystem.calcparams_desoto(1000., 25, *args)\r\nparams_series = pvlib.pvsystem.calcparams_desoto(pd.Series(data=[1000.]),\r\n                                                 pd.Series([25.]), *args)\r\nparams_series2 = pvlib.pvsystem.calcparams_desoto(pd.Series(data=[1000., 1000.]),\r\n                                                  pd.Series([25., 25.]), *args)\r\n# works with each input as float\r\nresult = pvlib.pvsystem.singlediode(*params, method='newton')\r\n\r\n# works with Series if length > 1\r\nresult_series2 = pvlib.pvsystem.singlediode(*params_series2, method='newton')\r\n\r\n# errors with Series if length is 1\r\nresult_series = pvlib.pvsystem.singlediode(*params_series, method='newton')\r\n```\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.5\r\n\n",
    "input": "        parameters of real solar cells using Lambert W-function\", Solar\n        Energy Materials and Solar Cells, 81 (2004) 269-277.\n     '''\n     if method.lower() == 'lambertw':\n        return _singlediode._lambertw_v_from_i(\n            current, photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth\n        )\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\n         # methods. Voltages are determined by first solving the single diode\n         # equation for the diode voltage V_d then backing out voltage\n        args = (current, photocurrent, saturation_current,\n                resistance_series, resistance_shunt, nNsVth)\n         V = _singlediode.bishop88_v_from_i(*args, method=method.lower())\n        # find the right size and shape for returns\n        size, shape = _singlediode._get_size_and_shape(args)\n        if size <= 1:\n            if shape is not None:\n                V = np.tile(V, shape)\n        if np.isnan(V).any() and size <= 1:\n            V = np.repeat(V, size)\n            if shape is not None:\n                V = V.reshape(shape)\n        return V\n \n \n def i_from_v(voltage, photocurrent, saturation_current, resistance_series,\n        parameters of real solar cells using Lambert W-function\", Solar\n        Energy Materials and Solar Cells, 81 (2004) 269-277.\n     '''\n     if method.lower() == 'lambertw':\n        return _singlediode._lambertw_i_from_v(\n            voltage, photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth\n        )\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\n         # methods. Voltages are determined by first solving the single diode\n         # equation for the diode voltage V_d then backing out voltage\n        args = (voltage, photocurrent, saturation_current, resistance_series,\n                resistance_shunt, nNsVth)\n         current = _singlediode.bishop88_i_from_v(*args, method=method.lower())\n        # find the right size and shape for returns\n        size, shape = _singlediode._get_size_and_shape(args)\n        if size <= 1:\n            if shape is not None:\n                current = np.tile(current, shape)\n        if np.isnan(current).any() and size <= 1:\n            current = np.repeat(current, size)\n            if shape is not None:\n                current = current.reshape(shape)\n        return current\n \n \n def scale_voltage_current_power(data, voltage=1, current=1):\ndiff --git a/pvlib/singlediode.py b/pvlib/singlediode.py\n     ...     method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n    args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth, d2mutau, NsVbi,\n             breakdown_factor, breakdown_voltage, breakdown_exp)\n     method = method.lower()\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n     elif method == 'newton':\n        # make sure all args are numpy arrays if max size > 1\n        # if voltage is an array, then make a copy to use for initial guess, v0\n        args, v0, method_kwargs = \\\n            _prepare_newton_inputs((voltage,), args, voltage, method_kwargs)\n        vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n                    args=args,\n                    **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n \n     ...     method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n    args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n            breakdown_voltage, breakdown_exp)\n     method = method.lower()\n \n     # method_kwargs create dict if not provided\n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, current, *args)\n     elif method == 'newton':\n        # make sure all args are numpy arrays if max size > 1\n        # if voc_est is an array, then make a copy to use for initial guess, v0\n        args, v0, method_kwargs = \\\n            _prepare_newton_inputs((current,), args, voc_est, method_kwargs)\n        vd = newton(func=lambda x, *a: fi(x, current, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[3],\n                    args=args,\n                    **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n \n     ...     method='newton', method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n    args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n            breakdown_voltage, breakdown_exp)\n     method = method.lower()\n \n     # method_kwargs create dict if not provided\n     elif method == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voc_est is an array, then make a copy to use for initial guess, v0\n        args, v0, method_kwargs = \\\n            _prepare_newton_inputs((), args, voc_est, method_kwargs)\n        vd = newton(\n            func=fmpp, x0=v0,\n            fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args,\n            **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n \n         return bishop88(vd, *args)\n \n \ndef _get_size_and_shape(args):\n    # find the right size and shape for returns\n    size, shape = 0, None  # 0 or None both mean scalar\n    for arg in args:\n        try:\n            this_shape = arg.shape  # try to get shape\n        except AttributeError:\n            this_shape = None\n            try:\n                this_size = len(arg)  # try to get the size\n            except TypeError:\n                this_size = 0\n        else:\n            this_size = arg.size  # if it has shape then it also has size\n            if shape is None:\n                shape = this_shape  # set the shape if None\n        # update size and shape\n        if this_size > size:\n            size = this_size\n            if this_shape is not None:\n                shape = this_shape\n    return size, shape\n\n\ndef _prepare_newton_inputs(i_or_v_tup, args, v0, method_kwargs):\n    # broadcast arguments for newton method\n    # the first argument should be a tuple, eg: (i,), (v,) or ()\n    size, shape = _get_size_and_shape(i_or_v_tup + args)\n    if size > 1:\n        args = [np.asarray(arg) for arg in args]\n    # newton uses initial guess for the output shape\n    # copy v0 to a new array and broadcast it to the shape of max size\n    if shape is not None:\n        v0 = np.broadcast_to(v0, shape).copy()\n \n     # set abs tolerance and maxiter from method_kwargs if not provided\n     # apply defaults, but giving priority to user-specified values\n     method_kwargs = {**NEWTON_DEFAULT_PARAMS, **method_kwargs}\n \n    return args, v0, method_kwargs\n \n \n def _lambertw_v_from_i(current, photocurrent, saturation_current,",
    "output": "        parameters of real solar cells using Lambert W-function\", Solar\n        Energy Materials and Solar Cells, 81 (2004) 269-277.\n     '''\n    args = (current, photocurrent, saturation_current,\n            resistance_series, resistance_shunt, nNsVth)\n     if method.lower() == 'lambertw':\n        return _singlediode._lambertw_v_from_i(*args)\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\n         # methods. Voltages are determined by first solving the single diode\n         # equation for the diode voltage V_d then backing out voltage\n         V = _singlediode.bishop88_v_from_i(*args, method=method.lower())\n        if all(map(np.isscalar, args)):\n            return V\n        shape = _singlediode._shape_of_max_size(*args)\n        return np.broadcast_to(V, shape)\n \n \n def i_from_v(voltage, photocurrent, saturation_current, resistance_series,\n        parameters of real solar cells using Lambert W-function\", Solar\n        Energy Materials and Solar Cells, 81 (2004) 269-277.\n     '''\n    args = (voltage, photocurrent, saturation_current,\n            resistance_series, resistance_shunt, nNsVth)\n     if method.lower() == 'lambertw':\n        return _singlediode._lambertw_i_from_v(*args)\n     else:\n         # Calculate points on the IV curve using either 'newton' or 'brentq'\n         # methods. Voltages are determined by first solving the single diode\n         # equation for the diode voltage V_d then backing out voltage\n         current = _singlediode.bishop88_i_from_v(*args, method=method.lower())\n        if all(map(np.isscalar, args)):\n            return current\n        shape = _singlediode._shape_of_max_size(*args)\n        return np.broadcast_to(current, shape)\n \n \n def scale_voltage_current_power(data, voltage=1, current=1):\ndiff --git a/pvlib/singlediode.py b/pvlib/singlediode.py\n     ...     method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n    args = (photocurrent, saturation_current,\n            resistance_series, resistance_shunt, nNsVth, d2mutau, NsVbi,\n             breakdown_factor, breakdown_voltage, breakdown_exp)\n     method = method.lower()\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n     elif method == 'newton':\n        x0, (voltage, *args), method_kwargs = \\\n            _prepare_newton_inputs(voltage, (voltage, *args), method_kwargs)\n        vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=x0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n                    args=args, **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n \n     ...     method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n    args = (photocurrent, saturation_current,\n            resistance_series, resistance_shunt, nNsVth, d2mutau, NsVbi,\n            breakdown_factor, breakdown_voltage, breakdown_exp)\n     method = method.lower()\n \n     # method_kwargs create dict if not provided\n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, current, *args)\n     elif method == 'newton':\n        x0, (current, *args), method_kwargs = \\\n            _prepare_newton_inputs(voc_est, (current, *args), method_kwargs)\n        vd = newton(func=lambda x, *a: fi(x, current, *a), x0=x0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[3],\n                    args=args, **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n \n     ...     method='newton', method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n    args = (photocurrent, saturation_current,\n            resistance_series, resistance_shunt, nNsVth, d2mutau, NsVbi,\n            breakdown_factor, breakdown_voltage, breakdown_exp)\n     method = method.lower()\n \n     # method_kwargs create dict if not provided\n     elif method == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voc_est is an array, then make a copy to use for initial guess, v0\n        x0, args, method_kwargs = \\\n            _prepare_newton_inputs(voc_est, args, method_kwargs)\n        vd = newton(func=fmpp, x0=x0,\n                    fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7],\n                    args=args, **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n \n         return bishop88(vd, *args)\n \n \ndef _shape_of_max_size(*args):\n    return max(((np.size(a), np.shape(a)) for a in args),\n               key=lambda t: t[0])[1]\n\n\ndef _prepare_newton_inputs(x0, args, method_kwargs):\n    \"\"\"\n    Make inputs compatible with Scipy's newton by:\n    - converting all arugments (`x0` and `args`) into numpy.ndarrays if any\n      argument is not a scalar.\n    - broadcasting the initial guess `x0` to the shape of the argument with\n      the greatest size.\n\n    Parameters\n    ----------\n    x0: numeric\n        Initial guess for newton.\n    args: Iterable(numeric)\n        Iterable of additional arguments to use in SciPy's newton.\n    method_kwargs: dict\n        Options to pass to newton.\n\n    Returns\n    -------\n    tuple\n        The updated initial guess, arguments, and options for newton.\n    \"\"\"\n    if not (np.isscalar(x0) and all(map(np.isscalar, args))):\n        args = tuple(map(np.asarray, args))\n        x0 = np.broadcast_to(x0, _shape_of_max_size(x0, *args))\n \n     # set abs tolerance and maxiter from method_kwargs if not provided\n     # apply defaults, but giving priority to user-specified values\n     method_kwargs = {**NEWTON_DEFAULT_PARAMS, **method_kwargs}\n \n    return x0, args, method_kwargs\n \n \n def _lambertw_v_from_i(current, photocurrent, saturation_current,"
  },
  {
    "instruction": "ValueError: ModelChain.run_from_effective_irradiance([weather]) when only providing temp_air and wind_speed\n**Describe the bug**\r\nAccording to the (new) docstring for `ModelChain.run_from_effective_irradiance`, cell temperature can be calculated from temperature_model using `'effective_irradiance'`. This is not the case when using one or more arrays \r\nhttps://github.com/pvlib/pvlib-python/blame/master/pvlib/modelchain.py#L1589-L1606\r\n\r\n**To Reproduce**\r\n```python\r\nfrom copy import deepcopy\r\nimport pandas as pd\r\nfrom pvlib.location import Location\r\nfrom pvlib.pvsystem import Array, PVSystem\r\nfrom pvlib.modelchain import ModelChain\r\n\r\n\r\narray_params = {\r\n    \"surface_tilt\": 32.0,\r\n    \"surface_azimuth\": 180.0,\r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\r\n    \"albedo\": 0.2,\r\n    \"temperature_model_parameters\": {\r\n        \"u_c\": 29.0,\r\n        \"u_v\": 0.0,\r\n        \"eta_m\": 0.1,\r\n        \"alpha_absorption\": 0.9,\r\n    },\r\n    \"strings\": 5,\r\n    \"modules_per_string\": 7,\r\n    \"module_parameters\": {\r\n        \"alpha_sc\": 0.004539,\r\n        \"gamma_ref\": 1.2,\r\n        \"mu_gamma\": -0.003,\r\n        \"I_L_ref\": 5.11426,\r\n        \"I_o_ref\": 8.10251e-10,\r\n        \"R_sh_ref\": 381.254,\r\n        \"R_sh_0\": 400.0,\r\n        \"R_s\": 1.06602,\r\n        \"cells_in_series\": 96,\r\n        \"R_sh_exp\": 5.5,\r\n        \"EgRef\": 1.121,\r\n    },\r\n}\r\ninverter_parameters = {\r\n    \"Paco\": 250.0,\r\n    \"Pdco\": 259.589,\r\n    \"Vdco\": 40.0,\r\n    \"Pso\": 2.08961,\r\n    \"C0\": -4.1e-05,\r\n    \"C1\": -9.1e-05,\r\n    \"C2\": 0.000494,\r\n    \"C3\": -0.013171,\r\n    \"Pnt\": 0.075,\r\n}\r\n\r\n\r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)\r\n\r\narray_sys = PVSystem(\r\n    arrays=[\r\n        Array(**array_params, name=0),\r\n    ],\r\n    inverter_parameters=inverter_parameters,\r\n)\r\nweather = pd.DataFrame(\r\n    {\r\n        \"effective_irradiance\": [1100.0, 1101.0],\r\n        \"temp_air\": [25.0, 26.0],\r\n        \"wind_speed\": [10.0, 10.0],\r\n    },\r\n    index=pd.DatetimeIndex(\r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\r\n    ),\r\n)\r\nmc0 = ModelChain(\r\n    array_sys,\r\n    location,\r\n    aoi_model=\"no_loss\",\r\n    spectral_model=\"no_loss\",\r\n)\r\nmc1 = deepcopy(mc0)\r\n\r\nmc0.run_model_from_effective_irradiance(weather)\r\nassert isinstance(mc0.results.cell_temperature, pd.Series)\r\n\r\n\r\nmc1.run_model_from_effective_irradiance([weather])  # ValueError\r\n\r\n```\r\n\r\n**Expected behavior**\r\nRunning the model with both `weather` and `[weather]` work\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.0-alpha.2+5.gb40df75\n",
    "input": "         \"\"\"\n         poa = _irrad_for_celltemp(self.results.total_irrad,\n                                   self.results.effective_irradiance)\n        if not isinstance(data, tuple) and self.system.num_arrays > 1:\n             # broadcast data to all arrays\n             data = (data,) * self.system.num_arrays\n        elif not isinstance(data, tuple):\n            return self._prepare_temperature_single_array(data, poa)\n         given_cell_temperature = tuple(itertools.starmap(\n             self._get_cell_temperature,\n             zip(data, poa, self.system.temperature_model_parameters)\n             self.results.cell_temperature = given_cell_temperature\n             return self\n         # Calculate cell temperature from weather data. If cell_temperature\n        # has not been provided for some arrays then it is computed with\n        # ModelChain.temperature_model(). Because this operates on all Arrays\n        # simultaneously, 'poa_global' must be known for all arrays, including\n        # those that have a known cell temperature.\n        try:\n            self._verify_df(self.results.total_irrad, ['poa_global'])\n        except ValueError:\n            # Provide a more informative error message. Because only\n            # run_model_from_effective_irradiance() can get to this point\n            # without known POA we can suggest a very specific remedy in the\n            # error message.\n            raise ValueError(\"Incomplete input data. Data must contain \"\n                             \"'poa_global'. For systems with multiple Arrays \"\n                             \"if you have provided 'cell_temperature' for \"\n                             \"only a subset of Arrays you must provide \"\n                             \"'poa_global' for all Arrays, including those \"\n                             \"that have a known 'cell_temperature'.\")\n         self.temperature_model()\n         # replace calculated cell temperature with temperature given in `data`\n         # where available.\n         \"\"\"\n         data = _to_tuple(data)\n         self._check_multiple_input(data)\n         self._assign_weather(data)\n         self._assign_total_irrad(data)\n         self.results.effective_irradiance = _tuple_from_dfs(",
    "output": "         \"\"\"\n         poa = _irrad_for_celltemp(self.results.total_irrad,\n                                   self.results.effective_irradiance)\n        # handle simple case first, single array, data not iterable\n        if not isinstance(data, tuple) and self.system.num_arrays == 1:\n            return self._prepare_temperature_single_array(data, poa)\n        if not isinstance(data, tuple):\n             # broadcast data to all arrays\n             data = (data,) * self.system.num_arrays\n        # find where cell or module temperature is specified in input data\n         given_cell_temperature = tuple(itertools.starmap(\n             self._get_cell_temperature,\n             zip(data, poa, self.system.temperature_model_parameters)\n             self.results.cell_temperature = given_cell_temperature\n             return self\n         # Calculate cell temperature from weather data. If cell_temperature\n        # has not been provided for some arrays then it is computed.\n         self.temperature_model()\n         # replace calculated cell temperature with temperature given in `data`\n         # where available.\n         \"\"\"\n         data = _to_tuple(data)\n         self._check_multiple_input(data)\n        self._verify_df(data, required=['effective_irradiance'])\n         self._assign_weather(data)\n         self._assign_total_irrad(data)\n         self.results.effective_irradiance = _tuple_from_dfs("
  },
  {
    "instruction": "make read_crn accomodate bad files\nA couple of issues with our `read_crn` function. \r\n\r\nFirst, the character sequence '\\x00\\x00\\x00\\x00\\x00\\x00' occasionally shows up and trips up pandas. This can be fixed by adding `na_values=['\\x00\\x00\\x00\\x00\\x00\\x00']` to the reader.\r\n\r\nSecond, we try to set the `CRX_VN` column to dtype int, but it occasionally has floats that cannot be coerced. The [documentation](https://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01/README.txt) says it should be treated like a string.\r\n\r\nExample below shows both issues in `'CRNS0101-05-2020-FL_Titusville_7_E.txt'`\r\n\r\n```\r\n92821 20200706 1145 20200706 0645      3  -80.69   28.62    24.5     0.0    151 0    24.7 C 0    94 0 -99.000 -9999.0   990 0   1.23 0\r\n92821 20200706 1150 20200706 0650      3  -80.69   28.62    24.7     0.0    168 0    25.0 C 0    94 0 -99.000 -9999.0   990 0   1.28 0\r\n92821 20200706 1155 20200706 0655      3  -80.69   28.62    24.9     0.0    173 0    25.3 C 0    93 0 -99.000 -9999.0   990 0   1.48 0\r\n92821 20200706 1200 20200706 0700      3  -80.69   28.62    24.9     0.0    190 0    25.5 C 0    93 0 -99.000 -9999.0   990 0   1.57 0\r\n\\x00\\x00\\x00\\x00\\x00\\x00 repeated\r\n92821 20200706 1305 20200706 0805  2.623  -80.69   28.62    26.8     0.0    409 0    30.0 C 0    87 0 -99.000 -9999.0   988 0   1.44 0\r\n92821 20200706 1310 20200706 0810  2.623  -80.69   28.62    26.9     0.0    430 0    30.2 C 0    87 0 -99.000 -9999.0   989 0   1.64 0\r\n92821 20200706 1315 20200706 0815  2.623  -80.69   28.62    27.0     0.0    445 0    30.4 C 0    86 0 -99.000 -9999.0   989 0   1.94 0\r\n92821 20200706 1320 20200706 0820  2.623  -80.69   28.62    27.3     0.0    463 0    30.8 C 0    86 0 -99.000 -9999.0   988 0   1.50 0\r\n92821 20200706 1325 20200706 0825  2.623  -80.69   28.62    27.6     0.0    478 0    31.1 C 0    85 0 -99.000 -9999.0   988 0   1.54 0\r\n92821 20200706 1330 20200706 0830  2.623  -80.69   28.62    27.6     0.0    496 0    31.5 C 0    84 0 -99.000 -9999.0   988 0   1.48 0\r\n```\r\n\r\nfyi @lboeman \n",
    "input": " \n # specify dtypes for potentially problematic values\n DTYPES = [\n    'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'float64', 'float64',\n     'float64', 'float64', 'float64', 'int64', 'float64', 'O', 'int64',\n     'float64', 'int64', 'float64', 'float64', 'int64', 'int64', 'float64',\n     'int64'\n     e.g. `SOLAR_RADIATION` becomes `ghi`. See the\n     `pvlib.iotools.crn.VARIABLE_MAP` dict for the complete mapping.\n \n     References\n     ----------\n     .. [1] U.S. Climate Reference Network\n        Amer. Meteor. Soc., 94, 489-498. :doi:`10.1175/BAMS-D-12-00170.1`\n     \"\"\"\n \n    # read in data\n     data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n                       widths=WIDTHS)\n     # loop here because dtype kwarg not supported in read_fwf until 0.20\n     for (col, _dtype) in zip(data.columns, DTYPES):\n         data[col] = data[col].astype(_dtype)\n     except TypeError:\n         pass\n \n    # set nans\n     for val in [-99, -999, -9999]:\n         data = data.where(data != val, np.nan)\n \n     data = data.rename(columns=VARIABLE_MAP)",
    "output": " \n # specify dtypes for potentially problematic values\n DTYPES = [\n    'int64', 'int64', 'int64', 'int64', 'int64', 'str', 'float64', 'float64',\n     'float64', 'float64', 'float64', 'int64', 'float64', 'O', 'int64',\n     'float64', 'int64', 'float64', 'float64', 'int64', 'int64', 'float64',\n     'int64'\n     e.g. `SOLAR_RADIATION` becomes `ghi`. See the\n     `pvlib.iotools.crn.VARIABLE_MAP` dict for the complete mapping.\n \n    CRN files occasionally have a set of null characters on a line\n    instead of valid data. This function drops those lines. Sometimes\n    these null characters appear on a line of their own and sometimes\n    they occur on the same line as valid data. In the latter case, the\n    valid data will not be returned. Users may manually remove the null\n    characters and reparse the file if they need that line.\n\n     References\n     ----------\n     .. [1] U.S. Climate Reference Network\n        Amer. Meteor. Soc., 94, 489-498. :doi:`10.1175/BAMS-D-12-00170.1`\n     \"\"\"\n \n    # read in data. set fields with NUL characters to NaN\n     data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n                       widths=WIDTHS, na_values=['\\x00\\x00\\x00\\x00\\x00\\x00'])\n    # at this point we only have NaNs from NUL characters, not -999 etc.\n    # these bad rows need to be removed so that dtypes can be set.\n    # NaNs require float dtype so we run into errors if we don't do this.\n    data = data.dropna(axis=0)\n     # loop here because dtype kwarg not supported in read_fwf until 0.20\n     for (col, _dtype) in zip(data.columns, DTYPES):\n         data[col] = data[col].astype(_dtype)\n     except TypeError:\n         pass\n \n    # Now we can set nans. This could be done a per column basis to be\n    # safer, since in principle a real -99 value could occur in a -9999\n    # column. Very unlikely to see that in the real world.\n     for val in [-99, -999, -9999]:\n        # consider replacing with .replace([-99, -999, -9999])\n         data = data.where(data != val, np.nan)\n \n     data = data.rename(columns=VARIABLE_MAP)"
  },
  {
    "instruction": "IAM that supports AR coating like Fresnel\n# Problem\r\nCurrently pvlib supports the DeSoto physical model (similar to normal glass), ASHRAE, Martin & Ruiz, and SAPM polynomial, but it doesn't have a pure Fresnel model that allows additional interfaces like an AR coating. \r\n\r\n* DeSoto physical model is most similar to the Fresnel for normal glass but only has one interface, so is limited to IAM curves below it only, while an AR coating would have a greater &rho; \r\n* Martin & Ruiz could be used to approximate an AR coated glass if the correct `a_r` were known. The default of `a_r=0.16` is slightly above the normal glass Fresnel IAM, but an `a_r=0.14` seems to match an AR coating with index of refraction of 1.2 most closely.\r\n\r\n![pvlib_iam](https://user-images.githubusercontent.com/1385621/180581071-0ff411f1-144a-40b6-a6a9-189ef55f019f.png)\r\n\r\n\r\n# Proposal\r\na new method in `pvl.iam.fresnel_ar(aoi, n_ar=1.2, n_air=1.0, n_glass=1.56)` that implements the [Fresnel equation](https://en.wikipedia.org/wiki/Fresnel_equations)\r\n\r\n# Alternative\r\nSuggest readers to use Martin & Ruiz with `a_r=0.14` instead of default.\r\n\r\n# additional content\r\nPVsyst has switched to Fresnel equations. We can duplicate [their methods](https://www.pvsyst.com/help/iam_loss.htm) ignoring additional reflections and the encapsulant layer:\r\n![Fresnel-v-ASHRAE](https://user-images.githubusercontent.com/1385621/180581112-67f3ed9d-5bd3-4dfe-8180-8b5d173fcdd2.png)\r\n\r\n<details>\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nplt.ion()\r\n\r\n\r\n# constants\r\nn_glass = 1.56\r\nn_air = 1.0\r\ntheta_inc = np.linspace(0, 88, 100)\r\n\r\n\r\ndef snell(theta_1, n1, n2):\r\n    \"\"\"Snell's equation\"\"\"\r\n    sintheta_2 = n1/n2 * np.sin(np.radians(theta_1))\r\n    return sintheta_2, np.degrees(np.arcsin(sintheta_2))\r\n\r\n\r\ndef refl_s(theta_1, theta_2, n1, n2):\r\n    \"\"\"Fresnel's equation\"\"\"\r\n    n1_costheta_1 = n1*np.cos(np.radians(theta_1))\r\n    n2_costheta_2 = n2*np.cos(np.radians(theta_2))\r\n    return np.abs((n1_costheta_1 - n2_costheta_2)/(n1_costheta_1 + n2_costheta_2))**2\r\n\r\n\r\ndef refl_p(theta_1, theta_2, n1, n2):\r\n    \"\"\"Fresnel's equation\"\"\"\r\n    n1_costheta_2 = n1*np.cos(np.radians(theta_2))\r\n    n2_costheta_1 = n2*np.cos(np.radians(theta_1))\r\n    return np.abs((n1_costheta_2 - n2_costheta_1)/(n1_costheta_2 + n2_costheta_1))**2\r\n\r\n\r\ndef refl_eff(rs, rp):\r\n    \"\"\"effective reflectivity\"\"\"\r\n    return (rs+rp)/2\r\n\r\n\r\ndef trans(refl):\r\n    \"\"\"transmissivity\"\"\"\r\n    return 1-refl\r\n\r\n\r\ndef refl0(n1, n2):\r\n    \"\"\"reflectivity at normal incidence\"\"\"\r\n    return np.abs((n1-n2)/(n1+n2))**2\r\n\r\n\r\ndef fresnel(theta_inc, n1=n_air, n2=n_glass):\r\n    \"\"\"calculate IAM using Fresnel's Law\"\"\"\r\n    _, theta_tr = snell(theta_inc, n1, n2)\r\n    rs = refl_s(theta_inc, theta_tr, n1, n2)\r\n    rp = refl_p(theta_inc, theta_tr, n1, n2)\r\n    reff = refl_eff(rs, rp)\r\n    r0 = refl0(n1, n2)\r\n    return trans(reff)/trans(r0)\r\n\r\n\r\ndef ashrae(theta_inc, b0=0.05):\r\n    \"\"\"ASHRAE equation\"\"\"\r\n    return 1 - b0*(1/np.cos(np.radians(theta_inc)) - 1)\r\n\r\n\r\ndef fresnel_ar(theta_inc, n_ar, n1=n_air, n2=n_glass):\r\n    \"\"\"calculate IAM using Fresnel's law with AR\"\"\"\r\n    # use fresnel() for n2=n_ar\r\n    _, theta_ar = snell(theta_inc, n1, n_ar)\r\n    rs_ar1 = refl_s(theta_inc, theta_ar, n1, n_ar)\r\n    rp_ar1 = refl_p(theta_inc, theta_ar, n1, n_ar)\r\n    r0_ar1 = refl0(n1, n_ar)\r\n    # repeat with fresnel() with n1=n_ar\r\n    _, theta_tr = snell(theta_ar, n_ar, n2)\r\n    rs = refl_s(theta_ar, theta_tr, n_ar, n2)\r\n    rp = refl_p(theta_ar, theta_tr, n_ar, n2)\r\n    # note that combined reflectivity is product of transmissivity!\r\n    # so... rho12 = 1 - (1-rho1)(1-rho2) \r\n    reff = refl_eff(1-(1-rs_ar1)*(1-rs), 1-(1-rp_ar1)*(1-rp))\r\n    r0 = 1-(1-refl0(n_ar, n2))*(1-r0_ar1)\r\n    return trans(reff)/trans(r0)\r\n\r\n\r\n# plot Fresnel for normal glass and ASHRAE\r\nplt.plot(theta_inc, fresnel(theta_inc))\r\nplt.plot(theta_inc, ashrae(theta_inc))\r\n\r\n# calculate IAM for AR with n=1.1 and plot\r\niam_ar11 = fresnel_ar(theta_inc, n_ar=1.1)\r\nplt.plot(theta_inc, iam_ar11)\r\n\r\n# repeat for AR with n=1.2\r\niam_ar12 = fresnel_ar(theta_inc, n_ar=1.2)\r\nplt.plot(theta_inc, iam_ar12)\r\n\r\n# make plot pretty\r\nplt.legend(['Fresnel, normal glass', 'ASHRAE, $b_0=0.05$', 'Fresnel $n_{AR}=1.1$', 'Fresnel $n_{AR}=1.2$'])\r\nplt.title(\"IAM correction, Fresnel vs. ASHRAE, using basic eqn's\")\r\nplt.ylabel('IAM')\r\nplt.xlabel(r'incidence angle $\\theta_{inc} [\\degree]$')\r\nplt.grid()\r\nplt.ylim([0.55,1.05])\r\n```\r\n</details>\r\n\r\n\n",
    "input": " import numpy as np\n import pandas as pd\n import functools\nfrom pvlib.tools import cosd, sind, tand, asind\n \n # a dict of required parameter names for each IAM model\n # keys are the function names for the IAM models\n     return iam\n \n \ndef physical(aoi, n=1.526, K=4., L=0.002):\n     r\"\"\"\n     Determine the incidence angle modifier using refractive index ``n``,\n    extinction coefficient ``K``, and glazing thickness ``L``.\n \n     ``iam.physical`` calculates the incidence angle modifier as described in\n    [1]_, Section 3. The calculation is based on a physical model of absorbtion\n     and transmission through a transparent cover.\n \n     Parameters\n     ----------\n     aoi : numeric\n         The angle of incidence between the module normal vector and the\n        sun-beam vector in degrees. Angles of 0 are replaced with 1e-06\n        to ensure non-nan results. Angles of nan will result in nan.\n \n     n : numeric, default 1.526\n         The effective index of refraction (unitless). Reference [1]_\n         indicates that 0.002 meters (2 mm) is reasonable for most\n         glass-covered PV panels.\n \n     Returns\n     -------\n     iam : numeric\n     pvlib.iam.interp\n     pvlib.iam.sapm\n     \"\"\"\n    zeroang = 1e-06\n\n    # hold a new reference to the input aoi object since we're going to\n    # overwrite the aoi reference below, but we'll need it for the\n    # series check at the end of the function\n    aoi_input = aoi\n\n    aoi = np.where(aoi == 0, zeroang, aoi)\n\n    # angle of reflection\n    thetar_deg = asind(1.0 / n * (sind(aoi)))\n\n    # reflectance and transmittance for normal incidence light\n    rho_zero = ((1-n) / (1+n)) ** 2\n    tau_zero = np.exp(-K*L)\n\n    # reflectance for parallel and perpendicular polarized light\n    rho_para = (tand(thetar_deg - aoi) / tand(thetar_deg + aoi)) ** 2\n    rho_perp = (sind(thetar_deg - aoi) / sind(thetar_deg + aoi)) ** 2\n\n    # transmittance for non-normal light\n    tau = np.exp(-K * L / cosd(thetar_deg))\n\n    # iam is ratio of non-normal to normal incidence transmitted light\n    # after deducting the reflected portion of each\n    iam = ((1 - (rho_para + rho_perp) / 2) / (1 - rho_zero) * tau / tau_zero)\n\n    with np.errstate(invalid='ignore'):\n        # angles near zero produce nan, but iam is defined as one\n        small_angle = 1e-06\n        iam = np.where(np.abs(aoi) < small_angle, 1.0, iam)\n\n        # angles at 90 degrees can produce tiny negative values,\n        # which should be zero. this is a result of calculation precision\n        # rather than the physical model\n        iam = np.where(iam < 0, 0, iam)\n\n        # for light coming from behind the plane, none can enter the module\n        iam = np.where(aoi > 90, 0, iam)\n\n    if isinstance(aoi_input, pd.Series):\n        iam = pd.Series(iam, index=aoi_input.index)\n \n     return iam\n ",
    "output": " import numpy as np\n import pandas as pd\n import functools\nfrom pvlib.tools import cosd, sind\n \n # a dict of required parameter names for each IAM model\n # keys are the function names for the IAM models\n     return iam\n \n \ndef physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):\n     r\"\"\"\n     Determine the incidence angle modifier using refractive index ``n``,\n    extinction coefficient ``K``, glazing thickness ``L`` and refractive\n    index ``n_ar`` of an optional anti-reflective coating.\n \n     ``iam.physical`` calculates the incidence angle modifier as described in\n    [1]_, Section 3, with additional support of an anti-reflective coating.\n    The calculation is based on a physical model of reflections, absorption,\n     and transmission through a transparent cover.\n \n     Parameters\n     ----------\n     aoi : numeric\n         The angle of incidence between the module normal vector and the\n        sun-beam vector in degrees. Angles of nan will result in nan.\n \n     n : numeric, default 1.526\n         The effective index of refraction (unitless). Reference [1]_\n         indicates that 0.002 meters (2 mm) is reasonable for most\n         glass-covered PV panels.\n \n    n_ar : numeric, optional\n        The effective index of refraction of the anti-reflective (AR) coating\n        (unitless). If n_ar is None (default), no AR coating is applied.\n        A typical value for the effective index of an AR coating is 1.29.\n\n     Returns\n     -------\n     iam : numeric\n     pvlib.iam.interp\n     pvlib.iam.sapm\n     \"\"\"\n    n1, n3 = 1, n\n    if n_ar is None or np.allclose(n_ar, n1):\n        # no AR coating\n        n2 = n\n    else:\n        n2 = n_ar\n\n    # incidence angle\n    costheta = np.maximum(0, cosd(aoi))  # always >= 0\n    sintheta = np.sqrt(1 - costheta**2)  # always >= 0\n    n1costheta1 = n1 * costheta\n    n2costheta1 = n2 * costheta\n\n    # refraction angle of first interface\n    sintheta = n1 / n2 * sintheta\n    costheta = np.sqrt(1 - sintheta**2)\n    n1costheta2 = n1 * costheta\n    n2costheta2 = n2 * costheta\n\n    # reflectance of s-, p-polarized, and normal light by the first interface\n    rho12_s = ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2\n    rho12_p = ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2\n    rho12_0 = ((n1 - n2) / (n1 + n2)) ** 2\n\n    # transmittance through the first interface\n    tau_s = 1 - rho12_s\n    tau_p = 1 - rho12_p\n    tau_0 = 1 - rho12_0\n\n    if not np.allclose(n3, n2):  # AR coated glass\n        n3costheta2 = n3 * costheta\n        # refraction angle of second interface\n        sintheta = n2 / n3 * sintheta\n        costheta = np.sqrt(1 - sintheta**2)\n        n2costheta3 = n2 * costheta\n        n3costheta3 = n3 * costheta\n\n        # reflectance by the second interface\n        rho23_s = (\n            (n2costheta2 - n3costheta3) / (n2costheta2 + n3costheta3)\n        ) ** 2\n        rho23_p = (\n            (n2costheta3 - n3costheta2) / (n2costheta3 + n3costheta2)\n        ) ** 2\n        rho23_0 = ((n2 - n3) / (n2 + n3)) ** 2\n\n        # transmittance through the coating, including internal reflections\n        # 1 + rho23*rho12 + (rho23*rho12)^2 + ... = 1/(1 - rho23*rho12)\n        tau_s *= (1 - rho23_s) / (1 - rho23_s * rho12_s)\n        tau_p *= (1 - rho23_p) / (1 - rho23_p * rho12_p)\n        tau_0 *= (1 - rho23_0) / (1 - rho23_0 * rho12_0)\n\n    # transmittance after absorption in the glass\n    tau_s *= np.exp(-K * L / costheta)\n    tau_p *= np.exp(-K * L / costheta)\n    tau_0 *= np.exp(-K * L)\n\n    # incidence angle modifier\n    iam = (tau_s + tau_p) / 2 / tau_0\n \n     return iam\n "
  },
  {
    "instruction": "Update CAMS/SoDa URL\nSoDa has developed a new load-balancing solution, such that requests are automatically redirected to the fastest server. This means that it might be advisable for us to update the URL in the [``pvlib.iotools.get_cams``](https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.iotools.get_cams.html?highlight=get_cams#pvlib.iotools.get_cams) function. \r\n\r\nEmail from SoDa (March 7th, 2023):\r\n> Our beta load balancing system for SoDa/CAMS API requests is extended to March 13th. All requests made on the beta-api.soda-solardata.com WILL NOT BE COUNTED in your subscription. The beta access will last until then. **From March 14th, the service will be fully operational and you have to use api.soda-solardata.com to process your API (machine to machine) requests.**\r\n\r\nand email from February 22nd, 2023:\r\n> This new functionality will automatically redirect any request to the fastest available SoDa server. As a result, future updates/maintenances won't need any action from your part as server switches will be completely autonomous.\r\n\r\nI will be following up on this issue in a couple of weeks.\r\n\r\n*Edit: email from March 20th, 2023*\r\n> We strongly advise you to switch your automatic commands on the load balancing system (api.soda-solardata.com). In that way, all future updates won't need any actions from your side. \n",
    "input": " import warnings\n \n \n CAMS_INTEGRATED_COLUMNS = [\n     'TOA', 'Clear sky GHI', 'Clear sky BHI', 'Clear sky DHI', 'Clear sky BNI',\n     'GHI', 'BHI', 'DHI', 'BNI',\n def get_cams(latitude, longitude, start, end, email, identifier='mcclear',\n              altitude=None, time_step='1h', time_ref='UT', verbose=False,\n              integrated=False, label=None, map_variables=True,\n             server='www.soda-is.com', timeout=30):\n     \"\"\"\n     Retrieve time-series of radiation and/or clear-sky global, beam, and\n     diffuse radiation from CAMS (see [1]_). Data is retrieved from SoDa [2]_.\n     map_variables: bool, default: True\n         When true, renames columns of the DataFrame to pvlib variable names\n         where applicable. See variable :const:`VARIABLE_MAP`.\n    server: str, default: 'www.soda-is.com'\n        Main server (www.soda-is.com) or backup mirror server (pro.soda-is.com)\n     timeout : int, default: 30\n         Time in seconds to wait for server response before timeout\n \n         all time steps except for '1M' which has a default of 'right'.\n     map_variables: bool, default: True\n         When true, renames columns of the Dataframe to pvlib variable names\n        where applicable. See variable VARIABLE_MAP.\n \n     Returns\n     -------\n     data: pandas.DataFrame\n        Timeseries data from CAMS Radiation or McClear\n        :func:`pvlib.iotools.get_cams` for fields\n     metadata: dict\n         Metadata available in the file.\n ",
    "output": " import warnings\n \n \nURL = 'api.soda-solardata.com'\n\n CAMS_INTEGRATED_COLUMNS = [\n     'TOA', 'Clear sky GHI', 'Clear sky BHI', 'Clear sky DHI', 'Clear sky BNI',\n     'GHI', 'BHI', 'DHI', 'BNI',\n def get_cams(latitude, longitude, start, end, email, identifier='mcclear',\n              altitude=None, time_step='1h', time_ref='UT', verbose=False,\n              integrated=False, label=None, map_variables=True,\n             server=URL, timeout=30):\n     \"\"\"\n     Retrieve time-series of radiation and/or clear-sky global, beam, and\n     diffuse radiation from CAMS (see [1]_). Data is retrieved from SoDa [2]_.\n     map_variables: bool, default: True\n         When true, renames columns of the DataFrame to pvlib variable names\n         where applicable. See variable :const:`VARIABLE_MAP`.\n    server: str, default: :const:`pvlib.iotools.sodapro.URL`\n        Base url of the SoDa Pro CAMS Radiation API.\n     timeout : int, default: 30\n         Time in seconds to wait for server response before timeout\n \n         all time steps except for '1M' which has a default of 'right'.\n     map_variables: bool, default: True\n         When true, renames columns of the Dataframe to pvlib variable names\n        where applicable. See variable :const:`VARIABLE_MAP`.\n \n     Returns\n     -------\n     data: pandas.DataFrame\n        Timeseries data from CAMS Radiation or McClear.\n        See :func:`pvlib.iotools.get_cams` for fields.\n     metadata: dict\n         Metadata available in the file.\n "
  },
  {
    "instruction": "Improve docstring or behavior for irradiance.get_total_irradiance and irradiance.get_sky_diffuse\n`pvlib.irradiance.get_total_irradiance` accepts kwargs `dni_extra` and `airmass`, both default to `None`. However, values for these kwargs are required for several of the irradiance transposition models. \r\n\r\nSee discussion [here](https://groups.google.com/d/msg/pvlib-python/ZPMdpQOD6F4/cs1t23w8AwAJ)\r\n\r\nDocstring should specify when `dni_extra` and `airmass` are required, and which airmass is appropriate for each model.\r\n\r\nCould also test for kwarg values if e.g. `model=='perez'`\n",
    "input": " from pvlib import atmosphere, solarposition, tools\n \n \n# see References section of grounddiffuse function\n SURFACE_ALBEDOS = {'urban': 0.18,\n                    'grass': 0.20,\n                    'fresh grass': 0.26,\n     Parameters\n     ----------\n     surface_tilt : numeric\n        Panel tilt from horizontal.\n     surface_azimuth : numeric\n        Panel azimuth from north.\n     solar_zenith : numeric\n        Solar zenith angle.\n     solar_azimuth : numeric\n        Solar azimuth angle.\n     dni : numeric\n        Direct Normal Irradiance\n     ghi : numeric\n        Global horizontal irradiance\n     dhi : numeric\n        Diffuse horizontal irradiance\n     dni_extra : None or numeric, default None\n        Extraterrestrial direct normal irradiance\n     airmass : None or numeric, default None\n        Airmass\n     albedo : numeric, default 0.25\n        Surface albedo\n    surface_type : None or String, default None\n        Surface type. See grounddiffuse.\n    model : String, default 'isotropic'\n        Irradiance model.\n    model_perez : String, default 'allsitescomposite1990'\n        Used only if model='perez'. See :py:func:`perez`.\n \n     Returns\n     -------\n     total_irrad : OrderedDict or DataFrame\n         Contains keys/columns ``'poa_global', 'poa_direct', 'poa_diffuse',\n         'poa_sky_diffuse', 'poa_ground_diffuse'``.\n     \"\"\"\n     poa_sky_diffuse = get_sky_diffuse(\n         surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,\n         dni, ghi, dhi, dni_extra=dni_extra, airmass=airmass, model=model,\n     Parameters\n     ----------\n     surface_tilt : numeric\n        Panel tilt from horizontal.\n     surface_azimuth : numeric\n        Panel azimuth from north.\n     solar_zenith : numeric\n        Solar zenith angle.\n     solar_azimuth : numeric\n        Solar azimuth angle.\n     dni : numeric\n        Direct Normal Irradiance\n     ghi : numeric\n        Global horizontal irradiance\n     dhi : numeric\n        Diffuse horizontal irradiance\n     dni_extra : None or numeric, default None\n        Extraterrestrial direct normal irradiance\n     airmass : None or numeric, default None\n        Airmass\n    model : String, default 'isotropic'\n        Irradiance model.\n    model_perez : String, default 'allsitescomposite1990'\n        See perez.\n \n     Returns\n     -------\n     poa_sky_diffuse : numeric\n     \"\"\"\n \n     model = model.lower()\n     if model == 'isotropic':\n         sky = isotropic(surface_tilt, dhi)\n     elif model == 'klucher':\n     elif model == 'king':\n         sky = king(surface_tilt, dhi, ghi, solar_zenith)\n     elif model == 'perez':\n         sky = perez(surface_tilt, surface_azimuth, dhi, dni, dni_extra,\n                     solar_zenith, solar_azimuth, airmass,\n                     model=model_perez)\n def get_ground_diffuse(surface_tilt, ghi, albedo=.25, surface_type=None):\n     '''\n     Estimate diffuse irradiance from ground reflections given\n    irradiance, albedo, and surface tilt\n \n     Function to determine the portion of irradiance on a tilted surface\n     due to ground reflections. Any of the inputs may be DataFrames or\n         (e.g. surface facing up = 0, surface facing horizon = 90).\n \n     ghi : numeric\n        Global horizontal irradiance in W/m^2.\n \n     albedo : numeric, default 0.25\n         Ground reflectance, typically 0.1-0.4 for surfaces on Earth\n     Returns\n     -------\n     grounddiffuse : numeric\n        Ground reflected irradiances in W/m^2.\n \n \n     References",
    "output": " from pvlib import atmosphere, solarposition, tools\n \n \n# see References section of get_ground_diffuse function\n SURFACE_ALBEDOS = {'urban': 0.18,\n                    'grass': 0.20,\n                    'fresh grass': 0.26,\n     Parameters\n     ----------\n     surface_tilt : numeric\n        Panel tilt from horizontal. [degree]\n     surface_azimuth : numeric\n        Panel azimuth from north. [degree]\n     solar_zenith : numeric\n        Solar zenith angle. [degree]\n     solar_azimuth : numeric\n        Solar azimuth angle. [degree]\n     dni : numeric\n        Direct Normal Irradiance. [W/m2]\n     ghi : numeric\n        Global horizontal irradiance. [W/m2]\n     dhi : numeric\n        Diffuse horizontal irradiance. [W/m2]\n     dni_extra : None or numeric, default None\n        Extraterrestrial direct normal irradiance. [W/m2]\n     airmass : None or numeric, default None\n        Relative airmass (not adjusted for pressure). [unitless]\n     albedo : numeric, default 0.25\n        Surface albedo. [unitless]\n    surface_type : None or str, default None\n        Surface type. See :py:func:`~pvlib.irradiance.get_ground_diffuse` for\n        the list of accepted values.\n    model : str, default 'isotropic'\n        Irradiance model. Can be one of ``'isotropic'``, ``'klucher'``,\n        ``'haydavies'``, ``'reindl'``, ``'king'``, ``'perez'``.\n    model_perez : str, default 'allsitescomposite1990'\n        Used only if ``model='perez'``. See :py:func:`~pvlib.irradiance.perez`.\n \n     Returns\n     -------\n     total_irrad : OrderedDict or DataFrame\n         Contains keys/columns ``'poa_global', 'poa_direct', 'poa_diffuse',\n         'poa_sky_diffuse', 'poa_ground_diffuse'``.\n\n    Notes\n    -----\n    Models ``'haydavies'``, ``'reindl'``, or ``'perez'`` require\n    ``'dni_extra'``. Values can be calculated using\n    :py:func:`~pvlib.irradiance.get_extra_radiation`.\n\n    The ``'perez'`` model requires relative airmass (``airmass``) as input. If\n    ``airmass`` is not provided, it is calculated using the defaults in\n    :py:func:`~pvlib.atmosphere.get_relative_airmass`.\n     \"\"\"\n\n     poa_sky_diffuse = get_sky_diffuse(\n         surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,\n         dni, ghi, dhi, dni_extra=dni_extra, airmass=airmass, model=model,\n     Parameters\n     ----------\n     surface_tilt : numeric\n        Panel tilt from horizontal. [degree]\n     surface_azimuth : numeric\n        Panel azimuth from north. [degree]\n     solar_zenith : numeric\n        Solar zenith angle. [degree]\n     solar_azimuth : numeric\n        Solar azimuth angle. [degree]\n     dni : numeric\n        Direct Normal Irradiance. [W/m2]\n     ghi : numeric\n        Global horizontal irradiance. [W/m2]\n     dhi : numeric\n        Diffuse horizontal irradiance. [W/m2]\n     dni_extra : None or numeric, default None\n        Extraterrestrial direct normal irradiance. [W/m2]\n     airmass : None or numeric, default None\n        Relative airmass (not adjusted for pressure). [unitless]\n    model : str, default 'isotropic'\n        Irradiance model. Can be one of ``'isotropic'``, ``'klucher'``,\n        ``'haydavies'``, ``'reindl'``, ``'king'``, ``'perez'``.\n    model_perez : str, default 'allsitescomposite1990'\n        Used only if ``model='perez'``. See :py:func:`~pvlib.irradiance.perez`.\n \n     Returns\n     -------\n     poa_sky_diffuse : numeric\n        Sky diffuse irradiance in the plane of array. [W/m2]\n\n    Raises\n    ------\n    ValueError\n        If model is one of ``'haydavies'``, ``'reindl'``, or ``'perez'`` and\n        ``dni_extra`` is ``None``.\n\n    Notes\n    -----\n    Models ``'haydavies'``, ``'reindl'``, and ``'perez``` require 'dni_extra'.\n    Values can be calculated using\n    :py:func:`~pvlib.irradiance.get_extra_radiation`.\n\n    The ``'perez'`` model requires relative airmass (``airmass``) as input. If\n    ``airmass`` is not provided, it is calculated using the defaults in\n    :py:func:`~pvlib.atmosphere.get_relative_airmass`.\n     \"\"\"\n \n     model = model.lower()\n\n    if (model in {'haydavies', 'reindl', 'perez'}) and (dni_extra is None):\n        raise ValueError(f'dni_extra is required for model {model}')\n\n     if model == 'isotropic':\n         sky = isotropic(surface_tilt, dhi)\n     elif model == 'klucher':\n     elif model == 'king':\n         sky = king(surface_tilt, dhi, ghi, solar_zenith)\n     elif model == 'perez':\n        if airmass is None:\n            airmass = atmosphere.get_relative_airmass(solar_zenith)\n         sky = perez(surface_tilt, surface_azimuth, dhi, dni, dni_extra,\n                     solar_zenith, solar_azimuth, airmass,\n                     model=model_perez)\n def get_ground_diffuse(surface_tilt, ghi, albedo=.25, surface_type=None):\n     '''\n     Estimate diffuse irradiance from ground reflections given\n    irradiance, albedo, and surface tilt.\n \n     Function to determine the portion of irradiance on a tilted surface\n     due to ground reflections. Any of the inputs may be DataFrames or\n         (e.g. surface facing up = 0, surface facing horizon = 90).\n \n     ghi : numeric\n        Global horizontal irradiance. [W/m^2]\n \n     albedo : numeric, default 0.25\n         Ground reflectance, typically 0.1-0.4 for surfaces on Earth\n     Returns\n     -------\n     grounddiffuse : numeric\n        Ground reflected irradiance. [W/m^2]\n \n \n     References"
  },
  {
    "instruction": "ModelChain.prepare_inputs can succeed with missing dhi\nFrom the docstring for `ModelChain.prepare_inputs()` I believe the method should fail if `weather` does not have a `dhi` column.\r\n\r\nThe validation checks for `'ghi'` twice, but not `'dhi`'\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/11c356f9a89fc88b4d3ff368ce1aae170a97ebd7/pvlib/modelchain.py#L1136\n",
    "input": "         ModelChain.complete_irradiance\n         \"\"\"\n \n        self._verify_df(weather, required=['ghi', 'dni', 'ghi'])\n         self._assign_weather(weather)\n \n         self.times = self.weather.index",
    "output": "         ModelChain.complete_irradiance\n         \"\"\"\n \n        self._verify_df(weather, required=['ghi', 'dni', 'dhi'])\n         self._assign_weather(weather)\n \n         self.times = self.weather.index"
  },
  {
    "instruction": "TypeError: running ModelChain with Arrays and module_temperature\n**Describe the bug**\r\nAnother bug using Arrays. This time a TypeError is raised in `pvlib.modelchain._get_cell_temperature` because `self.system.temperature_model_parameters` is zipped with dataframe tuples but is never a tuple itself\r\nhttps://github.com/pvlib/pvlib-python/blob/dc617d0c182bc8eec57898a039cb5115b425645f/pvlib/modelchain.py#L1525\r\n\r\n**To Reproduce**\r\n```python\r\nimport traceback\r\nimport pandas as pd\r\nfrom pvlib.location import Location\r\nfrom pvlib.pvsystem import Array, PVSystem\r\nfrom pvlib.modelchain import ModelChain\r\n\r\n\r\narray_params = {\r\n    \"surface_tilt\": 32.0,\r\n    \"surface_azimuth\": 180.0,\r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",\r\n    \"albedo\": 0.2,\r\n    \"temperature_model_parameters\": {\r\n        \"a\": -3.47,\r\n        \"b\": -0.0594,\r\n        \"deltaT\": 3.0,\r\n    },\r\n    \"strings\": 5,\r\n    \"modules_per_string\": 7,\r\n    \"module_parameters\": {\r\n        \"alpha_sc\": 0.004539,\r\n        \"gamma_ref\": 1.2,\r\n        \"mu_gamma\": -0.003,\r\n        \"I_L_ref\": 5.11426,\r\n        \"I_o_ref\": 8.10251e-10,\r\n        \"R_sh_ref\": 381.254,\r\n        \"R_sh_0\": 400.0,\r\n        \"R_s\": 1.06602,\r\n        \"cells_in_series\": 96,\r\n        \"R_sh_exp\": 5.5,\r\n        \"EgRef\": 1.121,\r\n    },\r\n}\r\ninverter_parameters = {\r\n    \"Paco\": 250.0,\r\n    \"Pdco\": 259.589,\r\n    \"Vdco\": 40.0,\r\n    \"Pso\": 2.08961,\r\n    \"C0\": -4.1e-05,\r\n    \"C1\": -9.1e-05,\r\n    \"C2\": 0.000494,\r\n    \"C3\": -0.013171,\r\n    \"Pnt\": 0.075,\r\n}\r\n\r\n\r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)\r\n\r\narray_sys = PVSystem(\r\n    arrays=[\r\n        Array(**array_params, name=0),\r\n    ],\r\n    inverter_parameters=inverter_parameters,\r\n)\r\npoa = pd.DataFrame(\r\n    {\r\n        \"poa_global\": [1100.0, 1101.0],\r\n        \"poa_direct\": [1000.0, 1001.0],\r\n        \"poa_diffuse\": [100.0, 100.0],\r\n        \"module_temperature\": [35.0, 33.0],\r\n    },\r\n    index=pd.DatetimeIndex(\r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]\r\n    ),\r\n)\r\nstandard = poa.copy().rename(\r\n    columns={\"poa_global\": \"ghi\", \"poa_direct\": \"dni\", \"poa_diffuse\": \"dhi\"}\r\n)\r\neffective = poa.copy()[[\"module_temperature\", \"poa_global\"]].rename(\r\n    columns={\"poa_global\": \"effective_irradiance\"}\r\n)\r\nmc = ModelChain(\r\n    array_sys,\r\n    location,\r\n    aoi_model=\"no_loss\",\r\n    spectral_model=\"no_loss\",\r\n)\r\ntry:\r\n    mc.run_model([standard])\r\nexcept TypeError:\r\n    print(traceback.format_exc())\r\nelse:\r\n    raise RuntimeError(\"expected a type error\")\r\ntry:\r\n    mc.run_model_from_poa([poa])\r\nexcept TypeError:\r\n    print(traceback.format_exc())\r\nelse:\r\n    raise RuntimeError(\"expected a type error\")\r\ntry:\r\n    mc.run_model_from_effective_irradiance([effective])\r\nexcept TypeError:\r\n    print(traceback.format_exc())\r\nelse:\r\n    raise RuntimeError(\"expected a type error\")\r\n\r\n```\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  master/g684b247\r\n\n",
    "input": "         if not isinstance(data, tuple):\n             # broadcast data to all arrays\n             data = (data,) * self.system.num_arrays\n         # find where cell or module temperature is specified in input data\n         given_cell_temperature = tuple(itertools.starmap(\n            self._get_cell_temperature,\n            zip(data, poa, self.system.temperature_model_parameters)\n         ))\n         # If cell temperature has been specified for all arrays return\n         # immediately and do not try to compute it.",
    "output": "         if not isinstance(data, tuple):\n             # broadcast data to all arrays\n             data = (data,) * self.system.num_arrays\n        # data is tuple, so temperature_model_parameters must also be\n        # tuple. system.temperature_model_parameters is reduced to a dict\n        # if system.num_arrays == 1, so manually access parameters. GH 1192\n        t_mod_params = tuple(array.temperature_model_parameters\n                             for array in self.system.arrays)\n         # find where cell or module temperature is specified in input data\n         given_cell_temperature = tuple(itertools.starmap(\n            self._get_cell_temperature, zip(data, poa, t_mod_params)\n         ))\n         # If cell temperature has been specified for all arrays return\n         # immediately and do not try to compute it."
  },
  {
    "instruction": "make Array play nicely with fixed tilt systems and trackers\n#1076 is adding an `Array` class that largely describes a fixed-tilt array. However, the composition logic of `PVSystem: def __init__(arrays,...)` combined with the inheritance logic of `SingleAxisTracker(PVSystem)` makes for an odd combination of `Array` objects within `SingleAxisTrackers`. See, for example, https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539704316. \r\n\r\nIn https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539686448 I proposed roughly:\r\n\r\nSplit the `Array` into `BaseArray`, `FixedTiltArray(BaseArray)`, `SingleAxisTrackingArray(BaseArray)`? Basic idea:\r\n\r\n```python\r\nclass FixedTiltArray(BaseArray)\r\n    \"\"\"\r\n    Parameters\r\n    ----------\r\n    surface_tilt: float or array-like, default 0\r\n        Surface tilt angles in decimal degrees.\r\n        The tilt angle is defined as degrees from horizontal\r\n        (e.g. surface facing up = 0, surface facing horizon = 90)\r\n\r\n    surface_azimuth: float or array-like, default 180\r\n        Azimuth angle of the module surface.\r\n        North=0, East=90, South=180, West=270.\r\n\r\n    **kwargs\r\n        Passed to Array. Or copy remainder of Array doc string to be explicit.\r\n    \"\"\"\r\n\r\n\r\n# could be in pvsystem.py (module is gradually becoming just the objects) or could be in tracking.py\r\nclass SingleAxisTrackerArray(BaseArray)\r\n    \"\"\"\r\n    Parameters\r\n    ----------\r\n    axis_tilt : float, default 0\r\n        The tilt of the axis of rotation (i.e, the y-axis defined by\r\n        axis_azimuth) with respect to horizontal, in decimal degrees.\r\n\r\n    etc.\r\n\r\n    **kwargs\r\n        Passed to Array. Or copy remainder of Array doc string to be explicit.\r\n    \"\"\"\r\n```\r\n\r\nI believe the only major challenge is that the `get_aoi` and `get_irradiance` methods would either need to differ in signature (as they do now, and thus present a challenge to a `PVSystem` wrapper) or in implementation (tracker methods would include a call to `singleaxis`, and thus would be less efficient in some workflows). @wfvining suggests that the consistent signature is more important and I'm inclined to agree.\r\n\r\nWe'd also deprecate the old `SingleAxisTracking` class.\r\n\r\nWe should resolve this issue before releasing the new Array code into the wild in 0.9.\n",
    "input": " from urllib.request import urlopen\n import numpy as np\n import pandas as pd\n \n from pvlib._deprecation import deprecated\n \n                 array_losses_parameters = _build_kwargs(['dc_ohmic_percent'],\n                                                         losses_parameters)\n             self.arrays = (Array(\n                surface_tilt,\n                surface_azimuth,\n                 albedo,\n                 surface_type,\n                 module,\n                 temperature_model_parameters,\n                 modules_per_string,\n                 strings_per_inverter,\n                racking_model,\n                 array_losses_parameters,\n             ),)\n         elif len(arrays) == 0:\n \n     @_unwrap_single_value\n     def _infer_cell_type(self):\n\n         \"\"\"\n         Examines module_parameters and maps the Technology key for the CEC\n         database and the Material key for the Sandia database to a common\n         The Fuentes thermal model uses the module surface tilt for convection\n         modeling. The SAM implementation of PVWatts hardcodes the surface tilt\n         value at 30 degrees, ignoring whatever value is used for irradiance\n        transposition. This method defaults to using ``self.surface_tilt``, but\n        if you want to match the PVWatts behavior, you can override it by\n        including a ``surface_tilt`` value in ``temperature_model_parameters``.\n \n        The `temp_air` and `wind_speed` parameters may be passed as tuples\n         to provide different values for each Array in the system. If not\n         passed as a tuple then the same value is used for input to each Array.\n         If passed as a tuple the length must be the same as the number of\n \n     @_unwrap_single_value\n     def first_solar_spectral_loss(self, pw, airmass_absolute):\n\n         \"\"\"\n         Use the :py:func:`first_solar_spectral_correction` function to\n         calculate the spectral loss modifier. The model coefficients are\n     @_unwrap_single_value\n     @_check_deprecated_passthrough\n     def surface_tilt(self):\n        return tuple(array.surface_tilt for array in self.arrays)\n \n     @surface_tilt.setter\n     @_check_deprecated_passthrough\n     def surface_tilt(self, value):\n         for array in self.arrays:\n            array.surface_tilt = value\n \n     @property\n     @_unwrap_single_value\n     @_check_deprecated_passthrough\n     def surface_azimuth(self):\n        return tuple(array.surface_azimuth for array in self.arrays)\n \n     @surface_azimuth.setter\n     @_check_deprecated_passthrough\n     def surface_azimuth(self, value):\n         for array in self.arrays:\n            array.surface_azimuth = value\n \n     @property\n     @_unwrap_single_value\n     @_unwrap_single_value\n     @_check_deprecated_passthrough\n     def racking_model(self):\n        return tuple(array.racking_model for array in self.arrays)\n \n     @racking_model.setter\n     @_check_deprecated_passthrough\n     def racking_model(self, value):\n         for array in self.arrays:\n            array.racking_model = value\n \n     @property\n     @_unwrap_single_value\n     \"\"\"\n     An Array is a set of of modules at the same orientation.\n \n    Specifically, an array is defined by tilt, azimuth, the\n     module parameters, the number of parallel strings of modules\n     and the number of modules on each string.\n \n     Parameters\n     ----------\n    surface_tilt: float or array-like, default 0\n        Surface tilt angles in decimal degrees.\n        The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth: float or array-like, default 180\n        Azimuth angle of the module surface.\n        North=0, East=90, South=180, West=270.\n \n     albedo : None or float, default None\n         The ground albedo. If ``None``, will attempt to use\n     strings: int, default 1\n         Number of parallel strings in the array.\n \n    racking_model : None or string, default None\n        Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n        Used to identify a parameter set for the SAPM cell temperature model.\n\n     array_losses_parameters: None, dict or Series, default None.\n         Supported keys are 'dc_ohmic_percent'.\n \n     \"\"\"\n \n    def __init__(self,\n                 surface_tilt=0, surface_azimuth=180,\n                  albedo=None, surface_type=None,\n                  module=None, module_type=None,\n                  module_parameters=None,\n                  temperature_model_parameters=None,\n                  modules_per_string=1, strings=1,\n                 racking_model=None, array_losses_parameters=None,\n                  name=None):\n        self.surface_tilt = surface_tilt\n        self.surface_azimuth = surface_azimuth\n \n         self.surface_type = surface_type\n         if albedo is None:\n             self.module_parameters = module_parameters\n \n         self.module_type = module_type\n        self.racking_model = racking_model\n \n         self.strings = strings\n         self.modules_per_string = modules_per_string\n         self.name = name\n \n     def __repr__(self):\n        attrs = ['name', 'surface_tilt', 'surface_azimuth', 'module',\n                 'albedo', 'racking_model', 'module_type',\n                  'temperature_model_parameters',\n                  'strings', 'modules_per_string']\n         return 'Array:\\n  ' + '\\n  '.join(\n             f'{attr}: {getattr(self, attr)}' for attr in attrs\n         )\n     def _infer_temperature_model_params(self):\n         # try to infer temperature model parameters from from racking_model\n         # and module_type\n        param_set = f'{self.racking_model}_{self.module_type}'\n         if param_set in temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']:\n             return temperature._temperature_model_params('sapm', param_set)\n         elif 'freestanding' in param_set:\n         aoi : Series\n             Then angle of incidence.\n         \"\"\"\n        return irradiance.aoi(self.surface_tilt, self.surface_azimuth,\n                               solar_zenith, solar_azimuth)\n \n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n         if airmass is None:\n             airmass = atmosphere.get_relative_airmass(solar_zenith)\n \n        return irradiance.get_total_irradiance(self.surface_tilt,\n                                               self.surface_azimuth,\n                                                solar_zenith, solar_azimuth,\n                                                dni, ghi, dhi,\n                                                dni_extra=dni_extra,\n             func = temperature.fuentes\n             required = _build_tcell_args(['noct_installed'])\n             optional = _build_kwargs([\n                'module_height', 'wind_height', 'emissivity', 'absorption',\n                 'surface_tilt', 'module_width', 'module_length'],\n                 self.temperature_model_parameters)\n            # default to using the Array attribute, but allow user to override\n            # with a custom surface_tilt value in temperature_model_parameters\n            if 'surface_tilt' not in optional:\n                optional['surface_tilt'] = self.surface_tilt\n         elif model == 'noct_sam':\n             func = functools.partial(temperature.noct_sam,\n                                      effective_irradiance=effective_irradiance)\n             self.strings)\n \n \n def calcparams_desoto(effective_irradiance, temp_cell,\n                       alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,\n                       EgRef=1.121, dEgdT=-0.0002677,\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n import pandas as pd\n \n from pvlib.tools import cosd, sind, tand\nfrom pvlib.pvsystem import PVSystem, _unwrap_single_value\n from pvlib import irradiance, atmosphere\n \n \n class SingleAxisTracker(PVSystem):\n     \"\"\"\n     A class for single-axis trackers that inherits the PV modeling methods from\n     def __init__(self, axis_tilt=0, axis_azimuth=0, max_angle=90,\n                  backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0.0, **kwargs):\n \n        arrays = kwargs.get('arrays', [])\n        if len(arrays) > 1:\n            raise ValueError(\"SingleAxisTracker does not support \"\n                             \"multiple arrays.\")\n        elif len(arrays) == 1:\n            surface_tilt = arrays[0].surface_tilt\n            surface_azimuth = arrays[0].surface_azimuth\n            if surface_tilt is not None or surface_azimuth is not None:\n                raise ValueError(\n                    \"Array must not have surface_tilt or \"\n                    \"surface_azimuth assigned. You must pass an \"\n                    \"Array with these fields set to None.\"\n                )\n\n         self.axis_tilt = axis_tilt\n         self.axis_azimuth = axis_azimuth\n         self.max_angle = max_angle\n         self.gcr = gcr\n         self.cross_axis_tilt = cross_axis_tilt\n \n        kwargs['surface_tilt'] = None\n        kwargs['surface_azimuth'] = None\n \n        super().__init__(**kwargs)\n \n     def __repr__(self):\n         attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr',",
    "output": " from urllib.request import urlopen\n import numpy as np\n import pandas as pd\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n \n from pvlib._deprecation import deprecated\n \n                 array_losses_parameters = _build_kwargs(['dc_ohmic_percent'],\n                                                         losses_parameters)\n             self.arrays = (Array(\n                FixedMount(surface_tilt, surface_azimuth, racking_model),\n                 albedo,\n                 surface_type,\n                 module,\n                 temperature_model_parameters,\n                 modules_per_string,\n                 strings_per_inverter,\n                 array_losses_parameters,\n             ),)\n         elif len(arrays) == 0:\n \n     @_unwrap_single_value\n     def _infer_cell_type(self):\n         \"\"\"\n         Examines module_parameters and maps the Technology key for the CEC\n         database and the Material key for the Sandia database to a common\n         The Fuentes thermal model uses the module surface tilt for convection\n         modeling. The SAM implementation of PVWatts hardcodes the surface tilt\n         value at 30 degrees, ignoring whatever value is used for irradiance\n        transposition.  If you want to match the PVWatts behavior you can\n        either leave ``surface_tilt`` unspecified to use the PVWatts default\n        of 30, or specify a ``surface_tilt`` value in the Array's\n        ``temperature_model_parameters``.\n \n        The `temp_air`, `wind_speed`, and `surface_tilt` parameters may be\n        passed as tuples\n         to provide different values for each Array in the system. If not\n         passed as a tuple then the same value is used for input to each Array.\n         If passed as a tuple the length must be the same as the number of\n \n     @_unwrap_single_value\n     def first_solar_spectral_loss(self, pw, airmass_absolute):\n         \"\"\"\n         Use the :py:func:`first_solar_spectral_correction` function to\n         calculate the spectral loss modifier. The model coefficients are\n     @_unwrap_single_value\n     @_check_deprecated_passthrough\n     def surface_tilt(self):\n        return tuple(array.mount.surface_tilt for array in self.arrays)\n \n     @surface_tilt.setter\n     @_check_deprecated_passthrough\n     def surface_tilt(self, value):\n         for array in self.arrays:\n            array.mount.surface_tilt = value\n \n     @property\n     @_unwrap_single_value\n     @_check_deprecated_passthrough\n     def surface_azimuth(self):\n        return tuple(array.mount.surface_azimuth for array in self.arrays)\n \n     @surface_azimuth.setter\n     @_check_deprecated_passthrough\n     def surface_azimuth(self, value):\n         for array in self.arrays:\n            array.mount.surface_azimuth = value\n \n     @property\n     @_unwrap_single_value\n     @_unwrap_single_value\n     @_check_deprecated_passthrough\n     def racking_model(self):\n        return tuple(array.mount.racking_model for array in self.arrays)\n \n     @racking_model.setter\n     @_check_deprecated_passthrough\n     def racking_model(self, value):\n         for array in self.arrays:\n            array.mount.racking_model = value\n \n     @property\n     @_unwrap_single_value\n     \"\"\"\n     An Array is a set of of modules at the same orientation.\n \n    Specifically, an array is defined by its mount, the\n     module parameters, the number of parallel strings of modules\n     and the number of modules on each string.\n \n     Parameters\n     ----------\n    mount: FixedMount, SingleAxisTrackerMount, or other\n        Mounting for the array, either on fixed-tilt racking or horizontal\n        single axis tracker. Mounting is used to determine module orientation.\n        If not provided, a FixedMount with zero tilt is used.\n \n     albedo : None or float, default None\n         The ground albedo. If ``None``, will attempt to use\n     strings: int, default 1\n         Number of parallel strings in the array.\n \n     array_losses_parameters: None, dict or Series, default None.\n         Supported keys are 'dc_ohmic_percent'.\n \n    name: None or str, default None\n        Name of Array instance.\n     \"\"\"\n \n    def __init__(self, mount,\n                  albedo=None, surface_type=None,\n                  module=None, module_type=None,\n                  module_parameters=None,\n                  temperature_model_parameters=None,\n                  modules_per_string=1, strings=1,\n                 array_losses_parameters=None,\n                  name=None):\n        self.mount = mount\n \n         self.surface_type = surface_type\n         if albedo is None:\n             self.module_parameters = module_parameters\n \n         self.module_type = module_type\n \n         self.strings = strings\n         self.modules_per_string = modules_per_string\n         self.name = name\n \n     def __repr__(self):\n        attrs = ['name', 'mount', 'module',\n                 'albedo', 'module_type',\n                  'temperature_model_parameters',\n                  'strings', 'modules_per_string']\n\n         return 'Array:\\n  ' + '\\n  '.join(\n             f'{attr}: {getattr(self, attr)}' for attr in attrs\n         )\n     def _infer_temperature_model_params(self):\n         # try to infer temperature model parameters from from racking_model\n         # and module_type\n        param_set = f'{self.mount.racking_model}_{self.module_type}'\n         if param_set in temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']:\n             return temperature._temperature_model_params('sapm', param_set)\n         elif 'freestanding' in param_set:\n         aoi : Series\n             Then angle of incidence.\n         \"\"\"\n        orientation = self.mount.get_orientation(solar_zenith, solar_azimuth)\n        return irradiance.aoi(orientation['surface_tilt'],\n                              orientation['surface_azimuth'],\n                               solar_zenith, solar_azimuth)\n \n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n         if airmass is None:\n             airmass = atmosphere.get_relative_airmass(solar_zenith)\n \n        orientation = self.mount.get_orientation(solar_zenith, solar_azimuth)\n        return irradiance.get_total_irradiance(orientation['surface_tilt'],\n                                               orientation['surface_azimuth'],\n                                                solar_zenith, solar_azimuth,\n                                                dni, ghi, dhi,\n                                                dni_extra=dni_extra,\n             func = temperature.fuentes\n             required = _build_tcell_args(['noct_installed'])\n             optional = _build_kwargs([\n                'wind_height', 'emissivity', 'absorption',\n                 'surface_tilt', 'module_width', 'module_length'],\n                 self.temperature_model_parameters)\n            if self.mount.module_height is not None:\n                optional['module_height'] = self.mount.module_height\n         elif model == 'noct_sam':\n             func = functools.partial(temperature.noct_sam,\n                                      effective_irradiance=effective_irradiance)\n             self.strings)\n \n \n@dataclass\nclass AbstractMount(ABC):\n    \"\"\"\n    A base class for Mount classes to extend. It is not intended to be\n    instantiated directly.\n    \"\"\"\n\n    @abstractmethod\n    def get_orientation(self, solar_zenith, solar_azimuth):\n        \"\"\"\n        Determine module orientation.\n\n        Parameters\n        ----------\n        solar_zenith : numeric\n            Solar apparent zenith angle [degrees]\n        solar_azimuth : numeric\n            Solar azimuth angle [degrees]\n\n        Returns\n        -------\n        orientation : dict-like\n            A dict-like object with keys `'surface_tilt', 'surface_azimuth'`\n            (typically a dict or pandas.DataFrame)\n        \"\"\"\n\n\n@dataclass\nclass FixedMount(AbstractMount):\n    \"\"\"\n    Racking at fixed (static) orientation.\n\n    Parameters\n    ----------\n    surface_tilt : float, default 0\n        Surface tilt angle. The tilt angle is defined as angle from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90) [degrees]\n\n    surface_azimuth : float, default 180\n        Azimuth angle of the module surface. North=0, East=90, South=180,\n        West=270. [degrees]\n\n    racking_model : str, optional\n        Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n        Used to identify a parameter set for the SAPM cell temperature model.\n\n    module_height : float, optional\n       The height above ground of the center of the module [m]. Used for\n       the Fuentes cell temperature model.\n    \"\"\"\n\n    surface_tilt: float = 0.0\n    surface_azimuth: float = 180.0\n    racking_model: Optional[str] = None\n    module_height: Optional[float] = None\n\n    def get_orientation(self, solar_zenith, solar_azimuth):\n        # note -- docstring is automatically inherited from AbstractMount\n        return {\n            'surface_tilt': self.surface_tilt,\n            'surface_azimuth': self.surface_azimuth,\n        }\n\n\n@dataclass\nclass SingleAxisTrackerMount(AbstractMount):\n    \"\"\"\n    Single-axis tracker racking for dynamic solar tracking.\n\n    Parameters\n    ----------\n    axis_tilt : float, default 0\n        The tilt of the axis of rotation (i.e, the y-axis defined by\n        axis_azimuth) with respect to horizontal. [degrees]\n\n    axis_azimuth : float, default 180\n        A value denoting the compass direction along which the axis of\n        rotation lies, measured east of north. [degrees]\n\n    max_angle : float, default 90\n        A value denoting the maximum rotation angle\n        of the one-axis tracker from its horizontal position (horizontal\n        if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n        to rotate to a vertical position to point the panel towards a\n        horizon. max_angle of 180 degrees allows for full rotation. [degrees]\n\n    backtrack : bool, default True\n        Controls whether the tracker has the capability to \"backtrack\"\n        to avoid row-to-row shading. False denotes no backtrack\n        capability. True denotes backtrack capability.\n\n    gcr : float, default 2.0/7.0\n        A value denoting the ground coverage ratio of a tracker system\n        which utilizes backtracking; i.e. the ratio between the PV array\n        surface area to total ground area. A tracker system with modules\n        2 meters wide, centered on the tracking axis, with 6 meters\n        between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n        provided, a gcr of 2/7 is default. gcr must be <=1. [unitless]\n\n    cross_axis_tilt : float, default 0.0\n        The angle, relative to horizontal, of the line formed by the\n        intersection between the slope containing the tracker axes and a plane\n        perpendicular to the tracker axes. Cross-axis tilt should be specified\n        using a right-handed convention. For example, trackers with axis\n        azimuth of 180 degrees (heading south) will have a negative cross-axis\n        tilt if the tracker axes plane slopes down to the east and positive\n        cross-axis tilt if the tracker axes plane slopes up to the east. Use\n        :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n        `cross_axis_tilt`. [degrees]\n\n    racking_model : str, optional\n        Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n        Used to identify a parameter set for the SAPM cell temperature model.\n\n    module_height : float, optional\n       The height above ground of the center of the module [m]. Used for\n       the Fuentes cell temperature model.\n    \"\"\"\n    axis_tilt: float = 0.0\n    axis_azimuth: float = 0.0\n    max_angle: float = 90.0\n    backtrack: bool = True\n    gcr: float = 2.0/7.0\n    cross_axis_tilt: float = 0.0\n    racking_model: Optional[str] = None\n    module_height: Optional[float] = None\n\n    def get_orientation(self, solar_zenith, solar_azimuth):\n        # note -- docstring is automatically inherited from AbstractMount\n        from pvlib import tracking  # avoid circular import issue\n        tracking_data = tracking.singleaxis(\n            solar_zenith, solar_azimuth,\n            self.axis_tilt, self.axis_azimuth,\n            self.max_angle, self.backtrack,\n            self.gcr, self.cross_axis_tilt\n        )\n        return tracking_data\n\n\n def calcparams_desoto(effective_irradiance, temp_cell,\n                       alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,\n                       EgRef=1.121, dEgdT=-0.0002677,\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n import pandas as pd\n \n from pvlib.tools import cosd, sind, tand\nfrom pvlib.pvsystem import (\n    PVSystem, Array, SingleAxisTrackerMount, _unwrap_single_value\n)\n from pvlib import irradiance, atmosphere\nfrom pvlib._deprecation import deprecated\n \n \n@deprecated('0.9.0', alternative='PVSystem with SingleAxisTrackerMount')\n class SingleAxisTracker(PVSystem):\n     \"\"\"\n     A class for single-axis trackers that inherits the PV modeling methods from\n     def __init__(self, axis_tilt=0, axis_azimuth=0, max_angle=90,\n                  backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0.0, **kwargs):\n \n        mount_kwargs = {\n            k: kwargs.pop(k) for k in ['racking_model', 'module_height']\n            if k in kwargs\n        }\n        mount = SingleAxisTrackerMount(axis_tilt, axis_azimuth, max_angle,\n                                       backtrack, gcr, cross_axis_tilt,\n                                       **mount_kwargs)\n\n        array_defaults = {\n            'albedo': None, 'surface_type': None, 'module': None,\n            'module_type': None, 'module_parameters': None,\n            'temperature_model_parameters': None,\n            'modules_per_string': 1,\n        }\n        array_kwargs = {\n            key: kwargs.get(key, array_defaults[key]) for key in array_defaults\n        }\n        # strings/strings_per_inverter is a special case\n        array_kwargs['strings'] = kwargs.get('strings_per_inverter', 1)\n\n        array = Array(mount=mount, **array_kwargs)\n        pass_through_kwargs = {  # other args to pass to PVSystem()\n            k: v for k, v in kwargs.items() if k not in array_defaults\n        }\n        # leave these in case someone is using them\n         self.axis_tilt = axis_tilt\n         self.axis_azimuth = axis_azimuth\n         self.max_angle = max_angle\n         self.gcr = gcr\n         self.cross_axis_tilt = cross_axis_tilt\n \n        pass_through_kwargs['surface_tilt'] = None\n        pass_through_kwargs['surface_azimuth'] = None\n \n        super().__init__(arrays=[array], **pass_through_kwargs)\n \n     def __repr__(self):\n         attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr',"
  },
  {
    "instruction": "ModelChainResult.cell_temperature is not always a pandas.Series \nFor a `ModelChain` configured as below, the `cell_temperature` when running the model chain with a list of data like `ModelChain.run_model([data])` is a tuple with a single number instead of the expected Series\r\n\r\n**To Reproduce**\r\n```python\r\nimport pandas as pd                                                                                                                                                                                                                                           \r\nfrom pvlib.location import Location                                                                                                                                                                                                                           \r\nfrom pvlib.pvsystem import PVSystem, Array                                                                                                                                                                                                                    \r\nfrom pvlib.modelchain import ModelChain                                                                                                                                                                                                                       \r\n                                                                                                                                                                                                                                                              \r\ndata = pd.DataFrame(                                                                                                                                                                                                                                          \r\n    {                                                                                                                                                                                                                                                         \r\n        \"ghi\": [1100.0, 1101.0],                                                                                                                                                                                                                              \r\n        \"dni\": [1000.0, 1001],                                                                                                                                                                                                                                \r\n        \"dhi\": [100.0, 100],                                                                                                                                                                                                                                  \r\n        \"temp_air\": [25.0, 25],                                                                                                                                                                                                                               \r\n        \"wind_speed\": [10.0, 10],                                                                                                                                                                                                                             \r\n    },                                                                                                                                                                                                                                                        \r\n    index=pd.DatetimeIndex(                                                                                                                                                                                                                                   \r\n        [pd.Timestamp(\"2021-01-20T12:00-05:00\"), pd.Timestamp(\"2021-01-20T12:05-05:00\")]                                                                                                                                                                      \r\n    ),                                                                                                                                                                                                                                                        \r\n)                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\narray_params = {                                                                                                                                                                                                                                              \r\n    \"name\": None,                                                                                                                                                                                                                                             \r\n    \"surface_tilt\": 20.0,                                                                                                                                                                                                                                     \r\n    \"surface_azimuth\": 180.0,                                                                                                                                                                                                                                 \r\n    \"module\": \"Canadian_Solar_Inc__CS5P_220M\",                                                                                                                                                                                                                \r\n    \"albedo\": 0.2,                                                                                                                                                                                                                                            \r\n    \"temperature_model_parameters\": {                                                                                                                                                                                                                         \r\n        \"u_c\": 29.0,                                                                                                                                                                                                                                          \r\n        \"u_v\": 0.0,                                                                                                                                                                                                                                           \r\n        \"eta_m\": 0.1,                                                                                                                                                                                                                                         \r\n        \"alpha_absorption\": 0.9,                                                                                                                                                                                                                              \r\n    },                                                                                                                                                                                                                                                        \r\n    \"strings\": 5,                                                                                                                                                                                                                                             \r\n    \"modules_per_string\": 7,                                                                                                                                                                                                                                  \r\n    \"module_parameters\": {                                                                                                                                                                                                                                    \r\n        \"alpha_sc\": 0.004539,                                                                                                                                                                                                                                 \r\n        \"gamma_ref\": 1.2,                                                                                                                                                                                                                                     \r\n        \"mu_gamma\": -0.003,                                                                                                                                                                                                                                   \r\n        \"I_L_ref\": 5.11426,                                                                                                                                                                                                                                   \r\n        \"I_o_ref\": 8.10251e-10,                                                                                                                                                                                                                               \r\n        \"R_sh_ref\": 381.254,                                                                                                                                                                                                                                  \r\n        \"R_sh_0\": 400.0,                                                                                                                                                                                                                                      \r\n        \"R_s\": 1.06602,                                                                                                                                                                                                                                       \r\n        \"cells_in_series\": 96,                                                                                                                                                                                                                                \r\n        \"R_sh_exp\": 5.5,                                                                                                                                                                                                                                      \r\n        \"EgRef\": 1.121,                                                                                                                                                                                                                                       \r\n    },                                                                                                                                                                                                                                                        \r\n}\r\ninverter_parameters = {                                                                                                                                                                                                                                       \r\n    \"Paco\": 250.0,                                                                                                                                                                                                                                            \r\n    \"Pdco\": 259.589,                                                                                                                                                                                                                                          \r\n    \"Vdco\": 40.0,                                                                                                                                                                                                                                             \r\n    \"Pso\": 2.08961,                                                                                                                                                                                                                                           \r\n    \"C0\": -4.1e-05,                                                                                                                                                                                                                                           \r\n    \"C1\": -9.1e-05,                                                                                                                                                                                                                                           \r\n    \"C2\": 0.000494,                                                                                                                                                                                                                                           \r\n    \"C3\": -0.013171,                                                                                                                                                                                                                                          \r\n    \"Pnt\": 0.075,                                                                                                                                                                                                                                             \r\n}                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                              \r\nlocation = Location(latitude=33.98, longitude=-115.323, altitude=2300)                                                                                                                                                                                        \r\n                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                              \r\narray_sys = PVSystem(                                                                                                                                                                                                                                         \r\n    arrays=[Array(**array_params)], inverter_parameters=inverter_parameters                                                                                                                                                                                   \r\n)                                                                                                                                                                                                                                                             \r\nassert isinstance(                                                                                                                                                                                                                                            \r\n    ModelChain(array_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\")                                                                                                                                                                            \r\n    .run_model(data)                                                                                                                                                                                                                                          \r\n    .results.cell_temperature,                                                                                                                                                                                                                                \r\n    pd.Series,                                                                                                                                                                                                                                                \r\n)                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\narray_run = ModelChain(                                                                                                                                                                                                                                       \r\n    array_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\"                                                                                                                                                                                        \r\n).run_model([data])                                                                                                                                                                                                                                           \r\nassert array_run.results.cell_temperature == array_run.cell_temperature                                                                                                                                                                                       \r\nprint(array_run.results.cell_temperature)  # (45.329789874660285,)                                                                                                                                                                                            \r\n                                                                                                                                                                                                                                                              \r\n                                                                                                                                                                                                                                                              \r\narray_params[\"strings_per_inverter\"] = array_params.pop(\"strings\")                                                                                                                                                                                            \r\nstandard_sys = PVSystem(**array_params, inverter_parameters=inverter_parameters)                                                                                                                                                                              \r\nassert isinstance(                                                                                                                                                                                                                                            \r\n    ModelChain(standard_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\")                                                                                                                                                                         \r\n    .run_model(data)                                                                                                                                                                                                                                          \r\n    .results.cell_temperature,                                                                                                                                                                                                                                \r\n    pd.Series,                                                                                                                                                                                                                                                \r\n)                                                                                                                                                                                                                                                             \r\n                                                                                                                                                                                                                                                              \r\nstandard_run = ModelChain(                                                                                                                                                                                                                                    \r\n    standard_sys, location, aoi_model=\"no_loss\", spectral_model=\"no_loss\"                                                                                                                                                                                     \r\n).run_model([data])                                                                                                                                                                                                                                           \r\nassert standard_run.results.cell_temperature == standard_run.cell_temperature                                                                                                                                                                                 \r\nprint(standard_run.results.cell_temperature)  # (45.329789874660285,)                                                                                                                                                                                         \r\nassert not isinstance(standard_run.results.cell_temperature, pd.Series)                                                                                                                                                                                       \r\n                                                                                \r\n```\r\n\r\n**Expected behavior**\r\n`type(ModelChain.run_model([data]).results.cell_temperature) == pd.Series`\r\n__\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``:  0.8.1+4.gba4a199\r\n - ``pandas.__version__``:  1.1.4\r\n - python: 3.8.5\r\n\n",
    "input": "     _T = TypeVar('T')\n     PerArray = Union[_T, Tuple[_T, ...]]\n     \"\"\"Type for fields that vary between arrays\"\"\"\n     # system-level information\n     solar_position: Optional[pd.DataFrame] = field(default=None)\n     airmass: Optional[pd.DataFrame] = field(default=None)\n     ac: Optional[pd.Series] = field(default=None)\n    # per DC array information\n     tracking: Optional[pd.DataFrame] = field(default=None)\n     total_irrad: Optional[PerArray[pd.DataFrame]] = field(default=None)\n     aoi: Optional[PerArray[pd.Series]] = field(default=None)\n    aoi_modifier: Optional[PerArray[pd.Series]] = field(default=None)\n    spectral_modifier: Optional[PerArray[pd.Series]] = field(default=None)\n     cell_temperature: Optional[PerArray[pd.Series]] = field(default=None)\n     effective_irradiance: Optional[PerArray[pd.Series]] = field(default=None)\n     dc: Optional[PerArray[Union[pd.Series, pd.DataFrame]]] = \\\n         field(default=None)\n     diode_params: Optional[PerArray[pd.DataFrame]] = field(default=None)\n \n \n class ModelChain:\n     \"\"\"\n                              'set the model with the dc_model kwarg.')\n \n     def sapm(self):\n        self.results.dc = self.system.sapm(self.results.effective_irradiance,\n                                           self.results.cell_temperature)\n\n        self.results.dc = self.system.scale_voltage_current_power(\n            self.results.dc)\n\n         return self\n \n     def _singlediode(self, calcparams_model_function):\n         pvlib.pvsystem.PVSystem.pvwatts_dc\n         pvlib.pvsystem.PVSystem.scale_voltage_current_power\n         \"\"\"\n        self.results.dc = self.system.pvwatts_dc(\n            self.results.effective_irradiance, self.results.cell_temperature)\n        if isinstance(self.results.dc, tuple):\n            temp = tuple(\n                pd.DataFrame(s, columns=['p_mp']) for s in self.results.dc)\n        else:\n            temp = pd.DataFrame(self.results.dc, columns=['p_mp'])\n        scaled = self.system.scale_voltage_current_power(temp)\n        if isinstance(scaled, tuple):\n            self.results.dc = tuple(s['p_mp'] for s in scaled)\n        else:\n            self.results.dc = scaled['p_mp']\n         return self\n \n     @property\n \n     def ashrae_aoi_loss(self):\n         self.results.aoi_modifier = self.system.get_iam(\n            self.results.aoi, iam_model='ashrae')\n         return self\n \n     def physical_aoi_loss(self):\n        self.results.aoi_modifier = self.system.get_iam(self.results.aoi,\n                                                        iam_model='physical')\n         return self\n \n     def sapm_aoi_loss(self):\n        self.results.aoi_modifier = self.system.get_iam(self.results.aoi,\n                                                        iam_model='sapm')\n         return self\n \n     def martin_ruiz_aoi_loss(self):\n         self.results.aoi_modifier = self.system.get_iam(\n            self.results.aoi,\n            iam_model='martin_ruiz')\n         return self\n \n     def no_aoi_loss(self):\n \n     def first_solar_spectral_loss(self):\n         self.results.spectral_modifier = self.system.first_solar_spectral_loss(\n            self.weather['precipitable_water'],\n            self.results.airmass['airmass_absolute'])\n         return self\n \n     def sapm_spectral_loss(self):\n         self.results.spectral_modifier = self.system.sapm_spectral_loss(\n            self.results.airmass['airmass_absolute'])\n         return self\n \n     def no_spectral_loss(self):\n \n     def pvwatts_losses(self):\n         self.losses = (100 - self.system.pvwatts_losses()) / 100.\n        if self.system.num_arrays > 1:\n             for dc in self.results.dc:\n                 dc *= self.losses\n         else:\n             for (i, array_data) in enumerate(data):\n                 _verify(array_data, i)\n \n     def _assign_weather(self, data):\n         def _build_weather(data):\n             key_list = [k for k in WEATHER_KEYS if k in data]\n             self.weather = tuple(\n                 _build_weather(weather) for weather in data\n             )\n         return self\n \n     def _assign_total_irrad(self, data):\n             _tuple_from_dfs(self.weather, 'ghi'),\n             _tuple_from_dfs(self.weather, 'dhi'),\n             airmass=self.results.airmass['airmass_relative'],\n            model=self.transposition_model)\n \n         return self\n \ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n from collections import OrderedDict\n import functools\n import io\n import os\n from urllib.request import urlopen\n import numpy as np\n             effective irradiance, i.e., the irradiance that is converted to\n             electrical current.\n         \"\"\"\n \n        def _spectral_correction(array):\n             if 'first_solar_spectral_coefficients' in \\\n                     array.module_parameters.keys():\n                 coefficients = \\\n                 pw, airmass_absolute,\n                 module_type, coefficients\n             )\n        return tuple(_spectral_correction(array) for array in self.arrays)\n \n     def singlediode(self, photocurrent, saturation_current,\n                     resistance_series, resistance_shunt, nNsVth,\n         model = model.lower()\n         multiple_arrays = self.num_arrays > 1\n         if model == 'sandia':\n             if multiple_arrays:\n                p_dc = self._validate_per_array(p_dc)\n                v_dc = self._validate_per_array(v_dc)\n                inv_fun = inverter.sandia_multi\n            else:\n                inv_fun = inverter.sandia\n            return inv_fun(v_dc, p_dc, self.inverter_parameters)\n         elif model == 'pvwatts':\n             kwargs = _build_kwargs(['eta_inv_nom', 'eta_inv_ref'],\n                                    self.inverter_parameters)\n             if multiple_arrays:\n                p_dc = self._validate_per_array(p_dc)\n                inv_fun = inverter.pvwatts_multi\n            else:\n                inv_fun = inverter.pvwatts\n            return inv_fun(p_dc, self.inverter_parameters['pdc0'], **kwargs)\n         elif model == 'adr':\n             if multiple_arrays:\n                 raise ValueError(\n                     'The adr inverter function cannot be used for an inverter',\n                     ' with multiple MPPT inputs')\n            else:\n                return inverter.adr(v_dc, p_dc, self.inverter_parameters)\n         else:\n             raise ValueError(\n                 model + ' is not a valid AC power model.',",
    "output": "     _T = TypeVar('T')\n     PerArray = Union[_T, Tuple[_T, ...]]\n     \"\"\"Type for fields that vary between arrays\"\"\"\n\n    # these attributes are used in __setattr__ to determine the correct type.\n    _singleton_tuples: bool = field(default=False)\n    _per_array_fields = {'total_irrad', 'aoi', 'aoi_modifier',\n                         'spectral_modifier', 'cell_temperature',\n                         'effective_irradiance', 'dc', 'diode_params'}\n\n     # system-level information\n     solar_position: Optional[pd.DataFrame] = field(default=None)\n     airmass: Optional[pd.DataFrame] = field(default=None)\n     ac: Optional[pd.Series] = field(default=None)\n     tracking: Optional[pd.DataFrame] = field(default=None)\n\n    # per DC array information\n     total_irrad: Optional[PerArray[pd.DataFrame]] = field(default=None)\n     aoi: Optional[PerArray[pd.Series]] = field(default=None)\n    aoi_modifier: Optional[PerArray[Union[pd.Series, float]]] = \\\n        field(default=None)\n    spectral_modifier: Optional[PerArray[Union[pd.Series, float]]] = \\\n        field(default=None)\n     cell_temperature: Optional[PerArray[pd.Series]] = field(default=None)\n     effective_irradiance: Optional[PerArray[pd.Series]] = field(default=None)\n     dc: Optional[PerArray[Union[pd.Series, pd.DataFrame]]] = \\\n         field(default=None)\n     diode_params: Optional[PerArray[pd.DataFrame]] = field(default=None)\n \n    def _result_type(self, value):\n        \"\"\"Coerce `value` to the correct type according to\n        ``self._singleton_tuples``.\"\"\"\n        # Allow None to pass through without being wrapped in a tuple\n        if (self._singleton_tuples\n                and not isinstance(value, tuple)\n                and value is not None):\n            return (value,)\n        return value\n\n    def __setattr__(self, key, value):\n        if key in ModelChainResult._per_array_fields:\n            value = self._result_type(value)\n        super().__setattr__(key, value)\n\n \n class ModelChain:\n     \"\"\"\n                              'set the model with the dc_model kwarg.')\n \n     def sapm(self):\n        dc = self.system.sapm(self.results.effective_irradiance,\n                              self.results.cell_temperature)\n        self.results.dc = self.system.scale_voltage_current_power(dc)\n         return self\n \n     def _singlediode(self, calcparams_model_function):\n         pvlib.pvsystem.PVSystem.pvwatts_dc\n         pvlib.pvsystem.PVSystem.scale_voltage_current_power\n         \"\"\"\n        dc = self.system.pvwatts_dc(\n            self.results.effective_irradiance,\n            self.results.cell_temperature,\n            unwrap=False\n        )\n        p_mp = tuple(pd.DataFrame(s, columns=['p_mp']) for s in dc)\n        scaled = self.system.scale_voltage_current_power(p_mp)\n        self.results.dc = _tuple_from_dfs(scaled, \"p_mp\")\n         return self\n \n     @property\n \n     def ashrae_aoi_loss(self):\n         self.results.aoi_modifier = self.system.get_iam(\n            self.results.aoi,\n            iam_model='ashrae'\n        )\n         return self\n \n     def physical_aoi_loss(self):\n        self.results.aoi_modifier = self.system.get_iam(\n            self.results.aoi,\n            iam_model='physical'\n        )\n         return self\n \n     def sapm_aoi_loss(self):\n        self.results.aoi_modifier = self.system.get_iam(\n            self.results.aoi,\n            iam_model='sapm'\n        )\n         return self\n \n     def martin_ruiz_aoi_loss(self):\n         self.results.aoi_modifier = self.system.get_iam(\n            self.results.aoi, iam_model='martin_ruiz'\n        )\n         return self\n \n     def no_aoi_loss(self):\n \n     def first_solar_spectral_loss(self):\n         self.results.spectral_modifier = self.system.first_solar_spectral_loss(\n            _tuple_from_dfs(self.weather, 'precipitable_water'),\n            self.results.airmass['airmass_absolute']\n        )\n         return self\n \n     def sapm_spectral_loss(self):\n         self.results.spectral_modifier = self.system.sapm_spectral_loss(\n            self.results.airmass['airmass_absolute']\n        )\n         return self\n \n     def no_spectral_loss(self):\n \n     def pvwatts_losses(self):\n         self.losses = (100 - self.system.pvwatts_losses()) / 100.\n        if isinstance(self.results.dc, tuple):\n             for dc in self.results.dc:\n                 dc *= self.losses\n         else:\n             for (i, array_data) in enumerate(data):\n                 _verify(array_data, i)\n \n    def _configure_results(self):\n        \"\"\"Configure the type used for per-array fields in ModelChainResult.\n\n        Must be called after ``self.weather`` has been assigned. If\n        ``self.weather`` is a tuple and the number of arrays in the system\n        is 1, then per-array results are stored as length-1 tuples.\n        \"\"\"\n        self.results._singleton_tuples = (\n            self.system.num_arrays == 1 and isinstance(self.weather, tuple)\n        )\n\n     def _assign_weather(self, data):\n         def _build_weather(data):\n             key_list = [k for k in WEATHER_KEYS if k in data]\n             self.weather = tuple(\n                 _build_weather(weather) for weather in data\n             )\n        self._configure_results()\n         return self\n \n     def _assign_total_irrad(self, data):\n             _tuple_from_dfs(self.weather, 'ghi'),\n             _tuple_from_dfs(self.weather, 'dhi'),\n             airmass=self.results.airmass['airmass_relative'],\n            model=self.transposition_model\n        )\n \n         return self\n \ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n from collections import OrderedDict\n import functools\n import io\nimport itertools\n import os\n from urllib.request import urlopen\n import numpy as np\n             effective irradiance, i.e., the irradiance that is converted to\n             electrical current.\n         \"\"\"\n        pw = self._validate_per_array(pw, system_wide=True)\n \n        def _spectral_correction(array, pw):\n             if 'first_solar_spectral_coefficients' in \\\n                     array.module_parameters.keys():\n                 coefficients = \\\n                 pw, airmass_absolute,\n                 module_type, coefficients\n             )\n        return tuple(\n            itertools.starmap(_spectral_correction, zip(self.arrays, pw))\n        )\n \n     def singlediode(self, photocurrent, saturation_current,\n                     resistance_series, resistance_shunt, nNsVth,\n         model = model.lower()\n         multiple_arrays = self.num_arrays > 1\n         if model == 'sandia':\n            p_dc = self._validate_per_array(p_dc)\n            v_dc = self._validate_per_array(v_dc)\n             if multiple_arrays:\n                return inverter.sandia_multi(\n                    v_dc, p_dc, self.inverter_parameters)\n            return inverter.sandia(v_dc[0], p_dc[0], self.inverter_parameters)\n         elif model == 'pvwatts':\n             kwargs = _build_kwargs(['eta_inv_nom', 'eta_inv_ref'],\n                                    self.inverter_parameters)\n            p_dc = self._validate_per_array(p_dc)\n             if multiple_arrays:\n                return inverter.pvwatts_multi(\n                    p_dc, self.inverter_parameters['pdc0'], **kwargs)\n            return inverter.pvwatts(\n                p_dc[0], self.inverter_parameters['pdc0'], **kwargs)\n         elif model == 'adr':\n             if multiple_arrays:\n                 raise ValueError(\n                     'The adr inverter function cannot be used for an inverter',\n                     ' with multiple MPPT inputs')\n            # While this is only used for single-array systems, calling\n            # _validate_per_arry lets us pass in singleton tuples.\n            p_dc = self._validate_per_array(p_dc)\n            v_dc = self._validate_per_array(v_dc)\n            return inverter.adr(v_dc[0], p_dc[0], self.inverter_parameters)\n         else:\n             raise ValueError(\n                 model + ' is not a valid AC power model.',"
  },
  {
    "instruction": "SolarAnywhere File -- pvlib.iotools.read_tmy3 Bug\n@AdamRJensen \r\n\r\nThere's a bug report for reading SolarAnywhere Files with using the pvlib.iotools.read_tmy3 function. This bug is in the TMY3 file (I think?)\r\n\r\n\r\n![TMY3](https://github.com/pvlib/pvlib-python/assets/74630912/1f85b014-a40a-42af-9c07-76e51ccc606e)\r\n\n",
    "input": " }\n \n \ndef read_tmy3(filename, coerce_year=None, map_variables=None, recolumn=None):\n     \"\"\"Read a TMY3 file into a pandas dataframe.\n \n     Note that values contained in the metadata dictionary are unchanged\n         If ``True``, apply standard names to TMY3 columns. Typically this\n         results in stripping the units from the column name.\n         Cannot be used in combination with ``map_variables``.\n \n     Returns\n     -------\n     data : DataFrame\n         A pandas dataframe with the columns described in the table\n         below. For more detailed descriptions of each component, please\n        consult the TMY3 User's Manual ([1]_), especially tables 1-1\n         through 1-6.\n \n     metadata : dict\n     \"\"\"  # noqa: E501\n     head = ['USAF', 'Name', 'State', 'TZ', 'latitude', 'longitude', 'altitude']\n \n    try:\n        with open(str(filename), 'r') as fbuf:\n            firstline, data = _parse_tmy3(fbuf)\n    # SolarAnywhere files contain non-UTF8 characters and may require\n    # encoding='iso-8859-1' in order to be parsed\n    except UnicodeDecodeError:\n        with open(str(filename), 'r', encoding='iso-8859-1') as fbuf:\n            firstline, data = _parse_tmy3(fbuf)\n \n     meta = dict(zip(head, firstline.rstrip('\\n').split(\",\")))\n     # convert metadata strings to numeric types\n \n     # get the date column as a pd.Series of numpy datetime64\n     data_ymd = pd.to_datetime(data['Date (MM/DD/YYYY)'], format='%m/%d/%Y')\n     # shift the time column so that midnite is 00:00 instead of 24:00\n    shifted_hour = data['Time (HH:MM)'].str[:2].astype(int) % 24\n     # shift the dates at midnight (24:00) so they correspond to the next day.\n     # If midnight is specified as 00:00 do not shift date.\n     data_ymd[data['Time (HH:MM)'].str[:2] == '24'] += datetime.timedelta(days=1)  # noqa: E501\n         data_ymd.iloc[-1] = data_ymd.iloc[-1].replace(year=coerce_year+1)\n     # NOTE: as of pvlib-0.6.3, min req is pandas-0.18.1, so pd.to_timedelta\n     # unit must be in (D,h,m,s,ms,us,ns), but pandas>=0.24 allows unit='hour'\n    data.index = data_ymd + pd.to_timedelta(shifted_hour, unit='h')\n     # shouldnt' specify both recolumn and map_variables\n     if recolumn is not None and map_variables is not None:\n         msg = \"`map_variables` and `recolumn` cannot both be specified\"\n     return data, meta\n \n \ndef _parse_tmy3(fbuf):\n    # header information on the 1st line (0 indexing)\n    firstline = fbuf.readline()\n    # use pandas to read the csv file buffer\n    # header is actually the second line, but tell pandas to look for\n    data = pd.read_csv(fbuf, header=0)\n    return firstline, data\n\n\n def _recolumn(tmy3_dataframe):\n     \"\"\"\n     Rename the columns of the TMY3 DataFrame.\n     data : DataFrame\n         A dataframe with the columns described in the table below. For a\n         more detailed descriptions of each component, please consult the\n        TMY2 User's Manual ([1]_), especially tables 3-1 through 3-6, and\n         Appendix B.\n \n     metadata : dict\n     ----------\n     .. [1] Marion, W and Urban, K. \"Wilcox, S and Marion, W. \"User's Manual\n        for TMY2s\". NREL 1995.\n     \"\"\"  # noqa: E501\n     # paste in the column info as one long line\n     string = '%2d%2d%2d%2d%4d%4d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%2d%1s%1d%2d%1s%1d%4d%1s%1d%4d%1s%1d%3d%1s%1d%4d%1s%1d%3d%1s%1d%3d%1s%1d%4d%1s%1d%5d%1s%1d%10d%3d%1s%1d%3d%1s%1d%3d%1s%1d%2d%1s%1d'  # noqa: E501",
    "output": " }\n \n \ndef read_tmy3(filename, coerce_year=None, map_variables=None, recolumn=None,\n              encoding=None):\n     \"\"\"Read a TMY3 file into a pandas dataframe.\n \n     Note that values contained in the metadata dictionary are unchanged\n         If ``True``, apply standard names to TMY3 columns. Typically this\n         results in stripping the units from the column name.\n         Cannot be used in combination with ``map_variables``.\n    encoding : str, optional\n        Encoding of the file. For files that contain non-UTF8 characters it may\n        be necessary to specify an alternative encoding, e.g., for\n        SolarAnywhere TMY3 files the encoding should be 'iso-8859-1'. Users\n        may also consider using the 'utf-8-sig' encoding.\n \n     Returns\n     -------\n     data : DataFrame\n         A pandas dataframe with the columns described in the table\n         below. For more detailed descriptions of each component, please\n        consult the TMY3 User's Manual [1]_, especially tables 1-1\n         through 1-6.\n \n     metadata : dict\n     \"\"\"  # noqa: E501\n     head = ['USAF', 'Name', 'State', 'TZ', 'latitude', 'longitude', 'altitude']\n \n    with open(str(filename), 'r', encoding=encoding) as fbuf:\n        # header information on the 1st line (0 indexing)\n        firstline = fbuf.readline()\n        # use pandas to read the csv file buffer\n        # header is actually the second line, but tell pandas to look for\n        data = pd.read_csv(fbuf, header=0)\n \n     meta = dict(zip(head, firstline.rstrip('\\n').split(\",\")))\n     # convert metadata strings to numeric types\n \n     # get the date column as a pd.Series of numpy datetime64\n     data_ymd = pd.to_datetime(data['Date (MM/DD/YYYY)'], format='%m/%d/%Y')\n    # extract minutes\n    minutes = data['Time (HH:MM)'].str.split(':').str[1].astype(int)\n     # shift the time column so that midnite is 00:00 instead of 24:00\n    shifted_hour = data['Time (HH:MM)'].str.split(':').str[0].astype(int) % 24\n     # shift the dates at midnight (24:00) so they correspond to the next day.\n     # If midnight is specified as 00:00 do not shift date.\n     data_ymd[data['Time (HH:MM)'].str[:2] == '24'] += datetime.timedelta(days=1)  # noqa: E501\n         data_ymd.iloc[-1] = data_ymd.iloc[-1].replace(year=coerce_year+1)\n     # NOTE: as of pvlib-0.6.3, min req is pandas-0.18.1, so pd.to_timedelta\n     # unit must be in (D,h,m,s,ms,us,ns), but pandas>=0.24 allows unit='hour'\n    data.index = data_ymd + pd.to_timedelta(shifted_hour, unit='h') \\\n        + pd.to_timedelta(minutes, unit='min')\n     # shouldnt' specify both recolumn and map_variables\n     if recolumn is not None and map_variables is not None:\n         msg = \"`map_variables` and `recolumn` cannot both be specified\"\n     return data, meta\n \n \n def _recolumn(tmy3_dataframe):\n     \"\"\"\n     Rename the columns of the TMY3 DataFrame.\n     data : DataFrame\n         A dataframe with the columns described in the table below. For a\n         more detailed descriptions of each component, please consult the\n        TMY2 User's Manual [1]_, especially tables 3-1 through 3-6, and\n         Appendix B.\n \n     metadata : dict\n     ----------\n     .. [1] Marion, W and Urban, K. \"Wilcox, S and Marion, W. \"User's Manual\n        for TMY2s\". NREL 1995.\n       :doi:`10.2172/87130`\n     \"\"\"  # noqa: E501\n     # paste in the column info as one long line\n     string = '%2d%2d%2d%2d%4d%4d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%2d%1s%1d%2d%1s%1d%4d%1s%1d%4d%1s%1d%3d%1s%1d%4d%1s%1d%3d%1s%1d%3d%1s%1d%4d%1s%1d%5d%1s%1d%10d%3d%1s%1d%3d%1s%1d%3d%1s%1d%2d%1s%1d'  # noqa: E501"
  },
  {
    "instruction": "Add `min_angle` argument to `tracking.singleaxis`\nIn `tracking.singleaxis` the minimum angle of the tracker is assumed to be opposite of the maximum angle, although in some cases the minimum angle could be different. NREL SAM doesn't support that but PVsyst does.\r\n\r\nIn order to support non symmetrical limiting angles, `tracking.singleaxis` should have another, optional, input, `min_angle`. By default, if not supplied (i.e. value is `None`), the current behavior (`min_angle = -max_angle`) would apply.\r\n\r\nCan I propose a PR for this, with modifications to `tracking.singleaxis`, `tracking.SingleAxisTracker` and to `pvsystem.SingleAxisTrackerMount` + corresponding tests?\n",
    "input": " import pandas as pd\n from dataclasses import dataclass\n from abc import ABC, abstractmethod\nfrom typing import Optional\n \n from pvlib._deprecation import deprecated, warn_deprecated\n \n         A value denoting the compass direction along which the axis of\n         rotation lies, measured east of north. [degrees]\n \n    max_angle : float, default 90\n        A value denoting the maximum rotation angle\n         of the one-axis tracker from its horizontal position (horizontal\n        if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n        to rotate to a vertical position to point the panel towards a\n        horizon. max_angle of 180 degrees allows for full rotation. [degrees]\n \n     backtrack : bool, default True\n         Controls whether the tracker has the capability to \"backtrack\"\n     \"\"\"\n     axis_tilt: float = 0.0\n     axis_azimuth: float = 0.0\n    max_angle: float = 90.0\n     backtrack: bool = True\n     gcr: float = 2.0/7.0\n     cross_axis_tilt: float = 0.0\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n         A value denoting the compass direction along which the axis of\n         rotation lies. Measured in decimal degrees east of north.\n \n    max_angle : float, default 90\n         A value denoting the maximum rotation angle, in decimal degrees,\n         of the one-axis tracker from its horizontal position (horizontal\n        if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n        to rotate to a vertical position to point the panel towards a\n        horizon. max_angle of 180 degrees allows for full rotation.\n \n     backtrack : bool, default True\n         Controls whether the tracker has the capability to \"backtrack\"\n \n     # NOTE: max_angle defined relative to zero-point rotation, not the\n     # system-plane normal\n    tracker_theta = np.clip(tracker_theta, -max_angle, max_angle)\n \n     # Calculate auxiliary angles\n     surface = calc_surface_orientation(tracker_theta, axis_tilt, axis_azimuth)",
    "output": " import pandas as pd\n from dataclasses import dataclass\n from abc import ABC, abstractmethod\nfrom typing import Optional, Union\n \n from pvlib._deprecation import deprecated, warn_deprecated\n \n         A value denoting the compass direction along which the axis of\n         rotation lies, measured east of north. [degrees]\n \n    max_angle : float or tuple, default 90\n        A value denoting the maximum rotation angle, in decimal degrees,\n         of the one-axis tracker from its horizontal position (horizontal\n        if axis_tilt = 0). If a float is provided, it represents the maximum\n        rotation angle, and the minimum rotation angle is assumed to be the\n        opposite of the maximum angle. If a tuple of (min_angle, max_angle) is\n        provided, it represents both the minimum and maximum rotation angles.\n\n        A rotation to 'max_angle' is a counter-clockwise rotation about the\n        y-axis of the tracker coordinate system. For example, for a tracker\n        with 'axis_azimuth' oriented to the south, a rotation to 'max_angle'\n        is towards the west, and a rotation toward 'min_angle' is in the\n        opposite direction, toward the east. Hence a max_angle of 180 degrees\n        (equivalent to max_angle = (-180, 180)) allows the tracker to achieve\n        its full rotation capability.\n \n     backtrack : bool, default True\n         Controls whether the tracker has the capability to \"backtrack\"\n     \"\"\"\n     axis_tilt: float = 0.0\n     axis_azimuth: float = 0.0\n    max_angle: Union[float, tuple] = 90.0\n     backtrack: bool = True\n     gcr: float = 2.0/7.0\n     cross_axis_tilt: float = 0.0\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n         A value denoting the compass direction along which the axis of\n         rotation lies. Measured in decimal degrees east of north.\n \n    max_angle : float or tuple, default 90\n         A value denoting the maximum rotation angle, in decimal degrees,\n         of the one-axis tracker from its horizontal position (horizontal\n        if axis_tilt = 0). If a float is provided, it represents the maximum\n        rotation angle, and the minimum rotation angle is assumed to be the\n        opposite of the maximum angle. If a tuple of (min_angle, max_angle) is\n        provided, it represents both the minimum and maximum rotation angles.\n\n        A rotation to 'max_angle' is a counter-clockwise rotation about the\n        y-axis of the tracker coordinate system. For example, for a tracker\n        with 'axis_azimuth' oriented to the south, a rotation to 'max_angle'\n        is towards the west, and a rotation toward 'min_angle' is in the\n        opposite direction, toward the east. Hence a max_angle of 180 degrees\n        (equivalent to max_angle = (-180, 180)) allows the tracker to achieve\n        its full rotation capability.\n \n     backtrack : bool, default True\n         Controls whether the tracker has the capability to \"backtrack\"\n \n     # NOTE: max_angle defined relative to zero-point rotation, not the\n     # system-plane normal\n\n    # Determine minimum and maximum rotation angles based on max_angle.\n    # If max_angle is a single value, assume min_angle is the negative.\n    if np.isscalar(max_angle):\n        min_angle = -max_angle\n    else:\n        min_angle, max_angle = max_angle\n\n    # Clip tracker_theta between the minimum and maximum angles.\n    tracker_theta = np.clip(tracker_theta, min_angle, max_angle)\n \n     # Calculate auxiliary angles\n     surface = calc_surface_orientation(tracker_theta, axis_tilt, axis_azimuth)"
  },
  {
    "instruction": "ModelChain should accept albedo in weather dataframe\n**Is your feature request related to a problem? Please describe.**\r\nAlbedo is treated as a scalar constant in pvlib, but it is of course a function of the weather and changes throughout the year.  Albedo is currently set in the PVSystem or Array and cannot be altered using the ModelChain.  Albedo is provided as a timeseries from many weather data services as well as through NREL's NSRBD and it would be useful to provide this data to the ModelChain.\r\n\r\nAdditionally, treating albedo as property of the Array seems to conflict with the [PVSystem Design Philosophy](https://pvlib-python.readthedocs.io/en/stable/pvsystem.html#design-philosophy), which highlights the separation of the PV system and the exogenous variables, such as the weather.\r\n\r\n**Describe the solution you'd like**\r\nModelChain.run_model() should accept albedo in the weather dataframe, like temperature and ghi.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep.  This probably has some side effects that we are unaware of.\r\n\n",
    "input": "         Extraterrestrial radiation [W/m^2], defaults to 1364[W/m^2]\n     asymmetry : numeric\n         Asymmetry factor, defaults to 0.85\n    albedo : numeric\n        Albedo, defaults to 0.2\n \n     Returns\n     -------\ndiff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\n def get_total_irradiance(surface_tilt, surface_azimuth,\n                          solar_zenith, solar_azimuth,\n                          dni, ghi, dhi, dni_extra=None, airmass=None,\n                         albedo=.25, surface_type=None,\n                          model='isotropic',\n                          model_perez='allsitescomposite1990'):\n     r\"\"\"\n     airmass : None or numeric, default None\n         Relative airmass (not adjusted for pressure). [unitless]\n     albedo : numeric, default 0.25\n        Surface albedo. [unitless]\n     surface_type : None or str, default None\n         Surface type. See :py:func:`~pvlib.irradiance.get_ground_diffuse` for\n         the list of accepted values.\n         applied.\n \n     albedo : numeric, default 0.25\n        Surface albedo\n \n     model : String, default 'perez'\n         Irradiance model.  See :py:func:`get_sky_diffuse` for allowed values.\ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n             **kwargs)\n         return self\n \n     def _prep_inputs_airmass(self):\n         \"\"\"\n         Assign airmass\n \n         Parameters\n         ----------\n        weather : DataFrame, or tuple or list of DataFrame\n             Required column names include ``'dni'``, ``'ghi'``, ``'dhi'``.\n            Optional column names are ``'wind_speed'``, ``'temp_air'``; if not\n             provided, air temperature of 20 C and wind speed\n            of 0 m/s will be added to the DataFrame.\n \n             If `weather` is a tuple or list, it must be of the same length and\n             order as the Arrays of the ModelChain's PVSystem.\n         Notes\n         -----\n         Assigns attributes to ``results``: ``times``, ``weather``,\n        ``solar_position``, ``airmass``, ``total_irrad``, ``aoi``\n \n         See also\n         --------\n \n         self._prep_inputs_solar_pos(weather)\n         self._prep_inputs_airmass()\n \n         # PVSystem.get_irradiance and SingleAxisTracker.get_irradiance\n         # and PVSystem.get_aoi and SingleAxisTracker.get_aoi\n             _tuple_from_dfs(self.results.weather, 'dni'),\n             _tuple_from_dfs(self.results.weather, 'ghi'),\n             _tuple_from_dfs(self.results.weather, 'dhi'),\n             airmass=self.results.airmass['airmass_relative'],\n             model=self.transposition_model\n         )\n         Parameters\n         ----------\n         weather : DataFrame, or tuple or list of DataFrame\n            Irradiance column names must include ``'dni'``, ``'ghi'``, and\n            ``'dhi'``. If optional columns ``'temp_air'`` and ``'wind_speed'``\n             are not provided, air temperature of 20 C and wind speed of 0 m/s\n             are added to the DataFrame. If optional column\n             ``'cell_temperature'`` is provided, these values are used instead\n            of `temperature_model`. If optional column `module_temperature`\n             is provided, `temperature_model` must be ``'sapm'``.\n \n            If list or tuple, must be of the same length and order as the\n            Arrays of the ModelChain's PVSystem.\n \n         Returns\n         -------\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         a single array is created from the other parameters (e.g.\n         `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n         if length of arrays is 0 a ValueError is raised. If `arrays` is\n        specified the following parameters are ignored:\n \n         - `surface_tilt`\n         - `surface_azimuth`\n         North=0, East=90, South=180, West=270.\n \n     albedo : None or float, default None\n        The ground albedo. If ``None``, will attempt to use\n        ``surface_type`` and ``irradiance.SURFACE_ALBEDOS``\n        to lookup albedo.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n        for valid values.\n \n     module : None or string, default None\n         The model name of the modules.\n \n     @_unwrap_single_value\n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, model='haydavies',\n                       **kwargs):\n         \"\"\"\n         Uses the :py:func:`irradiance.get_total_irradiance` function to\n         calculate the plane of array irradiance components on a tilted\n        surface defined by ``self.surface_tilt``,\n        ``self.surface_azimuth``, and ``self.albedo``.\n \n         Parameters\n         ----------\n        solar_zenith : float or Series.\n             Solar zenith angle.\n        solar_azimuth : float or Series.\n             Solar azimuth angle.\n         dni : float or Series or tuple of float or Series\n            Direct Normal Irradiance\n         ghi : float or Series or tuple of float or Series\n            Global horizontal irradiance\n         dhi : float or Series or tuple of float or Series\n            Diffuse horizontal irradiance\n        dni_extra : None, float or Series, default None\n            Extraterrestrial direct normal irradiance\n         airmass : None, float or Series, default None\n            Airmass\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame or tuple of DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n         \"\"\"\n         dni = self._validate_per_array(dni, system_wide=True)\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n         return tuple(\n             array.get_irradiance(solar_zenith, solar_azimuth,\n                                  dni, ghi, dhi,\n                                 dni_extra, airmass, model,\n                                  **kwargs)\n            for array, dni, ghi, dhi in zip(\n                self.arrays, dni, ghi, dhi\n             )\n         )\n \n         If not provided, a FixedMount with zero tilt is used.\n \n     albedo : None or float, default None\n        The ground albedo. If ``None``, will attempt to use\n        ``surface_type`` to look up an albedo value in\n        ``irradiance.SURFACE_ALBEDOS``. If a surface albedo\n        cannot be found then 0.25 is used.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n        for valid values.\n \n     module : None or string, default None\n         The model name of the modules.\n                               solar_zenith, solar_azimuth)\n \n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, model='haydavies',\n                       **kwargs):\n         \"\"\"\n         Get plane of array irradiance components.\n \n         Uses the :py:func:`pvlib.irradiance.get_total_irradiance` function to\n         calculate the plane of array irradiance components for a surface\n        defined by ``self.surface_tilt`` and ``self.surface_azimuth`` with\n        albedo ``self.albedo``.\n \n         Parameters\n         ----------\n         solar_azimuth : float or Series.\n             Solar azimuth angle.\n         dni : float or Series\n            Direct Normal Irradiance\n        ghi : float or Series\n             Global horizontal irradiance\n         dhi : float or Series\n            Diffuse horizontal irradiance\n         dni_extra : None, float or Series, default None\n            Extraterrestrial direct normal irradiance\n         airmass : None, float or Series, default None\n            Airmass\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n         \"\"\"\n         # not needed for all models, but this is easier\n         if dni_extra is None:\n             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n                                                orientation['surface_azimuth'],\n                                                solar_zenith, solar_azimuth,\n                                                dni, ghi, dhi,\n                                                dni_extra=dni_extra,\n                                                airmass=airmass,\n                                                model=model,\n                                               albedo=self.albedo,\n                                                **kwargs)\n \n     def get_iam(self, aoi, iam_model='physical'):\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n     @_unwrap_single_value\n     def get_irradiance(self, surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, model='haydavies',\n                        **kwargs):\n         \"\"\"\n         Uses the :func:`irradiance.get_total_irradiance` function to\n             Global horizontal irradiance\n         dhi : float or Series\n             Diffuse horizontal irradiance\n         dni_extra : float or Series, default None\n             Extraterrestrial direct normal irradiance\n         airmass : float or Series, default None\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n \n         return tuple(\n             irradiance.get_total_irradiance(\n                 surface_tilt,\n                 dni_extra=dni_extra,\n                 airmass=airmass,\n                 model=model,\n                albedo=self.arrays[0].albedo,\n                 **kwargs)\n            for array, dni, ghi, dhi in zip(\n                self.arrays, dni, ghi, dhi\n             )\n         )\n ",
    "output": "         Extraterrestrial radiation [W/m^2], defaults to 1364[W/m^2]\n     asymmetry : numeric\n         Asymmetry factor, defaults to 0.85\n    albedo : numeric, default 0.2\n        Ground surface albedo. [unitless]\n \n     Returns\n     -------\ndiff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\n def get_total_irradiance(surface_tilt, surface_azimuth,\n                          solar_zenith, solar_azimuth,\n                          dni, ghi, dhi, dni_extra=None, airmass=None,\n                         albedo=0.25, surface_type=None,\n                          model='isotropic',\n                          model_perez='allsitescomposite1990'):\n     r\"\"\"\n     airmass : None or numeric, default None\n         Relative airmass (not adjusted for pressure). [unitless]\n     albedo : numeric, default 0.25\n        Ground surface albedo. [unitless]\n     surface_type : None or str, default None\n         Surface type. See :py:func:`~pvlib.irradiance.get_ground_diffuse` for\n         the list of accepted values.\n         applied.\n \n     albedo : numeric, default 0.25\n        Ground surface albedo. [unitless]\n \n     model : String, default 'perez'\n         Irradiance model.  See :py:func:`get_sky_diffuse` for allowed values.\ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n             **kwargs)\n         return self\n \n    def _prep_inputs_albedo(self, weather):\n        \"\"\"\n        Get albedo from weather\n        \"\"\"\n        try:\n            self.results.albedo = _tuple_from_dfs(weather, 'albedo')\n        except KeyError:\n            self.results.albedo = None\n        return self\n\n     def _prep_inputs_airmass(self):\n         \"\"\"\n         Assign airmass\n \n         Parameters\n         ----------\n        weather : DataFrame, or tuple or list of DataFrames\n             Required column names include ``'dni'``, ``'ghi'``, ``'dhi'``.\n            Optional column names are ``'wind_speed'``, ``'temp_air'``,\n            ``'albedo'``.\n\n            If optional columns ``'wind_speed'``, ``'temp_air'`` are not\n             provided, air temperature of 20 C and wind speed\n            of 0 m/s will be added to the `weather` DataFrame.\n\n            If optional column ``'albedo'`` is provided, albedo values in the\n            ModelChain's PVSystem.arrays are ignored.\n \n             If `weather` is a tuple or list, it must be of the same length and\n             order as the Arrays of the ModelChain's PVSystem.\n         Notes\n         -----\n         Assigns attributes to ``results``: ``times``, ``weather``,\n        ``solar_position``, ``airmass``, ``total_irrad``, ``aoi``, ``albedo``.\n \n         See also\n         --------\n \n         self._prep_inputs_solar_pos(weather)\n         self._prep_inputs_airmass()\n        self._prep_inputs_albedo(weather)\n \n         # PVSystem.get_irradiance and SingleAxisTracker.get_irradiance\n         # and PVSystem.get_aoi and SingleAxisTracker.get_aoi\n             _tuple_from_dfs(self.results.weather, 'dni'),\n             _tuple_from_dfs(self.results.weather, 'ghi'),\n             _tuple_from_dfs(self.results.weather, 'dhi'),\n            albedo=self.results.albedo,\n             airmass=self.results.airmass['airmass_relative'],\n             model=self.transposition_model\n         )\n         Parameters\n         ----------\n         weather : DataFrame, or tuple or list of DataFrame\n            Column names must include:\n\n            - ``'dni'``\n            - ``'ghi'``\n            - ``'dhi'``\n\n            Optional columns are:\n\n            - ``'temp_air'``\n            - ``'cell_temperature'``\n            - ``'module_temperature'``\n            - ``'wind_speed'``\n            - ``'albedo'``\n\n            If optional columns ``'temp_air'`` and ``'wind_speed'``\n             are not provided, air temperature of 20 C and wind speed of 0 m/s\n             are added to the DataFrame. If optional column\n             ``'cell_temperature'`` is provided, these values are used instead\n            of `temperature_model`. If optional column ``'module_temperature'``\n             is provided, `temperature_model` must be ``'sapm'``.\n \n            If optional column ``'albedo'`` is provided, ``'albedo'`` may not\n            be present on the ModelChain's PVSystem.Arrays.\n\n            If weather is a list or tuple, it must be of the same length and\n            order as the Arrays of the ModelChain's PVSystem.\n \n         Returns\n         -------\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         a single array is created from the other parameters (e.g.\n         `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n         if length of arrays is 0 a ValueError is raised. If `arrays` is\n        specified the following PVSystem parameters are ignored:\n \n         - `surface_tilt`\n         - `surface_azimuth`\n         North=0, East=90, South=180, West=270.\n \n     albedo : None or float, default None\n        Ground surface albedo. If ``None``, then ``surface_type`` is used\n        to look up a value in ``irradiance.SURFACE_ALBEDOS``.\n        If ``surface_type`` is also None then a ground surface albedo\n        of 0.25 is used. For time-dependent albedos, add ``'albedo'`` to\n        the input ``'weather'`` DataFrame for\n        :py:class:`pvlib.modelchain.ModelChain` methods.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for\n        valid values.\n \n     module : None or string, default None\n         The model name of the modules.\n \n     @_unwrap_single_value\n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       albedo=None, dni_extra=None, airmass=None,\n                       model='haydavies', **kwargs):\n         \"\"\"\n         Uses the :py:func:`irradiance.get_total_irradiance` function to\n         calculate the plane of array irradiance components on a tilted\n        surface defined by ``self.surface_tilt`` and ``self.surface_azimuth```.\n \n         Parameters\n         ----------\n        solar_zenith : float or Series\n             Solar zenith angle.\n        solar_azimuth : float or Series\n             Solar azimuth angle.\n         dni : float or Series or tuple of float or Series\n            Direct Normal Irradiance. [W/m2]\n         ghi : float or Series or tuple of float or Series\n            Global horizontal irradiance. [W/m2]\n         dhi : float or Series or tuple of float or Series\n            Diffuse horizontal irradiance. [W/m2]\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n        dni_extra : None, float, Series or tuple of float or Series,\n            default None\n            Extraterrestrial direct normal irradiance. [W/m2]\n         airmass : None, float or Series, default None\n            Airmass. [unitless]\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame or tuple of DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n\n        See also\n        --------\n        :py:func:`pvlib.irradiance.get_total_irradiance`\n         \"\"\"\n         dni = self._validate_per_array(dni, system_wide=True)\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n\n        albedo = self._validate_per_array(albedo, system_wide=True)\n\n         return tuple(\n             array.get_irradiance(solar_zenith, solar_azimuth,\n                                  dni, ghi, dhi,\n                                 albedo=albedo,\n                                 dni_extra=dni_extra, airmass=airmass,\n                                 model=model,\n                                  **kwargs)\n            for array, dni, ghi, dhi, albedo in zip(\n                self.arrays, dni, ghi, dhi, albedo\n             )\n         )\n \n         If not provided, a FixedMount with zero tilt is used.\n \n     albedo : None or float, default None\n        Ground surface albedo. If ``None``, then ``surface_type`` is used\n        to look up a value in ``irradiance.SURFACE_ALBEDOS``.\n        If ``surface_type`` is also None then a ground surface albedo\n        of 0.25 is used.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for valid\n        values.\n \n     module : None or string, default None\n         The model name of the modules.\n                               solar_zenith, solar_azimuth)\n \n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       albedo=None, dni_extra=None, airmass=None,\n                       model='haydavies', **kwargs):\n         \"\"\"\n         Get plane of array irradiance components.\n \n         Uses the :py:func:`pvlib.irradiance.get_total_irradiance` function to\n         calculate the plane of array irradiance components for a surface\n        defined by ``self.surface_tilt`` and ``self.surface_azimuth``.\n \n         Parameters\n         ----------\n         solar_azimuth : float or Series.\n             Solar azimuth angle.\n         dni : float or Series\n            Direct normal irradiance. [W/m2]\n        ghi : float or Series. [W/m2]\n             Global horizontal irradiance\n         dhi : float or Series\n            Diffuse horizontal irradiance. [W/m2]\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n         dni_extra : None, float or Series, default None\n            Extraterrestrial direct normal irradiance. [W/m2]\n         airmass : None, float or Series, default None\n            Airmass. [unitless]\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n\n        See also\n        --------\n        :py:func:`pvlib.irradiance.get_total_irradiance`\n         \"\"\"\n        if albedo is None:\n            albedo = self.albedo\n\n         # not needed for all models, but this is easier\n         if dni_extra is None:\n             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n                                                orientation['surface_azimuth'],\n                                                solar_zenith, solar_azimuth,\n                                                dni, ghi, dhi,\n                                               albedo=albedo,\n                                                dni_extra=dni_extra,\n                                                airmass=airmass,\n                                                model=model,\n                                                **kwargs)\n \n     def get_iam(self, aoi, iam_model='physical'):\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n     @_unwrap_single_value\n     def get_irradiance(self, surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       albedo=None, dni_extra=None, airmass=None,\n                       model='haydavies',\n                        **kwargs):\n         \"\"\"\n         Uses the :func:`irradiance.get_total_irradiance` function to\n             Global horizontal irradiance\n         dhi : float or Series\n             Diffuse horizontal irradiance\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n         dni_extra : float or Series, default None\n             Extraterrestrial direct normal irradiance\n         airmass : float or Series, default None\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n \n        if albedo is None:\n            # assign default albedo here because SingleAxisTracker\n            # initializes albedo to None\n            albedo = 0.25\n\n        albedo = self._validate_per_array(albedo, system_wide=True)\n\n         return tuple(\n             irradiance.get_total_irradiance(\n                 surface_tilt,\n                 dni_extra=dni_extra,\n                 airmass=airmass,\n                 model=model,\n                albedo=albedo,\n                 **kwargs)\n            for array, dni, ghi, dhi, albedo in zip(\n                self.arrays, dni, ghi, dhi, albedo\n             )\n         )\n "
  },
  {
    "instruction": "is vf_row_sky correct?\nhttps://github.com/pvlib/pvlib-python/blob/7e88d212c786d0ad334dce6fcafaf29339ff60ab/pvlib/bifacial/infinite_sheds.py#L146\r\n\r\nI think this should be:\r\n\r\n$$\\frac{1 + \\cos \\left( \\text{surface tilt} + \\psi_{t}\\ \\text{shaded} \\right)}{2}$$\r\n\r\nbecause in the reference frame of the module surface the angle pointing along the slant height to the sky is actually zero, $cos(0) = 1$, and the angle above the slant height to a horizontal line would be the `surface_tilt` itself, then the angle from the horizontal to the top of the next row is `psi_t_shaded` so finally this angle from the slant height all the way up to the top of the next row is `surface_tilt + psi_t_shaded`:\r\n\r\n![infinite_sheds](https://user-images.githubusercontent.com/1385621/218985907-7fced67c-ccff-439f-8fc8-0774026b9501.png)\r\n\r\nFor example, this is why if `psi_t_shaded` is zero, then the view factor should collapse to the isotropic view factor $(1+\\cos(\\beta))/2$ as given on the [PVPMC website modeling reference for POA sky diffuse](https://pvpmc.sandia.gov/modeling-steps/1-weather-design-inputs/plane-of-array-poa-irradiance/calculating-poa-irradiance/poa-sky-diffuse/isotropic-sky-diffuse-model/).\r\n\r\nThe actual value difference between the two formulas can be quite small when `psi_t_shaded` is close to zero (_eg_ less than 5&deg;), but it's significant when as the masking angle is larger (_eg_ greater than 5&deg;).\n",
    "input": " import pandas as pd\n from pvlib.tools import cosd, sind, tand\n from pvlib.bifacial import utils\nfrom pvlib.shading import masking_angle\n from pvlib.irradiance import beam_component, aoi, haydavies\n \n \ndef _vf_ground_sky_integ(surface_tilt, surface_azimuth, gcr, height,\n                         pitch, max_rows=10, npoints=100, vectorize=False):\n    \"\"\"\n    Integrated view factor to the sky from the ground underneath\n    interior rows of the array.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    surface_azimuth : numeric\n        Surface azimuth angles in decimal degrees east of north\n        (e.g. North = 0, South = 180, East = 90, West = 270).\n        ``surface_azimuth`` must be >=0 and <=360.\n    gcr : float\n        Ratio of row slant length to row spacing (pitch). [unitless]\n    height : float\n        Height of the center point of the row above the ground; must be in the\n        same units as ``pitch``.\n    pitch : float\n        Distance between two rows. Must be in the same units as ``height``.\n    max_rows : int, default 10\n        Maximum number of rows to consider in front and behind the current row.\n    npoints : int, default 100\n        Number of points used to discretize distance along the ground.\n    vectorize : bool, default False\n        If True, vectorize the view factor calculation across ``surface_tilt``.\n        This increases speed with the cost of increased memory usage.\n\n    Returns\n    -------\n    fgnd_sky : numeric\n        Integration of view factor over the length between adjacent, interior\n        rows.  Shape matches that of ``surface_tilt``. [unitless]\n    \"\"\"\n    # Abuse utils._vf_ground_sky_2d by supplying surface_tilt in place\n    # of a signed rotation. This is OK because\n    # 1) z span the full distance between 2 rows, and\n    # 2) max_rows is set to be large upstream, and\n    # 3) _vf_ground_sky_2d considers [-max_rows, +max_rows]\n    # The VFs to the sky will thus be symmetric around z=0.5\n    z = np.linspace(0, 1, npoints)\n    rotation = np.atleast_1d(surface_tilt)\n    if vectorize:\n        fz_sky = utils._vf_ground_sky_2d(z, rotation, gcr, pitch, height,\n                                         max_rows)\n    else:\n        fz_sky = np.zeros((npoints, len(rotation)))\n        for k, r in enumerate(rotation):\n            vf = utils._vf_ground_sky_2d(z, r, gcr, pitch, height, max_rows)\n            fz_sky[:, k] = vf[:, 0]  # remove spurious rotation dimension\n    # calculate the integrated view factor for all of the ground between rows\n    return np.trapz(fz_sky, z, axis=0)\n\n\n def _poa_ground_shadows(poa_ground, f_gnd_beam, df, vf_gnd_sky):\n     \"\"\"\n     Reduce ground-reflected irradiance to the tilted plane (poa_ground) to\n     return poa_ground * (f_gnd_beam*(1 - df) + df*vf_gnd_sky)\n \n \ndef _vf_row_sky_integ(f_x, surface_tilt, gcr, npoints=100):\n    \"\"\"\n     Integrated view factors from the shaded and unshaded parts of\n     the row slant height to the sky.\n \n     npoints : int, default 100\n         Number of points for integration. [unitless]\n \n    Returns\n    -------\n    vf_shade_sky_integ : numeric\n        Integrated view factor from the shaded part of the row to the sky.\n        [unitless]\n    vf_noshade_sky_integ : numeric\n        Integrated view factor from the unshaded part of the row to the sky.\n        [unitless]\n \n    Notes\n    -----\n    The view factor to the sky at a point x along the row slant height is\n    given by\n \n    .. math ::\n        \\\\large{f_{sky} = \\frac{1}{2} \\\\left(\\\\cos\\\\left(\\\\psi_t\\\\right) +\n        \\\\cos \\\\left(\\\\beta\\\\right) \\\\right)\n \n    where :math:`\\\\psi_t` is the angle from horizontal of the line from point\n    x to the top of the facing row, and :math:`\\\\beta` is the surface tilt.\n \n    View factors are integrated separately over shaded and unshaded portions\n    of the row slant height.\n \n    \"\"\"\n    # handle Series inputs\n    surface_tilt = np.array(surface_tilt)\n    cst = cosd(surface_tilt)\n    # shaded portion\n    x = np.linspace(0, f_x, num=npoints)\n    psi_t_shaded = masking_angle(surface_tilt, gcr, x)\n    y = 0.5 * (cosd(psi_t_shaded) + cst)\n    # integrate view factors from each point in the discretization. This is an\n    # improvement over the algorithm described in [2]\n    vf_shade_sky_integ = np.trapz(y, x, axis=0)\n    # unshaded portion\n    x = np.linspace(f_x, 1., num=npoints)\n    psi_t_unshaded = masking_angle(surface_tilt, gcr, x)\n    y = 0.5 * (cosd(psi_t_unshaded) + cst)\n    vf_noshade_sky_integ = np.trapz(y, x, axis=0)\n    return vf_shade_sky_integ, vf_noshade_sky_integ\n\n\ndef _poa_sky_diffuse_pv(f_x, dhi, vf_shade_sky_integ, vf_noshade_sky_integ):\n    \"\"\"\n    Sky diffuse POA from integrated view factors combined for both shaded and\n    unshaded parts of the surface.\n \n     Parameters\n     ----------\n         direct irradiance. [unitless]\n     dhi : numeric\n         Diffuse horizontal irradiance (DHI). [W/m^2]\n    vf_shade_sky_integ : numeric\n        Integrated view factor from the shaded part of the row to the sky.\n        [unitless]\n    vf_noshade_sky_integ : numeric\n        Integrated view factor from the unshaded part of the row to the sky.\n        [unitless]\n\n    Returns\n    -------\n    poa_sky_diffuse_pv : numeric\n        Total sky diffuse irradiance incident on the PV surface. [W/m^2]\n    \"\"\"\n    return dhi * (f_x * vf_shade_sky_integ + (1 - f_x) * vf_noshade_sky_integ)\n\n\ndef _ground_angle(x, surface_tilt, gcr):\n    \"\"\"\n    Angle from horizontal of the line from a point x on the row slant length\n    to the bottom of the facing row.\n\n    The angles are clockwise from horizontal, rather than the usual\n    counterclockwise direction.\n\n    Parameters\n    ----------\n    x : numeric\n        fraction of row slant length from bottom, ``x = 0`` is at the row\n        bottom, ``x = 1`` is at the top of the row.\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n     gcr : float\n         ground coverage ratio, ratio of row slant length to row spacing.\n         [unitless]\n\n    Returns\n    -------\n    psi : numeric\n        Angle [degree].\n    \"\"\"\n    #  : \\\\            \\\n    #  :  \\\\            \\\n    #  :   \\\\            \\\n    #  :    \\\\            \\  facing row\n    #  :     \\\\.___________\\\n    #  :       \\  ^*-.  psi \\\n    #  :        \\  x   *-.   \\\n    #  :         \\  v      *-.\\\n    #  :          \\<-----P---->\\\n\n    x1 = gcr * x * sind(surface_tilt)\n    x2 = gcr * x * cosd(surface_tilt) + 1\n    psi = np.arctan2(x1, x2)  # do this first because it handles 0 / 0\n    return np.rad2deg(psi)\n\n\ndef _vf_row_ground(x, surface_tilt, gcr):\n    \"\"\"\n    View factor from a point x on the row to the ground.\n\n    Parameters\n    ----------\n    x : numeric\n        Fraction of row slant height from the bottom. [unitless]\n     surface_tilt : numeric\n         Surface tilt angle in degrees from horizontal, e.g., surface facing up\n         = 0, surface facing horizon = 90. [degree]\n    gcr : float\n        Ground coverage ratio, ratio of row slant length to row spacing.\n        [unitless]\n \n     Returns\n     -------\n    vf : numeric\n        View factor from the point at x to the ground. [unitless]\n\n    \"\"\"\n    cst = cosd(surface_tilt)\n    # angle from horizontal at the point x on the row slant height to the\n    # bottom of the facing row\n    psi_t_shaded = _ground_angle(x, surface_tilt, gcr)\n    # view factor from the point on the row to the ground\n    return 0.5 * (cosd(psi_t_shaded) - cst)\n\n\ndef _vf_row_ground_integ(f_x, surface_tilt, gcr, npoints=100):\n     \"\"\"\n    View factors to the ground from shaded and unshaded parts of a row.\n\n    Parameters\n    ----------\n    f_x : numeric\n        Fraction of row slant height from the bottom that is shaded from\n        direct irradiance. [unitless]\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : float\n        Ground coverage ratio, ratio of row slant length to row spacing.\n        [unitless]\n    npoints : int, default 100\n        Number of points for integration. [unitless]\n\n    Returns\n    -------\n    vf_shade_ground_integ : numeric\n        View factor from the shaded portion of the row to the ground.\n        [unitless]\n    vf_noshade_ground_integ : numeric\n        View factor from the unshaded portion of the row to the ground.\n        [unitless]\n\n    Notes\n    -----\n    The view factor to the ground at a point x along the row slant height is\n    given by\n\n    .. math ::\n        \\\\large{f_{gr} = \\frac{1}{2} \\\\left(\\\\cos\\\\left(\\\\psi_t\\\\right) -\n        \\\\cos \\\\left(\\\\beta\\\\right) \\\\right)\n \n    where :math:`\\\\psi_t` is the angle from horizontal of the line from point\n    x to the bottom of the facing row, and :math:`\\\\beta` is the surface tilt.\n \n    Each view factor is integrated over the relevant portion of the row\n    slant height.\n    \"\"\"\n    # handle Series inputs\n    surface_tilt = np.array(surface_tilt)\n    # shaded portion of row slant height\n    x = np.linspace(0, f_x, num=npoints)\n    # view factor from the point on the row to the ground\n    y = _vf_row_ground(x, surface_tilt, gcr)\n    # integrate view factors along the shaded portion of the row slant height.\n    # This is an improvement over the algorithm described in [2]\n    vf_shade_ground_integ = np.trapz(y, x, axis=0)\n\n    # unshaded portion of row slant height\n    x = np.linspace(f_x, 1., num=npoints)\n    # view factor from the point on the row to the ground\n    y = _vf_row_ground(x, surface_tilt, gcr)\n    # integrate view factors along the unshaded portion.\n    # This is an improvement over the algorithm described in [2]\n    vf_noshade_ground_integ = np.trapz(y, x, axis=0)\n\n    return vf_shade_ground_integ, vf_noshade_ground_integ\n\n\ndef _poa_ground_pv(f_x, poa_ground, f_gnd_pv_shade, f_gnd_pv_noshade):\n     \"\"\"\n     Reduce ground-reflected irradiance to account for limited view of the\n     ground from the row surface.\n \n     Parameters\n     ----------\n    f_x : numeric\n        Fraction of row slant height from the bottom that is shaded from\n        direct irradiance. [unitless]\n     poa_ground : numeric\n         Ground-reflected irradiance that would reach the row surface if the\n         full ground was visible. poa_gnd_sky accounts for limited view of the\n         sky from the ground. [W/m^2]\n    f_gnd_pv_shade : numeric\n        fraction of ground visible from shaded part of PV surface. [unitless]\n    f_gnd_pv_noshade : numeric\n        fraction of ground visible from unshaded part of PV surface. [unitless]\n \n     Returns\n     -------\n     numeric\n         Ground diffuse irradiance on the row plane. [W/m^2]\n     \"\"\"\n    return poa_ground * (f_x * f_gnd_pv_shade + (1 - f_x) * f_gnd_pv_noshade)\n \n \n def _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n     # adjacent rows interior to the array\n     # method differs from [1], Eq. 7 and Eq. 8; height is defined at row\n     # center rather than at row lower edge as in [1].\n    vf_gnd_sky = _vf_ground_sky_integ(\n        surface_tilt, surface_azimuth, gcr, height, pitch, max_rows, npoints,\n         vectorize)\n     # fraction of row slant height that is shaded from direct irradiance\n     f_x = _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n                            surface_azimuth, gcr)\n \n    # Integrated view factors to the sky from the shaded and unshaded parts of\n    # the row slant height\n    # Differs from [1] Eq. 15 and Eq. 16. Here, we integrate over each\n    # interval (shaded or unshaded) rather than averaging values at each\n    # interval's end points.\n    vf_shade_sky, vf_noshade_sky = _vf_row_sky_integ(\n        f_x, surface_tilt, gcr, npoints)\n\n    # view factors from the ground to shaded and unshaded portions of the row\n    # slant height\n    # Differs from [1] Eq. 17 and Eq. 18. Here, we integrate over each\n    # interval (shaded or unshaded) rather than averaging values at each\n    # interval's end points.\n    f_gnd_pv_shade, f_gnd_pv_noshade = _vf_row_ground_integ(\n        f_x, surface_tilt, gcr, npoints)\n\n     # Total sky diffuse received by both shaded and unshaded portions\n    poa_sky_pv = _poa_sky_diffuse_pv(\n        f_x, dhi, vf_shade_sky, vf_noshade_sky)\n \n     # irradiance reflected from the ground before accounting for shadows\n     # and restricted views\n     # the usual ground-reflected irradiance includes the single row to ground\n     # view factor (1 - cos(tilt))/2, and Eq. 10, 11 and later multiply\n     # this quantity by a ratio of view factors.\n    poa_gnd_pv = _poa_ground_pv(\n        f_x, ground_diffuse, f_gnd_pv_shade, f_gnd_pv_noshade)\n \n     # add sky and ground-reflected irradiance on the row by irradiance\n     # component\ndiff --git a/pvlib/bifacial/utils.py b/pvlib/bifacial/utils.py\n import numpy as np\n from pvlib.tools import sind, cosd, tand\n \n def _solar_projection_tangent(solar_zenith, solar_azimuth, surface_azimuth):\n     \"\"\"\n     Tangent of the angle between the zenith vector and the sun vector\n     return f_gnd_beam  # 1 - min(1, abs()) < 1 always\n \n \ndef _vf_ground_sky_2d(x, rotation, gcr, pitch, height, max_rows=10):\n     r\"\"\"\n     Calculate the fraction of the sky dome visible from point x on the ground.\n \n \n     Parameters\n     ----------\n    x : numeric\n        Position on the ground between two rows, as a fraction of the pitch.\n        x = 0 corresponds to the point on the ground directly below the\n        center point of a row. Positive x is towards the right. [unitless]\n     rotation : numeric\n         Rotation angle of the row's right edge relative to row center.\n         [degree]\n     gcr : float\n         Ratio of the row slant length to the row spacing (pitch). [unitless]\n     height : float\n         Height of the center point of the row above the ground; must be in the\n         same units as ``pitch``.\n     np.clip(next_edge, a_min=0., a_max=None, out=next_edge)\n     vf = np.sum(next_edge, axis=-1) / 2\n     return vf\ndiff --git a/pvlib/shading.py b/pvlib/shading.py\n from pvlib.tools import sind, cosd\n \n \n def masking_angle(surface_tilt, gcr, slant_height):\n     \"\"\"\n     The elevation angle below which diffuse irradiance is blocked.",
    "output": " import pandas as pd\n from pvlib.tools import cosd, sind, tand\n from pvlib.bifacial import utils\n from pvlib.irradiance import beam_component, aoi, haydavies\n \n \n def _poa_ground_shadows(poa_ground, f_gnd_beam, df, vf_gnd_sky):\n     \"\"\"\n     Reduce ground-reflected irradiance to the tilted plane (poa_ground) to\n     return poa_ground * (f_gnd_beam*(1 - df) + df*vf_gnd_sky)\n \n \ndef _poa_sky_diffuse_pv(dhi, gcr, surface_tilt):\n    r\"\"\"\n     Integrated view factors from the shaded and unshaded parts of\n     the row slant height to the sky.\n \n     npoints : int, default 100\n         Number of points for integration. [unitless]\n \n    A detailed calculation would be\n \n        dhi * (f_x * vf_shade_sky_integ + (1 - f_x) * vf_noshade_sky_integ)\n \n    where vf_shade_sky_integ is the average view factor between 0 and f_x\n    (the shaded portion). But the average view factor is\n \n        1/(f_x - 0) Integral_0^f_x vf(x) dx\n \n    so the detailed calculation is equivalent to\n \n        dhi * 1/(1 - 0) Integral_0^1 vf(x) dx\n \n     Parameters\n     ----------\n         direct irradiance. [unitless]\n     dhi : numeric\n         Diffuse horizontal irradiance (DHI). [W/m^2]\n     gcr : float\n         ground coverage ratio, ratio of row slant length to row spacing.\n         [unitless]\n     surface_tilt : numeric\n         Surface tilt angle in degrees from horizontal, e.g., surface facing up\n         = 0, surface facing horizon = 90. [degree]\n \n     Returns\n     -------\n    poa_sky_diffuse_pv : numeric\n        Total sky diffuse irradiance incident on the PV surface. [W/m^2]\n     \"\"\"\n    vf_integ = utils.vf_row_sky_2d_integ(surface_tilt, gcr, 0., 1.)\n    return dhi * vf_integ\n \n \ndef _poa_ground_pv(poa_ground, gcr, surface_tilt):\n     \"\"\"\n     Reduce ground-reflected irradiance to account for limited view of the\n     ground from the row surface.\n \n     Parameters\n     ----------\n     poa_ground : numeric\n         Ground-reflected irradiance that would reach the row surface if the\n         full ground was visible. poa_gnd_sky accounts for limited view of the\n         sky from the ground. [W/m^2]\n    gcr : float\n        ground coverage ratio, ratio of row slant length to row spacing.\n        [unitless]\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n \n     Returns\n     -------\n     numeric\n         Ground diffuse irradiance on the row plane. [W/m^2]\n     \"\"\"\n    vf_integ = utils.vf_row_ground_2d_integ(surface_tilt, gcr, 0., 1.)\n    return poa_ground * vf_integ\n \n \n def _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n     # adjacent rows interior to the array\n     # method differs from [1], Eq. 7 and Eq. 8; height is defined at row\n     # center rather than at row lower edge as in [1].\n    vf_gnd_sky = utils.vf_ground_sky_2d_integ(\n        surface_tilt, gcr, height, pitch, max_rows, npoints,\n         vectorize)\n     # fraction of row slant height that is shaded from direct irradiance\n     f_x = _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n                            surface_azimuth, gcr)\n \n     # Total sky diffuse received by both shaded and unshaded portions\n    poa_sky_pv = _poa_sky_diffuse_pv(dhi, gcr, surface_tilt)\n \n     # irradiance reflected from the ground before accounting for shadows\n     # and restricted views\n     # the usual ground-reflected irradiance includes the single row to ground\n     # view factor (1 - cos(tilt))/2, and Eq. 10, 11 and later multiply\n     # this quantity by a ratio of view factors.\n    poa_gnd_pv = _poa_ground_pv(ground_diffuse, gcr, surface_tilt)\n \n     # add sky and ground-reflected irradiance on the row by irradiance\n     # component\ndiff --git a/pvlib/bifacial/utils.py b/pvlib/bifacial/utils.py\n import numpy as np\n from pvlib.tools import sind, cosd, tand\n \n\n def _solar_projection_tangent(solar_zenith, solar_azimuth, surface_azimuth):\n     \"\"\"\n     Tangent of the angle between the zenith vector and the sun vector\n     return f_gnd_beam  # 1 - min(1, abs()) < 1 always\n \n \ndef vf_ground_sky_2d(rotation, gcr, x, pitch, height, max_rows=10):\n     r\"\"\"\n     Calculate the fraction of the sky dome visible from point x on the ground.\n \n \n     Parameters\n     ----------\n     rotation : numeric\n         Rotation angle of the row's right edge relative to row center.\n         [degree]\n     gcr : float\n         Ratio of the row slant length to the row spacing (pitch). [unitless]\n    x : numeric\n        Position on the ground between two rows, as a fraction of the pitch.\n        x = 0 corresponds to the point on the ground directly below the\n        center point of a row. Positive x is towards the right. [unitless]\n     height : float\n         Height of the center point of the row above the ground; must be in the\n         same units as ``pitch``.\n     np.clip(next_edge, a_min=0., a_max=None, out=next_edge)\n     vf = np.sum(next_edge, axis=-1) / 2\n     return vf\n\n\ndef vf_ground_sky_2d_integ(surface_tilt, gcr, height, pitch, max_rows=10,\n                           npoints=100, vectorize=False):\n    \"\"\"\n    Integrated view factor to the sky from the ground underneath\n    interior rows of the array.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : float\n        Ratio of row slant length to row spacing (pitch). [unitless]\n    height : float\n        Height of the center point of the row above the ground; must be in the\n        same units as ``pitch``.\n    pitch : float\n        Distance between two rows. Must be in the same units as ``height``.\n    max_rows : int, default 10\n        Maximum number of rows to consider in front and behind the current row.\n    npoints : int, default 100\n        Number of points used to discretize distance along the ground.\n    vectorize : bool, default False\n        If True, vectorize the view factor calculation across ``surface_tilt``.\n        This increases speed with the cost of increased memory usage.\n\n    Returns\n    -------\n    fgnd_sky : numeric\n        Integration of view factor over the length between adjacent, interior\n        rows.  Shape matches that of ``surface_tilt``. [unitless]\n    \"\"\"\n    # Abuse vf_ground_sky_2d by supplying surface_tilt in place\n    # of a signed rotation. This is OK because\n    # 1) z span the full distance between 2 rows, and\n    # 2) max_rows is set to be large upstream, and\n    # 3) _vf_ground_sky_2d considers [-max_rows, +max_rows]\n    # The VFs to the sky will thus be symmetric around z=0.5\n    z = np.linspace(0, 1, npoints)\n    rotation = np.atleast_1d(surface_tilt)\n    if vectorize:\n        fz_sky = vf_ground_sky_2d(rotation, gcr, z, pitch, height, max_rows)\n    else:\n        fz_sky = np.zeros((npoints, len(rotation)))\n        for k, r in enumerate(rotation):\n            vf = vf_ground_sky_2d(r, gcr, z, pitch, height, max_rows)\n            fz_sky[:, k] = vf[:, 0]  # remove spurious rotation dimension\n    # calculate the integrated view factor for all of the ground between rows\n    return np.trapz(fz_sky, z, axis=0)\n\n\ndef _vf_poly(surface_tilt, gcr, x, delta):\n    r'''\n    A term common to many 2D view factor calculations\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : numeric\n        Ratio of the row slant length to the row spacing (pitch). [unitless]\n    x : numeric\n        Position on the row's slant length, as a fraction of the slant length.\n        x=0 corresponds to the bottom of the row. [unitless]\n    delta : -1 or +1\n        A sign indicator for the linear term of the polynomial\n\n    Returns\n    -------\n    numeric\n    '''\n    a = 1 / gcr\n    c = cosd(surface_tilt)\n    return np.sqrt(a*a + 2*delta*a*c*x + x*x)\n\n\ndef vf_row_sky_2d(surface_tilt, gcr, x):\n    r'''\n    Calculate the view factor to the sky from a point x on a row surface.\n\n    Assumes a PV system of infinitely long rows with uniform pitch on\n    horizontal ground. The view to the sky is restricted by the row's surface\n    tilt and the top of the adjacent row.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : numeric\n        Ratio of the row slant length to the row spacing (pitch). [unitless]\n    x : numeric\n        Position on the row's slant length, as a fraction of the slant length.\n        x=0 corresponds to the bottom of the row. [unitless]\n\n    Returns\n    -------\n    vf : numeric\n        Fraction of the sky dome visible from the point x. [unitless]\n\n    '''\n    p = _vf_poly(surface_tilt, gcr, 1 - x, -1)\n    return 0.5*(1 + (1/gcr * cosd(surface_tilt) - (1 - x)) / p)\n\n\ndef vf_row_sky_2d_integ(surface_tilt, gcr, x0=0, x1=1):\n    r'''\n    Calculate the average view factor to the sky from a segment of the row\n    surface between x0 and x1.\n\n    Assumes a PV system of infinitely long rows with uniform pitch on\n    horizontal ground. The view to the sky is restricted by the row's surface\n    tilt and the top of the adjacent row.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : numeric\n        Ratio of the row slant length to the row spacing (pitch). [unitless]\n    x0 : numeric, default 0\n        Position on the row's slant length, as a fraction of the slant length.\n        x0=0 corresponds to the bottom of the row. x0 should be less than x1.\n        [unitless]\n    x1 : numeric, default 1\n        Position on the row's slant length, as a fraction of the slant length.\n        x1 should be greater than x0. [unitless]\n\n    Returns\n    -------\n    vf : numeric\n        Average fraction of the sky dome visible from points in the segment\n        from x0 to x1. [unitless]\n\n    '''\n    u = np.abs(x1 - x0)\n    p0 = _vf_poly(surface_tilt, gcr, 1 - x0, -1)\n    p1 = _vf_poly(surface_tilt, gcr, 1 - x1, -1)\n    with np.errstate(divide='ignore'):\n        result = np.where(u < 1e-6,\n                          vf_row_sky_2d(surface_tilt, gcr, x0),\n                          0.5*(1 + 1/u * (p1 - p0))\n                          )\n    return result\n\n\ndef vf_row_ground_2d(surface_tilt, gcr, x):\n    r'''\n    Calculate the view factor to the ground from a point x on a row surface.\n\n    Assumes a PV system of infinitely long rows with uniform pitch on\n    horizontal ground. The view to the ground is restricted by the row's\n    tilt and the bottom of the facing row.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : numeric\n        Ratio of the row slant length to the row spacing (pitch). [unitless]\n    x : numeric\n        Position on the row's slant length, as a fraction of the slant length.\n        x=0 corresponds to the bottom of the row. [unitless]\n\n    Returns\n    -------\n    vf : numeric\n        View factor to the visible ground from the point x. [unitless]\n\n    '''\n    p = _vf_poly(surface_tilt, gcr, x, 1)\n    return 0.5 * (1 - (1/gcr * cosd(surface_tilt) + x)/p)\n\n\ndef vf_row_ground_2d_integ(surface_tilt, gcr, x0=0, x1=1):\n    r'''\n    Calculate the average view factor to the ground from a segment of the row\n    surface between x0 and x1.\n\n    Assumes a PV system of infinitely long rows with uniform pitch on\n    horizontal ground. The view to the ground is restricted by the row's\n    tilt and the bottom of the facing row.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : numeric\n        Ratio of the row slant length to the row spacing (pitch). [unitless]\n    x0 : numeric, default 0.\n        Position on the row's slant length, as a fraction of the slant length.\n        x0=0 corresponds to the bottom of the row. x0 should be less than x1.\n        [unitless]\n    x1 : numeric, default 1.\n        Position on the row's slant length, as a fraction of the slant length.\n        x1 should be greater than x0. [unitless]\n\n    Returns\n    -------\n    vf : numeric\n        Integrated view factor to the visible ground on the interval (x0, x1).\n        [unitless]\n\n    '''\n    u = np.abs(x1 - x0)\n    p0 = _vf_poly(surface_tilt, gcr, x0, 1)\n    p1 = _vf_poly(surface_tilt, gcr, x1, 1)\n    with np.errstate(divide='ignore'):\n        result = np.where(u < 1e-6,\n                          vf_row_ground_2d(surface_tilt, gcr, x0),\n                          0.5*(1 - 1/u * (p1 - p0))\n                          )\n    return result\ndiff --git a/pvlib/shading.py b/pvlib/shading.py\n from pvlib.tools import sind, cosd\n \n \ndef ground_angle(surface_tilt, gcr, slant_height):\n    \"\"\"\n    Angle from horizontal of the line from a point on the row slant length\n    to the bottom of the facing row.\n\n    The angles are clockwise from horizontal, rather than the usual\n    counterclockwise direction.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angle in degrees from horizontal, e.g., surface facing up\n        = 0, surface facing horizon = 90. [degree]\n    gcr : float\n        ground coverage ratio, ratio of row slant length to row spacing.\n        [unitless]\n    slant_height : numeric\n        The distance up the module's slant height to evaluate the ground\n        angle, as a fraction [0-1] of the module slant height [unitless].\n\n    Returns\n    -------\n    psi : numeric\n        Angle [degree].\n    \"\"\"\n    #  : \\\\            \\\n    #  :  \\\\            \\\n    #  :   \\\\            \\\n    #  :    \\\\            \\  facing row\n    #  :     \\\\.___________\\\n    #  :       \\  ^*-.  psi \\\n    #  :        \\  x   *-.   \\\n    #  :         \\  v      *-.\\\n    #  :          \\<-----P---->\\\n\n    x1 = gcr * slant_height * sind(surface_tilt)\n    x2 = gcr * slant_height * cosd(surface_tilt) + 1\n    psi = np.arctan2(x1, x2)  # do this before rad2deg because it handles 0 / 0\n    return np.rad2deg(psi)\n\n\n def masking_angle(surface_tilt, gcr, slant_height):\n     \"\"\"\n     The elevation angle below which diffuse irradiance is blocked."
  },
  {
    "instruction": "Allow user to set tol and maxiter for singlediode newton method\nThe first few lines of `pvlib.singlediode` set `tol` and `maxiter` for all the solvers using the newton method:\r\n\r\n```\r\nfrom scipy.optimize import brentq, newton\r\nfrom scipy.special import lambertw\r\n\r\n# set keyword arguments for all uses of newton in this module\r\nnewton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\r\n```\r\n\r\nHowever, I would like to change `tol` and `maxiter` for my application. It would be great if these could be added instead as keyword arguments to the various functions so they can be adjusted by the user. Using a variety of singlediode model params, I have found that by setting tol=0.1 and maxiter=10, I can realize a 1.4x speedup in the `singeldiode.bishop88_mpp` algorithm while incurring a maximum error of 0.007038% and a mean absolute error of  0.000042% in calculated V_mp.\r\n\r\n\n",
    "input": " Low-level functions for solving the single diode equation.\n \"\"\"\n \nfrom functools import partial\n import numpy as np\n from pvlib.tools import _golden_sect_DataFrame\n \n from scipy.optimize import brentq, newton\n from scipy.special import lambertw\n \n# set keyword arguments for all uses of newton in this module\nnewton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\n \n # intrinsic voltage per cell junction for a:Si, CdTe, Mertens et al.\n VOLTAGE_BUILTIN = 0.9  # [V]\n                       resistance_series, resistance_shunt, nNsVth,\n                       d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n                       breakdown_voltage=-5.5, breakdown_exp=3.28,\n                      method='newton'):\n     \"\"\"\n     Find current given any voltage.\n \n     method : str, default 'newton'\n        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n        if ``breakdown_factor`` is not 0.\n \n     Returns\n     -------\n     current : numeric\n         current (I) at the specified voltage (V). [A]\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, d2mutau, NsVbi,\n             breakdown_factor, breakdown_voltage, breakdown_exp)\n \n     def fv(x, v, *a):\n         # calculate voltage residual given diode voltage \"x\"\n         return bishop88(x, *a)[1] - v\n \n    if method.lower() == 'brentq':\n         # first bound the search using voc\n         voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n             return brentq(fv, 0.0, voc,\n                           args=(v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                                 breakdown_factor, breakdown_voltage,\n                                breakdown_exp))\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n    elif method.lower() == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voltage is an array, then make a copy to use for initial guess, v0\n        args, v0 = _prepare_newton_inputs((voltage,), args, voltage)\n         vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n                    args=args)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n    return bishop88(vd, *args)[0]\n \n \n def bishop88_v_from_i(current, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth,\n                       d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n                       breakdown_voltage=-5.5, breakdown_exp=3.28,\n                      method='newton'):\n     \"\"\"\n     Find voltage given any current.\n \n     method : str, default 'newton'\n        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n        if ``breakdown_factor`` is not 0.\n \n     Returns\n     -------\n     voltage : numeric\n         voltage (V) at the specified current (I) in volts [V]\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n             breakdown_voltage, breakdown_exp)\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n         # calculate current residual given diode voltage \"x\"\n         return bishop88(x, *a)[0] - i\n \n    if method.lower() == 'brentq':\n         # brentq only works with scalar inputs, so we need a set up function\n         # and np.vectorize to repeatedly call the optimizer with the right\n         # arguments for possible array input\n             return brentq(fi, 0.0, voc,\n                           args=(i, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                                 breakdown_factor, breakdown_voltage,\n                                breakdown_exp))\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, current, *args)\n    elif method.lower() == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voc_est is an array, then make a copy to use for initial guess, v0\n        args, v0 = _prepare_newton_inputs((current,), args, voc_est)\n         vd = newton(func=lambda x, *a: fi(x, current, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[3],\n                    args=args)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n    return bishop88(vd, *args)[1]\n \n \n def bishop88_mpp(photocurrent, saturation_current, resistance_series,\n                  resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n                  breakdown_factor=0., breakdown_voltage=-5.5,\n                 breakdown_exp=3.28, method='newton'):\n     \"\"\"\n     Find max power point.\n \n     method : str, default 'newton'\n        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n        if ``breakdown_factor`` is not 0.\n \n     Returns\n     -------\n     tuple\n         max power current ``i_mp`` [A], max power voltage ``v_mp`` [V], and\n         max power ``p_mp`` [W]\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n             breakdown_voltage, breakdown_exp)\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     def fmpp(x, *a):\n         return bishop88(x, *a, gradients=True)[6]\n \n    if method.lower() == 'brentq':\n         # break out arguments for numpy.vectorize to handle broadcasting\n         vec_fun = np.vectorize(\n             lambda voc, iph, isat, rs, rsh, gamma, d2mutau, NsVbi, vbr_a, vbr,\n             vbr_exp: brentq(fmpp, 0.0, voc,\n                             args=(iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                                  vbr_a, vbr, vbr_exp))\n         )\n         vd = vec_fun(voc_est, *args)\n    elif method.lower() == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voc_est is an array, then make a copy to use for initial guess, v0\n        args, v0 = _prepare_newton_inputs((), args, voc_est)\n         vd = newton(\n             func=fmpp, x0=v0,\n            fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args\n        )\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n    return bishop88(vd, *args)\n \n \n def _get_size_and_shape(args):\n     return size, shape\n \n \ndef _prepare_newton_inputs(i_or_v_tup, args, v0):\n     # broadcast arguments for newton method\n     # the first argument should be a tuple, eg: (i,), (v,) or ()\n     size, shape = _get_size_and_shape(i_or_v_tup + args)\n     # copy v0 to a new array and broadcast it to the shape of max size\n     if shape is not None:\n         v0 = np.broadcast_to(v0, shape).copy()\n    return args, v0\n \n \n def _lambertw_v_from_i(current, photocurrent, saturation_current,",
    "output": " Low-level functions for solving the single diode equation.\n \"\"\"\n \n import numpy as np\n from pvlib.tools import _golden_sect_DataFrame\n \n from scipy.optimize import brentq, newton\n from scipy.special import lambertw\n \n# newton method default parameters for this module\nNEWTON_DEFAULT_PARAMS = {\n    'tol': 1e-6,\n    'maxiter': 100\n}\n \n # intrinsic voltage per cell junction for a:Si, CdTe, Mertens et al.\n VOLTAGE_BUILTIN = 0.9  # [V]\n                       resistance_series, resistance_shunt, nNsVth,\n                       d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n                       breakdown_voltage=-5.5, breakdown_exp=3.28,\n                      method='newton', method_kwargs=None):\n     \"\"\"\n     Find current given any voltage.\n \n     method : str, default 'newton'\n        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n        if ``breakdown_factor`` is not 0.\n    method_kwargs : dict, optional\n        Keyword arguments passed to root finder method. See\n        :py:func:`scipy:scipy.optimize.brentq` and\n        :py:func:`scipy:scipy.optimize.newton` parameters.\n        ``'full_output': True`` is allowed, and ``optimizer_output`` would be\n        returned. See examples section.\n \n     Returns\n     -------\n     current : numeric\n         current (I) at the specified voltage (V). [A]\n    optimizer_output : tuple, optional, if specified in ``method_kwargs``\n        see root finder documentation for selected method.\n        Found root is diode voltage in [1]_.\n\n    Examples\n    --------\n    Using the following arguments that may come from any\n    `calcparams_.*` function in :py:mod:`pvlib.pvsystem`:\n\n    >>> args = {'photocurrent': 1., 'saturation_current': 9e-10, 'nNsVth': 4.,\n    ...         'resistance_series': 4., 'resistance_shunt': 5000.0}\n\n    Use default values:\n\n    >>> i = bishop88_i_from_v(0.0, **args)\n\n    Specify tolerances and maximum number of iterations:\n\n    >>> i = bishop88_i_from_v(0.0, **args, method='newton',\n    ...     method_kwargs={'tol': 1e-3, 'rtol': 1e-3, 'maxiter': 20})\n\n    Retrieve full output from the root finder:\n\n    >>> i, method_output = bishop88_i_from_v(0.0, **args, method='newton',\n    ...     method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, d2mutau, NsVbi,\n             breakdown_factor, breakdown_voltage, breakdown_exp)\n    method = method.lower()\n\n    # method_kwargs create dict if not provided\n    # this pattern avoids bugs with Mutable Default Parameters\n    if not method_kwargs:\n        method_kwargs = {}\n \n     def fv(x, v, *a):\n         # calculate voltage residual given diode voltage \"x\"\n         return bishop88(x, *a)[1] - v\n \n    if method == 'brentq':\n         # first bound the search using voc\n         voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n             return brentq(fv, 0.0, voc,\n                           args=(v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                                 breakdown_factor, breakdown_voltage,\n                                breakdown_exp),\n                          **method_kwargs)\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n    elif method == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voltage is an array, then make a copy to use for initial guess, v0\n        args, v0, method_kwargs = \\\n            _prepare_newton_inputs((voltage,), args, voltage, method_kwargs)\n         vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n                    args=args,\n                    **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n\n    # When 'full_output' parameter is specified, returned 'vd' is a tuple with\n    # many elements, where the root is the first one. So we use it to output\n    # the bishop88 result and return tuple(scalar, tuple with method results)\n    if method_kwargs.get('full_output') is True:\n        return (bishop88(vd[0], *args)[0], vd)\n    else:\n        return bishop88(vd, *args)[0]\n \n \n def bishop88_v_from_i(current, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth,\n                       d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n                       breakdown_voltage=-5.5, breakdown_exp=3.28,\n                      method='newton', method_kwargs=None):\n     \"\"\"\n     Find voltage given any current.\n \n     method : str, default 'newton'\n        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n        if ``breakdown_factor`` is not 0.\n    method_kwargs : dict, optional\n        Keyword arguments passed to root finder method. See\n        :py:func:`scipy:scipy.optimize.brentq` and\n        :py:func:`scipy:scipy.optimize.newton` parameters.\n        ``'full_output': True`` is allowed, and ``optimizer_output`` would be\n        returned. See examples section.\n \n     Returns\n     -------\n     voltage : numeric\n         voltage (V) at the specified current (I) in volts [V]\n    optimizer_output : tuple, optional, if specified in ``method_kwargs``\n        see root finder documentation for selected method.\n        Found root is diode voltage in [1]_.\n\n    Examples\n    --------\n    Using the following arguments that may come from any\n    `calcparams_.*` function in :py:mod:`pvlib.pvsystem`:\n\n    >>> args = {'photocurrent': 1., 'saturation_current': 9e-10, 'nNsVth': 4.,\n    ...         'resistance_series': 4., 'resistance_shunt': 5000.0}\n\n    Use default values:\n\n    >>> v = bishop88_v_from_i(0.0, **args)\n\n    Specify tolerances and maximum number of iterations:\n\n    >>> v = bishop88_v_from_i(0.0, **args, method='newton',\n    ...     method_kwargs={'tol': 1e-3, 'rtol': 1e-3, 'maxiter': 20})\n\n    Retrieve full output from the root finder:\n\n    >>> v, method_output = bishop88_v_from_i(0.0, **args, method='newton',\n    ...     method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n             breakdown_voltage, breakdown_exp)\n    method = method.lower()\n\n    # method_kwargs create dict if not provided\n    # this pattern avoids bugs with Mutable Default Parameters\n    if not method_kwargs:\n        method_kwargs = {}\n\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n         # calculate current residual given diode voltage \"x\"\n         return bishop88(x, *a)[0] - i\n \n    if method == 'brentq':\n         # brentq only works with scalar inputs, so we need a set up function\n         # and np.vectorize to repeatedly call the optimizer with the right\n         # arguments for possible array input\n             return brentq(fi, 0.0, voc,\n                           args=(i, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                                 breakdown_factor, breakdown_voltage,\n                                breakdown_exp),\n                          **method_kwargs)\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, current, *args)\n    elif method == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voc_est is an array, then make a copy to use for initial guess, v0\n        args, v0, method_kwargs = \\\n            _prepare_newton_inputs((current,), args, voc_est, method_kwargs)\n         vd = newton(func=lambda x, *a: fi(x, current, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[3],\n                    args=args,\n                    **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n\n    # When 'full_output' parameter is specified, returned 'vd' is a tuple with\n    # many elements, where the root is the first one. So we use it to output\n    # the bishop88 result and return tuple(scalar, tuple with method results)\n    if method_kwargs.get('full_output') is True:\n        return (bishop88(vd[0], *args)[1], vd)\n    else:\n        return bishop88(vd, *args)[1]\n \n \n def bishop88_mpp(photocurrent, saturation_current, resistance_series,\n                  resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n                  breakdown_factor=0., breakdown_voltage=-5.5,\n                 breakdown_exp=3.28, method='newton', method_kwargs=None):\n     \"\"\"\n     Find max power point.\n \n     method : str, default 'newton'\n        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n        if ``breakdown_factor`` is not 0.\n    method_kwargs : dict, optional\n        Keyword arguments passed to root finder method. See\n        :py:func:`scipy:scipy.optimize.brentq` and\n        :py:func:`scipy:scipy.optimize.newton` parameters.\n        ``'full_output': True`` is allowed, and ``optimizer_output`` would be\n        returned. See examples section.\n \n     Returns\n     -------\n     tuple\n         max power current ``i_mp`` [A], max power voltage ``v_mp`` [V], and\n         max power ``p_mp`` [W]\n    optimizer_output : tuple, optional, if specified in ``method_kwargs``\n        see root finder documentation for selected method.\n        Found root is diode voltage in [1]_.\n\n    Examples\n    --------\n    Using the following arguments that may come from any\n    `calcparams_.*` function in :py:mod:`pvlib.pvsystem`:\n\n    >>> args = {'photocurrent': 1., 'saturation_current': 9e-10, 'nNsVth': 4.,\n    ...         'resistance_series': 4., 'resistance_shunt': 5000.0}\n\n    Use default values:\n\n    >>> i_mp, v_mp, p_mp = bishop88_mpp(**args)\n\n    Specify tolerances and maximum number of iterations:\n\n    >>> i_mp, v_mp, p_mp = bishop88_mpp(**args, method='newton',\n    ...     method_kwargs={'tol': 1e-3, 'rtol': 1e-3, 'maxiter': 20})\n\n    Retrieve full output from the root finder:\n\n    >>> (i_mp, v_mp, p_mp), method_output = bishop88_mpp(**args,\n    ...     method='newton', method_kwargs={'full_output': True})\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n             breakdown_voltage, breakdown_exp)\n    method = method.lower()\n\n    # method_kwargs create dict if not provided\n    # this pattern avoids bugs with Mutable Default Parameters\n    if not method_kwargs:\n        method_kwargs = {}\n\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     def fmpp(x, *a):\n         return bishop88(x, *a, gradients=True)[6]\n \n    if method == 'brentq':\n         # break out arguments for numpy.vectorize to handle broadcasting\n         vec_fun = np.vectorize(\n             lambda voc, iph, isat, rs, rsh, gamma, d2mutau, NsVbi, vbr_a, vbr,\n             vbr_exp: brentq(fmpp, 0.0, voc,\n                             args=(iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                                  vbr_a, vbr, vbr_exp),\n                            **method_kwargs)\n         )\n         vd = vec_fun(voc_est, *args)\n    elif method == 'newton':\n         # make sure all args are numpy arrays if max size > 1\n         # if voc_est is an array, then make a copy to use for initial guess, v0\n        args, v0, method_kwargs = \\\n            _prepare_newton_inputs((), args, voc_est, method_kwargs)\n         vd = newton(\n             func=fmpp, x0=v0,\n            fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args,\n            **method_kwargs)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n\n    # When 'full_output' parameter is specified, returned 'vd' is a tuple with\n    # many elements, where the root is the first one. So we use it to output\n    # the bishop88 result and return\n    # tuple(tuple with bishop88 solution, tuple with method results)\n    if method_kwargs.get('full_output') is True:\n        return (bishop88(vd[0], *args), vd)\n    else:\n        return bishop88(vd, *args)\n \n \n def _get_size_and_shape(args):\n     return size, shape\n \n \ndef _prepare_newton_inputs(i_or_v_tup, args, v0, method_kwargs):\n     # broadcast arguments for newton method\n     # the first argument should be a tuple, eg: (i,), (v,) or ()\n     size, shape = _get_size_and_shape(i_or_v_tup + args)\n     # copy v0 to a new array and broadcast it to the shape of max size\n     if shape is not None:\n         v0 = np.broadcast_to(v0, shape).copy()\n\n    # set abs tolerance and maxiter from method_kwargs if not provided\n    # apply defaults, but giving priority to user-specified values\n    method_kwargs = {**NEWTON_DEFAULT_PARAMS, **method_kwargs}\n\n    return args, v0, method_kwargs\n \n \n def _lambertw_v_from_i(current, photocurrent, saturation_current,"
  },
  {
    "instruction": "ModelChain should accept albedo in weather dataframe\n**Is your feature request related to a problem? Please describe.**\r\nAlbedo is treated as a scalar constant in pvlib, but it is of course a function of the weather and changes throughout the year.  Albedo is currently set in the PVSystem or Array and cannot be altered using the ModelChain.  Albedo is provided as a timeseries from many weather data services as well as through NREL's NSRBD and it would be useful to provide this data to the ModelChain.\r\n\r\nAdditionally, treating albedo as property of the Array seems to conflict with the [PVSystem Design Philosophy](https://pvlib-python.readthedocs.io/en/stable/pvsystem.html#design-philosophy), which highlights the separation of the PV system and the exogenous variables, such as the weather.\r\n\r\n**Describe the solution you'd like**\r\nModelChain.run_model() should accept albedo in the weather dataframe, like temperature and ghi.\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative we have implemented is calling ModelChain.run_model() on each row of a dataframe and manually updating the albedo of the array in each tilmestep.  This probably has some side effects that we are unaware of.\r\n\n",
    "input": "         Extraterrestrial radiation [W/m^2], defaults to 1364[W/m^2]\n     asymmetry : numeric\n         Asymmetry factor, defaults to 0.85\n    albedo : numeric\n        Albedo, defaults to 0.2\n \n     Returns\n     -------\ndiff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\n def get_total_irradiance(surface_tilt, surface_azimuth,\n                          solar_zenith, solar_azimuth,\n                          dni, ghi, dhi, dni_extra=None, airmass=None,\n                         albedo=.25, surface_type=None,\n                          model='isotropic',\n                          model_perez='allsitescomposite1990'):\n     r\"\"\"\n     airmass : None or numeric, default None\n         Relative airmass (not adjusted for pressure). [unitless]\n     albedo : numeric, default 0.25\n        Surface albedo. [unitless]\n     surface_type : None or str, default None\n         Surface type. See :py:func:`~pvlib.irradiance.get_ground_diffuse` for\n         the list of accepted values.\n         applied.\n \n     albedo : numeric, default 0.25\n        Surface albedo\n \n     model : String, default 'perez'\n         Irradiance model.  See :py:func:`get_sky_diffuse` for allowed values.\ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n     _per_array_fields = {'total_irrad', 'aoi', 'aoi_modifier',\n                          'spectral_modifier', 'cell_temperature',\n                          'effective_irradiance', 'dc', 'diode_params',\n                         'dc_ohmic_losses', 'weather'}\n \n     # system-level information\n     solar_position: Optional[pd.DataFrame] = field(default=None)\n     \"\"\"DatetimeIndex containing a copy of the index of the input weather data.\n     \"\"\"\n \n     def _result_type(self, value):\n         \"\"\"Coerce `value` to the correct type according to\n         ``self._singleton_tuples``.\"\"\"\n             **kwargs)\n         return self\n \n     def _prep_inputs_airmass(self):\n         \"\"\"\n         Assign airmass\n \n         Parameters\n         ----------\n        weather : DataFrame, or tuple or list of DataFrame\n             Required column names include ``'dni'``, ``'ghi'``, ``'dhi'``.\n            Optional column names are ``'wind_speed'``, ``'temp_air'``; if not\n             provided, air temperature of 20 C and wind speed\n            of 0 m/s will be added to the DataFrame.\n \n             If `weather` is a tuple or list, it must be of the same length and\n             order as the Arrays of the ModelChain's PVSystem.\n         Notes\n         -----\n         Assigns attributes to ``results``: ``times``, ``weather``,\n        ``solar_position``, ``airmass``, ``total_irrad``, ``aoi``\n \n         See also\n         --------\n \n         self._prep_inputs_solar_pos(weather)\n         self._prep_inputs_airmass()\n \n         # PVSystem.get_irradiance and SingleAxisTracker.get_irradiance\n         # and PVSystem.get_aoi and SingleAxisTracker.get_aoi\n             _tuple_from_dfs(self.results.weather, 'dni'),\n             _tuple_from_dfs(self.results.weather, 'ghi'),\n             _tuple_from_dfs(self.results.weather, 'dhi'),\n             airmass=self.results.airmass['airmass_relative'],\n             model=self.transposition_model\n         )\n         Parameters\n         ----------\n         weather : DataFrame, or tuple or list of DataFrame\n            Irradiance column names must include ``'dni'``, ``'ghi'``, and\n            ``'dhi'``. If optional columns ``'temp_air'`` and ``'wind_speed'``\n             are not provided, air temperature of 20 C and wind speed of 0 m/s\n             are added to the DataFrame. If optional column\n             ``'cell_temperature'`` is provided, these values are used instead\n            of `temperature_model`. If optional column `module_temperature`\n            is provided, `temperature_model` must be ``'sapm'``.\n \n            If list or tuple, must be of the same length and order as the\n            Arrays of the ModelChain's PVSystem.\n \n         Returns\n         -------\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         a single array is created from the other parameters (e.g.\n         `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n         if length of arrays is 0 a ValueError is raised. If `arrays` is\n        specified the following parameters are ignored:\n \n         - `surface_tilt`\n         - `surface_azimuth`\n         North=0, East=90, South=180, West=270.\n \n     albedo : None or float, default None\n        The ground albedo. If ``None``, will attempt to use\n        ``surface_type`` and ``irradiance.SURFACE_ALBEDOS``\n        to lookup albedo.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n        for valid values.\n \n     module : None or string, default None\n         The model name of the modules.\n \n     @_unwrap_single_value\n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, model='haydavies',\n                       **kwargs):\n         \"\"\"\n         Uses the :py:func:`irradiance.get_total_irradiance` function to\n        calculate the plane of array irradiance components on a tilted\n        surface defined by ``self.surface_tilt``,\n        ``self.surface_azimuth``, and ``self.albedo``.\n \n         Parameters\n         ----------\n        solar_zenith : float or Series.\n             Solar zenith angle.\n        solar_azimuth : float or Series.\n             Solar azimuth angle.\n         dni : float or Series or tuple of float or Series\n            Direct Normal Irradiance\n         ghi : float or Series or tuple of float or Series\n            Global horizontal irradiance\n         dhi : float or Series or tuple of float or Series\n            Diffuse horizontal irradiance\n        dni_extra : None, float or Series, default None\n            Extraterrestrial direct normal irradiance\n         airmass : None, float or Series, default None\n            Airmass\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame or tuple of DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n         \"\"\"\n         dni = self._validate_per_array(dni, system_wide=True)\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n         return tuple(\n             array.get_irradiance(solar_zenith, solar_azimuth,\n                                  dni, ghi, dhi,\n                                 dni_extra, airmass, model,\n                                 **kwargs)\n            for array, dni, ghi, dhi in zip(\n                self.arrays, dni, ghi, dhi\n             )\n         )\n \n         If not provided, a FixedMount with zero tilt is used.\n \n     albedo : None or float, default None\n        The ground albedo. If ``None``, will attempt to use\n        ``surface_type`` to look up an albedo value in\n        ``irradiance.SURFACE_ALBEDOS``. If a surface albedo\n        cannot be found then 0.25 is used.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n        for valid values.\n \n     module : None or string, default None\n         The model name of the modules.\n                               solar_zenith, solar_azimuth)\n \n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, model='haydavies',\n                       **kwargs):\n         \"\"\"\n         Get plane of array irradiance components.\n \n         Uses the :py:func:`pvlib.irradiance.get_total_irradiance` function to\n         calculate the plane of array irradiance components for a surface\n        defined by ``self.surface_tilt`` and ``self.surface_azimuth`` with\n        albedo ``self.albedo``.\n \n         Parameters\n         ----------\n         solar_azimuth : float or Series.\n             Solar azimuth angle.\n         dni : float or Series\n            Direct Normal Irradiance\n        ghi : float or Series\n             Global horizontal irradiance\n         dhi : float or Series\n            Diffuse horizontal irradiance\n         dni_extra : None, float or Series, default None\n            Extraterrestrial direct normal irradiance\n         airmass : None, float or Series, default None\n            Airmass\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n         \"\"\"\n         # not needed for all models, but this is easier\n         if dni_extra is None:\n             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n                                                dni, ghi, dhi,\n                                                dni_extra=dni_extra,\n                                                airmass=airmass,\n                                                model=model,\n                                               albedo=self.albedo,\n                                                **kwargs)\n \n     def get_iam(self, aoi, iam_model='physical'):\n \n     See Also\n     --------\n    :py:func:`~pvlib.pvsystem.dc_ohmic_losses`\n \n     References\n     ----------\n \n     See Also\n     --------\n    :py:func:`~pvlib.pvsystem.dc_ohms_from_percent`\n \n     References\n     ----------\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n     @_unwrap_single_value\n     def get_irradiance(self, surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, model='haydavies',\n                        **kwargs):\n         \"\"\"\n         Uses the :func:`irradiance.get_total_irradiance` function to\n             Global horizontal irradiance\n         dhi : float or Series\n             Diffuse horizontal irradiance\n         dni_extra : float or Series, default None\n             Extraterrestrial direct normal irradiance\n         airmass : float or Series, default None\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n \n         return tuple(\n             irradiance.get_total_irradiance(\n                 surface_tilt,\n                 dni_extra=dni_extra,\n                 airmass=airmass,\n                 model=model,\n                albedo=self.arrays[0].albedo,\n                 **kwargs)\n            for array, dni, ghi, dhi in zip(\n                self.arrays, dni, ghi, dhi\n             )\n         )\n ",
    "output": "         Extraterrestrial radiation [W/m^2], defaults to 1364[W/m^2]\n     asymmetry : numeric\n         Asymmetry factor, defaults to 0.85\n    albedo : numeric, default 0.2\n        Ground surface albedo. [unitless]\n \n     Returns\n     -------\ndiff --git a/pvlib/irradiance.py b/pvlib/irradiance.py\n def get_total_irradiance(surface_tilt, surface_azimuth,\n                          solar_zenith, solar_azimuth,\n                          dni, ghi, dhi, dni_extra=None, airmass=None,\n                         albedo=0.25, surface_type=None,\n                          model='isotropic',\n                          model_perez='allsitescomposite1990'):\n     r\"\"\"\n     airmass : None or numeric, default None\n         Relative airmass (not adjusted for pressure). [unitless]\n     albedo : numeric, default 0.25\n        Ground surface albedo. [unitless]\n     surface_type : None or str, default None\n         Surface type. See :py:func:`~pvlib.irradiance.get_ground_diffuse` for\n         the list of accepted values.\n         applied.\n \n     albedo : numeric, default 0.25\n        Ground surface albedo. [unitless]\n \n     model : String, default 'perez'\n         Irradiance model.  See :py:func:`get_sky_diffuse` for allowed values.\ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n     _per_array_fields = {'total_irrad', 'aoi', 'aoi_modifier',\n                          'spectral_modifier', 'cell_temperature',\n                          'effective_irradiance', 'dc', 'diode_params',\n                         'dc_ohmic_losses', 'weather', 'albedo'}\n \n     # system-level information\n     solar_position: Optional[pd.DataFrame] = field(default=None)\n     \"\"\"DatetimeIndex containing a copy of the index of the input weather data.\n     \"\"\"\n \n    albedo: Optional[PerArray[pd.Series]] = None\n    \"\"\"Series (or tuple of Series, one for each array) containing albedo.\n    \"\"\"\n\n     def _result_type(self, value):\n         \"\"\"Coerce `value` to the correct type according to\n         ``self._singleton_tuples``.\"\"\"\n             **kwargs)\n         return self\n \n    def _prep_inputs_albedo(self, weather):\n        \"\"\"\n        Get albedo from weather\n        \"\"\"\n        try:\n            self.results.albedo = _tuple_from_dfs(weather, 'albedo')\n        except KeyError:\n            self.results.albedo = tuple([\n                a.albedo for a in self.system.arrays])\n        return self\n\n     def _prep_inputs_airmass(self):\n         \"\"\"\n         Assign airmass\n \n         Parameters\n         ----------\n        weather : DataFrame, or tuple or list of DataFrames\n             Required column names include ``'dni'``, ``'ghi'``, ``'dhi'``.\n            Optional column names are ``'wind_speed'``, ``'temp_air'``,\n            ``'albedo'``.\n\n            If optional columns ``'wind_speed'``, ``'temp_air'`` are not\n             provided, air temperature of 20 C and wind speed\n            of 0 m/s will be added to the ``weather`` DataFrame.\n\n            If optional column ``'albedo'`` is provided, albedo values in the\n            ModelChain's PVSystem.arrays are ignored.\n \n             If `weather` is a tuple or list, it must be of the same length and\n             order as the Arrays of the ModelChain's PVSystem.\n         Notes\n         -----\n         Assigns attributes to ``results``: ``times``, ``weather``,\n        ``solar_position``, ``airmass``, ``total_irrad``, ``aoi``, ``albedo``.\n \n         See also\n         --------\n \n         self._prep_inputs_solar_pos(weather)\n         self._prep_inputs_airmass()\n        self._prep_inputs_albedo(weather)\n \n         # PVSystem.get_irradiance and SingleAxisTracker.get_irradiance\n         # and PVSystem.get_aoi and SingleAxisTracker.get_aoi\n             _tuple_from_dfs(self.results.weather, 'dni'),\n             _tuple_from_dfs(self.results.weather, 'ghi'),\n             _tuple_from_dfs(self.results.weather, 'dhi'),\n            albedo=self.results.albedo,\n             airmass=self.results.airmass['airmass_relative'],\n             model=self.transposition_model\n         )\n         Parameters\n         ----------\n         weather : DataFrame, or tuple or list of DataFrame\n            Column names must include:\n\n            - ``'dni'``\n            - ``'ghi'``\n            - ``'dhi'``\n\n            Optional columns are:\n\n            - ``'temp_air'``\n            - ``'cell_temperature'``\n            - ``'module_temperature'``\n            - ``'wind_speed'``\n            - ``'albedo'``\n\n            If optional columns ``'temp_air'`` and ``'wind_speed'``\n             are not provided, air temperature of 20 C and wind speed of 0 m/s\n             are added to the DataFrame. If optional column\n             ``'cell_temperature'`` is provided, these values are used instead\n            of `temperature_model`. If optional column ``'module_temperature'``\n            is provided, ``temperature_model`` must be ``'sapm'``.\n \n            If optional column ``'albedo'`` is provided, ``'albedo'`` may not\n            be present on the ModelChain's PVSystem.Arrays.\n\n            If weather is a list or tuple, it must be of the same length and\n            order as the Arrays of the ModelChain's PVSystem.\n \n         Returns\n         -------\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         a single array is created from the other parameters (e.g.\n         `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n         if length of arrays is 0 a ValueError is raised. If `arrays` is\n        specified the following PVSystem parameters are ignored:\n \n         - `surface_tilt`\n         - `surface_azimuth`\n         North=0, East=90, South=180, West=270.\n \n     albedo : None or float, default None\n        Ground surface albedo. If ``None``, then ``surface_type`` is used\n        to look up a value in ``irradiance.SURFACE_ALBEDOS``.\n        If ``surface_type`` is also None then a ground surface albedo\n        of 0.25 is used.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for\n        valid values.\n \n     module : None or string, default None\n         The model name of the modules.\n \n     @_unwrap_single_value\n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, albedo=None,\n                       model='haydavies', **kwargs):\n         \"\"\"\n         Uses the :py:func:`irradiance.get_total_irradiance` function to\n        calculate the plane of array irradiance components on the tilted\n        surfaces defined by each array's ``surface_tilt`` and\n        ``surface_azimuth``.\n \n         Parameters\n         ----------\n        solar_zenith : float or Series\n             Solar zenith angle.\n        solar_azimuth : float or Series\n             Solar azimuth angle.\n         dni : float or Series or tuple of float or Series\n            Direct Normal Irradiance. [W/m2]\n         ghi : float or Series or tuple of float or Series\n            Global horizontal irradiance. [W/m2]\n         dhi : float or Series or tuple of float or Series\n            Diffuse horizontal irradiance. [W/m2]\n        dni_extra : None, float, Series or tuple of float or Series,\\\n            default None\n            Extraterrestrial direct normal irradiance. [W/m2]\n         airmass : None, float or Series, default None\n            Airmass. [unitless]\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame or tuple of DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n\n        See also\n        --------\n        pvlib.irradiance.get_total_irradiance\n         \"\"\"\n         dni = self._validate_per_array(dni, system_wide=True)\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n\n        albedo = self._validate_per_array(albedo, system_wide=True)\n\n         return tuple(\n             array.get_irradiance(solar_zenith, solar_azimuth,\n                                  dni, ghi, dhi,\n                                 dni_extra=dni_extra, airmass=airmass,\n                                 albedo=albedo, model=model, **kwargs)\n            for array, dni, ghi, dhi, albedo in zip(\n                self.arrays, dni, ghi, dhi, albedo\n             )\n         )\n \n         If not provided, a FixedMount with zero tilt is used.\n \n     albedo : None or float, default None\n        Ground surface albedo. If ``None``, then ``surface_type`` is used\n        to look up a value in ``irradiance.SURFACE_ALBEDOS``.\n        If ``surface_type`` is also None then a ground surface albedo\n        of 0.25 is used.\n \n     surface_type : None or string, default None\n        The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for valid\n        values.\n \n     module : None or string, default None\n         The model name of the modules.\n                               solar_zenith, solar_azimuth)\n \n     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, albedo=None,\n                       model='haydavies', **kwargs):\n         \"\"\"\n         Get plane of array irradiance components.\n \n         Uses the :py:func:`pvlib.irradiance.get_total_irradiance` function to\n         calculate the plane of array irradiance components for a surface\n        defined by ``self.surface_tilt`` and ``self.surface_azimuth``.\n \n         Parameters\n         ----------\n         solar_azimuth : float or Series.\n             Solar azimuth angle.\n         dni : float or Series\n            Direct normal irradiance. [W/m2]\n        ghi : float or Series. [W/m2]\n             Global horizontal irradiance\n         dhi : float or Series\n            Diffuse horizontal irradiance. [W/m2]\n         dni_extra : None, float or Series, default None\n            Extraterrestrial direct normal irradiance. [W/m2]\n         airmass : None, float or Series, default None\n            Airmass. [unitless]\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n         model : String, default 'haydavies'\n             Irradiance model.\n \n         poa_irradiance : DataFrame\n             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',\n             'poa_sky_diffuse', 'poa_ground_diffuse'``.\n\n        See also\n        --------\n        :py:func:`pvlib.irradiance.get_total_irradiance`\n         \"\"\"\n        if albedo is None:\n            albedo = self.albedo\n\n         # not needed for all models, but this is easier\n         if dni_extra is None:\n             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n                                                dni, ghi, dhi,\n                                                dni_extra=dni_extra,\n                                                airmass=airmass,\n                                               albedo=albedo,\n                                                model=model,\n                                                **kwargs)\n \n     def get_iam(self, aoi, iam_model='physical'):\n \n     See Also\n     --------\n    pvlib.pvsystem.dc_ohmic_losses\n \n     References\n     ----------\n \n     See Also\n     --------\n    pvlib.pvsystem.dc_ohms_from_percent\n \n     References\n     ----------\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n     @_unwrap_single_value\n     def get_irradiance(self, surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       albedo=None, dni_extra=None, airmass=None,\n                       model='haydavies',\n                        **kwargs):\n         \"\"\"\n         Uses the :func:`irradiance.get_total_irradiance` function to\n             Global horizontal irradiance\n         dhi : float or Series\n             Diffuse horizontal irradiance\n        albedo : None, float or Series, default None\n            Ground surface albedo. [unitless]\n         dni_extra : float or Series, default None\n             Extraterrestrial direct normal irradiance\n         airmass : float or Series, default None\n         ghi = self._validate_per_array(ghi, system_wide=True)\n         dhi = self._validate_per_array(dhi, system_wide=True)\n \n        if albedo is None:\n            # assign default albedo here because SingleAxisTracker\n            # initializes albedo to None\n            albedo = 0.25\n\n        albedo = self._validate_per_array(albedo, system_wide=True)\n\n         return tuple(\n             irradiance.get_total_irradiance(\n                 surface_tilt,\n                 dni_extra=dni_extra,\n                 airmass=airmass,\n                 model=model,\n                albedo=albedo,\n                 **kwargs)\n            for array, dni, ghi, dhi, albedo in zip(\n                self.arrays, dni, ghi, dhi, albedo\n             )\n         )\n "
  },
  {
    "instruction": "temperature.fuentes errors when given tz-aware inputs on pandas>=1.0.0\n**Describe the bug**\r\nWhen the weather timeseries inputs to `temperature.fuentes` have tz-aware index, an internal call to `np.diff(index)` returns an array of `Timedelta` objects instead of an array of nanosecond ints, throwing an error immediately after.  The error only happens when using pandas>=1.0.0; using 0.25.3 runs successfully, but emits the warning:\r\n\r\n```\r\n  /home/kevin/anaconda3/envs/pvlib-dev/lib/python3.7/site-packages/numpy/lib/function_base.py:1243: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\r\n  \tTo accept the future behavior, pass 'dtype=object'.\r\n  \tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\r\n    a = asanyarray(a)\r\n```\r\n\r\n**To Reproduce**\r\n```python\r\nIn [1]: import pvlib\r\n   ...: import pandas as pd\r\n   ...: \r\n   ...: index_naive = pd.date_range('2019-01-01', freq='h', periods=3)\r\n   ...: \r\n   ...: kwargs = {\r\n   ...:     'poa_global': pd.Series(1000, index_naive),\r\n   ...:     'temp_air': pd.Series(20, index_naive),\r\n   ...:     'wind_speed': pd.Series(1, index_naive),\r\n   ...:     'noct_installed': 45\r\n   ...: }\r\n   ...: \r\n\r\nIn [2]: print(pvlib.temperature.fuentes(**kwargs))\r\n2019-01-01 00:00:00    47.85\r\n2019-01-01 01:00:00    50.85\r\n2019-01-01 02:00:00    50.85\r\nFreq: H, Name: tmod, dtype: float64\r\n\r\nIn [3]: kwargs['poa_global'].index = index_naive.tz_localize('UTC')\r\n   ...: print(pvlib.temperature.fuentes(**kwargs))\r\n   ...: \r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-ff99badadc91>\", line 2, in <module>\r\n    print(pvlib.temperature.fuentes(**kwargs))\r\n\r\n  File \"/home/kevin/anaconda3/lib/python3.7/site-packages/pvlib/temperature.py\", line 602, in fuentes\r\n    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\r\n\r\nTypeError: float() argument must be a string or a number, not 'Timedelta'\r\n```\r\n\r\n**Expected behavior**\r\n`temperature.fuentes` should work with both tz-naive and tz-aware inputs.\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.8.0\r\n - ``pandas.__version__``: 1.0.0+\r\n - python: 3.7.4 (default, Aug 13 2019, 20:35:49) \\n[GCC 7.3.0]\r\n\r\n\n",
    "input": "     # n.b. the way Fuentes calculates the first timedelta makes it seem like\n     # the value doesn't matter -- rather than recreate it here, just assume\n     # it's the same as the second timedelta:\n    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\n    timedelta_hours = np.append([timedelta_hours[0]], timedelta_hours)\n \n     tamb_array = temp_air + 273.15\n     sun_array = poa_global * absorp",
    "output": "     # n.b. the way Fuentes calculates the first timedelta makes it seem like\n     # the value doesn't matter -- rather than recreate it here, just assume\n     # it's the same as the second timedelta:\n    timedelta_seconds = poa_global.index.to_series().diff().dt.total_seconds()\n    timedelta_hours = timedelta_seconds / 3600\n    timedelta_hours.iloc[0] = timedelta_hours.iloc[1]\n \n     tamb_array = temp_air + 273.15\n     sun_array = poa_global * absorp"
  },
  {
    "instruction": "expose pvlib.temperature.fuentes in PVSystem and ModelChain\nFollow up to #1032 and #1037 \n",
    "input": "         as the first argument to a user-defined function.\n \n     temperature_model: None, str or function, default None\n        Valid strings are 'sapm', 'pvsyst', and 'faiman'. The ModelChain\n        instance will be passed as the first argument to a user-defined\n        function.\n \n     losses_model: str or function, default 'no_loss'\n         Valid strings are 'pvwatts', 'no_loss'. The ModelChain instance\n                 self._temperature_model = self.pvsyst_temp\n             elif model == 'faiman':\n                 self._temperature_model = self.faiman_temp\n             else:\n                 raise ValueError(model + ' is not a valid temperature model')\n             # check system.temperature_model_parameters for consistency\n             return self.pvsyst_temp\n         elif {'u0', 'u1'} <= params:\n             return self.faiman_temp\n         else:\n             raise ValueError('could not infer temperature model from '\n                              'system.temperature_module_parameters {}.'\n             self.weather['wind_speed'])\n         return self\n \n     @property\n     def losses_model(self):\n         return self._losses_model\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         return temperature.faiman(poa_global, temp_air, wind_speed,\n                                   **kwargs)\n \n     def first_solar_spectral_loss(self, pw, airmass_absolute):\n \n         \"\"\"",
    "output": "         as the first argument to a user-defined function.\n \n     temperature_model: None, str or function, default None\n        Valid strings are 'sapm', 'pvsyst', 'faiman', and 'fuentes'.\n        The ModelChain instance will be passed as the first argument to a\n        user-defined function.\n \n     losses_model: str or function, default 'no_loss'\n         Valid strings are 'pvwatts', 'no_loss'. The ModelChain instance\n                 self._temperature_model = self.pvsyst_temp\n             elif model == 'faiman':\n                 self._temperature_model = self.faiman_temp\n            elif model == 'fuentes':\n                self._temperature_model = self.fuentes_temp\n             else:\n                 raise ValueError(model + ' is not a valid temperature model')\n             # check system.temperature_model_parameters for consistency\n             return self.pvsyst_temp\n         elif {'u0', 'u1'} <= params:\n             return self.faiman_temp\n        elif {'noct_installed'} <= params:\n            return self.fuentes_temp\n         else:\n             raise ValueError('could not infer temperature model from '\n                              'system.temperature_module_parameters {}.'\n             self.weather['wind_speed'])\n         return self\n \n    def fuentes_temp(self):\n        self.cell_temperature = self.system.fuentes_celltemp(\n            self.total_irrad['poa_global'], self.weather['temp_air'],\n            self.weather['wind_speed'])\n        return self\n\n     @property\n     def losses_model(self):\n         return self._losses_model\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         return temperature.faiman(poa_global, temp_air, wind_speed,\n                                   **kwargs)\n \n    def fuentes_celltemp(self, poa_global, temp_air, wind_speed):\n        \"\"\"\n        Use :py:func:`temperature.fuentes` to calculate cell temperature.\n\n        Parameters\n        ----------\n        poa_global : pandas Series\n            Total incident irradiance [W/m^2]\n\n        temp_air : pandas Series\n            Ambient dry bulb temperature [C]\n\n        wind_speed : pandas Series\n            Wind speed [m/s]\n\n        Returns\n        -------\n        temperature_cell : pandas Series\n            The modeled cell temperature [C]\n\n        Notes\n        -----\n        The Fuentes thermal model uses the module surface tilt for convection\n        modeling. The SAM implementation of PVWatts hardcodes the surface tilt\n        value at 30 degrees, ignoring whatever value is used for irradiance\n        transposition. This method defaults to using ``self.surface_tilt``, but\n        if you want to match the PVWatts behavior, you can override it by\n        including a ``surface_tilt`` value in ``temperature_model_parameters``.\n        \"\"\"\n        # default to using the PVSystem attribute, but allow user to\n        # override with a custom surface_tilt value\n        kwargs = {'surface_tilt': self.surface_tilt}\n        temp_model_kwargs = _build_kwargs([\n            'noct_installed', 'module_height', 'wind_height', 'emissivity',\n            'absorption', 'surface_tilt', 'module_width', 'module_length'],\n            self.temperature_model_parameters)\n        kwargs.update(temp_model_kwargs)\n        return temperature.fuentes(poa_global, temp_air, wind_speed,\n                                   **kwargs)\n\n     def first_solar_spectral_loss(self, pw, airmass_absolute):\n \n         \"\"\""
  },
  {
    "instruction": "remove ModelChain.orientation_strategy\nI don't like that `ModelChain(system, location, orientation_strategy='flat`|`south_at_latitude_tilt`) modifies the `system` object. It's not something we do anywhere else in pvlib. `orientation_strategy` only supports flat and south_at_latitude_tilt, neither of which are commonly used in the real world in 2020. \r\n\r\nI think we should remove it, maybe even without deprecation, in 0.8.\r\n\r\nI'm ok with keeping the `modelchain.get_orientation` function for now.\n",
    "input": " \n \n def basic_chain(times, latitude, longitude,\n                 module_parameters, temperature_model_parameters,\n                 inverter_parameters,\n                 irradiance=None, weather=None,\n                surface_tilt=None, surface_azimuth=None,\n                orientation_strategy=None,\n                 transposition_model='haydavies',\n                 solar_position_method='nrel_numpy',\n                 airmass_model='kastenyoung1989',\n         Positive is east of the prime meridian.\n         Use decimal degrees notation.\n \n     module_parameters : None, dict or Series\n         Module parameters as defined by the SAPM. See pvsystem.sapm for\n         details.\n         wind speed is 0 m/s.\n         Columns must be 'wind_speed', 'temp_air'.\n \n    surface_tilt : None, float or Series, default None\n        Surface tilt angles in decimal degrees.\n        The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth : None, float or Series, default None\n        Surface azimuth angles in decimal degrees.\n        The azimuth convention is defined\n        as degrees east of north\n        (North=0, South=180, East=90, West=270).\n\n    orientation_strategy : None or str, default None\n        The strategy for aligning the modules.\n        If not None, sets the ``surface_azimuth`` and ``surface_tilt``\n        properties of the ``system``. Allowed strategies include 'flat',\n        'south_at_latitude_tilt'. Ignored for SingleAxisTracker systems.\n\n     transposition_model : str, default 'haydavies'\n         Passed to system.get_irradiance.\n \n         power (Series).\n     \"\"\"\n \n    # use surface_tilt and surface_azimuth if provided,\n    # otherwise set them using the orientation_strategy\n    if surface_tilt is not None and surface_azimuth is not None:\n        pass\n    elif orientation_strategy is not None:\n        surface_tilt, surface_azimuth = \\\n            get_orientation(orientation_strategy, latitude=latitude)\n    else:\n        raise ValueError('orientation_strategy or surface_tilt and '\n                         'surface_azimuth must be provided')\n\n     if altitude is None and pressure is None:\n         altitude = 0.\n         pressure = 101325.\n         A :py:class:`~pvlib.location.Location` object that represents\n         the physical location at which to evaluate the model.\n \n    orientation_strategy : None or str, default None\n        The strategy for aligning the modules. If not None, sets the\n        ``surface_azimuth`` and ``surface_tilt`` properties of the\n        ``system``. Allowed strategies include 'flat',\n        'south_at_latitude_tilt'. Ignored for SingleAxisTracker systems.\n\n     clearsky_model : str, default 'ineichen'\n         Passed to location.get_clearsky.\n \n                          'dc', 'ac', 'diode_params', 'tracking']\n \n     def __init__(self, system, location,\n                 orientation_strategy=None,\n                  clearsky_model='ineichen',\n                  transposition_model='haydavies',\n                  solar_position_method='nrel_numpy',\n         self.temperature_model = temperature_model\n \n         self.losses_model = losses_model\n        self.orientation_strategy = orientation_strategy\n \n         self.weather = None\n         self.times = None\n \n     @classmethod\n     def with_pvwatts(cls, system, location,\n                     orientation_strategy=None,\n                      clearsky_model='ineichen',\n                      airmass_model='kastenyoung1989',\n                      name=None,\n             A :py:class:`~pvlib.location.Location` object that represents\n             the physical location at which to evaluate the model.\n \n        orientation_strategy : None or str, default None\n            The strategy for aligning the modules. If not None, sets the\n            ``surface_azimuth`` and ``surface_tilt`` properties of the\n            ``system``. Allowed strategies include 'flat',\n            'south_at_latitude_tilt'. Ignored for SingleAxisTracker systems.\n\n         clearsky_model : str, default 'ineichen'\n             Passed to location.get_clearsky.\n \n         >>> ModelChain.with_pvwatts(system, location)\n         ModelChain:\n           name: None\n          orientation_strategy: None\n           clearsky_model: ineichen\n           transposition_model: perez\n           solar_position_method: nrel_numpy\n         config.update(kwargs)\n         return ModelChain(\n             system, location,\n            orientation_strategy=orientation_strategy,\n             clearsky_model=clearsky_model,\n             airmass_model=airmass_model,\n             name=name,\n \n     @classmethod\n     def with_sapm(cls, system, location,\n                  orientation_strategy=None,\n                   clearsky_model='ineichen',\n                   transposition_model='haydavies',\n                   solar_position_method='nrel_numpy',\n             A :py:class:`~pvlib.location.Location` object that represents\n             the physical location at which to evaluate the model.\n \n        orientation_strategy : None or str, default None\n            The strategy for aligning the modules. If not None, sets the\n            ``surface_azimuth`` and ``surface_tilt`` properties of the\n            ``system``. Allowed strategies include 'flat',\n            'south_at_latitude_tilt'. Ignored for SingleAxisTracker systems.\n\n         clearsky_model : str, default 'ineichen'\n             Passed to location.get_clearsky.\n \n         >>> ModelChain.with_sapm(system, location)\n         ModelChain:\n           name: None\n          orientation_strategy: None\n           clearsky_model: ineichen\n           transposition_model: haydavies\n           solar_position_method: nrel_numpy\n         config.update(kwargs)\n         return ModelChain(\n             system, location,\n            orientation_strategy=orientation_strategy,\n             clearsky_model=clearsky_model,\n             transposition_model=transposition_model,\n             solar_position_method=solar_position_method,\n \n     def __repr__(self):\n         attrs = [\n            'name', 'orientation_strategy', 'clearsky_model',\n             'transposition_model', 'solar_position_method',\n             'airmass_model', 'dc_model', 'ac_model', 'aoi_model',\n             'spectral_model', 'temperature_model', 'losses_model'\n         return ('ModelChain: \\n  ' + '\\n  '.join(\n             f'{attr}: {getmcattr(self, attr)}' for attr in attrs))\n \n    @property\n    def orientation_strategy(self):\n        return self._orientation_strategy\n\n    @orientation_strategy.setter\n    def orientation_strategy(self, strategy):\n        if strategy == 'None':\n            strategy = None\n\n        if strategy is not None:\n            self.system.surface_tilt, self.system.surface_azimuth = \\\n                get_orientation(strategy, latitude=self.location.latitude)\n\n        self._orientation_strategy = strategy\n\n     @property\n     def dc_model(self):\n         return self._dc_model",
    "output": " \n \n def basic_chain(times, latitude, longitude,\n                surface_tilt, surface_azimuth,\n                 module_parameters, temperature_model_parameters,\n                 inverter_parameters,\n                 irradiance=None, weather=None,\n                 transposition_model='haydavies',\n                 solar_position_method='nrel_numpy',\n                 airmass_model='kastenyoung1989',\n         Positive is east of the prime meridian.\n         Use decimal degrees notation.\n \n    surface_tilt : numeric\n        Surface tilt angles in decimal degrees.\n        The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth : numeric\n        Surface azimuth angles in decimal degrees.\n        The azimuth convention is defined\n        as degrees east of north\n        (North=0, South=180, East=90, West=270).\n\n     module_parameters : None, dict or Series\n         Module parameters as defined by the SAPM. See pvsystem.sapm for\n         details.\n         wind speed is 0 m/s.\n         Columns must be 'wind_speed', 'temp_air'.\n \n     transposition_model : str, default 'haydavies'\n         Passed to system.get_irradiance.\n \n         power (Series).\n     \"\"\"\n \n     if altitude is None and pressure is None:\n         altitude = 0.\n         pressure = 101325.\n         A :py:class:`~pvlib.location.Location` object that represents\n         the physical location at which to evaluate the model.\n \n     clearsky_model : str, default 'ineichen'\n         Passed to location.get_clearsky.\n \n                          'dc', 'ac', 'diode_params', 'tracking']\n \n     def __init__(self, system, location,\n                  clearsky_model='ineichen',\n                  transposition_model='haydavies',\n                  solar_position_method='nrel_numpy',\n         self.temperature_model = temperature_model\n \n         self.losses_model = losses_model\n \n         self.weather = None\n         self.times = None\n \n     @classmethod\n     def with_pvwatts(cls, system, location,\n                      clearsky_model='ineichen',\n                      airmass_model='kastenyoung1989',\n                      name=None,\n             A :py:class:`~pvlib.location.Location` object that represents\n             the physical location at which to evaluate the model.\n \n         clearsky_model : str, default 'ineichen'\n             Passed to location.get_clearsky.\n \n         >>> ModelChain.with_pvwatts(system, location)\n         ModelChain:\n           name: None\n           clearsky_model: ineichen\n           transposition_model: perez\n           solar_position_method: nrel_numpy\n         config.update(kwargs)\n         return ModelChain(\n             system, location,\n             clearsky_model=clearsky_model,\n             airmass_model=airmass_model,\n             name=name,\n \n     @classmethod\n     def with_sapm(cls, system, location,\n                   clearsky_model='ineichen',\n                   transposition_model='haydavies',\n                   solar_position_method='nrel_numpy',\n             A :py:class:`~pvlib.location.Location` object that represents\n             the physical location at which to evaluate the model.\n \n         clearsky_model : str, default 'ineichen'\n             Passed to location.get_clearsky.\n \n         >>> ModelChain.with_sapm(system, location)\n         ModelChain:\n           name: None\n           clearsky_model: ineichen\n           transposition_model: haydavies\n           solar_position_method: nrel_numpy\n         config.update(kwargs)\n         return ModelChain(\n             system, location,\n             clearsky_model=clearsky_model,\n             transposition_model=transposition_model,\n             solar_position_method=solar_position_method,\n \n     def __repr__(self):\n         attrs = [\n            'name', 'clearsky_model',\n             'transposition_model', 'solar_position_method',\n             'airmass_model', 'dc_model', 'ac_model', 'aoi_model',\n             'spectral_model', 'temperature_model', 'losses_model'\n         return ('ModelChain: \\n  ' + '\\n  '.join(\n             f'{attr}: {getmcattr(self, attr)}' for attr in attrs))\n \n     @property\n     def dc_model(self):\n         return self._dc_model"
  },
  {
    "instruction": "golden-section search fails when upper and lower bounds are equal\n**Describe the bug**\r\nI was using pvlib for sometime now and until now I was always passing a big dataframe containing readings of a long period. Because of some changes in our software architecture, I need to pass the weather readings as a single reading (a dataframe with only one row) and I noticed that for readings that GHI-DHI are zero pvlib fails to calculate the output and returns below error while the same code executes correctly with weather information that has non-zero GHI-DHI:\r\n```python\r\nimport os\r\nimport pathlib\r\nimport time\r\nimport json\r\nfrom datetime import datetime\r\nfrom time import mktime, gmtime\r\n\r\nimport pandas as pd\r\n\r\nfrom pvlib import pvsystem\r\nfrom pvlib import location as pvlocation\r\nfrom pvlib import modelchain\r\nfrom pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS as PARAMS # not used -- to remove\r\nfrom pvlib.bifacial.pvfactors import pvfactors_timeseries\r\nfrom pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\r\n\r\nclass PV:\r\n    def pv_transform_time(self, val):\r\n        # tt = gmtime(val / 1000)\r\n        tt = gmtime(val)\r\n        dd = datetime.fromtimestamp(mktime(tt))\r\n        timestamp = pd.Timestamp(dd)\r\n        return timestamp\r\n\r\n    def __init__(self, model: str, inverter: str, latitude: float, longitude: float, **kwargs):\r\n        # super().__init__(**kwargs)\r\n\r\n        temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS[\"sapm\"][\r\n            \"open_rack_glass_glass\"\r\n        ]\r\n        # Load the database of CEC module model parameters\r\n        modules = pvsystem.retrieve_sam(\"cecmod\")\r\n        # Load the database of CEC inverter model parameters\r\n        inverters = pvsystem.retrieve_sam(\"cecinverter\")\r\n\r\n\r\n        # A bare bone PV simulator\r\n\r\n        # Load the database of CEC module model parameters\r\n        modules = pvsystem.retrieve_sam('cecmod')\r\n        inverters = pvsystem.retrieve_sam('cecinverter')\r\n        module_parameters = modules[model]\r\n        inverter_parameters = inverters[inverter]\r\n\r\n        location = pvlocation.Location(latitude=latitude, longitude=longitude)\r\n        system = pvsystem.PVSystem(module_parameters=module_parameters, inverter_parameters=inverter_parameters, temperature_model_parameters=temperature_model_parameters)\r\n        self.modelchain = modelchain.ModelChain(system, location, aoi_model='no_loss', spectral_model=\"no_loss\")\r\n\r\n    def process(self, data):\r\n        weather = pd.read_json(data)\r\n        # print(f\"raw_weather: {weather}\")\r\n        weather.drop('time.1', axis=1, inplace=True)\r\n        weather['time'] = pd.to_datetime(weather['time']).map(datetime.timestamp) # --> this works for the new process_weather code and also the old weather file\r\n        weather[\"time\"] = weather[\"time\"].apply(self.pv_transform_time)\r\n        weather.index = weather[\"time\"]\r\n        # print(f\"weather: {weather}\")\r\n        # print(weather.dtypes)\r\n        # print(weather['ghi'][0])\r\n        # print(type(weather['ghi'][0]))\r\n\r\n        # simulate\r\n        self.modelchain.run_model(weather)\r\n        # print(self.modelchain.results.ac.to_frame().to_json())\r\n        print(self.modelchain.results.ac)\r\n\r\n\r\n# good data\r\ngood_data = \"{\\\"time\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"},\\\"ghi\\\":{\\\"12\\\":36},\\\"dhi\\\":{\\\"12\\\":36},\\\"dni\\\":{\\\"12\\\":0},\\\"Tamb\\\":{\\\"12\\\":8.0},\\\"WindVel\\\":{\\\"12\\\":5.0},\\\"WindDir\\\":{\\\"12\\\":270},\\\"time.1\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"}}\"\r\n\r\n# data that causes error\r\ndata = \"{\\\"time\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"},\\\"ghi\\\":{\\\"4\\\":0},\\\"dhi\\\":{\\\"4\\\":0},\\\"dni\\\":{\\\"4\\\":0},\\\"Tamb\\\":{\\\"4\\\":8.0},\\\"WindVel\\\":{\\\"4\\\":4.0},\\\"WindDir\\\":{\\\"4\\\":240},\\\"time.1\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"}}\"\r\np1 = PV(model=\"Trina_Solar_TSM_300DEG5C_07_II_\", inverter=\"ABB__MICRO_0_25_I_OUTD_US_208__208V_\", latitude=51.204483, longitude=5.265472)\r\np1.process(good_data)\r\nprint(\"=====\")\r\np1.process(data)\r\n```\r\nError:\r\n```log\r\n$ python3 ./tmp-pv.py \r\ntime\r\n2010-01-01 13:30:00    7.825527\r\ndtype: float64\r\n=====\r\n/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py:340: RuntimeWarning: divide by zero encountered in divide\r\n  np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))\r\nTraceback (most recent call last):\r\n  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 88, in <module>\r\n    p1.process(data)\r\n  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 75, in process\r\n    self.modelchain.run_model(weather)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1770, in run_model\r\n    self._run_from_effective_irrad(weather)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1858, in _run_from_effective_irrad\r\n    self.dc_model()\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 790, in cec\r\n    return self._singlediode(self.system.calcparams_cec)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 772, in _singlediode\r\n    self.results.dc = tuple(itertools.starmap(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 931, in singlediode\r\n    return singlediode(photocurrent, saturation_current,\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 2826, in singlediode\r\n    out = _singlediode._lambertw(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/singlediode.py\", line 651, in _lambertw\r\n    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py\", line 364, in _golden_sect_DataFrame\r\n    raise Exception(\"Iterations exceeded maximum. Check that func\",\r\nException: ('Iterations exceeded maximum. Check that func', ' is not NaN in (lower, upper)')\r\n```\r\n\r\nI have to mention that for now the workaround that I am using is to pass the weather data as a dataframe with two rows, the first row is a good weather data that pvlib can process and the second row is the incoming weather reading (I can also post that code if you want).\r\n\r\n**Expected behavior**\r\nPVlib should have consistent behavior and regardless of GHI-DHI readings.\r\n\r\n**Versions:**\r\n```python\r\n>>> import pvlib\r\n>>> import pandas\r\n>>> pvlib.__version__\r\n'0.9.1'\r\n>>> pandas.__version__\r\n'1.4.3'\r\n``` \r\n - python: 3.10.6\r\n- OS: Ubuntu 22.04.1 LTS\n",
    "input": "     --------\n     pvlib.singlediode._pwr_optfcn\n     \"\"\"\n \n     phim1 = (np.sqrt(5) - 1) / 2\n \n     df['VL'] = lower\n \n     converged = False\n    iterations = 0\n \n    # handle all NaN case gracefully\n    with warnings.catch_warnings():\n        warnings.filterwarnings(action='ignore',\n                                message='All-NaN slice encountered')\n        iterlimit = 1 + np.nanmax(\n            np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))\n\n    while not converged and (iterations <= iterlimit):\n \n         phi = phim1 * (df['VH'] - df['VL'])\n         df['V1'] = df['VL'] + phi\n \n         err = abs(df['V2'] - df['V1'])\n \n        # works with single value because err is np.float64\n        converged = (err[~np.isnan(err)] < atol).all()\n        # err will be less than atol before iterations hit the limit\n        # but just to be safe\n        iterations += 1\n\n    if iterations > iterlimit:\n        raise Exception(\"Iterations exceeded maximum. Check that func\",\n                        \" is not NaN in (lower, upper)\")  # pragma: no cover\n \n    try:\n        func_result = func(df, 'V1')\n        x = np.where(np.isnan(func_result), np.nan, df['V1'])\n    except KeyError:\n        func_result = np.full_like(upper, np.nan)\n        x = func_result.copy()\n \n     return func_result, x\n ",
    "output": "     --------\n     pvlib.singlediode._pwr_optfcn\n     \"\"\"\n    if np.any(upper - lower < 0.):\n        raise ValueError('upper >= lower is required')\n \n     phim1 = (np.sqrt(5) - 1) / 2\n \n     df['VL'] = lower\n \n     converged = False\n \n    while not converged:\n \n         phi = phim1 * (df['VH'] - df['VL'])\n         df['V1'] = df['VL'] + phi\n \n         err = abs(df['V2'] - df['V1'])\n \n        # handle all NaN case gracefully\n        with warnings.catch_warnings():\n            warnings.filterwarnings(action='ignore',\n                                    message='All-NaN slice encountered')\n            converged = np.all(err[~np.isnan(err)] < atol)\n \n    # best estimate of location of maximum\n    df['max'] = 0.5 * (df['V1'] + df['V2'])\n    func_result = func(df, 'max')\n    x = np.where(np.isnan(func_result), np.nan, df['max'])\n \n     return func_result, x\n "
  },
  {
    "instruction": "Consider extracting the surface orientation calculation in pvlib.tracking.singleaxis() to its own function\n**Is your feature request related to a problem? Please describe.**\r\nThe usual workflow for modeling single-axis tracking in pvlib is to treat tracker rotation (`tracker_theta`) as an unknown to be calculated from solar position and array geometry.  However, sometimes a user might have their own tracker rotations but not have the corresponding `surface_tilt` and `surface_azimuth` values.  Here are a few motivating examples:\r\n- Using measured rotation angles\r\n- Post-processing the output of `tracking.singleaxis()` to include wind stow events or tracker stalls\r\n- Other tracking algorithms that determine rotation differently from the astronomical method\r\n\r\nAssuming I have my tracker rotations already in hand, getting the corresponding `surface_tilt` and `surface_azimuth` angles is not as easy as it should be.  For the specific case of horizontal N-S axis the math isn't so bad, but either way it's annoying to have to DIY when pvlib already has code to calculate those angles from tracker rotation.\r\n\r\n**Describe the solution you'd like**\r\nA function `pvlib.tracking.rotation_to_orientation` that implements the same math in `pvlib.tracking.singleaxis` to go from `tracker_theta` to `surface_tilt` and `surface_azimuth`.  Basically extract out the second half of `tracking.singleaxis` into a new function.  Suggestions for the function name are welcome.  To be explicit, this is more or less what I'm imagining:\r\n\r\n```python\r\ndef rotation_to_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0, max_angle=90):\r\n    # insert math from second half of tracking.singleaxis() here\r\n    out = {'tracker_theta': tracker_theta, 'aoi': aoi,\r\n           'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\r\n    return pandas_if_needed(out)\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nContinue suffering\r\n\r\n**Additional context**\r\nThis is one step towards a broader goal I have for `pvlib.tracking` to house other methods to determine tracker rotation in addition to the current astronomical method, the same way we have multiple temperature and transposition models.  These functions would be responsible for determining tracker rotations, and they'd all use this `rotation_to_orientation` function to convert rotation to module orientation.\r\n\r\nSeparately, I wonder if the code could be simplified using the tilt and azimuth equations in Bill's technical report (https://www.nrel.gov/docs/fy13osti/58891.pdf) -- seems like what we're doing is overly complicated, although maybe I've just not studied it closely enough.\r\n\r\ncc @williamhobbs @spaneja \n",
    "input": "     return res\n \n \n def localize_to_utc(time, location):\n     \"\"\"\n     Converts or localizes a time series to UTC.\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n import numpy as np\n import pandas as pd\n \nfrom pvlib.tools import cosd, sind, tand\n from pvlib.pvsystem import (\n     PVSystem, Array, SingleAxisTrackerMount, _unwrap_single_value\n )\n     Returns\n     -------\n     dict or DataFrame with the following columns:\n        * `tracker_theta`: The rotation angle of the tracker.\n          tracker_theta = 0 is horizontal, and positive rotation angles are\n          clockwise. [degrees]\n         * `aoi`: The angle-of-incidence of direct irradiance onto the\n           rotated panel surface. [degrees]\n         * `surface_tilt`: The angle between the panel surface and the earth\n     --------\n     pvlib.tracking.calc_axis_tilt\n     pvlib.tracking.calc_cross_axis_tilt\n \n     References\n     ----------\n     cos_axis_tilt = cosd(axis_tilt)\n     sin_axis_tilt = sind(axis_tilt)\n     xp = x*cos_axis_azimuth - y*sin_axis_azimuth\n    yp = (x*cos_axis_tilt*sin_axis_azimuth\n          + y*cos_axis_tilt*cos_axis_azimuth\n          - z*sin_axis_tilt)\n     zp = (x*sin_axis_tilt*sin_axis_azimuth\n           + y*sin_axis_tilt*cos_axis_azimuth\n           + z*cos_axis_tilt)\n     # system-plane normal\n     tracker_theta = np.clip(tracker_theta, -max_angle, max_angle)\n \n    # Calculate panel normal vector in panel-oriented x, y, z coordinates.\n    # y-axis is axis of tracker rotation. tracker_theta is a compass angle\n    # (clockwise is positive) rather than a trigonometric angle.\n    # NOTE: the *0 is a trick to preserve NaN values.\n    panel_norm = np.array([sind(tracker_theta),\n                           tracker_theta*0,\n                           cosd(tracker_theta)])\n\n    # sun position in vector format in panel-oriented x, y, z coordinates\n    sun_vec = np.array([xp, yp, zp])\n\n    # calculate angle-of-incidence on panel\n    # TODO: use irradiance.aoi\n    projection = np.clip(np.sum(sun_vec*panel_norm, axis=0), -1, 1)\n    aoi = np.degrees(np.arccos(projection))\n\n    # Calculate panel tilt and azimuth in a coordinate system where the panel\n    # tilt is the angle from horizontal, and the panel azimuth is the compass\n    # angle (clockwise from north) to the projection of the panel's normal to\n    # the earth's surface. These outputs are provided for convenience and\n    # comparison with other PV software which use these angle conventions.\n\n    # Project normal vector to earth surface. First rotate about x-axis by\n    # angle -axis_tilt so that y-axis is also parallel to earth surface, then\n    # project.\n\n    # Calculate standard rotation matrix\n    rot_x = np.array([[1, 0, 0],\n                      [0, cosd(-axis_tilt), -sind(-axis_tilt)],\n                      [0, sind(-axis_tilt), cosd(-axis_tilt)]])\n\n    # panel_norm_earth contains the normal vector expressed in earth-surface\n    # coordinates (z normal to surface, y aligned with tracker axis parallel to\n    # earth)\n    panel_norm_earth = np.dot(rot_x, panel_norm).T\n\n    # projection to plane tangent to earth surface, in earth surface\n    # coordinates\n    projected_normal = np.array([panel_norm_earth[:, 0],\n                                 panel_norm_earth[:, 1],\n                                 panel_norm_earth[:, 2]*0]).T\n\n    # calculate vector magnitudes\n    projected_normal_mag = np.sqrt(np.nansum(projected_normal**2, axis=1))\n\n    # renormalize the projected vector, avoid creating nan values.\n    non_zeros = projected_normal_mag != 0\n    projected_normal[non_zeros] = (projected_normal[non_zeros].T /\n                                   projected_normal_mag[non_zeros]).T\n\n    # calculation of surface_azimuth\n    surface_azimuth = \\\n        np.degrees(np.arctan2(projected_normal[:, 1], projected_normal[:, 0]))\n\n    # Rotate 0 reference from panel's x-axis to its y-axis and then back to\n    # north.\n    surface_azimuth = 90 - surface_azimuth + axis_azimuth\n\n    # Map azimuth into [0,360) domain.\n    with np.errstate(invalid='ignore'):\n        surface_azimuth = surface_azimuth % 360\n\n    # Calculate surface_tilt\n    dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n    # for edge cases like axis_tilt=90, numpy's SIMD can produce values like\n    # dotproduct = (1 + 2e-16). Clip off the excess so that arccos works:\n    dotproduct = np.clip(dotproduct, -1, 1)\n    surface_tilt = 90 - np.degrees(np.arccos(dotproduct))\n \n     # Bundle DataFrame for return values and filter for sun below horizon.\n     out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n           'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\n     if index is not None:\n         out = pd.DataFrame(out, index=index)\n        out = out[['tracker_theta', 'aoi', 'surface_azimuth', 'surface_tilt']]\n         out[zen_gt_90] = np.nan\n     else:\n         out = {k: np.where(zen_gt_90, np.nan, v) for k, v in out.items()}\n     return out\n \n \n def calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth):\n     \"\"\"\n     Calculate tracker axis tilt in the global reference frame when on a sloped",
    "output": "     return res\n \n \ndef acosd(number):\n    \"\"\"\n    Inverse Cosine returning an angle in degrees\n\n    Parameters\n    ----------\n    number : float\n        Input number\n\n    Returns\n    -------\n    result : float\n        arccos result\n    \"\"\"\n\n    res = np.degrees(np.arccos(number))\n    return res\n\n\n def localize_to_utc(time, location):\n     \"\"\"\n     Converts or localizes a time series to UTC.\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n import numpy as np\n import pandas as pd\n \nfrom pvlib.tools import cosd, sind, tand, acosd, asind\n from pvlib.pvsystem import (\n     PVSystem, Array, SingleAxisTrackerMount, _unwrap_single_value\n )\n     Returns\n     -------\n     dict or DataFrame with the following columns:\n        * `tracker_theta`: The rotation angle of the tracker is a right-handed\n          rotation defined by `axis_azimuth`.\n          tracker_theta = 0 is horizontal. [degrees]\n         * `aoi`: The angle-of-incidence of direct irradiance onto the\n           rotated panel surface. [degrees]\n         * `surface_tilt`: The angle between the panel surface and the earth\n     --------\n     pvlib.tracking.calc_axis_tilt\n     pvlib.tracking.calc_cross_axis_tilt\n    pvlib.tracking.calc_surface_orientation\n \n     References\n     ----------\n     cos_axis_tilt = cosd(axis_tilt)\n     sin_axis_tilt = sind(axis_tilt)\n     xp = x*cos_axis_azimuth - y*sin_axis_azimuth\n    # not necessary to calculate y'\n    # yp = (x*cos_axis_tilt*sin_axis_azimuth\n    #       + y*cos_axis_tilt*cos_axis_azimuth\n    #       - z*sin_axis_tilt)\n     zp = (x*sin_axis_tilt*sin_axis_azimuth\n           + y*sin_axis_tilt*cos_axis_azimuth\n           + z*cos_axis_tilt)\n     # system-plane normal\n     tracker_theta = np.clip(tracker_theta, -max_angle, max_angle)\n \n    # Calculate auxiliary angles\n    surface = calc_surface_orientation(tracker_theta, axis_tilt, axis_azimuth)\n    surface_tilt = surface['surface_tilt']\n    surface_azimuth = surface['surface_azimuth']\n    aoi = irradiance.aoi(surface_tilt, surface_azimuth,\n                         apparent_zenith, apparent_azimuth)\n \n     # Bundle DataFrame for return values and filter for sun below horizon.\n     out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n           'surface_azimuth': surface_azimuth, 'surface_tilt': surface_tilt}\n     if index is not None:\n         out = pd.DataFrame(out, index=index)\n         out[zen_gt_90] = np.nan\n     else:\n         out = {k: np.where(zen_gt_90, np.nan, v) for k, v in out.items()}\n     return out\n \n \ndef calc_surface_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0):\n    \"\"\"\n    Calculate the surface tilt and azimuth angles for a given tracker rotation.\n\n    Parameters\n    ----------\n    tracker_theta : numeric\n        Tracker rotation angle as a right-handed rotation around\n        the axis defined by ``axis_tilt`` and ``axis_azimuth``.  For example,\n        with ``axis_tilt=0`` and ``axis_azimuth=180``, ``tracker_theta > 0``\n        results in ``surface_azimuth`` to the West while ``tracker_theta < 0``\n        results in ``surface_azimuth`` to the East. [degree]\n    axis_tilt : float, default 0\n        The tilt of the axis of rotation with respect to horizontal. [degree]\n    axis_azimuth : float, default 0\n        A value denoting the compass direction along which the axis of\n        rotation lies. Measured east of north. [degree]\n\n    Returns\n    -------\n    dict or DataFrame\n        Contains keys ``'surface_tilt'`` and ``'surface_azimuth'`` representing\n        the module orientation accounting for tracker rotation and axis\n        orientation. [degree]\n\n    References\n    ----------\n    .. [1] William F. Marion and Aron P. Dobos, \"Rotation Angle for the Optimum\n       Tracking of One-Axis Trackers\", Technical Report NREL/TP-6A20-58891,\n       July 2013. :doi:`10.2172/1089596`\n    \"\"\"\n    with np.errstate(invalid='ignore', divide='ignore'):\n        surface_tilt = acosd(cosd(tracker_theta) * cosd(axis_tilt))\n\n        # clip(..., -1, +1) to prevent arcsin(1 + epsilon) issues:\n        azimuth_delta = asind(np.clip(sind(tracker_theta) / sind(surface_tilt),\n                                      a_min=-1, a_max=1))\n        # Combine Eqs 2, 3, and 4:\n        azimuth_delta = np.where(abs(tracker_theta) < 90,\n                                 azimuth_delta,\n                                 -azimuth_delta + np.sign(tracker_theta) * 180)\n        # handle surface_tilt=0 case:\n        azimuth_delta = np.where(sind(surface_tilt) != 0, azimuth_delta, 90)\n        surface_azimuth = (axis_azimuth + azimuth_delta) % 360\n\n    out = {\n        'surface_tilt': surface_tilt,\n        'surface_azimuth': surface_azimuth,\n    }\n    if hasattr(tracker_theta, 'index'):\n        out = pd.DataFrame(out)\n    return out\n\n\n def calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth):\n     \"\"\"\n     Calculate tracker axis tilt in the global reference frame when on a sloped"
  },
  {
    "instruction": "Infinite sheds perf improvement: vectorize over surface_tilt\nInfinite sheds is quite a bit slower than the modelchain POA modeling we use for frontside (as expected). I see a TODO comment in the code for _vf_ground_sky_integ (`_TODO: vectorize over surface_tilt_`) that could potentially result in some perf improvement for Infinite sheds calls with tracking systems.\n",
    "input": " \n class InfiniteSheds:\n \n    def setup(self):\n         self.times = pd.date_range(start='20180601', freq='1min',\n                                    periods=1440)\n         self.location = location.Location(40, -80)\n                 gcr=self.gcr\n             )\n \n    def time_get_irradiance_poa_fixed(self):\n         infinite_sheds.get_irradiance_poa(\n             surface_tilt=self.surface_tilt,\n             surface_azimuth=self.surface_azimuth,\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints\n         )\n \n    def time_get_irradiance_poa_tracking(self):\n         infinite_sheds.get_irradiance_poa(\n             surface_tilt=self.tracking['surface_tilt'],\n             surface_azimuth=self.tracking['surface_azimuth'],\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints\n         )\n \n    def time_get_irradiance_fixed(self):\n         infinite_sheds.get_irradiance(\n             surface_tilt=self.surface_tilt,\n             surface_azimuth=self.surface_azimuth,\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints\n         )\n \n    def time_get_irradiance_tracking(self):\n         infinite_sheds.get_irradiance(\n             surface_tilt=self.tracking['surface_tilt'],\n             surface_azimuth=self.tracking['surface_azimuth'],\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints\n         )\ndiff --git a/pvlib/bifacial/infinite_sheds.py b/pvlib/bifacial/infinite_sheds.py\n from pvlib.irradiance import beam_component, aoi, haydavies\n \n def _vf_ground_sky_integ(surface_tilt, surface_azimuth, gcr, height,\n                         pitch, max_rows=10, npoints=100):\n     \"\"\"\n    Integrated and per-point view factors from the ground to the sky at points\n    between interior rows of the array.\n \n     Parameters\n     ----------\n         Maximum number of rows to consider in front and behind the current row.\n     npoints : int, default 100\n         Number of points used to discretize distance along the ground.\n \n     Returns\n     -------\n    fgnd_sky : float\n         Integration of view factor over the length between adjacent, interior\n        rows. [unitless]\n    fz : ndarray\n        Fraction of distance from the previous row to the next row. [unitless]\n    fz_sky : ndarray\n        View factors at discrete points between adjacent, interior rows.\n        [unitless]\n\n     \"\"\"\n    # TODO: vectorize over surface_tilt\n     # Abuse utils._vf_ground_sky_2d by supplying surface_tilt in place\n     # of a signed rotation. This is OK because\n     # 1) z span the full distance between 2 rows, and\n     # The VFs to the sky will thus be symmetric around z=0.5\n     z = np.linspace(0, 1, npoints)\n     rotation = np.atleast_1d(surface_tilt)\n    fz_sky = np.zeros((len(rotation), npoints))\n    for k, r in enumerate(rotation):\n        vf, _ = utils._vf_ground_sky_2d(z, r, gcr, pitch, height, max_rows)\n        fz_sky[k, :] = vf\n     # calculate the integrated view factor for all of the ground between rows\n    return np.trapz(fz_sky, z, axis=1)\n \n \n def _poa_ground_shadows(poa_ground, f_gnd_beam, df, vf_gnd_sky):\n def get_irradiance_poa(surface_tilt, surface_azimuth, solar_zenith,\n                        solar_azimuth, gcr, height, pitch, ghi, dhi, dni,\n                        albedo, model='isotropic', dni_extra=None, iam=1.0,\n                       npoints=100):\n     r\"\"\"\n     Calculate plane-of-array (POA) irradiance on one side of a row of modules.\n \n         on the surface that is not reflected away. [unitless]\n \n     npoints : int, default 100\n        Number of points used to discretize distance along the ground.\n \n     Returns\n     -------\n     # method differs from [1], Eq. 7 and Eq. 8; height is defined at row\n     # center rather than at row lower edge as in [1].\n     vf_gnd_sky = _vf_ground_sky_integ(\n        surface_tilt, surface_azimuth, gcr, height, pitch, max_rows, npoints)\n     # fraction of row slant height that is shaded from direct irradiance\n     f_x = _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n                            surface_azimuth, gcr)\n                    gcr, height, pitch, ghi, dhi, dni,\n                    albedo, model='isotropic', dni_extra=None, iam_front=1.0,\n                    iam_back=1.0, bifaciality=0.8, shade_factor=-0.02,\n                   transmission_factor=0, npoints=100):\n     \"\"\"\n     Get front and rear irradiance using the infinite sheds model.\n \n         etc. A negative value is a reduction in back irradiance. [unitless]\n \n     npoints : int, default 100\n        Number of points used to discretize distance along the ground.\n \n     Returns\n     -------\n         solar_zenith=solar_zenith, solar_azimuth=solar_azimuth,\n         gcr=gcr, height=height, pitch=pitch, ghi=ghi, dhi=dhi, dni=dni,\n         albedo=albedo, model=model, dni_extra=dni_extra, iam=iam_front,\n        npoints=npoints)\n     # back side POA irradiance\n     irrad_back = get_irradiance_poa(\n         surface_tilt=backside_tilt, surface_azimuth=backside_sysaz,\n         solar_zenith=solar_zenith, solar_azimuth=solar_azimuth,\n         gcr=gcr, height=height, pitch=pitch, ghi=ghi, dhi=dhi, dni=dni,\n         albedo=albedo, model=model, dni_extra=dni_extra, iam=iam_back,\n        npoints=npoints)\n \n     colmap_front = {\n         'poa_global': 'poa_front',\ndiff --git a/pvlib/bifacial/utils.py b/pvlib/bifacial/utils.py\n import numpy as np\n from pvlib.tools import sind, cosd, tand\n \n\n def _solar_projection_tangent(solar_zenith, solar_azimuth, surface_azimuth):\n     \"\"\"\n     Tangent of the angle between the zenith vector and the sun vector\n         Position on the ground between two rows, as a fraction of the pitch.\n         x = 0 corresponds to the point on the ground directly below the\n         center point of a row. Positive x is towards the right. [unitless]\n    rotation : float\n         Rotation angle of the row's right edge relative to row center.\n         [degree]\n     gcr : float\n \n     Returns\n     -------\n    vf : numeric\n        Fraction of sky dome visible from each point on the ground. [unitless]\n    wedge_angles : array\n        Angles defining each wedge of sky that is blocked by a row. Shape is\n        (2, len(x), 2*max_rows+1). ``wedge_angles[0,:,:]`` is the\n        starting angle of each wedge, ``wedge_angles[1,:,:]`` is the end angle.\n        [degree]\n     \"\"\"\n    x = np.atleast_1d(x)  # handle float\n     all_k = np.arange(-max_rows, max_rows + 1)\n     width = gcr * pitch / 2.\n     # angles from x to right edge of each row\n    a1 = height + width * sind(rotation)\n    b1 = (all_k - x[:, np.newaxis]) * pitch + width * cosd(rotation)\n    phi_1 = np.degrees(np.arctan2(a1, b1))\n     # angles from x to left edge of each row\n    a2 = height - width * sind(rotation)\n    b2 = (all_k - x[:, np.newaxis]) * pitch - width * cosd(rotation)\n    phi_2 = np.degrees(np.arctan2(a2, b2))\n    phi = np.stack([phi_1, phi_2])\n    swap = phi[0, :, :] > phi[1, :, :]\n    # swap where phi_1 > phi_2 so that phi_1[0,:,:] is the lesser angle\n    phi = np.where(swap, phi[::-1], phi)\n    # right edge of next row - left edge of previous row\n    wedge_vfs = 0.5 * (cosd(phi[1, :, 1:]) - cosd(phi[0, :, :-1]))\n    vf = np.sum(np.where(wedge_vfs > 0, wedge_vfs, 0.), axis=1)\n    return vf, phi",
    "output": " \n class InfiniteSheds:\n \n    # benchmark variant parameters (run both vectorize=True and False)\n    params = [True, False]\n    param_names = ['vectorize']\n\n    def setup(self, vectorize):\n         self.times = pd.date_range(start='20180601', freq='1min',\n                                    periods=1440)\n         self.location = location.Location(40, -80)\n                 gcr=self.gcr\n             )\n \n    def time_get_irradiance_poa_fixed(self, vectorize):\n         infinite_sheds.get_irradiance_poa(\n             surface_tilt=self.surface_tilt,\n             surface_azimuth=self.surface_azimuth,\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints,\n            vectorize=vectorize,\n         )\n \n    def time_get_irradiance_poa_tracking(self, vectorize):\n         infinite_sheds.get_irradiance_poa(\n             surface_tilt=self.tracking['surface_tilt'],\n             surface_azimuth=self.tracking['surface_azimuth'],\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints,\n            vectorize=vectorize,\n         )\n \n    def time_get_irradiance_fixed(self, vectorize):\n         infinite_sheds.get_irradiance(\n             surface_tilt=self.surface_tilt,\n             surface_azimuth=self.surface_azimuth,\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints,\n            vectorize=vectorize,\n         )\n \n    def time_get_irradiance_tracking(self, vectorize):\n         infinite_sheds.get_irradiance(\n             surface_tilt=self.tracking['surface_tilt'],\n             surface_azimuth=self.tracking['surface_azimuth'],\n             dhi=self.clearsky_irradiance['dhi'],\n             dni=self.clearsky_irradiance['dni'],\n             albedo=self.albedo,\n            npoints=self.npoints,\n            vectorize=vectorize,\n         )\ndiff --git a/pvlib/bifacial/infinite_sheds.py b/pvlib/bifacial/infinite_sheds.py\n from pvlib.irradiance import beam_component, aoi, haydavies\n \n def _vf_ground_sky_integ(surface_tilt, surface_azimuth, gcr, height,\n                         pitch, max_rows=10, npoints=100, vectorize=False):\n     \"\"\"\n    Integrated view factor to the sky from the ground underneath\n    interior rows of the array.\n \n     Parameters\n     ----------\n         Maximum number of rows to consider in front and behind the current row.\n     npoints : int, default 100\n         Number of points used to discretize distance along the ground.\n    vectorize : bool, default False\n        If True, vectorize the view factor calculation across ``surface_tilt``.\n        This increases speed with the cost of increased memory usage.\n \n     Returns\n     -------\n    fgnd_sky : numeric\n         Integration of view factor over the length between adjacent, interior\n        rows.  Shape matches that of ``surface_tilt``. [unitless]\n     \"\"\"\n     # Abuse utils._vf_ground_sky_2d by supplying surface_tilt in place\n     # of a signed rotation. This is OK because\n     # 1) z span the full distance between 2 rows, and\n     # The VFs to the sky will thus be symmetric around z=0.5\n     z = np.linspace(0, 1, npoints)\n     rotation = np.atleast_1d(surface_tilt)\n    if vectorize:\n        fz_sky = utils._vf_ground_sky_2d(z, rotation, gcr, pitch, height,\n                                         max_rows)\n    else:\n        fz_sky = np.zeros((npoints, len(rotation)))\n        for k, r in enumerate(rotation):\n            vf = utils._vf_ground_sky_2d(z, r, gcr, pitch, height, max_rows)\n            fz_sky[:, k] = vf[:, 0]  # remove spurious rotation dimension\n     # calculate the integrated view factor for all of the ground between rows\n    return np.trapz(fz_sky, z, axis=0)\n \n \n def _poa_ground_shadows(poa_ground, f_gnd_beam, df, vf_gnd_sky):\n def get_irradiance_poa(surface_tilt, surface_azimuth, solar_zenith,\n                        solar_azimuth, gcr, height, pitch, ghi, dhi, dni,\n                        albedo, model='isotropic', dni_extra=None, iam=1.0,\n                       npoints=100, vectorize=False):\n     r\"\"\"\n     Calculate plane-of-array (POA) irradiance on one side of a row of modules.\n \n         on the surface that is not reflected away. [unitless]\n \n     npoints : int, default 100\n        Number of discretization points for calculating integrated view\n        factors.\n\n    vectorize : bool, default False\n        If True, vectorize the view factor calculation across ``surface_tilt``.\n        This increases speed with the cost of increased memory usage.\n \n     Returns\n     -------\n     # method differs from [1], Eq. 7 and Eq. 8; height is defined at row\n     # center rather than at row lower edge as in [1].\n     vf_gnd_sky = _vf_ground_sky_integ(\n        surface_tilt, surface_azimuth, gcr, height, pitch, max_rows, npoints,\n        vectorize)\n     # fraction of row slant height that is shaded from direct irradiance\n     f_x = _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n                            surface_azimuth, gcr)\n                    gcr, height, pitch, ghi, dhi, dni,\n                    albedo, model='isotropic', dni_extra=None, iam_front=1.0,\n                    iam_back=1.0, bifaciality=0.8, shade_factor=-0.02,\n                   transmission_factor=0, npoints=100, vectorize=False):\n     \"\"\"\n     Get front and rear irradiance using the infinite sheds model.\n \n         etc. A negative value is a reduction in back irradiance. [unitless]\n \n     npoints : int, default 100\n        Number of discretization points for calculating integrated view\n        factors.\n\n    vectorize : bool, default False\n        If True, vectorize the view factor calculation across ``surface_tilt``.\n        This increases speed with the cost of increased memory usage.\n \n     Returns\n     -------\n         solar_zenith=solar_zenith, solar_azimuth=solar_azimuth,\n         gcr=gcr, height=height, pitch=pitch, ghi=ghi, dhi=dhi, dni=dni,\n         albedo=albedo, model=model, dni_extra=dni_extra, iam=iam_front,\n        npoints=npoints, vectorize=vectorize)\n     # back side POA irradiance\n     irrad_back = get_irradiance_poa(\n         surface_tilt=backside_tilt, surface_azimuth=backside_sysaz,\n         solar_zenith=solar_zenith, solar_azimuth=solar_azimuth,\n         gcr=gcr, height=height, pitch=pitch, ghi=ghi, dhi=dhi, dni=dni,\n         albedo=albedo, model=model, dni_extra=dni_extra, iam=iam_back,\n        npoints=npoints, vectorize=vectorize)\n \n     colmap_front = {\n         'poa_global': 'poa_front',\ndiff --git a/pvlib/bifacial/utils.py b/pvlib/bifacial/utils.py\n import numpy as np\n from pvlib.tools import sind, cosd, tand\n \n def _solar_projection_tangent(solar_zenith, solar_azimuth, surface_azimuth):\n     \"\"\"\n     Tangent of the angle between the zenith vector and the sun vector\n         Position on the ground between two rows, as a fraction of the pitch.\n         x = 0 corresponds to the point on the ground directly below the\n         center point of a row. Positive x is towards the right. [unitless]\n    rotation : numeric\n         Rotation angle of the row's right edge relative to row center.\n         [degree]\n     gcr : float\n \n     Returns\n     -------\n    vf : array\n        Fraction of sky dome visible from each point on the ground.\n        Shape is (len(x), len(rotation)). [unitless]\n     \"\"\"\n    # This function creates large float64 arrays of size\n    # (2*len(x)*len(rotation)*len(max_rows)) or ~100 MB for\n    # typical time series inputs.  This function makes heavy\n    # use of numpy's out parameter to avoid allocating new\n    # memory.  Unfortunately that comes at the cost of some\n    # readability: because arrays get reused to avoid new allocations,\n    # variable names don't always match what they hold.\n\n    # handle floats:\n    x = np.atleast_1d(x)[:, np.newaxis, np.newaxis]\n    rotation = np.atleast_1d(rotation)[np.newaxis, :, np.newaxis]\n     all_k = np.arange(-max_rows, max_rows + 1)\n     width = gcr * pitch / 2.\n    distance_to_row_centers = (all_k - x) * pitch\n    dy = width * sind(rotation)\n    dx = width * cosd(rotation)\n\n    phi = np.empty((2, x.shape[0], rotation.shape[1], len(all_k)))\n\n     # angles from x to right edge of each row\n    a1 = height + dy\n    # temporarily store one leg of the triangle in phi:\n    np.add(distance_to_row_centers, dx, out=phi[0])\n    np.arctan2(a1, phi[0], out=phi[0])\n\n     # angles from x to left edge of each row\n    a2 = height - dy\n    np.subtract(distance_to_row_centers, dx, out=phi[1])\n    np.arctan2(a2, phi[1], out=phi[1])\n\n    # swap angles so that phi[0,:,:,:] is the lesser angle\n    phi.sort(axis=0)\n\n    # now re-use phi's memory again, this time storing cos(phi).\n    next_edge = phi[1, :, :, 1:]\n    np.cos(next_edge, out=next_edge)\n    prev_edge = phi[0, :, :, :-1]\n    np.cos(prev_edge, out=prev_edge)\n    # right edge of next row - left edge of previous row, again\n    # reusing memory so that the difference is stored in next_edge.\n    # Note that the 0.5 view factor coefficient is applied after summing\n    # as a minor speed optimization.\n    np.subtract(next_edge, prev_edge, out=next_edge)\n    np.clip(next_edge, a_min=0., a_max=None, out=next_edge)\n    vf = np.sum(next_edge, axis=-1) / 2\n    return vf"
  },
  {
    "instruction": "Add interp method for modelchain aoi model.\nI would like to simulate the effect of different IAM functions on performance. Pvlib already has an `interp` method for the iam_loss function. However, it is not possible to use `interp` within model chain. Can we add this feature?\n",
    "input": "     'physical': {'n', 'K', 'L'},\n     'martin_ruiz': {'a_r'},\n     'sapm': {'B0', 'B1', 'B2', 'B3', 'B4', 'B5'},\n    'interp': set()\n }\n \n \ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n from typing import Union, Tuple, Optional, TypeVar\n \n from pvlib import (atmosphere, clearsky, inverter, pvsystem, solarposition,\n                   temperature)\n import pvlib.irradiance  # avoid name conflict with full import\n from pvlib.pvsystem import _DC_MODEL_PARAMS\nfrom pvlib._deprecation import pvlibDeprecationWarning\n from pvlib.tools import _build_kwargs\n \n from pvlib._deprecation import deprecated\n     # scalar, None, other?\n     return repr(obj)\n \n    \n # Type for fields that vary between arrays\n T = TypeVar('T')\n \n         If None, the model will be inferred from the parameters that\n         are common to all of system.arrays[i].module_parameters.\n         Valid strings are 'physical', 'ashrae', 'sapm', 'martin_ruiz',\n        'no_loss'. The ModelChain instance will be passed as the\n         first argument to a user-defined function.\n \n     spectral_model: None, str, or function, default None\n                 self._aoi_model = self.sapm_aoi_loss\n             elif model == 'martin_ruiz':\n                 self._aoi_model = self.martin_ruiz_aoi_loss\n             elif model == 'no_loss':\n                 self._aoi_model = self.no_aoi_loss\n             else:\n         module_parameters = tuple(\n             array.module_parameters for array in self.system.arrays)\n         params = _common_keys(module_parameters)\n        if {'K', 'L', 'n'} <= params:\n             return self.physical_aoi_loss\n        elif {'B5', 'B4', 'B3', 'B2', 'B1', 'B0'} <= params:\n             return self.sapm_aoi_loss\n        elif {'b'} <= params:\n             return self.ashrae_aoi_loss\n        elif {'a_r'} <= params:\n             return self.martin_ruiz_aoi_loss\n         else:\n             raise ValueError('could not infer AOI model from '\n                              'system.arrays[i].module_parameters. Check that '\n                              'the module_parameters for all Arrays in '\n                             'system.arrays contain parameters for '\n                             'the physical, aoi, ashrae or martin_ruiz model; '\n                             'explicitly set the model with the aoi_model '\n                             'kwarg; or set aoi_model=\"no_loss\".')\n \n     def ashrae_aoi_loss(self):\n         self.results.aoi_modifier = self.system.get_iam(\n         )\n         return self\n \n     def no_aoi_loss(self):\n         if self.system.num_arrays == 1:\n             self.results.aoi_modifier = 1.0\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n import io\n import itertools\n import os\n from urllib.request import urlopen\n import numpy as np\n from scipy import constants\n \n         aoi_model : string, default 'physical'\n             The IAM model to be used. Valid strings are 'physical', 'ashrae',\n            'martin_ruiz' and 'sapm'.\n         Returns\n         -------\n         iam : numeric or tuple of numeric\n \n         aoi_model : string, default 'physical'\n             The IAM model to be used. Valid strings are 'physical', 'ashrae',\n            'martin_ruiz' and 'sapm'.\n \n         Returns\n         -------\n             if `iam_model` is not a valid model name.\n         \"\"\"\n         model = iam_model.lower()\n        if model in ['ashrae', 'physical', 'martin_ruiz']:\n            param_names = iam._IAM_MODEL_PARAMS[model]\n            kwargs = _build_kwargs(param_names, self.module_parameters)\n            func = getattr(iam, model)\n             return func(aoi, **kwargs)\n         elif model == 'sapm':\n             return iam.sapm(aoi, self.module_parameters)\n        elif model == 'interp':\n            raise ValueError(model + ' is not implemented as an IAM model '\n                             'option for Array')\n         else:\n             raise ValueError(model + ' is not a valid IAM model')\n ",
    "output": "     'physical': {'n', 'K', 'L'},\n     'martin_ruiz': {'a_r'},\n     'sapm': {'B0', 'B1', 'B2', 'B3', 'B4', 'B5'},\n    'interp': {'theta_ref', 'iam_ref'}\n }\n \n \ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n from typing import Union, Tuple, Optional, TypeVar\n \n from pvlib import (atmosphere, clearsky, inverter, pvsystem, solarposition,\n                   temperature, iam)\n import pvlib.irradiance  # avoid name conflict with full import\n from pvlib.pvsystem import _DC_MODEL_PARAMS\n from pvlib.tools import _build_kwargs\n \n from pvlib._deprecation import deprecated\n     # scalar, None, other?\n     return repr(obj)\n \n\n # Type for fields that vary between arrays\n T = TypeVar('T')\n \n         If None, the model will be inferred from the parameters that\n         are common to all of system.arrays[i].module_parameters.\n         Valid strings are 'physical', 'ashrae', 'sapm', 'martin_ruiz',\n        'interp' and 'no_loss'. The ModelChain instance will be passed as the\n         first argument to a user-defined function.\n \n     spectral_model: None, str, or function, default None\n                 self._aoi_model = self.sapm_aoi_loss\n             elif model == 'martin_ruiz':\n                 self._aoi_model = self.martin_ruiz_aoi_loss\n            elif model == 'interp':\n                self._aoi_model = self.interp_aoi_loss\n             elif model == 'no_loss':\n                 self._aoi_model = self.no_aoi_loss\n             else:\n         module_parameters = tuple(\n             array.module_parameters for array in self.system.arrays)\n         params = _common_keys(module_parameters)\n        if iam._IAM_MODEL_PARAMS['physical'] <= params:\n             return self.physical_aoi_loss\n        elif iam._IAM_MODEL_PARAMS['sapm'] <= params:\n             return self.sapm_aoi_loss\n        elif iam._IAM_MODEL_PARAMS['ashrae'] <= params:\n             return self.ashrae_aoi_loss\n        elif iam._IAM_MODEL_PARAMS['martin_ruiz'] <= params:\n             return self.martin_ruiz_aoi_loss\n        elif iam._IAM_MODEL_PARAMS['interp'] <= params:\n            return self.interp_aoi_loss\n         else:\n             raise ValueError('could not infer AOI model from '\n                              'system.arrays[i].module_parameters. Check that '\n                              'the module_parameters for all Arrays in '\n                             'system.arrays contain parameters for the '\n                             'physical, aoi, ashrae, martin_ruiz or interp '\n                             'model; explicitly set the model with the '\n                             'aoi_model kwarg; or set aoi_model=\"no_loss\".')\n \n     def ashrae_aoi_loss(self):\n         self.results.aoi_modifier = self.system.get_iam(\n         )\n         return self\n \n    def interp_aoi_loss(self):\n        self.results.aoi_modifier = self.system.get_iam(\n            self.results.aoi,\n            iam_model='interp'\n        )\n        return self\n\n     def no_aoi_loss(self):\n         if self.system.num_arrays == 1:\n             self.results.aoi_modifier = 1.0\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n import io\n import itertools\n import os\nimport inspect\n from urllib.request import urlopen\n import numpy as np\n from scipy import constants\n \n         aoi_model : string, default 'physical'\n             The IAM model to be used. Valid strings are 'physical', 'ashrae',\n            'martin_ruiz', 'sapm' and 'interp'.\n         Returns\n         -------\n         iam : numeric or tuple of numeric\n \n         aoi_model : string, default 'physical'\n             The IAM model to be used. Valid strings are 'physical', 'ashrae',\n            'martin_ruiz', 'sapm' and 'interp'.\n \n         Returns\n         -------\n             if `iam_model` is not a valid model name.\n         \"\"\"\n         model = iam_model.lower()\n        if model in ['ashrae', 'physical', 'martin_ruiz', 'interp']:\n            func = getattr(iam, model)  # get function at pvlib.iam\n            # get all parameters from function signature to retrieve them from\n            # module_parameters if present\n            params = set(inspect.signature(func).parameters.keys())\n            params.discard('aoi')  # exclude aoi so it can't be repeated\n            kwargs = _build_kwargs(params, self.module_parameters)\n             return func(aoi, **kwargs)\n         elif model == 'sapm':\n             return iam.sapm(aoi, self.module_parameters)\n         else:\n             raise ValueError(model + ' is not a valid IAM model')\n "
  },
  {
    "instruction": "Apparent numerical instability in I_mp calculation using PVsyst model\n**Describe the bug**\r\n\r\nI used these parameters in `pvlib.pvsystem.calcparams_pvsyst()` in order to calculate `I_mp` vs. `T` using `pvlib.pvsystem.singlediode()` with `effective_irradiance` fixed at 1000 W/m^2 and `temp_cell` having 1001 values ranging from 15 to 50 degC:\r\n\r\n`{'alpha_sc': 0.006, 'gamma_ref': 1.009, 'mu_gamma': -0.0005, 'I_L_ref': 13.429, 'I_o_ref': 3.719506010004821e-11, 'R_sh_ref': 800.0, 'R_sh_0': 3200.0, 'R_s': 0.187, 'cells_in_series': 72, 'R_sh_exp': 5.5, 'EgRef': 1.121, 'irrad_ref': 1000, 'temp_ref': 25}`\r\n\r\nMy purpose was to investigate the temperature coefficient of `I_mp`, and I got the following result, which appears to suffer from a numeric instability:\r\n\r\n![image](https://user-images.githubusercontent.com/1125363/98264917-ab2d2880-1f45-11eb-83a2-e146774abf44.png)\r\n\r\nFor comparison, the corresponding `V_mp` vs. `T` plot:\r\n\r\n![image](https://user-images.githubusercontent.com/1125363/98264984-bc763500-1f45-11eb-9012-7c29efa25e1e.png)\r\n\r\n**To Reproduce**\r\n\r\nRun the above calculations using the parameters provided.\r\n\r\n**Expected behavior**\r\n\r\nBetter numerical stability in `I_mp` vs. `T`.\r\n\r\n**Screenshots**\r\n\r\nSee above.\r\n\r\n**Versions:**\r\n\r\n - ``pvlib.__version__``: 0.8.0\r\n - ``numpy.__version__``: 1.19.2\r\n - ``scipy.__version__``: 1.5.2\r\n - ``pandas.__version__``: 1.1.3\r\n - python: 3.8.5\r\n\r\n**Additional context**\r\n\r\nI was going to attempt a numerical computation of the temperature coefficient of `I_mp` for a model translation to the SAPM. I have seen reports from CFV in which this coefficient is actually negative, and I have computed it alternately using the `P_mp` and `V_mp` temperature coefficients, and gotten a negative value for this particular PV module. Despite the apparent numerical instability in the above plot, it still suggests that the coefficient should be positive, not negative. Perhaps I am missing something here?\r\n\r\nAlso, I have not dug deep enough to figure out if the underlying issue is in `pvlib.pvsystem.singlediode()`.\n",
    "input": "     negrs = rs < 0.\n     badrs = np.logical_or(rs > rsh, np.isnan(rs))\n     imagrs = ~(np.isreal(rs))\n    badio = np.logical_or(~(np.isreal(rs)), io <= 0)\n     goodr = np.logical_and(~badrsh, ~imagrs)\n     goodr = np.logical_and(goodr, ~negrs)\n     goodr = np.logical_and(goodr, ~badrs)\ndiff --git a/pvlib/tools.py b/pvlib/tools.py\n \n # Created April,2014\n # Author: Rob Andrews, Calama Consulting\n\ndef _golden_sect_DataFrame(params, VL, VH, func):\n     \"\"\"\n    Vectorized golden section search for finding MPP from a dataframe\n    timeseries.\n \n     Parameters\n     ----------\n    params : dict\n        Dictionary containing scalars or arrays\n        of inputs to the function to be optimized.\n        Each row should represent an independent optimization.\n \n    VL: float\n        Lower bound of the optimization\n \n    VH: float\n        Upper bound of the optimization\n \n     func: function\n        Function to be optimized must be in the form f(array-like, x)\n \n     Returns\n     -------\n    func(df,'V1') : DataFrame\n        function evaluated at the optimal point\n \n    df['V1']: Dataframe\n        Dataframe of optimal points\n \n     Notes\n     -----\n    This function will find the MAXIMUM of a function\n     \"\"\"\n \n     df = params\n    df['VH'] = VH\n    df['VL'] = VL\n \n    errflag = True\n     iterations = 0\n \n    while errflag:\n \n        phi = (np.sqrt(5)-1)/2*(df['VH']-df['VL'])\n         df['V1'] = df['VL'] + phi\n         df['V2'] = df['VH'] - phi\n \n         df['VL'] = df['V2']*df['SW_Flag'] + df['VL']*(~df['SW_Flag'])\n         df['VH'] = df['V1']*~df['SW_Flag'] + df['VH']*(df['SW_Flag'])\n \n        err = df['V1'] - df['V2']\n        try:\n            errflag = (abs(err) > .01).any()\n        except ValueError:\n            errflag = (abs(err) > .01)\n \n         iterations += 1\n \n        if iterations > 50:\n            raise Exception(\"EXCEPTION:iterations exceeded maximum (50)\")\n \n     return func(df, 'V1'), df['V1']",
    "output": "     negrs = rs < 0.\n     badrs = np.logical_or(rs > rsh, np.isnan(rs))\n     imagrs = ~(np.isreal(rs))\n    badio = np.logical_or(np.logical_or(~(np.isreal(rs)), io <= 0),\n                          np.isnan(io))\n     goodr = np.logical_and(~badrsh, ~imagrs)\n     goodr = np.logical_and(goodr, ~negrs)\n     goodr = np.logical_and(goodr, ~badrs)\ndiff --git a/pvlib/tools.py b/pvlib/tools.py\n \n # Created April,2014\n # Author: Rob Andrews, Calama Consulting\n# Modified: November, 2020 by C. W. Hansen, to add atol and change exit\n# criteria\ndef _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):\n     \"\"\"\n    Vectorized golden section search for finding maximum of a function of a\n    single variable.\n \n     Parameters\n     ----------\n    params : dict or Dataframe\n        Parameters to be passed to `func`.\n \n    lower: numeric\n        Lower bound for the optimization\n \n    upper: numeric\n        Upper bound for the optimization\n \n     func: function\n        Function to be optimized. Must be in the form\n        result = f(dict or DataFrame, str), where result is a dict or DataFrame\n        that also contains the function output, and str is the key\n        corresponding to the function's input variable.\n \n     Returns\n     -------\n    numeric\n        function evaluated at the optimal points\n \n    numeric\n        optimal points\n \n     Notes\n     -----\n    This function will find the points where the function is maximized.\n\n    See also\n    --------\n    pvlib.singlediode._pwr_optfcn\n     \"\"\"\n \n    phim1 = (np.sqrt(5) - 1) / 2\n\n     df = params\n    df['VH'] = upper\n    df['VL'] = lower\n \n    converged = False\n     iterations = 0\n    iterlimit = 1 + np.max(\n        np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))\n \n    while not converged and (iterations < iterlimit):\n \n        phi = phim1 * (df['VH'] - df['VL'])\n         df['V1'] = df['VL'] + phi\n         df['V2'] = df['VH'] - phi\n \n         df['VL'] = df['V2']*df['SW_Flag'] + df['VL']*(~df['SW_Flag'])\n         df['VH'] = df['V1']*~df['SW_Flag'] + df['VH']*(df['SW_Flag'])\n \n        err = abs(df['V2'] - df['V1'])\n \n        # works with single value because err is np.float64\n        converged = (err < atol).all()\n        # err will be less than atol before iterations hit the limit\n        # but just to be safe\n         iterations += 1\n \n    if iterations > iterlimit:\n        raise Exception(\"iterations exceeded maximum\")  # pragma: no cover\n \n     return func(df, 'V1'), df['V1']"
  },
  {
    "instruction": "PVSystem.temperature_model_parameters requirement\nThe `temperature_model_parameters` handling code below suggests to me that in 0.8 we're going to \r\n\r\n1. set default values `module_type=None` and `racking_model=None`.\r\n2. require user to specify either `temperature_model_parameters` or both `module_type` and `racking_model`.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L208-L221\r\n\r\n@cwhanse is that correct?\r\n\r\nThe problem is that the only way to see this warning is to supply an invalid `module_type` or `racking_model`. That's because `PVSystem._infer_temperature_model` is called before the code above, and it looks up the default `module_type` and `racking_model` and successfully finds temperature coefficients.\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/27872b83b0932cc419116f79e442963cced935bb/pvlib/pvsystem.py#L201-L203\r\n\r\nSo I'm guessing that this warning has been seen by only a small fraction of people that need to see it. I'm ok moving forward with the removal in 0.8 or pushing to 0.9. \nremove deprecated functions in 0.8\n`pvsystem`:\r\n* `sapm_celltemp`\r\n* `pvsyst_celltemp`\r\n* `ashraeiam`\r\n* `physicaliam`\r\n* `sapm_aoi_loss`\r\n* `PVSystem.ashraeiam`\r\n* `PVSystem.physicaliam`\r\n* `PVSystem.sapm_aoi_loss`\r\n* inference of `PVSystem.temperature_model_parameters`\r\n\r\n`modelchain.ModelChain`:\r\n* remove `times` from `complete_irradiance`, `prepare_inputs`, `run_model`\r\n* remove `temp_model` kwarg\n",
    "input": "             linke_turbidity,\n             altitude=altitude,\n             dni_extra=dni_extra\n            )\n \n     total_irrad = pvlib.irradiance.get_total_irradiance(\n         surface_tilt,\n         self.ac_model = ac_model\n         self.aoi_model = aoi_model\n         self.spectral_model = spectral_model\n\n        # TODO: deprecated kwarg temp_model. Remove use of temp_model in v0.8\n        temp_model = kwargs.pop('temp_model', None)\n        if temp_model is not None:\n            if temperature_model is None:\n                warnings.warn('The temp_model keyword argument is deprecated.'\n                              ' Use temperature_model instead',\n                              pvlibDeprecationWarning)\n                temperature_model = temp_model\n            elif temp_model == temperature_model:\n                warnings.warn('Provide only one of temperature_model or '\n                              'temp_model (deprecated).',\n                              pvlibDeprecationWarning)\n            else:\n                raise ValueError(\n                    'Conflicting temperature_model {} and temp_model {}. '\n                    'temp_model is deprecated. Specify only temperature_model.'\n                    .format(temperature_model, temp_model))\n         self.temperature_model = temperature_model\n \n         self.losses_model = losses_model\n             'transposition_model', 'solar_position_method',\n             'airmass_model', 'dc_model', 'ac_model', 'aoi_model',\n             'spectral_model', 'temperature_model', 'losses_model'\n            ]\n \n         def getmcattr(self, attr):\n             \"\"\"needed to avoid recursion in property lookups\"\"\"\n             model = model.lower()\n             if model in _DC_MODEL_PARAMS.keys():\n                 # validate module parameters\n                missing_params = _DC_MODEL_PARAMS[model] - \\\n                                 set(self.system.module_parameters.keys())\n                 if missing_params:  # some parameters are not in module.keys()\n                     raise ValueError(model + ' selected for the DC model but '\n                                      'one or more required parameters are '\n \n     def first_solar_spectral_loss(self):\n         self.spectral_modifier = self.system.first_solar_spectral_loss(\n                                        self.weather['precipitable_water'],\n                                        self.airmass['airmass_absolute'])\n         return self\n \n     def sapm_spectral_loss(self):\n \n     def infer_temperature_model(self):\n         params = set(self.system.temperature_model_parameters.keys())\n        if set(['a', 'b', 'deltaT']) <= params:\n             return self.sapm_temp\n         elif set(['u_c', 'u_v']) <= params:\n             return self.pvsyst_temp\n             fd*self.total_irrad['poa_diffuse'])\n         return self\n \n    def complete_irradiance(self, weather, times=None):\n         \"\"\"\n         Determine the missing irradiation columns. Only two of the\n         following data columns (dni, ghi, dhi) are needed to calculate\n             ``'wind_speed'``, ``'temp_air'``. All irradiance components\n             are required. Air temperature of 20 C and wind speed\n             of 0 m/s will be added to the DataFrame if not provided.\n        times : None, deprecated\n            Deprecated argument included for API compatibility, but not\n            used internally. The index of the weather DataFrame is used\n            for times.\n \n         Returns\n         -------\n         \"\"\"\n         self.weather = weather\n \n        if times is not None:\n            warnings.warn('times keyword argument is deprecated and will be '\n                          'removed in 0.8. The index of the weather DataFrame '\n                          'is used for times.', pvlibDeprecationWarning)\n\n         self.solar_position = self.location.get_solarposition(\n             self.weather.index, method=self.solar_position_method)\n \n \n         return self\n \n    def prepare_inputs(self, weather, times=None):\n         \"\"\"\n         Prepare the solar position, irradiance, and weather inputs to\n         the model.\n             ``'wind_speed'``, ``'temp_air'``. All irradiance components\n             are required. Air temperature of 20 C and wind speed\n             of 0 m/s will be added to the DataFrame if not provided.\n        times : None, deprecated\n            Deprecated argument included for API compatibility, but not\n            used internally. The index of the weather DataFrame is used\n            for times.\n \n         Notes\n         -----\n \n         self.weather = weather\n \n        if times is not None:\n            warnings.warn('times keyword argument is deprecated and will be '\n                          'removed in 0.8. The index of the weather DataFrame '\n                          'is used for times.', pvlibDeprecationWarning)\n\n         self.times = self.weather.index\n         try:\n             kwargs = _build_kwargs(['pressure', 'temp_air'], weather)\n             self.weather['temp_air'] = 20\n         return self\n \n    def run_model(self, weather, times=None):\n         \"\"\"\n         Run the model.\n \n             ``'wind_speed'``, ``'temp_air'``. All irradiance components\n             are required. Air temperature of 20 C and wind speed\n             of 0 m/s will be added to the DataFrame if not provided.\n        times : None, deprecated\n            Deprecated argument included for API compatibility, but not\n            used internally. The index of the weather DataFrame is used\n            for times.\n \n         Returns\n         -------\n         ``dc``, ``ac``, ``losses``,\n         ``diode_params`` (if dc_model is a single diode model)\n         \"\"\"\n        if times is not None:\n            warnings.warn('times keyword argument is deprecated and will be '\n                          'removed in 0.8. The index of the weather DataFrame '\n                          'is used for times.', pvlibDeprecationWarning)\n\n         self.prepare_inputs(weather)\n         self.aoi_model()\n         self.spectral_model()\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n     def __init__(self,\n                  surface_tilt=0, surface_azimuth=180,\n                  albedo=None, surface_type=None,\n                 module=None, module_type='glass_polymer',\n                  module_parameters=None,\n                  temperature_model_parameters=None,\n                  modules_per_string=1, strings_per_inverter=1,\n                  inverter=None, inverter_parameters=None,\n                 racking_model='open_rack', losses_parameters=None, name=None,\n                  **kwargs):\n \n         self.surface_tilt = surface_tilt\n         if temperature_model_parameters is None:\n             self.temperature_model_parameters = \\\n                 self._infer_temperature_model_params()\n            # TODO: in v0.8 check if an empty dict is returned and raise error\n         else:\n             self.temperature_model_parameters = temperature_model_parameters\n \n        # TODO: deprecated behavior if PVSystem.temperature_model_parameters\n        # are not specified. Remove in v0.8\n        if not any(self.temperature_model_parameters):\n            warnings.warn(\n                'Required temperature_model_parameters is not specified '\n                'and parameters are not inferred from racking_model and '\n                'module_type. Reverting to deprecated default: SAPM cell '\n                'temperature model parameters for a glass/glass module in '\n                'open racking. In the future '\n                'PVSystem.temperature_model_parameters will be required',\n                pvlibDeprecationWarning)\n            params = temperature._temperature_model_params(\n                'sapm', 'open_rack_glass_glass')\n            self.temperature_model_parameters = params\n\n         self.modules_per_string = modules_per_string\n         self.strings_per_inverter = strings_per_inverter\n \n         else:\n             raise ValueError(model + ' is not a valid IAM model')\n \n    def ashraeiam(self, aoi):\n        \"\"\"\n        Deprecated. Use ``PVSystem.get_iam`` instead.\n        \"\"\"\n        import warnings\n        warnings.warn('PVSystem.ashraeiam is deprecated and will be removed in'\n                      'v0.8, use PVSystem.get_iam instead',\n                      pvlibDeprecationWarning)\n        return PVSystem.get_iam(self, aoi, iam_model='ashrae')\n\n    def physicaliam(self, aoi):\n        \"\"\"\n        Deprecated. Use ``PVSystem.get_iam`` instead.\n        \"\"\"\n        import warnings\n        warnings.warn('PVSystem.physicaliam is deprecated and will be removed'\n                      ' in v0.8, use PVSystem.get_iam instead',\n                      pvlibDeprecationWarning)\n        return PVSystem.get_iam(self, aoi, iam_model='physical')\n\n     def calcparams_desoto(self, effective_irradiance, temp_cell, **kwargs):\n         \"\"\"\n         Use the :py:func:`calcparams_desoto` function, the input\n         -------\n         numeric, values in degrees C.\n         \"\"\"\n         kwargs = _build_kwargs(['a', 'b', 'deltaT'],\n                                self.temperature_model_parameters)\n         return temperature.sapm_cell(poa_global, temp_air, wind_speed,\n     def _infer_temperature_model_params(self):\n         # try to infer temperature model parameters from from racking_model\n         # and module_type\n        param_set = self.racking_model + '_' + self.module_type\n         if param_set in temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']:\n             return temperature._temperature_model_params('sapm', param_set)\n         elif 'freestanding' in param_set:\n         \"\"\"\n         return sapm_spectral_loss(airmass_absolute, self.module_parameters)\n \n    def sapm_aoi_loss(self, aoi):\n        \"\"\"\n        Deprecated. Use ``PVSystem.get_iam`` instead.\n        \"\"\"\n        import warnings\n        warnings.warn('PVSystem.sapm_aoi_loss is deprecated and will be'\n                      ' removed in v0.8, use PVSystem.get_iam instead',\n                      pvlibDeprecationWarning)\n        return PVSystem.get_iam(self, aoi, iam_model='sapm')\n\n     def sapm_effective_irradiance(self, poa_direct, poa_diffuse,\n                                   airmass_absolute, aoi,\n                                   reference_irradiance=1000):\n         if 'first_solar_spectral_coefficients' in \\\n                 self.module_parameters.keys():\n             coefficients = \\\n                   self.module_parameters['first_solar_spectral_coefficients']\n             module_type = None\n         else:\n             module_type = self._infer_cell_type()\n          Source: [4]\n     '''\n \n    # test for use of function pre-v0.6.0 API change\n    if isinstance(a_ref, dict) or \\\n       (isinstance(a_ref, pd.Series) and ('a_ref' in a_ref.keys())):\n        import warnings\n        warnings.warn('module_parameters detected as fourth positional'\n                      + ' argument of calcparams_desoto. calcparams_desoto'\n                      + ' will require one argument for each module model'\n                      + ' parameter in v0.7.0 and later', DeprecationWarning)\n        try:\n            module_parameters = a_ref\n            a_ref = module_parameters['a_ref']\n            I_L_ref = module_parameters['I_L_ref']\n            I_o_ref = module_parameters['I_o_ref']\n            R_sh_ref = module_parameters['R_sh_ref']\n            R_s = module_parameters['R_s']\n        except Exception as e:\n            raise e('Module parameters could not be extracted from fourth'\n                    + ' positional argument of calcparams_desoto. Check that'\n                    + ' parameters are from the CEC database and/or update'\n                    + ' your code for the new API for calcparams_desoto')\n\n     # Boltzmann constant in eV/K\n     k = 8.617332478e-05\n \n     # reference_irradiance and expose\n     temp_ref = 25\n     irrad_ref = 1000\n    # TODO: remove this warning in v0.8 after deprecation period for change in\n    # effective irradiance units, made in v0.7\n    with np.errstate(invalid='ignore'):  # turn off warning for NaN\n        ee = np.asarray(effective_irradiance)\n        ee_gt0 = ee[ee > 0.0]\n        if ee_gt0.size > 0 and np.all(ee_gt0 < 2.0):\n            import warnings\n            msg = 'effective_irradiance inputs appear to be in suns. Units ' \\\n                  'changed in v0.7 from suns to W/m2'\n            warnings.warn(msg, RuntimeWarning)\n \n     q = 1.60218e-19  # Elementary charge in units of coulombs\n     kb = 1.38066e-23  # Boltzmann's constant in units of J/K\n     return out\n \n \ndef _sapm_celltemp_translator(*args, **kwargs):\n    # TODO: remove this function after deprecation period for sapm_celltemp\n    new_kwargs = {}\n    # convert position arguments to kwargs\n    old_arg_list = ['poa_global', 'wind_speed', 'temp_air', 'model']\n    for pos in range(len(args)):\n        new_kwargs[old_arg_list[pos]] = args[pos]\n    # determine value for new kwarg 'model'\n    try:\n        param_set = new_kwargs['model']\n        new_kwargs.pop('model')  # model is not a new kwarg\n    except KeyError:\n        # 'model' not in positional arguments, check kwargs\n        try:\n            param_set = kwargs['model']\n            kwargs.pop('model')\n        except KeyError:\n            # 'model' not in kwargs, use old default value\n            param_set = 'open_rack_glass_glass'\n    if type(param_set) is list:\n        new_kwargs.update({'a': param_set[0],\n                           'b': param_set[1],\n                           'deltaT': param_set[2]})\n    elif type(param_set) is dict:\n        new_kwargs.update(param_set)\n    else:  # string\n        params = temperature._temperature_model_params('sapm', param_set)\n        new_kwargs.update(params)\n    new_kwargs.update(kwargs)  # kwargs with unchanged names\n    new_kwargs['irrad_ref'] = 1000  # default for new kwarg\n    # convert old positional arguments to named kwargs\n    return temperature.sapm_cell(**new_kwargs)\n\n\nsapm_celltemp = deprecated('0.7', alternative='temperature.sapm_cell',\n                           name='sapm_celltemp', removal='0.8',\n                           addendum='Note that the arguments and argument '\n                           'order for temperature.sapm_cell are different '\n                           'than for sapm_celltemp')(_sapm_celltemp_translator)\n\n\ndef _pvsyst_celltemp_translator(*args, **kwargs):\n    # TODO: remove this function after deprecation period for pvsyst_celltemp\n    new_kwargs = {}\n    # convert position arguments to kwargs\n    old_arg_list = ['poa_global', 'temp_air', 'wind_speed', 'eta_m',\n                    'alpha_absorption', 'model_params']\n    for pos in range(len(args)):\n        new_kwargs[old_arg_list[pos]] = args[pos]\n    # determine value for new kwarg 'model'\n    try:\n        param_set = new_kwargs['model_params']\n        new_kwargs.pop('model_params')  # model_params is not a new kwarg\n    except KeyError:\n        # 'model_params' not in positional arguments, check kwargs\n        try:\n            param_set = kwargs['model_params']\n            kwargs.pop('model_params')\n        except KeyError:\n            # 'model_params' not in kwargs, use old default value\n            param_set = 'freestanding'\n    if type(param_set) in (list, tuple):\n        new_kwargs.update({'u_c': param_set[0],\n                           'u_v': param_set[1]})\n    else:  # string\n        params = temperature._temperature_model_params('pvsyst', param_set)\n        new_kwargs.update(params)\n    new_kwargs.update(kwargs)  # kwargs with unchanged names\n    # convert old positional arguments to named kwargs\n    return temperature.pvsyst_cell(**new_kwargs)\n\n\npvsyst_celltemp = deprecated(\n    '0.7', alternative='temperature.pvsyst_cell', name='pvsyst_celltemp',\n    removal='0.8', addendum='Note that the argument names for '\n    'temperature.pvsyst_cell are different than '\n    'for pvsyst_celltemp')(_pvsyst_celltemp_translator)\n\n\n def sapm_spectral_loss(airmass_absolute, module):\n     \"\"\"\n     Calculates the SAPM spectral loss coefficient, F1.\n         # calculate the IV curve if requested using bishop88\n         if ivcurve_pnts:\n             vd = v_oc * (\n                    (11.0 - np.logspace(np.log10(11.0), 0.0,\n                                        ivcurve_pnts)) / 10.0\n             )\n             ivcurve_i, ivcurve_v, _ = _singlediode.bishop88(vd, *args)\n \n         # equation for the diode voltage V_d then backing out voltage\n         args = (voltage, photocurrent, saturation_current, resistance_series,\n                 resistance_shunt, nNsVth)\n        I = _singlediode.bishop88_i_from_v(*args, method=method.lower())\n         # find the right size and shape for returns\n         size, shape = _singlediode._get_size_and_shape(args)\n         if size <= 1:\n             if shape is not None:\n                I = np.tile(I, shape)\n        if np.isnan(I).any() and size <= 1:\n            I = np.repeat(I, size)\n             if shape is not None:\n                I = I.reshape(shape)\n        return I\n \n \n def scale_voltage_current_power(data, voltage=1, current=1):\n     .. [1] A. P. Dobos, \"PVWatts Version 5 Manual\"\n            http://pvwatts.nrel.gov/downloads/pvwattsv5.pdf\n            (2014).\n    \"\"\"\n \n     pdc = (g_poa_effective * 0.001 * pdc0 *\n            (1 + gamma_pdc * (temp_cell - temp_ref)))\n     return losses\n \n \nashraeiam = deprecated('0.7', alternative='iam.ashrae', name='ashraeiam',\n                       removal='0.8')(iam.ashrae)\n\n\nphysicaliam = deprecated('0.7', alternative='iam.physical', name='physicaliam',\n                         removal='0.8')(iam.physical)\n\n\nsapm_aoi_loss = deprecated('0.7', alternative='iam.sapm', name='sapm_aoi_loss',\n                           removal='0.8')(iam.sapm)\n\n\n snlinverter = deprecated('0.8', alternative='inverter.sandia',\n                          name='snlinverter', removal='0.9')(inverter.sandia)\n ",
    "output": "             linke_turbidity,\n             altitude=altitude,\n             dni_extra=dni_extra\n        )\n \n     total_irrad = pvlib.irradiance.get_total_irradiance(\n         surface_tilt,\n         self.ac_model = ac_model\n         self.aoi_model = aoi_model\n         self.spectral_model = spectral_model\n         self.temperature_model = temperature_model\n \n         self.losses_model = losses_model\n             'transposition_model', 'solar_position_method',\n             'airmass_model', 'dc_model', 'ac_model', 'aoi_model',\n             'spectral_model', 'temperature_model', 'losses_model'\n        ]\n \n         def getmcattr(self, attr):\n             \"\"\"needed to avoid recursion in property lookups\"\"\"\n             model = model.lower()\n             if model in _DC_MODEL_PARAMS.keys():\n                 # validate module parameters\n                missing_params = (_DC_MODEL_PARAMS[model]\n                                  - set(self.system.module_parameters.keys()))\n                 if missing_params:  # some parameters are not in module.keys()\n                     raise ValueError(model + ' selected for the DC model but '\n                                      'one or more required parameters are '\n \n     def first_solar_spectral_loss(self):\n         self.spectral_modifier = self.system.first_solar_spectral_loss(\n            self.weather['precipitable_water'],\n            self.airmass['airmass_absolute'])\n         return self\n \n     def sapm_spectral_loss(self):\n \n     def infer_temperature_model(self):\n         params = set(self.system.temperature_model_parameters.keys())\n        # remove or statement in v0.9\n        if set(['a', 'b', 'deltaT']) <= params or (\n                not params and self.system.racking_model is None\n                and self.system.module_type is None):\n             return self.sapm_temp\n         elif set(['u_c', 'u_v']) <= params:\n             return self.pvsyst_temp\n             fd*self.total_irrad['poa_diffuse'])\n         return self\n \n    def complete_irradiance(self, weather):\n         \"\"\"\n         Determine the missing irradiation columns. Only two of the\n         following data columns (dni, ghi, dhi) are needed to calculate\n             ``'wind_speed'``, ``'temp_air'``. All irradiance components\n             are required. Air temperature of 20 C and wind speed\n             of 0 m/s will be added to the DataFrame if not provided.\n \n         Returns\n         -------\n         \"\"\"\n         self.weather = weather\n \n         self.solar_position = self.location.get_solarposition(\n             self.weather.index, method=self.solar_position_method)\n \n \n         return self\n \n    def prepare_inputs(self, weather):\n         \"\"\"\n         Prepare the solar position, irradiance, and weather inputs to\n         the model.\n             ``'wind_speed'``, ``'temp_air'``. All irradiance components\n             are required. Air temperature of 20 C and wind speed\n             of 0 m/s will be added to the DataFrame if not provided.\n \n         Notes\n         -----\n \n         self.weather = weather\n \n         self.times = self.weather.index\n         try:\n             kwargs = _build_kwargs(['pressure', 'temp_air'], weather)\n             self.weather['temp_air'] = 20\n         return self\n \n    def run_model(self, weather):\n         \"\"\"\n         Run the model.\n \n             ``'wind_speed'``, ``'temp_air'``. All irradiance components\n             are required. Air temperature of 20 C and wind speed\n             of 0 m/s will be added to the DataFrame if not provided.\n \n         Returns\n         -------\n         ``dc``, ``ac``, ``losses``,\n         ``diode_params`` (if dc_model is a single diode model)\n         \"\"\"\n         self.prepare_inputs(weather)\n         self.aoi_model()\n         self.spectral_model()\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n     def __init__(self,\n                  surface_tilt=0, surface_azimuth=180,\n                  albedo=None, surface_type=None,\n                 module=None, module_type=None,\n                  module_parameters=None,\n                  temperature_model_parameters=None,\n                  modules_per_string=1, strings_per_inverter=1,\n                  inverter=None, inverter_parameters=None,\n                 racking_model=None, losses_parameters=None, name=None,\n                  **kwargs):\n \n         self.surface_tilt = surface_tilt\n         if temperature_model_parameters is None:\n             self.temperature_model_parameters = \\\n                 self._infer_temperature_model_params()\n         else:\n             self.temperature_model_parameters = temperature_model_parameters\n \n         self.modules_per_string = modules_per_string\n         self.strings_per_inverter = strings_per_inverter\n \n         else:\n             raise ValueError(model + ' is not a valid IAM model')\n \n     def calcparams_desoto(self, effective_irradiance, temp_cell, **kwargs):\n         \"\"\"\n         Use the :py:func:`calcparams_desoto` function, the input\n         -------\n         numeric, values in degrees C.\n         \"\"\"\n        # warn user about change in default behavior in 0.9.\n        if (self.temperature_model_parameters == {} and self.module_type\n                is None and self.racking_model is None):\n            warnings.warn(\n                'temperature_model_parameters, racking_model, and module_type '\n                'are not specified. Reverting to deprecated default: SAPM '\n                'cell temperature model parameters for a glass/glass module '\n                'in open racking. In v0.9, temperature_model_parameters or a '\n                'valid combination of racking_model and module_type will be '\n                'required.',\n                pvlibDeprecationWarning)\n            params = temperature._temperature_model_params(\n                'sapm', 'open_rack_glass_glass')\n            self.temperature_model_parameters = params\n\n         kwargs = _build_kwargs(['a', 'b', 'deltaT'],\n                                self.temperature_model_parameters)\n         return temperature.sapm_cell(poa_global, temp_air, wind_speed,\n     def _infer_temperature_model_params(self):\n         # try to infer temperature model parameters from from racking_model\n         # and module_type\n        param_set = '{}_{}'.format(self.racking_model, self.module_type)\n         if param_set in temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']:\n             return temperature._temperature_model_params('sapm', param_set)\n         elif 'freestanding' in param_set:\n         \"\"\"\n         return sapm_spectral_loss(airmass_absolute, self.module_parameters)\n \n     def sapm_effective_irradiance(self, poa_direct, poa_diffuse,\n                                   airmass_absolute, aoi,\n                                   reference_irradiance=1000):\n         if 'first_solar_spectral_coefficients' in \\\n                 self.module_parameters.keys():\n             coefficients = \\\n                self.module_parameters['first_solar_spectral_coefficients']\n             module_type = None\n         else:\n             module_type = self._infer_cell_type()\n          Source: [4]\n     '''\n \n     # Boltzmann constant in eV/K\n     k = 8.617332478e-05\n \n     # reference_irradiance and expose\n     temp_ref = 25\n     irrad_ref = 1000\n \n     q = 1.60218e-19  # Elementary charge in units of coulombs\n     kb = 1.38066e-23  # Boltzmann's constant in units of J/K\n     return out\n \n \n def sapm_spectral_loss(airmass_absolute, module):\n     \"\"\"\n     Calculates the SAPM spectral loss coefficient, F1.\n         # calculate the IV curve if requested using bishop88\n         if ivcurve_pnts:\n             vd = v_oc * (\n                (11.0 - np.logspace(np.log10(11.0), 0.0, ivcurve_pnts)) / 10.0\n             )\n             ivcurve_i, ivcurve_v, _ = _singlediode.bishop88(vd, *args)\n \n         # equation for the diode voltage V_d then backing out voltage\n         args = (voltage, photocurrent, saturation_current, resistance_series,\n                 resistance_shunt, nNsVth)\n        current = _singlediode.bishop88_i_from_v(*args, method=method.lower())\n         # find the right size and shape for returns\n         size, shape = _singlediode._get_size_and_shape(args)\n         if size <= 1:\n             if shape is not None:\n                current = np.tile(current, shape)\n        if np.isnan(current).any() and size <= 1:\n            current = np.repeat(current, size)\n             if shape is not None:\n                current = current.reshape(shape)\n        return current\n \n \n def scale_voltage_current_power(data, voltage=1, current=1):\n     .. [1] A. P. Dobos, \"PVWatts Version 5 Manual\"\n            http://pvwatts.nrel.gov/downloads/pvwattsv5.pdf\n            (2014).\n    \"\"\"  # noqa: E501\n \n     pdc = (g_poa_effective * 0.001 * pdc0 *\n            (1 + gamma_pdc * (temp_cell - temp_ref)))\n     return losses\n \n \n snlinverter = deprecated('0.8', alternative='inverter.sandia',\n                          name='snlinverter', removal='0.9')(inverter.sandia)\n "
  },
  {
    "instruction": "CEC 6-parameter coefficient generation\nSAM is able to extract the CEC parameters required for calcparams_desoto.  This is done through the 'CEC Performance Model with User Entered Specifications' module model, and coefficients are automatically extracted given nameplate parameters Voc, Isc, Imp, Vmp and TempCoeff.  The method is based on Aron Dobos' \"An Improved Coefficient Calculator for the California Energy Commission 6 Parameter Photovoltaic Module Model \", 2012\r\n\r\nIdeally we should be able to work with the SAM open source code, extract the bit that does the coefficient generation, and put it into a PVLib function that would allow users to run calcparams_desoto with any arbitrary module type.  At the moment we are dependent on PV modules loaded into the SAM or CEC database.\r\n\r\nThank you!\r\n\n",
    "input": " from pvlib import location\n from pvlib import solarposition\n from pvlib import iotools\n from pvlib import tracking\n from pvlib import pvsystem\n from pvlib import spa\ndiff --git a/pvlib/ivtools.py b/pvlib/ivtools.py\nnew file mode 100644\ndiff --git a/setup.py b/setup.py\n TESTS_REQUIRE = ['nose', 'pytest', 'pytest-cov', 'pytest-mock',\n                  'pytest-timeout']\n EXTRAS_REQUIRE = {\n    'optional': ['ephem', 'cython', 'netcdf4', 'numba', 'pvfactors', 'scipy',\n                 'siphon', 'tables'],\n     'doc': ['ipython', 'matplotlib', 'sphinx', 'sphinx_rtd_theme'],\n     'test': TESTS_REQUIRE\n }",
    "output": " from pvlib import location\n from pvlib import solarposition\n from pvlib import iotools\nfrom pvlib import ivtools\n from pvlib import tracking\n from pvlib import pvsystem\n from pvlib import spa\ndiff --git a/pvlib/ivtools.py b/pvlib/ivtools.py\nnew file mode 100644\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Mar 29 10:34:10 2019\n\n@author: cwhanse\n\"\"\"\n\nimport numpy as np\n\n\ndef fit_sdm_cec_sam(celltype, v_mp, i_mp, v_oc, i_sc, alpha_sc, beta_voc,\n                    gamma_pmp, cells_in_series, temp_ref=25):\n    \"\"\"\n    Estimates parameters for the CEC single diode model (SDM) using the SAM\n    SDK.\n\n    Parameters\n    ----------\n    celltype : str\n        Value is one of 'monoSi', 'multiSi', 'polySi', 'cis', 'cigs', 'cdte',\n        'amorphous'\n    v_mp : float\n        Voltage at maximum power point [V]\n    i_mp : float\n        Current at maximum power point [A]\n    v_oc : float\n        Open circuit voltage [V]\n    i_sc : float\n        Short circuit current [A]\n    alpha_sc : float\n        Temperature coefficient of short circuit current [A/C]\n    beta_voc : float\n        Temperature coefficient of open circuit voltage [V/C]\n    gamma_pmp : float\n        Temperature coefficient of power at maximum point point [%/C]\n    cells_in_series : int\n        Number of cells in series\n    temp_ref : float, default 25\n        Reference temperature condition [C]\n\n    Returns\n    -------\n    tuple of the following elements:\n\n        * I_L_ref : float\n            The light-generated current (or photocurrent) at reference\n            conditions [A]\n\n        * I_o_ref : float\n            The dark or diode reverse saturation current at reference\n            conditions [A]\n\n        * R_sh_ref : float\n            The shunt resistance at reference conditions, in ohms.\n\n        * R_s : float\n            The series resistance at reference conditions, in ohms.\n\n        * a_ref : float\n            The product of the usual diode ideality factor ``n`` (unitless),\n            number of cells in series ``Ns``, and cell thermal voltage at\n            reference conditions [V]\n\n        * Adjust : float\n            The adjustment to the temperature coefficient for short circuit\n            current, in percent.\n\n    Raises\n    ------\n        ImportError if NREL-PySAM is not installed.\n\n        RuntimeError if parameter extraction is not successful.\n\n    Notes\n    -----\n    Inputs ``v_mp``, ``v_oc``, ``i_mp`` and ``i_sc`` are assumed to be from a\n    single IV curve at constant irradiance and cell temperature. Irradiance is\n    not explicitly used by the fitting procedure. The irradiance level at which\n    the input IV curve is determined and the specified cell temperature\n    ``temp_ref`` are the reference conditions for the output parameters\n    ``I_L_ref``, ``I_o_ref``, ``R_sh_ref``, ``R_s``, ``a_ref`` and ``Adjust``.\n\n    References\n    ----------\n    [1] A. Dobos, \"An Improved Coefficient Calculator for the California\n    Energy Commission 6 Parameter Photovoltaic Module Model\", Journal of\n    Solar Energy Engineering, vol 134, 2012.\n    \"\"\"\n\n    try:\n        from PySAM import PySSC\n    except ImportError:\n        raise ImportError(\"Requires NREL's PySAM package at \"\n                          \"https://pypi.org/project/NREL-PySAM/.\")\n\n    datadict = {'tech_model': '6parsolve', 'financial_model': 'none',\n                'celltype': celltype, 'Vmp': v_mp,\n                'Imp': i_mp, 'Voc': v_oc, 'Isc': i_sc, 'alpha_isc': alpha_sc,\n                'beta_voc': beta_voc, 'gamma_pmp': gamma_pmp,\n                'Nser': cells_in_series, 'Tref': temp_ref}\n\n    result = PySSC.ssc_sim_from_dict(datadict)\n    if result['cmod_success'] == 1:\n        return tuple([result[k] for k in ['Il', 'Io', 'Rsh', 'Rs', 'a',\n                      'Adj']])\n    else:\n        raise RuntimeError('Parameter estimation failed')\n\n\ndef fit_sde_sandia(voltage, current, v_oc=None, i_sc=None, v_mp_i_mp=None,\n                   vlim=0.2, ilim=0.1):\n    r\"\"\"\n    Fits the single diode equation (SDE) to an IV curve.\n\n    Parameters\n    ----------\n    voltage : ndarray\n        1D array of `float` type containing voltage at each point on the IV\n        curve, increasing from 0 to ``v_oc`` inclusive [V]\n\n    current : ndarray\n        1D array of `float` type containing current at each point on the IV\n        curve, from ``i_sc`` to 0 inclusive [A]\n\n    v_oc : float, default None\n        Open circuit voltage [V]. If not provided, ``v_oc`` is taken as the\n        last point in the ``voltage`` array.\n\n    i_sc : float, default None\n        Short circuit current [A]. If not provided, ``i_sc`` is taken as the\n        first point in the ``current`` array.\n\n    v_mp_i_mp : tuple of float, default None\n        Voltage, current at maximum power point in units of [V], [A].\n        If not provided, the maximum power point is found at the maximum of\n        ``voltage`` \\times ``current``.\n\n    vlim : float, default 0.2\n        Defines portion of IV curve where the exponential term in the single\n        diode equation can be neglected, i.e.\n        ``voltage`` <= ``vlim`` x ``v_oc`` [V]\n\n    ilim : float, default 0.1\n        Defines portion of the IV curve where the exponential term in the\n        single diode equation is signficant, approximately defined by\n        ``current`` < (1 - ``ilim``) x ``i_sc`` [A]\n\n    Returns\n    -------\n    tuple of the following elements:\n\n        * photocurrent : float\n            photocurrent [A]\n        * saturation_current : float\n            dark (saturation) current [A]\n        * resistance_shunt : float\n            shunt (parallel) resistance, in ohms\n        * resistance_series : float\n            series resistance, in ohms\n        * nNsVth : float\n            product of thermal voltage ``Vth`` [V], diode ideality factor\n            ``n``, and number of series cells ``Ns``\n\n    Raises\n    ------\n    RuntimeError if parameter extraction is not successful.\n\n    Notes\n    -----\n    Inputs ``voltage``, ``current``, ``v_oc``, ``i_sc`` and ``v_mp_i_mp`` are\n    assumed to be from a single IV curve at constant irradiance and cell\n    temperature.\n\n    :py:func:`fit_single_diode_sandia` obtains values for the five parameters\n    for the single diode equation [1]:\n\n    .. math::\n\n        I = I_{L} - I_{0} (\\exp \\frac{V + I R_{s}}{nNsVth} - 1)\n        - \\frac{V + I R_{s}}{R_{sh}}\n\n    See :py:func:`pvsystem.singlediode` for definition of the parameters.\n\n    The extraction method [2] proceeds in six steps.\n\n    1. In the single diode equation, replace :math:`R_{sh} = 1/G_{p}` and\n       re-arrange\n\n    .. math::\n\n        I = \\frac{I_{L}}{1 + G_{p} R_{s}} - \\frac{G_{p} V}{1 + G_{p} R_{s}}\n        - \\frac{I_{0}}{1 + G_{p} R_{s}} (\\exp(\\frac{V + I R_{s}}{nNsVth}) - 1)\n\n    2. The linear portion of the IV curve is defined as\n       :math:`V \\le vlim \\times v_oc`. Over this portion of the IV curve,\n\n    .. math::\n\n        \\frac{I_{0}}{1 + G_{p} R_{s}} (\\exp(\\frac{V + I R_{s}}{nNsVth}) - 1)\n        \\approx 0\n\n    3. Fit the linear portion of the IV curve with a line.\n\n    .. math::\n\n        I &\\approx \\frac{I_{L}}{1 + G_{p} R_{s}} - \\frac{G_{p} V}{1 + G_{p}\n        R_{s}} \\\\\n        &= \\beta_{0} + \\beta_{1} V\n\n    4. The exponential portion of the IV curve is defined by\n       :math:`\\beta_{0} + \\beta_{1} \\times V - I > ilim \\times i_sc`.\n       Over this portion of the curve, :math:`exp((V + IRs)/nNsVth) >> 1`\n       so that\n\n    .. math::\n\n        \\exp(\\frac{V + I R_{s}}{nNsVth}) - 1 \\approx\n        \\exp(\\frac{V + I R_{s}}{nNsVth})\n\n    5. Fit the exponential portion of the IV curve.\n\n    .. math::\n\n        \\log(\\beta_{0} - \\beta_{1} V - I)\n        &\\approx \\log(\\frac{I_{0}}{1 + G_{p} R_{s}} + \\frac{V}{nNsVth}\n        + \\frac{I R_{s}}{nNsVth} \\\\\n        &= \\beta_{2} + beta_{3} V + \\beta_{4} I\n\n    6. Calculate values for ``IL, I0, Rs, Rsh,`` and ``nNsVth`` from the\n       regression coefficents :math:`\\beta_{0}, \\beta_{1}, \\beta_{3}` and\n       :math:`\\beta_{4}`.\n\n\n    References\n    ----------\n    [1] S.R. Wenham, M.A. Green, M.E. Watt, \"Applied Photovoltaics\" ISBN\n    0 86758 909 4\n    [2] C. B. Jones, C. W. Hansen, Single Diode Parameter Extraction from\n    In-Field Photovoltaic I-V Curves on a Single Board Computer, 46th IEEE\n    Photovoltaic Specialist Conference, Chicago, IL, 2019\n    \"\"\"\n\n    # If not provided, extract v_oc, i_sc, v_mp and i_mp from the IV curve data\n    if v_oc is None:\n        v_oc = voltage[-1]\n    if i_sc is None:\n        i_sc = current[0]\n    if v_mp_i_mp is not None:\n        v_mp, i_mp = v_mp_i_mp\n    else:\n        v_mp, i_mp = _find_mp(voltage, current)\n\n    # Find beta0 and beta1 from linear portion of the IV curve\n    beta0, beta1 = _find_beta0_beta1(voltage, current, vlim, v_oc)\n\n    # Find beta3 and beta4 from the exponential portion of the IV curve\n    beta3, beta4 = _find_beta3_beta4(voltage, current, beta0, beta1, ilim,\n                                     i_sc)\n\n    # calculate single diode parameters from regression coefficients\n    return _calculate_sde_parameters(beta0, beta1, beta3, beta4, v_mp, i_mp,\n                                     v_oc)\n\n\ndef _find_mp(voltage, current):\n    \"\"\"\n    Finds voltage and current at maximum power point.\n\n    Parameters\n    ----------\n    voltage : ndarray\n        1D array containing voltage at each point on the IV curve, increasing\n        from 0 to v_oc inclusive, of `float` type [V]\n\n    current : ndarray\n        1D array containing current at each point on the IV curve, decreasing\n        from i_sc to 0 inclusive, of `float` type [A]\n\n    Returns\n    -------\n    v_mp, i_mp : tuple\n        voltage ``v_mp`` and current ``i_mp`` at the maximum power point [V],\n        [A]\n    \"\"\"\n    p = voltage * current\n    idx = np.argmax(p)\n    return voltage[idx], current[idx]\n\n\ndef _calc_I0(IL, I, V, Gp, Rs, nNsVth):\n    return (IL - I - Gp * V - Gp * Rs * I) / np.exp((V + Rs * I) / nNsVth)\n\n\ndef _find_beta0_beta1(v, i, vlim, v_oc):\n    # Get intercept and slope of linear portion of IV curve.\n    # Start with V =< vlim * v_oc, extend by adding points until slope is\n    # negative (downward).\n    beta0 = np.nan\n    beta1 = np.nan\n    first_idx = np.searchsorted(v, vlim * v_oc)\n    for idx in range(first_idx, len(v)):\n        coef = np.polyfit(v[:idx], i[:idx], deg=1)\n        if coef[0] < 0:\n            # intercept term\n            beta0 = coef[1].item()\n            # sign change of slope to get positive parameter value\n            beta1 = -coef[0].item()\n            break\n    if any(np.isnan([beta0, beta1])):\n        raise RuntimeError(\"Parameter extraction failed: beta0={}, beta1={}\"\n                           .format(beta0, beta1))\n    else:\n        return beta0, beta1\n\n\ndef _find_beta3_beta4(voltage, current, beta0, beta1, ilim, i_sc):\n    # Subtract the IV curve from the linear fit.\n    y = beta0 - beta1 * voltage - current\n    x = np.array([np.ones_like(voltage), voltage, current]).T\n    # Select points where y > ilim * i_sc to regress log(y) onto x\n    idx = (y > ilim * i_sc)\n    result = np.linalg.lstsq(x[idx], np.log(y[idx]), rcond=None)\n    coef = result[0]\n    beta3 = coef[1].item()\n    beta4 = coef[2].item()\n    if any(np.isnan([beta3, beta4])):\n        raise RuntimeError(\"Parameter extraction failed: beta3={}, beta4={}\"\n                           .format(beta3, beta4))\n    else:\n        return beta3, beta4\n\n\ndef _calculate_sde_parameters(beta0, beta1, beta3, beta4, v_mp, i_mp, v_oc):\n    nNsVth = 1.0 / beta3\n    Rs = beta4 / beta3\n    Gp = beta1 / (1.0 - Rs * beta1)\n    Rsh = 1.0 / Gp\n    IL = (1 + Gp * Rs) * beta0\n    # calculate I0\n    I0_vmp = _calc_I0(IL, i_mp, v_mp, Gp, Rs, nNsVth)\n    I0_voc = _calc_I0(IL, 0, v_oc, Gp, Rs, nNsVth)\n    if any(np.isnan([I0_vmp, I0_voc])) or ((I0_vmp <= 0) and (I0_voc <= 0)):\n        raise RuntimeError(\"Parameter extraction failed: I0 is undetermined.\")\n    elif (I0_vmp > 0) and (I0_voc > 0):\n        I0 = 0.5 * (I0_vmp + I0_voc)\n    elif (I0_vmp > 0):\n        I0 = I0_vmp\n    else:  # I0_voc > 0\n        I0 = I0_voc\n    return (IL, I0, Rsh, Rs, nNsVth)\ndiff --git a/setup.py b/setup.py\n TESTS_REQUIRE = ['nose', 'pytest', 'pytest-cov', 'pytest-mock',\n                  'pytest-timeout']\n EXTRAS_REQUIRE = {\n    'optional': ['ephem', 'cython', 'netcdf4', 'nrel-pysam', 'numba',\n                 'pvfactors', 'scipy', 'siphon', 'tables'],\n     'doc': ['ipython', 'matplotlib', 'sphinx', 'sphinx_rtd_theme'],\n     'test': TESTS_REQUIRE\n }"
  },
  {
    "instruction": "Corrections to Townsend snow model\nPrivate communications with the model's author have turned up some issues with the pvlib implementation. Chief among the issues is  this part of the calculation:\r\n\r\n```\r\n    lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\r\n    gamma = (\r\n        slant_height\r\n        * effective_snow_weighted_m\r\n        * cosd(surface_tilt)\r\n        / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\r\n        * 2\r\n        * tand(angle_of_repose)\r\n    )\r\n\r\n    ground_interference_term = 1 - C2 * np.exp(-gamma)\r\n```\r\n\r\nWhen `lower_edge_height_clipped` < `effective_snow_weighted_m`, `gamma` < 0 and the `ground_interference_term` can become negative. In contrast, the author's intent is that C2 < `ground_interference_terms` < 1. The author recommends clipping the squared difference (lower bound being worked out but will be something like 0.01.).\r\n\r\nOther issues appear to arise from the unit conversions. The published model uses inches for distance and snow depth. The pvlib code uses cm for snow depth (convenience for working with external snow data) and m for distances (for consistency with the rest of pvlib). After several steps, including the `ground_interference_term` calculation, the code converts from cm or m to inches to apply the final formula for loss (since the formula involves some coefficients determined by a regression). It would be easier to trace the pvlib code back to the paper if the internal unit conversions (from cm / m to inches) were done earlier.\r\n\n",
    "input": " \n def loss_townsend(snow_total, snow_events, surface_tilt, relative_humidity,\n                   temp_air, poa_global, slant_height, lower_edge_height,\n                  angle_of_repose=40):\n     '''\n     Calculates monthly snow loss based on the Townsend monthly snow loss\n     model [1]_.\n         Snow received each month. Referred to as S in [1]_. [cm]\n \n     snow_events : array-like\n        Number of snowfall events each month. Referred to as N in [1]_. [-]\n \n     surface_tilt : float\n         Tilt angle of the array. [deg]\n     lower_edge_height : float\n         Distance from array lower edge to the ground. [m]\n \n     angle_of_repose : float, default 40\n         Piled snow angle, assumed to stabilize at 40\u00b0, the midpoint of\n         25\u00b0-55\u00b0 avalanching slope angles. [deg]\n     -----\n     This model has not been validated for tracking arrays; however, for\n     tracking arrays [1]_ suggests using the maximum rotation angle in place\n    of ``surface_tilt``.\n \n     References\n     ----------\n        :doi:`10.1109/PVSC.2011.6186627`\n     '''\n \n     C1 = 5.7e04\n     C2 = 0.51\n \n    snow_total_prev = np.roll(snow_total, 1)\n     snow_events_prev = np.roll(snow_events, 1)\n \n    effective_snow = _townsend_effective_snow(snow_total, snow_events)\n     effective_snow_prev = _townsend_effective_snow(\n         snow_total_prev,\n         snow_events_prev\n         1 / 3 * effective_snow_prev\n         + 2 / 3 * effective_snow\n     )\n    effective_snow_weighted_m = effective_snow_weighted / 100\n \n    lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\n     gamma = (\n        slant_height\n        * effective_snow_weighted_m\n         * cosd(surface_tilt)\n        / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\n         * 2\n         * tand(angle_of_repose)\n     )\n \n     ground_interference_term = 1 - C2 * np.exp(-gamma)\n    relative_humidity_fraction = relative_humidity / 100\n    temp_air_kelvin = temp_air + 273.15\n    effective_snow_weighted_in = effective_snow_weighted / 2.54\n    poa_global_kWh = poa_global / 1000\n \n     # Calculate Eqn. 3 in the reference.\n     # Although the reference says Eqn. 3 calculates percentage loss, the y-axis\n     # of Figure 7 indicates Eqn. 3 calculates fractional loss. Since the slope\n     # of the line in Figure 7 is the same as C1 in Eqn. 3, it is assumed that\n     # Eqn. 3 calculates fractional loss.\n     loss_fraction = (\n         C1\n        * effective_snow_weighted_in\n         * cosd(surface_tilt)**2\n         * ground_interference_term\n         * relative_humidity_fraction\n         / temp_air_kelvin**2\n         / poa_global_kWh**0.67\n     )\n \n     return np.clip(loss_fraction, 0, 1)",
    "output": " \n def loss_townsend(snow_total, snow_events, surface_tilt, relative_humidity,\n                   temp_air, poa_global, slant_height, lower_edge_height,\n                  string_factor=1.0, angle_of_repose=40):\n     '''\n     Calculates monthly snow loss based on the Townsend monthly snow loss\n     model [1]_.\n         Snow received each month. Referred to as S in [1]_. [cm]\n \n     snow_events : array-like\n        Number of snowfall events each month. May be int or float type for\n        the average events in a typical month. Referred to as N in [1]_.\n \n     surface_tilt : float\n         Tilt angle of the array. [deg]\n     lower_edge_height : float\n         Distance from array lower edge to the ground. [m]\n \n    string_factor : float, default 1.0\n        Multiplier applied to monthly loss fraction. Use 1.0 if the DC array\n        has only one string of modules in the slant direction, use 0.75\n        otherwise. [-]\n\n     angle_of_repose : float, default 40\n         Piled snow angle, assumed to stabilize at 40\u00b0, the midpoint of\n         25\u00b0-55\u00b0 avalanching slope angles. [deg]\n     -----\n     This model has not been validated for tracking arrays; however, for\n     tracking arrays [1]_ suggests using the maximum rotation angle in place\n    of ``surface_tilt``. The author of [1]_ recommends using one-half the\n    table width for ``slant_height``, i.e., the distance from the tracker\n    axis to the module edge.\n\n    The parameter `string_factor` is an enhancement added to the model after\n    publication of [1]_ per private communication with the model's author.\n \n     References\n     ----------\n        :doi:`10.1109/PVSC.2011.6186627`\n     '''\n \n    # unit conversions from cm and m to in, from C to K, and from % to fraction\n    # doing this early to facilitate comparison of this code with [1]\n    snow_total_inches = snow_total / 2.54  # to inches\n    relative_humidity_fraction = relative_humidity / 100.\n    poa_global_kWh = poa_global / 1000.\n    slant_height_inches = slant_height * 39.37\n    lower_edge_height_inches = lower_edge_height * 39.37\n    temp_air_kelvin = temp_air + 273.15\n\n     C1 = 5.7e04\n     C2 = 0.51\n \n    snow_total_prev = np.roll(snow_total_inches, 1)\n     snow_events_prev = np.roll(snow_events, 1)\n \n    effective_snow = _townsend_effective_snow(snow_total_inches, snow_events)\n     effective_snow_prev = _townsend_effective_snow(\n         snow_total_prev,\n         snow_events_prev\n         1 / 3 * effective_snow_prev\n         + 2 / 3 * effective_snow\n     )\n \n    # the lower limit of 0.1 in^2 is per private communication with the model's\n    # author. CWH 1/30/2023\n    lower_edge_distance = np.clip(\n        lower_edge_height_inches**2 - effective_snow_weighted**2, a_min=0.1,\n        a_max=None)\n     gamma = (\n        slant_height_inches\n        * effective_snow_weighted\n         * cosd(surface_tilt)\n        / lower_edge_distance\n         * 2\n         * tand(angle_of_repose)\n     )\n \n     ground_interference_term = 1 - C2 * np.exp(-gamma)\n \n     # Calculate Eqn. 3 in the reference.\n     # Although the reference says Eqn. 3 calculates percentage loss, the y-axis\n     # of Figure 7 indicates Eqn. 3 calculates fractional loss. Since the slope\n     # of the line in Figure 7 is the same as C1 in Eqn. 3, it is assumed that\n     # Eqn. 3 calculates fractional loss.\n\n     loss_fraction = (\n         C1\n        * effective_snow_weighted\n         * cosd(surface_tilt)**2\n         * ground_interference_term\n         * relative_humidity_fraction\n         / temp_air_kelvin**2\n         / poa_global_kWh**0.67\n        * string_factor\n     )\n \n     return np.clip(loss_fraction, 0, 1)"
  },
  {
    "instruction": "PVSystem with single Array generates an error\n**Is your feature request related to a problem? Please describe.**\r\n\r\nWhen a PVSystem has a single Array, you can't assign just the Array instance when constructing the PVSystem.\r\n\r\n```\r\nmount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\narray = pvlib.pvsystem.Array(mount=mount)\r\npv = pvlib.pvsystem.PVSystem(arrays=array)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-f5424e3db16a> in <module>\r\n      3 mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\n      4 array = pvlib.pvsystem.Array(mount=mount)\r\n----> 5 pv = pvlib.pvsystem.PVSystem(arrays=array)\r\n\r\n~\\anaconda3\\lib\\site-packages\\pvlib\\pvsystem.py in __init__(self, arrays, surface_tilt, surface_azimuth, albedo, surface_type, module, module_type, module_parameters, temperature_model_parameters, modules_per_string, strings_per_inverter, inverter, inverter_parameters, racking_model, losses_parameters, name)\r\n    251                 array_losses_parameters,\r\n    252             ),)\r\n--> 253         elif len(arrays) == 0:\r\n    254             raise ValueError(\"PVSystem must have at least one Array. \"\r\n    255                              \"If you want to create a PVSystem instance \"\r\n\r\nTypeError: object of type 'Array' has no len()\r\n\r\n```\r\n\r\nNot a bug per se, since the PVSystem docstring requests that `arrays` be iterable. Still, a bit inconvenient to have to do this\r\n\r\n```\r\nmount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\narray = pvlib.pvsystem.Array(mount=mount)\r\npv = pvlib.pvsystem.PVSystem(arrays=[array])\r\n```\r\n\r\n**Describe the solution you'd like**\r\nHandle `arrays=array` where `array` is an instance of `Array`\r\n\r\n**Describe alternatives you've considered**\r\nStatus quo - either make the single Array into a list, or use the PVSystem kwargs.\r\n\n",
    "input": " \n     Parameters\n     ----------\n    arrays : iterable of Array, optional\n        List of arrays that are part of the system. If not specified\n        a single array is created from the other parameters (e.g.\n        `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n         if length of arrays is 0 a ValueError is raised. If `arrays` is\n         specified the following PVSystem parameters are ignored:\n \n                 strings_per_inverter,\n                 array_losses_parameters,\n             ),)\n         elif len(arrays) == 0:\n             raise ValueError(\"PVSystem must have at least one Array. \"\n                              \"If you want to create a PVSystem instance \"",
    "output": " \n     Parameters\n     ----------\n    arrays : Array or iterable of Array, optional\n        An Array or list of arrays that are part of the system. If not\n        specified a single array is created from the other parameters (e.g.\n        `surface_tilt`, `surface_azimuth`). If specified as a list, the list\n        must contain at least one Array;\n         if length of arrays is 0 a ValueError is raised. If `arrays` is\n         specified the following PVSystem parameters are ignored:\n \n                 strings_per_inverter,\n                 array_losses_parameters,\n             ),)\n        elif isinstance(arrays, Array):\n            self.arrays = (arrays,)\n         elif len(arrays) == 0:\n             raise ValueError(\"PVSystem must have at least one Array. \"\n                              \"If you want to create a PVSystem instance \""
  },
  {
    "instruction": "Inconsistent default settings for _prep_inputs_solar_pos in prepare_inputs and prepare_inputs_from_poa\nHi there,\r\n\r\nI find that `_prep_inputs_solar_pos` method has been both called in [`prepare_inputs`](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/modelchain.html#ModelChain.prepare_inputs) and [`prepare_inputs_from_poa`](https://pvlib-python.readthedocs.io/en/stable/_modules/pvlib/modelchain.html#ModelChain.prepare_inputs_from_poa). However, the former takes an additional argument, press_temp that contains temperature pulled from the weather data provided outside. For the default `nrel_numpy` algorithm, I further checked its input requirement is [avg. yearly air temperature in degrees C](https://pvlib-python.readthedocs.io/en/stable/generated/pvlib.solarposition.spa_python.html#pvlib.solarposition.spa_python) rather than the instantaneous temperature provided in weather. Hence I would like to ask if the following codes in `prepare_inputs` are redundant at least for the default 'nrel_numpy' algorithm?\r\n```\r\n        # build kwargs for solar position calculation\r\n        try:\r\n            press_temp = _build_kwargs(['pressure', 'temp_air'], weather)\r\n            press_temp['temperature'] = press_temp.pop('temp_air')\r\n        except KeyError:\r\n            pass\r\n```\r\nAnd thereby we change `self._prep_inputs_solar_pos(press_temp)` to `self._prep_inputs_solar_pos()` in `prepare_inputs`?\r\n\r\nMeanwhile, does the temperature really matter? How much uncertainty will it cause in the calculation of the sun's position? Should we provide avg. local temperature data if for a global modelling purpose?\r\n\r\nAny help would be appreciated!\r\n\r\n\n",
    "input": "                 weather.ghi - weather.dni *\n                 tools.cosd(self.results.solar_position.zenith))\n \n    def _prep_inputs_solar_pos(self, kwargs={}):\n         \"\"\"\n         Assign solar position\n         \"\"\"\n         self.results.solar_position = self.location.get_solarposition(\n             self.times, method=self.solar_position_method,\n             **kwargs)\n         self._assign_weather(weather)\n         self._assign_times()\n \n        # build kwargs for solar position calculation\n        try:\n            press_temp = _build_kwargs(['pressure', 'temp_air'],\n                                       weather[0] if isinstance(weather, tuple)\n                                       else weather)\n            press_temp['temperature'] = press_temp.pop('temp_air')\n        except KeyError:\n            pass\n\n        self._prep_inputs_solar_pos(press_temp)\n         self._prep_inputs_airmass()\n \n         # PVSystem.get_irradiance and SingleAxisTracker.get_irradiance\n                                         'poa_diffuse'])\n         self._assign_total_irrad(data)\n \n        self._prep_inputs_solar_pos()\n         self._prep_inputs_airmass()\n \n         if isinstance(self.system, SingleAxisTracker):",
    "output": "                 weather.ghi - weather.dni *\n                 tools.cosd(self.results.solar_position.zenith))\n \n    def _prep_inputs_solar_pos(self, weather):\n         \"\"\"\n         Assign solar position\n         \"\"\"\n        # build weather kwargs for solar position calculation\n        kwargs = _build_kwargs(['pressure', 'temp_air'],\n                               weather[0] if isinstance(weather, tuple)\n                               else weather)\n        try:\n            kwargs['temperature'] = kwargs.pop('temp_air')\n        except KeyError:\n            pass\n\n         self.results.solar_position = self.location.get_solarposition(\n             self.times, method=self.solar_position_method,\n             **kwargs)\n         self._assign_weather(weather)\n         self._assign_times()\n \n        self._prep_inputs_solar_pos(weather)\n         self._prep_inputs_airmass()\n \n         # PVSystem.get_irradiance and SingleAxisTracker.get_irradiance\n                                         'poa_diffuse'])\n         self._assign_total_irrad(data)\n \n        self._prep_inputs_solar_pos(data)\n         self._prep_inputs_airmass()\n \n         if isinstance(self.system, SingleAxisTracker):"
  },
  {
    "instruction": "Add a model for spectral corrections\n**Additional context**\r\nFirst of all, I introduce myself, my name is Jose Antonio Caballero, and I have recently finished my PhD in photovoltaic engineering at the University of Ja\u00e9n, Spain.\r\n\r\nI have developed a python script to apply spectral corrections as a function of AM, AOD, PW based on this work (https://doi.org/10.1109/jphotov.2017.2787019).\r\n\r\nWe have found that in pvlib there is already a similar methodology developed by First solar, in which the spectral corrections are based only on the AM and PW parameters, so we intend to include our proposed method in pvlib in a similar way.\r\n\r\nAs an example, I attach the code developed in python (.zip file) to estimate the spectral effects related to different flat photovoltaic technologies from the AM, AOD and PW parameters included in a .csv file.\r\n[PV-MM-AM_AOD_PW_data.csv](https://github.com/pvlib/pvlib-python/files/6970716/PV-MM-AM_AOD_PW_data.csv)\r\n[PV_Spectral_Corrections.zip](https://github.com/pvlib/pvlib-python/files/6970727/PV_Spectral_Corrections.zip)\r\n\r\nKind regards\n",
    "input": "     calc_spectral_mismatch_field,\n     get_am15g,\n     get_example_spectral_response,\n     spectral_factor_firstsolar,\n     spectral_factor_sapm,\n )\ndiff --git a/pvlib/spectrum/mismatch.py b/pvlib/spectrum/mismatch.py\n         spectral_loss = pd.Series(spectral_loss, airmass_absolute.index)\n \n     return spectral_loss",
    "output": "     calc_spectral_mismatch_field,\n     get_am15g,\n     get_example_spectral_response,\n    spectral_factor_caballero,\n     spectral_factor_firstsolar,\n     spectral_factor_sapm,\n )\ndiff --git a/pvlib/spectrum/mismatch.py b/pvlib/spectrum/mismatch.py\n         spectral_loss = pd.Series(spectral_loss, airmass_absolute.index)\n \n     return spectral_loss\n\n\ndef spectral_factor_caballero(precipitable_water, airmass_absolute, aod500,\n                              module_type=None, coefficients=None):\n    r\"\"\"\n    Estimate a technology-specific spectral mismatch modifier from\n    airmass, aerosol optical depth, and atmospheric precipitable water,\n    using the Caballero model.\n\n    The model structure was motivated by examining the effect of these three\n    atmospheric parameters on simulated irradiance spectra and spectral\n    modifiers.  However, the coefficient values reported in [1]_ and\n    available here via the ``module_type`` parameter were determined\n    by fitting the model equations to spectral factors calculated from\n    global tilted spectral irradiance measurements taken in the city of\n    Ja\u00e9n, Spain.  See [1]_ for details.\n\n    Parameters\n    ----------\n    precipitable_water : numeric\n        atmospheric precipitable water. [cm]\n\n    airmass_absolute : numeric\n        absolute (pressure-adjusted) airmass. [unitless]\n\n    aod500 : numeric\n        atmospheric aerosol optical depth at 500 nm. [unitless]\n\n    module_type : str, optional\n        One of the following PV technology strings from [1]_:\n\n        * ``'cdte'`` - anonymous CdTe module.\n        * ``'monosi'``, - anonymous sc-si module.\n        * ``'multisi'``, - anonymous mc-si- module.\n        * ``'cigs'`` - anonymous copper indium gallium selenide module.\n        * ``'asi'`` - anonymous amorphous silicon module.\n        * ``'perovskite'`` - anonymous pervoskite module.\n\n    coefficients : array-like, optional\n        user-defined coefficients, if not using one of the default coefficient\n        sets via the ``module_type`` parameter.\n\n    Returns\n    -------\n    modifier: numeric\n        spectral mismatch factor (unitless) which is multiplied\n        with broadband irradiance reaching a module's cells to estimate\n        effective irradiance, i.e., the irradiance that is converted to\n        electrical current.\n\n    References\n    ----------\n    .. [1] Caballero, J.A., Fern\u00e1ndez, E., Theristis, M.,\n        Almonacid, F., and Nofuentes, G. \"Spectral Corrections Based on\n        Air Mass, Aerosol Optical Depth and Precipitable Water\n        for PV Performance Modeling.\"\n        IEEE Journal of Photovoltaics 2018, 8(2), 552-558.\n        :doi:`10.1109/jphotov.2017.2787019`\n    \"\"\"\n\n    if module_type is None and coefficients is None:\n        raise ValueError('Must provide either `module_type` or `coefficients`')\n    if module_type is not None and coefficients is not None:\n        raise ValueError('Only one of `module_type` and `coefficients` should '\n                         'be provided')\n\n    # Experimental coefficients from [1]_.\n    # The extra 0/1 coefficients at the end are used to enable/disable\n    # terms to match the different equation forms in Table 1.\n    _coefficients = {}\n    _coefficients['cdte'] = (\n        1.0044, 0.0095, -0.0037, 0.0002, 0.0000, -0.0046,\n        -0.0182, 0, 0.0095, 0.0068, 0, 1)\n    _coefficients['monosi'] = (\n        0.9706, 0.0377, -0.0123, 0.0025, -0.0002, 0.0159,\n        -0.0165, 0, -0.0016, -0.0027, 1, 0)\n    _coefficients['multisi'] = (\n        0.9836, 0.0254, -0.0085, 0.0016, -0.0001, 0.0094,\n        -0.0132, 0, -0.0002, -0.0011, 1, 0)\n    _coefficients['cigs'] = (\n        0.9801, 0.0283, -0.0092, 0.0019, -0.0001, 0.0117,\n        -0.0126, 0, -0.0011, -0.0019, 1, 0)\n    _coefficients['asi'] = (\n        1.1060, -0.0848, 0.0302, -0.0076, 0.0006, -0.1283,\n        0.0986, -0.0254, 0.0156, 0.0146, 1, 0)\n    _coefficients['perovskite'] = (\n        1.0637, -0.0491, 0.0180, -0.0047, 0.0004, -0.0773,\n        0.0583, -0.0159, 0.01251, 0.0109, 1, 0)\n\n    if module_type is not None:\n        coeff = _coefficients[module_type]\n    else:\n        coeff = coefficients\n\n    # Evaluate spectral correction factor\n    ama = airmass_absolute\n    aod500_ref = 0.084\n    pw_ref = 1.4164\n\n    f_AM = (\n        coeff[0]\n        + coeff[1] * ama\n        + coeff[2] * ama**2\n        + coeff[3] * ama**3\n        + coeff[4] * ama**4\n    )\n    # Eq 6, with Table 1\n    f_AOD = (aod500 - aod500_ref) * (\n        coeff[5]\n        + coeff[10] * coeff[6] * ama\n        + coeff[11] * coeff[6] * np.log(ama)\n        + coeff[7] * ama**2\n    )\n    # Eq 7, with Table 1\n    f_PW = (precipitable_water - pw_ref) * (\n        coeff[8]\n        + coeff[9] * np.log(ama)\n    )\n    modifier = f_AM + f_AOD + f_PW  # Eq 5\n    return modifier"
  },
  {
    "instruction": "`pvlib.iotools.get_pvgis_hourly`'s `surface_azimuth` parameter doesn't use pvlib's azimuth convention\nNearly everything in pvlib represents azimuth angles as values in [0, 360) clockwise from north, except `pvlib.iotools.get_pvgis_hourly`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/3def7e3375002ee3a5492b7bc609d3fb63a8edb1/pvlib/iotools/pvgis.py#L79-L81\r\n\r\nThis inconsistency is a shame.  However, I don't see any way to switch it to pvlib's convention without a hard break, which is also a shame.  I wonder how others view the cost/benefit analysis here.\r\n\r\nSee also https://github.com/pvlib/pvlib-python/pull/1395#discussion_r1181853794\r\n\r\n\n",
    "input": " \n def get_pvgis_hourly(latitude, longitude, start=None, end=None,\n                      raddatabase=None, components=True,\n                     surface_tilt=0, surface_azimuth=0,\n                      outputformat='json',\n                      usehorizon=True, userhorizon=None,\n                      pvcalculation=False,\n         Otherwise only global irradiance is returned.\n     surface_tilt: float, default: 0\n         Tilt angle from horizontal plane. Ignored for two-axis tracking.\n    surface_azimuth: float, default: 0\n        Orientation (azimuth angle) of the (fixed) plane. 0=south, 90=west,\n        -90: east. Ignored for tracking systems.\n     usehorizon: bool, default: True\n         Include effects of horizon\n     userhorizon: list of float, default: None\n     time stamp convention, e.g., SARAH and SARAH2 provide instantaneous values,\n     whereas values from ERA5 are averages for the hour.\n \n     Notes\n     -----\n     data includes the following fields:\n     \"\"\"  # noqa: E501\n     # use requests to format the query string by passing params dictionary\n     params = {'lat': latitude, 'lon': longitude, 'outputformat': outputformat,\n              'angle': surface_tilt, 'aspect': surface_azimuth,\n               'pvcalculation': int(pvcalculation),\n               'pvtechchoice': pvtechchoice, 'mountingplace': mountingplace,\n               'trackingtype': trackingtype, 'components': int(components),\n     metadata : dict\n         metadata\n \n     Raises\n     ------\n     ValueError",
    "output": " \n def get_pvgis_hourly(latitude, longitude, start=None, end=None,\n                      raddatabase=None, components=True,\n                     surface_tilt=0, surface_azimuth=180,\n                      outputformat='json',\n                      usehorizon=True, userhorizon=None,\n                      pvcalculation=False,\n         Otherwise only global irradiance is returned.\n     surface_tilt: float, default: 0\n         Tilt angle from horizontal plane. Ignored for two-axis tracking.\n    surface_azimuth: float, default: 180\n        Orientation (azimuth angle) of the (fixed) plane. Counter-clockwise\n        from north (north=0, south=180). This is offset 180 degrees from\n        the convention used by PVGIS. Ignored for tracking systems.\n\n        .. versionchanged:: 0.10.0\n           The `surface_azimuth` parameter now follows the pvlib convention, which\n           is counterclockwise from north. However, the convention used by the\n           PVGIS website and pvlib<=0.9.5 is offset by 180 degrees.\n     usehorizon: bool, default: True\n         Include effects of horizon\n     userhorizon: list of float, default: None\n     time stamp convention, e.g., SARAH and SARAH2 provide instantaneous values,\n     whereas values from ERA5 are averages for the hour.\n \n    Warning\n    -------\n    The azimuth orientation specified in the output metadata does not\n    correspond to the pvlib convention, but is offset 180 degrees. This is\n    despite the fact that the input parameter `surface_tilt` has to be\n    specified according to the pvlib convention.\n\n     Notes\n     -----\n     data includes the following fields:\n     \"\"\"  # noqa: E501\n     # use requests to format the query string by passing params dictionary\n     params = {'lat': latitude, 'lon': longitude, 'outputformat': outputformat,\n              'angle': surface_tilt, 'aspect': surface_azimuth-180,\n               'pvcalculation': int(pvcalculation),\n               'pvtechchoice': pvtechchoice, 'mountingplace': mountingplace,\n               'trackingtype': trackingtype, 'components': int(components),\n     metadata : dict\n         metadata\n \n    Warning\n    -------\n    The azimuth orientation specified in the output metadata does not\n    correspond to the pvlib convention, but is offset 180 degrees.\n\n     Raises\n     ------\n     ValueError"
  },
  {
    "instruction": "It should be impossible to instantiate a PVSystem with no Arrays\n**Describe the bug**\r\nIt should be impossible to instantiate a `PVSystem` with no `Arrays`. Currently this is possible via `PVSystem(arrays=[])`.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```python\r\nfrom pvlib import pvsystem\r\npvsystem.PVSystem(arrays=[])\r\n```\r\nresults in this PVSystem:\r\n```\r\nPVSystem:\r\n  name: None\r\n  inverter: None\r\n```\r\n**Expected behavior**\r\nA `ValueError` should be raised indicating that a PVSystem must have at least one `Array` and suggesting that a system with an arbitrary default array can be constructed by passing `arrays=None` or not passing the `arrays` parameter at all.\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.8.1+\r\n\r\n\n",
    "input": "     arrays : iterable of Array, optional\n         List of arrays that are part of the system. If not specified\n         a single array is created from the other parameters (e.g.\n        `surface_tilt`, `surface_azimuth`). If `arrays` is specified\n        the following parameters are ignored:\n \n         - `surface_tilt`\n         - `surface_azimuth`\n         Arbitrary keyword arguments.\n         Included for compatibility, but not used.\n \n     See also\n     --------\n     pvlib.location.Location\n                 racking_model,\n                 array_losses_parameters,\n             ),)\n         else:\n             self.arrays = tuple(arrays)\n ",
    "output": "     arrays : iterable of Array, optional\n         List of arrays that are part of the system. If not specified\n         a single array is created from the other parameters (e.g.\n        `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n        if length of arrays is 0 a ValueError is raised. If `arrays` is\n        specified the following parameters are ignored:\n \n         - `surface_tilt`\n         - `surface_azimuth`\n         Arbitrary keyword arguments.\n         Included for compatibility, but not used.\n \n    Raises\n    ------\n    ValueError\n        If `arrays` is not None and has length 0.\n\n     See also\n     --------\n     pvlib.location.Location\n                 racking_model,\n                 array_losses_parameters,\n             ),)\n        elif len(arrays) == 0:\n            raise ValueError(\"PVSystem must have at least one Array. \"\n                             \"If you want to create a PVSystem instance \"\n                             \"with a single Array pass `arrays=None` and pass \"\n                             \"values directly to PVSystem attributes, e.g., \"\n                             \"`surface_tilt=30`\")\n         else:\n             self.arrays = tuple(arrays)\n "
  },
  {
    "instruction": "Add recombination current params to all bishop88 functions\nThe changes made in #163 incorporate recombination current into the `bishop88()` function.  Functions that build on the `bishop88()` function should likewise accept these parameters.\r\n\n",
    "input": " \n \n def max_power_point(photocurrent, saturation_current, resistance_series,\n                    resistance_shunt, nNsVth, method='brentq'):\n     \"\"\"\n     Given the single diode equation coefficients, calculates the maximum power\n     point (MPP).\n     nNsVth : numeric\n         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n         and number of serices cells ``Ns``\n     method : str\n         either ``'newton'`` or ``'brentq'``\n \n     \"\"\"\n     i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(\n         photocurrent, saturation_current, resistance_series,\n        resistance_shunt, nNsVth, method=method.lower()\n     )\n     if isinstance(photocurrent, pd.Series):\n         ivp = {'i_mp': i_mp, 'v_mp': v_mp, 'p_mp': p_mp}\ndiff --git a/pvlib/singlediode.py b/pvlib/singlediode.py\n     nNsVth : numeric\n         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n         and number of series cells ``Ns``\n    d2mutau : numeric\n        PVSyst thin-film recombination parameter that is the ratio of thickness\n        of the intrinsic layer squared :math:`d^2` and the diffusion length of\n        charge carriers :math:`\\\\mu \\\\tau`, in volts [V], defaults to 0[V]\n    NsVbi : numeric\n        PVSyst thin-film recombination parameter that is the product of the PV\n        module number of series cells ``Ns`` and the builtin voltage ``Vbi`` of\n        the intrinsic layer, in volts [V], defaults to ``np.inf``\n     gradients : bool\n         False returns only I, V, and P. True also returns gradients\n \n     Notes\n     -----\n     The PVSyst thin-film recombination losses parameters ``d2mutau`` and\n    ``NsVbi`` are only applied to cadmium-telluride (CdTe) and amorphous-\n    silicon (a:Si) PV modules, [2]_, [3]_. The builtin voltage :math:`V_{bi}`\n     should account for all junctions. For example: tandem and triple junction\n     cells would have builtin voltages of 1.8[V] and 2.7[V] respectively, based\n     on the default of 0.9[V] for a single junction. The parameter ``NsVbi``\n \n def bishop88_i_from_v(voltage, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth,\n                      method='newton'):\n     \"\"\"\n     Find current given any voltage.\n \n     nNsVth : numeric\n         product of diode ideality factor (n), number of series cells (Ns), and\n         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n     method : str\n         one of two optional search methods: either ``'brentq'``, a reliable and\n         bounded method or ``'newton'`` which is the default.\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth)\n \n     def fv(x, v, *a):\n         # calculate voltage residual given diode voltage \"x\"\n         # brentq only works with scalar inputs, so we need a set up function\n         # and np.vectorize to repeatedly call the optimizer with the right\n         # arguments for possible array input\n        def vd_from_brent(voc, v, iph, isat, rs, rsh, gamma):\n            return brentq(fv, 0.0, voc, args=(v, iph, isat, rs, rsh, gamma))\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n \n def bishop88_v_from_i(current, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth,\n                      method='newton'):\n     \"\"\"\n     Find voltage given any current.\n \n     nNsVth : numeric\n         product of diode ideality factor (n), number of series cells (Ns), and\n         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n     method : str\n         one of two optional search methods: either ``'brentq'``, a reliable and\n         bounded method or ``'newton'`` which is the default.\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth)\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n         # brentq only works with scalar inputs, so we need a set up function\n         # and np.vectorize to repeatedly call the optimizer with the right\n         # arguments for possible array input\n        def vd_from_brent(voc, i, iph, isat, rs, rsh, gamma):\n            return brentq(fi, 0.0, voc, args=(i, iph, isat, rs, rsh, gamma))\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, current, *args)\n \n \n def bishop88_mpp(photocurrent, saturation_current, resistance_series,\n                 resistance_shunt, nNsVth, method='newton'):\n     \"\"\"\n     Find max power point.\n \n     nNsVth : numeric\n         product of diode ideality factor (n), number of series cells (Ns), and\n         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n     method : str\n         one of two optional search methods: either ``'brentq'``, a reliable and\n         bounded method or ``'newton'`` which is the default.\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth)\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     if method.lower() == 'brentq':\n         # break out arguments for numpy.vectorize to handle broadcasting\n         vec_fun = np.vectorize(\n            lambda voc, iph, isat, rs, rsh, gamma:\n                brentq(fmpp, 0.0, voc, args=(iph, isat, rs, rsh, gamma))\n         )\n         vd = vec_fun(voc_est, *args)\n     elif method.lower() == 'newton':",
    "output": " \n \n def max_power_point(photocurrent, saturation_current, resistance_series,\n                    resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n                    method='brentq'):\n     \"\"\"\n     Given the single diode equation coefficients, calculates the maximum power\n     point (MPP).\n     nNsVth : numeric\n         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n         and number of serices cells ``Ns``\n    d2mutau : numeric, default 0\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that accounts for recombination current in the\n        intrinsic layer. The value is the ratio of intrinsic layer thickness\n        squared :math:`d^2` to the diffusion length of charge carriers\n        :math:`\\\\mu \\\\tau`. [V]\n    NsVbi : numeric, default np.inf\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that is the product of the PV module number of series\n        cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n        [V].\n     method : str\n         either ``'newton'`` or ``'brentq'``\n \n     \"\"\"\n     i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(\n         photocurrent, saturation_current, resistance_series,\n        resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n        method=method.lower()\n     )\n     if isinstance(photocurrent, pd.Series):\n         ivp = {'i_mp': i_mp, 'v_mp': v_mp, 'p_mp': p_mp}\ndiff --git a/pvlib/singlediode.py b/pvlib/singlediode.py\n     nNsVth : numeric\n         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n         and number of series cells ``Ns``\n    d2mutau : numeric, default 0\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that accounts for recombination current in the\n        intrinsic layer. The value is the ratio of intrinsic layer thickness\n        squared :math:`d^2` to the diffusion length of charge carriers\n        :math:`\\\\mu \\\\tau`. [V]\n    NsVbi : numeric, default np.inf\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that is the product of the PV module number of series\n        cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n        [V].\n     gradients : bool\n         False returns only I, V, and P. True also returns gradients\n \n     Notes\n     -----\n     The PVSyst thin-film recombination losses parameters ``d2mutau`` and\n    ``NsVbi`` should only be applied to cadmium-telluride (CdTe) and amorphous-\n    silicon (a-Si) PV modules, [2]_, [3]_. The builtin voltage :math:`V_{bi}`\n     should account for all junctions. For example: tandem and triple junction\n     cells would have builtin voltages of 1.8[V] and 2.7[V] respectively, based\n     on the default of 0.9[V] for a single junction. The parameter ``NsVbi``\n \n def bishop88_i_from_v(voltage, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth,\n                      d2mutau=0, NsVbi=np.Inf, method='newton'):\n     \"\"\"\n     Find current given any voltage.\n \n     nNsVth : numeric\n         product of diode ideality factor (n), number of series cells (Ns), and\n         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n    d2mutau : numeric, default 0\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that accounts for recombination current in the\n        intrinsic layer. The value is the ratio of intrinsic layer thickness\n        squared :math:`d^2` to the diffusion length of charge carriers\n        :math:`\\\\mu \\\\tau`. [V]\n    NsVbi : numeric, default np.inf\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that is the product of the PV module number of series\n        cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n        [V].\n     method : str\n         one of two optional search methods: either ``'brentq'``, a reliable and\n         bounded method or ``'newton'`` which is the default.\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth, d2mutau, NsVbi)\n \n     def fv(x, v, *a):\n         # calculate voltage residual given diode voltage \"x\"\n         # brentq only works with scalar inputs, so we need a set up function\n         # and np.vectorize to repeatedly call the optimizer with the right\n         # arguments for possible array input\n        def vd_from_brent(voc, v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi):\n            return brentq(fv, 0.0, voc,\n                          args=(v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi))\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n \n def bishop88_v_from_i(current, photocurrent, saturation_current,\n                       resistance_series, resistance_shunt, nNsVth,\n                      d2mutau=0, NsVbi=np.Inf, method='newton'):\n     \"\"\"\n     Find voltage given any current.\n \n     nNsVth : numeric\n         product of diode ideality factor (n), number of series cells (Ns), and\n         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n    d2mutau : numeric, default 0\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that accounts for recombination current in the\n        intrinsic layer. The value is the ratio of intrinsic layer thickness\n        squared :math:`d^2` to the diffusion length of charge carriers\n        :math:`\\\\mu \\\\tau`. [V]\n    NsVbi : numeric, default np.inf\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that is the product of the PV module number of series\n        cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n        [V].\n     method : str\n         one of two optional search methods: either ``'brentq'``, a reliable and\n         bounded method or ``'newton'`` which is the default.\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth, d2mutau, NsVbi)\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n         # brentq only works with scalar inputs, so we need a set up function\n         # and np.vectorize to repeatedly call the optimizer with the right\n         # arguments for possible array input\n        def vd_from_brent(voc, i, iph, isat, rs, rsh, gamma, d2mutau, NsVbi):\n            return brentq(fi, 0.0, voc,\n                          args=(i, iph, isat, rs, rsh, gamma, d2mutau, NsVbi))\n \n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, current, *args)\n \n \n def bishop88_mpp(photocurrent, saturation_current, resistance_series,\n                 resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n                 method='newton'):\n     \"\"\"\n     Find max power point.\n \n     nNsVth : numeric\n         product of diode ideality factor (n), number of series cells (Ns), and\n         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n    d2mutau : numeric, default 0\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that accounts for recombination current in the\n        intrinsic layer. The value is the ratio of intrinsic layer thickness\n        squared :math:`d^2` to the diffusion length of charge carriers\n        :math:`\\\\mu \\\\tau`. [V]\n    NsVbi : numeric, default np.inf\n        PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n        (a-Si) modules that is the product of the PV module number of series\n        cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n        [V].\n     method : str\n         one of two optional search methods: either ``'brentq'``, a reliable and\n         bounded method or ``'newton'`` which is the default.\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth, d2mutau, NsVbi)\n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     if method.lower() == 'brentq':\n         # break out arguments for numpy.vectorize to handle broadcasting\n         vec_fun = np.vectorize(\n            lambda voc, iph, isat, rs, rsh, gamma, d2mutau, NsVbi:\n                brentq(fmpp, 0.0, voc,\n                       args=(iph, isat, rs, rsh, gamma, d2mutau, NsVbi))\n         )\n         vd = vec_fun(voc_est, *args)\n     elif method.lower() == 'newton':"
  },
  {
    "instruction": "access private _parse_pvgis_tmy_csv() function as read_pvgis_tmy_csv()\n**Is your feature request related to a problem? Please describe.**\r\nsomeone sent me a csv file they downloaded from pvgis, and I needed to parse it, so I had to call the private methods like this:\r\n\r\n```python\r\n>>> from pvlib.iotools.pvgis import _parse_pvgis_tmy_csv\r\n>>> with (path_to_folder / 'pvgis_tmy_lat_lon_years.csv').open('rb') as f:\r\n        pvgis_data = _parse_pvgis_tmy_csv(f)\r\n```\r\n\r\n**Describe the solution you'd like**\r\nIf I need this, others may also. I think a public method that takes either a string or a buffer could be useful? Something called `read_pvgis_tmy_csv()`\r\n\r\n**Describe alternatives you've considered**\r\nI was able to do it by just calling the private function and it worked, so that's an alternative also\r\n\r\n**Additional context**\r\nrelated to #845 and #849 \r\n\naccess private _parse_pvgis_tmy_csv() function as read_pvgis_tmy_csv()\n**Is your feature request related to a problem? Please describe.**\r\nsomeone sent me a csv file they downloaded from pvgis, and I needed to parse it, so I had to call the private methods like this:\r\n\r\n```python\r\n>>> from pvlib.iotools.pvgis import _parse_pvgis_tmy_csv\r\n>>> with (path_to_folder / 'pvgis_tmy_lat_lon_years.csv').open('rb') as f:\r\n        pvgis_data = _parse_pvgis_tmy_csv(f)\r\n```\r\n\r\n**Describe the solution you'd like**\r\nIf I need this, others may also. I think a public method that takes either a string or a buffer could be useful? Something called `read_pvgis_tmy_csv()`\r\n\r\n**Describe alternatives you've considered**\r\nI was able to do it by just calling the private function and it worked, so that's an alternative also\r\n\r\n**Additional context**\r\nrelated to #845 and #849 \r\n\n",
    "input": " from pvlib.iotools.psm3 import get_psm3  # noqa: F401\n from pvlib.iotools.psm3 import read_psm3  # noqa: F401\n from pvlib.iotools.psm3 import parse_psm3  # noqa: F401\nfrom pvlib.iotools.pvgis import get_pvgis_tmy  # noqa: F401\ndiff --git a/pvlib/iotools/pvgis.py b/pvlib/iotools/pvgis.py\n   <https://ec.europa.eu/jrc/en/PVGIS/tools/monthly-radiation>`_\n \"\"\"\n import io\n import requests\n import pandas as pd\nfrom pvlib.iotools import parse_epw\n \n URL = 'https://re.jrc.ec.europa.eu/api/'\n \n         the error message in the response will be raised as an exception,\n         otherwise raise whatever ``HTTP/1.1`` error occurred\n \n     References\n     ----------\n \n         data['time(UTC)'], format='%Y%m%d:%H%M', utc=True)\n     data = data.drop('time(UTC)', axis=1)\n     return data, None, None, None",
    "output": " from pvlib.iotools.psm3 import get_psm3  # noqa: F401\n from pvlib.iotools.psm3 import read_psm3  # noqa: F401\n from pvlib.iotools.psm3 import parse_psm3  # noqa: F401\nfrom pvlib.iotools.pvgis import get_pvgis_tmy, read_pvgis_tmy  # noqa: F401\ndiff --git a/pvlib/iotools/pvgis.py b/pvlib/iotools/pvgis.py\n   <https://ec.europa.eu/jrc/en/PVGIS/tools/monthly-radiation>`_\n \"\"\"\n import io\nimport json\nfrom pathlib import Path\n import requests\n import pandas as pd\nfrom pvlib.iotools import read_epw, parse_epw\n \n URL = 'https://re.jrc.ec.europa.eu/api/'\n \n         the error message in the response will be raised as an exception,\n         otherwise raise whatever ``HTTP/1.1`` error occurred\n \n    See also\n    --------\n    read_pvgis_tmy\n\n     References\n     ----------\n \n         data['time(UTC)'], format='%Y%m%d:%H%M', utc=True)\n     data = data.drop('time(UTC)', axis=1)\n     return data, None, None, None\n\n\ndef read_pvgis_tmy(filename, pvgis_format=None):\n    \"\"\"\n    Read a file downloaded from PVGIS.\n\n    Parameters\n    ----------\n    filename : str, pathlib.Path, or file-like buffer\n        Name, path, or buffer of file downloaded from PVGIS.\n    pvgis_format : str, default None\n        Format of PVGIS file or buffer. Equivalent to the ``outputformat``\n        parameter in the PVGIS TMY API. If `filename` is a file and\n        `pvgis_format` is ``None`` then the file extension will be used to\n        determine the PVGIS format to parse. For PVGIS files from the API with\n        ``outputformat='basic'``, please set `pvgis_format` to ``'basic'``. If\n        `filename` is a buffer, then `pvgis_format` is required and must be in\n        ``['csv', 'epw', 'json', 'basic']``.\n\n    Returns\n    -------\n    data : pandas.DataFrame\n        the weather data\n    months_selected : list\n        TMY year for each month, ``None`` for basic and EPW\n    inputs : dict\n        the inputs, ``None`` for basic and EPW\n    meta : list or dict\n        meta data, ``None`` for basic\n\n    Raises\n    ------\n    ValueError\n        if `pvgis_format` is ``None`` and the file extension is neither\n        ``.csv``, ``.json``, nor ``.epw``, or if `pvgis_format` is provided as\n        input but isn't in ``['csv', 'epw', 'json', 'basic']``\n    TypeError\n        if `pvgis_format` is ``None`` and `filename` is a buffer\n\n    See also\n    --------\n    get_pvgis_tmy\n    \"\"\"\n    # get the PVGIS outputformat\n    if pvgis_format is None:\n        # get the file extension from suffix, but remove the dot and make sure\n        # it's lower case to compare with epw, csv, or json\n        # NOTE: raises TypeError if filename is a buffer\n        outputformat = Path(filename).suffix[1:].lower()\n    else:\n        outputformat = pvgis_format\n\n    # parse the pvgis file based on the output format, either 'epw', 'json',\n    # 'csv', or 'basic'\n\n    # EPW: use the EPW parser from the pvlib.iotools epw.py module\n    if outputformat == 'epw':\n        try:\n            data, meta = parse_epw(filename)\n        except AttributeError:  # str/path has no .read() attribute\n            data, meta = read_epw(filename)\n        return data, None, None, meta\n\n    # NOTE: json, csv, and basic output formats have parsers defined as private\n    # functions in this module\n\n    # JSON: use Python built-in json module to convert file contents to a\n    # Python dictionary, and pass the dictionary to the _parse_pvgis_tmy_json()\n    # function from this module\n    if outputformat == 'json':\n        try:\n            src = json.load(filename)\n        except AttributeError:  # str/path has no .read() attribute\n            with open(str(filename), 'r') as fbuf:\n                src = json.load(fbuf)\n        return _parse_pvgis_tmy_json(src)\n\n    # CSV or basic: use the correct parser from this module\n    # eg: _parse_pvgis_tmy_csv() or _parse_pvgist_tmy_basic()\n    if outputformat in ['csv', 'basic']:\n        # get the correct parser function for this output format from globals()\n        pvgis_parser = globals()['_parse_pvgis_tmy_{:s}'.format(outputformat)]\n        # NOTE: pvgis_parse() is a pvgis parser function from this module,\n        # either _parse_pvgis_tmy_csv() or _parse_pvgist_tmy_basic()\n        try:\n            pvgis_data = pvgis_parser(filename)\n        except AttributeError:  # str/path has no .read() attribute\n            with open(str(filename), 'rb') as fbuf:\n                pvgis_data = pvgis_parser(fbuf)\n        return pvgis_data\n\n    # raise exception if pvgis format isn't in ['csv', 'basic', 'epw', 'json']\n    err_msg = (\n        \"pvgis format '{:s}' was unknown, must be either 'epw', 'json', 'csv'\"\n        \", or 'basic'\").format(outputformat)\n    raise ValueError(err_msg)"
  },
  {
    "instruction": "remove **kwargs from PVSystem, ModelChain, Location\nThese objects accept arbitrary kwargs so that users can be lazy about splatting dictionaries into the object constructors. I guess this is nice in some situations. But it also leads to bugs when users mistype a parameter name because python doesn't raise an exception. I ran into this when working on #1022 and #1027. \r\n\r\nI propose that we remove the kwargs without deprecation in 0.8.\n",
    "input": " # Will Holmgren, University of Arizona, 2014-2016.\n \n import datetime\n \n import pandas as pd\n import pytz\n \n from pvlib import solarposition, clearsky, atmosphere, irradiance\n \n \n class Location:\n     name : None or string, default None.\n         Sets the name attribute of the Location object.\n \n    **kwargs\n        Arbitrary keyword arguments.\n        Included for compatibility, but not used.\n\n     See also\n     --------\n     pvlib.pvsystem.PVSystem\n \n         self.name = name\n \n     def __repr__(self):\n         attrs = ['name', 'latitude', 'longitude', 'altitude', 'tz']\n         return ('Location: \\n  ' + '\\n  '.join(\ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n \n     name: None or str, default None\n         Name of ModelChain instance.\n\n    **kwargs\n        Arbitrary keyword arguments. Included for compatibility, but not\n        used.\n     \"\"\"\n \n     def __init__(self, system, location,\n         self.times = None\n         self.solar_position = None\n \n     @classmethod\n     def with_pvwatts(cls, system, location,\n                      orientation_strategy=None,\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n     :py:class:`~pvlib.modelchain.ModelChain`\n     objects.\n \n    See the :py:class:`LocalizedPVSystem` class for an object model that\n    describes an installed PV system.\n\n     The class supports basic system topologies consisting of:\n \n         * `N` total modules arranged in series\n     --------\n     pvlib.location.Location\n     pvlib.tracking.SingleAxisTracker\n    pvlib.pvsystem.LocalizedPVSystem\n     \"\"\"\n \n     def __init__(self,\n \n         self.name = name\n \n     def __repr__(self):\n         attrs = ['name', 'surface_tilt', 'surface_azimuth', 'module',\n                  'inverter', 'albedo', 'racking_model', 'module_type',\n         return inverter.pvwatts(pdc, self.inverter_parameters['pdc0'],\n                                 **kwargs)\n \n     def localize(self, location=None, latitude=None, longitude=None,\n                  **kwargs):\n        \"\"\"Creates a LocalizedPVSystem object using this object\n         and location data. Must supply either location object or\n         latitude, longitude, and any location kwargs\n \n         return LocalizedPVSystem(pvsystem=self, location=location)\n \n \n class LocalizedPVSystem(PVSystem, Location):\n     \"\"\"\n     The LocalizedPVSystem class defines a standard set of installed PV\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n from pvlib.pvsystem import PVSystem\n from pvlib.location import Location\n from pvlib import irradiance, atmosphere\n \n \n class SingleAxisTracker(PVSystem):\n         :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n         `cross_axis_tilt`. [degrees]\n \n     See also\n     --------\n     pvlib.tracking.singleaxis\n \n         return tracking_data\n \n     def localize(self, location=None, latitude=None, longitude=None,\n                  **kwargs):\n         \"\"\"\n                                                **kwargs)\n \n \n class LocalizedSingleAxisTracker(SingleAxisTracker, Location):\n     \"\"\"\n     The :py:class:`~pvlib.tracking.LocalizedSingleAxisTracker` class defines a",
    "output": " # Will Holmgren, University of Arizona, 2014-2016.\n \n import datetime\nimport warnings\n \n import pandas as pd\n import pytz\n \n from pvlib import solarposition, clearsky, atmosphere, irradiance\nfrom pvlib._deprecation import pvlibDeprecationWarning\n \n \n class Location:\n     name : None or string, default None.\n         Sets the name attribute of the Location object.\n \n     See also\n     --------\n     pvlib.pvsystem.PVSystem\n \n         self.name = name\n \n        if kwargs:\n            warnings.warn(\n                'Arbitrary Location kwargs are deprecated and will be '\n                'removed in v0.9', pvlibDeprecationWarning\n            )\n\n     def __repr__(self):\n         attrs = ['name', 'latitude', 'longitude', 'altitude', 'tz']\n         return ('Location: \\n  ' + '\\n  '.join(\ndiff --git a/pvlib/modelchain.py b/pvlib/modelchain.py\n \n     name: None or str, default None\n         Name of ModelChain instance.\n     \"\"\"\n \n     def __init__(self, system, location,\n         self.times = None\n         self.solar_position = None\n \n        if kwargs:\n            warnings.warn(\n                'Arbitrary ModelChain kwargs are deprecated and will be '\n                'removed in v0.9', pvlibDeprecationWarning\n            )\n\n     @classmethod\n     def with_pvwatts(cls, system, location,\n                      orientation_strategy=None,\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n     :py:class:`~pvlib.modelchain.ModelChain`\n     objects.\n \n     The class supports basic system topologies consisting of:\n \n         * `N` total modules arranged in series\n     --------\n     pvlib.location.Location\n     pvlib.tracking.SingleAxisTracker\n     \"\"\"\n \n     def __init__(self,\n \n         self.name = name\n \n        if kwargs:\n            warnings.warn(\n                'Arbitrary PVSystem kwargs are deprecated and will be '\n                'removed in v0.9', pvlibDeprecationWarning\n            )\n\n     def __repr__(self):\n         attrs = ['name', 'surface_tilt', 'surface_azimuth', 'module',\n                  'inverter', 'albedo', 'racking_model', 'module_type',\n         return inverter.pvwatts(pdc, self.inverter_parameters['pdc0'],\n                                 **kwargs)\n \n    @deprecated('0.8', alternative='PVSystem, Location, and ModelChain',\n                name='PVSystem.localize', removal='0.9')\n     def localize(self, location=None, latitude=None, longitude=None,\n                  **kwargs):\n        \"\"\"\n        Creates a LocalizedPVSystem object using this object\n         and location data. Must supply either location object or\n         latitude, longitude, and any location kwargs\n \n         return LocalizedPVSystem(pvsystem=self, location=location)\n \n \n@deprecated('0.8', alternative='PVSystem, Location, and ModelChain',\n            name='LocalizedPVSystem', removal='0.9')\n class LocalizedPVSystem(PVSystem, Location):\n     \"\"\"\n     The LocalizedPVSystem class defines a standard set of installed PV\ndiff --git a/pvlib/tracking.py b/pvlib/tracking.py\n from pvlib.pvsystem import PVSystem\n from pvlib.location import Location\n from pvlib import irradiance, atmosphere\nfrom pvlib._deprecation import deprecated\n \n \n class SingleAxisTracker(PVSystem):\n         :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n         `cross_axis_tilt`. [degrees]\n \n    **kwargs\n        Passed to :py:class:`~pvlib.pvsystem.PVSystem`.\n\n     See also\n     --------\n     pvlib.tracking.singleaxis\n \n         return tracking_data\n \n    @deprecated('0.8',\n                alternative='SingleAxisTracker, Location, and ModelChain',\n                name='SingleAxisTracker.localize', removal='0.9')\n     def localize(self, location=None, latitude=None, longitude=None,\n                  **kwargs):\n         \"\"\"\n                                                **kwargs)\n \n \n@deprecated('0.8', alternative='SingleAxisTracker, Location, and ModelChain',\n            name='LocalizedSingleAxisTracker', removal='0.9')\n class LocalizedSingleAxisTracker(SingleAxisTracker, Location):\n     \"\"\"\n     The :py:class:`~pvlib.tracking.LocalizedSingleAxisTracker` class defines a"
  },
  {
    "instruction": "Altitude lookup table\nCurrently, altitude for `pvlib.location` based algorithms defaults to zero, but if we include a low-resolution altitude lookup, we can provide better results when altitude is not specified.\r\nWe can make this altitude lookup the same format as [LinkeTurbidities.h5](https://github.com/pvlib/pvlib-python/blob/master/pvlib/data/LinkeTurbidities.h5), so it wouldn't require that much new code or any new dependencies.\r\nI was able to build an altitude map using [open data aggregated by tilezen](https://github.com/tilezen/joerd/blob/master/docs/data-sources.md). My test H5 file is currently `13 mb` using `4320x2160` resolution, `uint16` altitude, and `gzip` compression. We are free to distribute this data, but we do need to do is add [this attribution](https://github.com/tilezen/joerd/blob/master/docs/attribution.md) somewhere in the documentation.\r\nWould you guys be interested in this feature? Should I make a pull request?\r\n\r\nHere is a plot of my sample\r\n![altitude](https://user-images.githubusercontent.com/17040442/182914007-aedbdd53-5f74-4657-b0cb-60158b6aa26d.png)\r\n:\n",
    "input": " import h5py\n \n from pvlib import atmosphere, tools\n \n \n def ineichen(apparent_zenith, airmass_absolute, linke_turbidity,\n     return middles\n \n \ndef _degrees_to_index(degrees, coordinate):\n    \"\"\"Transform input degrees to an output index integer. The Linke\n    turbidity lookup tables have three dimensions, latitude, longitude, and\n    month. Specify a degree value and either 'latitude' or 'longitude' to get\n    the appropriate index number for the first two of these index numbers.\n\n    Parameters\n    ----------\n    degrees : float or int\n        Degrees of either latitude or longitude.\n    coordinate : string\n        Specify whether degrees arg is latitude or longitude. Must be set to\n        either 'latitude' or 'longitude' or an error will be raised.\n\n    Returns\n    -------\n    index : np.int16\n        The latitude or longitude index number to use when looking up values\n        in the Linke turbidity lookup table.\n    \"\"\"\n    # Assign inputmin, inputmax, and outputmax based on degree type.\n    if coordinate == 'latitude':\n        inputmin = 90\n        inputmax = -90\n        outputmax = 2160\n    elif coordinate == 'longitude':\n        inputmin = -180\n        inputmax = 180\n        outputmax = 4320\n    else:\n        raise IndexError(\"coordinate must be 'latitude' or 'longitude'.\")\n\n    inputrange = inputmax - inputmin\n    scale = outputmax/inputrange  # number of indices per degree\n    center = inputmin + 1 / scale / 2  # shift to center of index\n    outputmax -= 1  # shift index to zero indexing\n    index = (degrees - center) * scale\n    err = IndexError('Input, %g, is out of range (%g, %g).' %\n                     (degrees, inputmin, inputmax))\n\n    # If the index is still out of bounds after rounding, raise an error.\n    # 0.500001 is used in comparisons instead of 0.5 to allow for a small\n    # margin of error which can occur when dealing with floating point numbers.\n    if index > outputmax:\n        if index - outputmax <= 0.500001:\n            index = outputmax\n        else:\n            raise err\n    elif index < 0:\n        if -index <= 0.500001:\n            index = 0\n        else:\n            raise err\n    # If the index wasn't set to outputmax or 0, round it and cast it as an\n    # integer so it can be used in integer-based indexing.\n    else:\n        index = int(np.around(index))\n\n    return index\n\n\n def haurwitz(apparent_zenith):\n     '''\n     Determine clear sky GHI using the Haurwitz model.\ndiff --git a/pvlib/location.py b/pvlib/location.py\n \n # Will Holmgren, University of Arizona, 2014-2016.\n \n import datetime\n import warnings\n \n import pandas as pd\n import pytz\n \n from pvlib import solarposition, clearsky, atmosphere, irradiance\n \n class Location:\n     \"\"\"\n                              'one of pyephem, spa, geometric'\n                              .format(method))\n         return result\ndiff --git a/pvlib/tools.py b/pvlib/tools.py\n             'periods, leap days, etc.'\n         )\n         raise NotImplementedError(message)",
    "output": " import h5py\n \n from pvlib import atmosphere, tools\nfrom pvlib.tools import _degrees_to_index\n \n \n def ineichen(apparent_zenith, airmass_absolute, linke_turbidity,\n     return middles\n \n \n def haurwitz(apparent_zenith):\n     '''\n     Determine clear sky GHI using the Haurwitz model.\ndiff --git a/pvlib/location.py b/pvlib/location.py\n \n # Will Holmgren, University of Arizona, 2014-2016.\n \nimport os\n import datetime\n import warnings\n \n import pandas as pd\n import pytz\nimport h5py\n \n from pvlib import solarposition, clearsky, atmosphere, irradiance\nfrom pvlib.tools import _degrees_to_index\n \n class Location:\n     \"\"\"\n                              'one of pyephem, spa, geometric'\n                              .format(method))\n         return result\n\n\ndef lookup_altitude(latitude, longitude):\n    \"\"\"\n    Look up location altitude from low-resolution altitude map\n    supplied with pvlib. The data for this map comes from multiple open data\n    sources with varying resolutions aggregated by Mapzen.\n\n    More details can be found here\n    https://github.com/tilezen/joerd/blob/master/docs/data-sources.md\n\n    Altitudes from this map are a coarse approximation and can have\n    significant errors (100+ meters) introduced by downsampling and\n    source data resolution.\n\n    Parameters\n    ----------\n    latitude : float.\n        Positive is north of the equator.\n        Use decimal degrees notation.\n\n    longitude : float.\n        Positive is east of the prime meridian.\n        Use decimal degrees notation.\n\n    Returns\n    -------\n    altitude : float\n        The altitude of the location in meters.\n\n    Notes\n    -----------\n    Attributions:\n\n    * ArcticDEM terrain data DEM(s) were created from DigitalGlobe, Inc.,\n      imagery and funded under National Science Foundation awards 1043681,\n      1559691, and 1542736;\n    * Australia terrain data \u00a9 Commonwealth of Australia\n      (Geoscience Australia) 2017;\n    * Austria terrain data \u00a9 offene Daten \u00d6sterreichs - Digitales\n      Gel\u00e4ndemodell (DGM) \u00d6sterreich;\n    * Canada terrain data contains information licensed under the Open\n      Government Licence - Canada;\n    * Europe terrain data produced using Copernicus data and information\n      funded by the European Union - EU-DEM layers;\n    * Global ETOPO1 terrain data U.S. National Oceanic and Atmospheric\n      Administration\n    * Mexico terrain data source: INEGI, Continental relief, 2016;\n    * New Zealand terrain data Copyright 2011 Crown copyright (c) Land\n      Information New Zealand and the New Zealand Government\n      (All rights reserved);\n    * Norway terrain data \u00a9 Kartverket;\n    * United Kingdom terrain data \u00a9 Environment Agency copyright and/or\n      database right 2015. All rights reserved;\n    * United States 3DEP (formerly NED) and global GMTED2010 and SRTM\n      terrain data courtesy of the U.S. Geological Survey.\n\n    References\n    ----------\n    .. [1] `Mapzen, Linux foundation project for open data maps\n        <https://www.mapzen.com/>`_\n    .. [2] `Joerd, tool for downloading and processing DEMs, Used by Mapzen\n        <https://github.com/tilezen/joerd/>`_\n    .. [3] `AWS, Open Data Registry Terrain Tiles\n        <https://registry.opendata.aws/terrain-tiles/>`_\n\n    \"\"\"\n\n    pvlib_path = os.path.dirname(os.path.abspath(__file__))\n    filepath = os.path.join(pvlib_path, 'data', 'Altitude.h5')\n\n    latitude_index = _degrees_to_index(latitude, coordinate='latitude')\n    longitude_index = _degrees_to_index(longitude, coordinate='longitude')\n\n    with h5py.File(filepath, 'r') as alt_h5_file:\n        alt = alt_h5_file['Altitude'][latitude_index, longitude_index]\n\n    # 255 is a special value that means nodata. Fallback to 0 if nodata.\n    if alt == 255:\n        return 0\n    # Altitude is encoded in 28 meter steps from -450 meters to 6561 meters\n    # There are 0-254 possible altitudes, with 255 reserved for nodata.\n    alt *= 28\n    alt -= 450\n    return float(alt)\ndiff --git a/pvlib/tools.py b/pvlib/tools.py\n             'periods, leap days, etc.'\n         )\n         raise NotImplementedError(message)\n\n\ndef _degrees_to_index(degrees, coordinate):\n    \"\"\"Transform input degrees to an output index integer.\n    Specify a degree value and either 'latitude' or 'longitude' to get\n    the appropriate index number for these two index numbers.\n    Parameters\n    ----------\n    degrees : float or int\n        Degrees of either latitude or longitude.\n    coordinate : string\n        Specify whether degrees arg is latitude or longitude. Must be set to\n        either 'latitude' or 'longitude' or an error will be raised.\n    Returns\n    -------\n    index : np.int16\n        The latitude or longitude index number to use when looking up values\n        in the Linke turbidity lookup table.\n    \"\"\"\n    # Assign inputmin, inputmax, and outputmax based on degree type.\n    if coordinate == 'latitude':\n        inputmin = 90\n        inputmax = -90\n        outputmax = 2160\n    elif coordinate == 'longitude':\n        inputmin = -180\n        inputmax = 180\n        outputmax = 4320\n    else:\n        raise IndexError(\"coordinate must be 'latitude' or 'longitude'.\")\n\n    inputrange = inputmax - inputmin\n    scale = outputmax/inputrange  # number of indices per degree\n    center = inputmin + 1 / scale / 2  # shift to center of index\n    outputmax -= 1  # shift index to zero indexing\n    index = (degrees - center) * scale\n    err = IndexError('Input, %g, is out of range (%g, %g).' %\n                     (degrees, inputmin, inputmax))\n\n    # If the index is still out of bounds after rounding, raise an error.\n    # 0.500001 is used in comparisons instead of 0.5 to allow for a small\n    # margin of error which can occur when dealing with floating point numbers.\n    if index > outputmax:\n        if index - outputmax <= 0.500001:\n            index = outputmax\n        else:\n            raise err\n    elif index < 0:\n        if -index <= 0.500001:\n            index = 0\n        else:\n            raise err\n    # If the index wasn't set to outputmax or 0, round it and cast it as an\n    # integer so it can be used in integer-based indexing.\n    else:\n        index = int(np.around(index))\n\n    return index"
  },
  {
    "instruction": "The Perez diffuse model should not be forcing the horizon coefficient up to zero\nThe perez model in irradiance.py forces F2, and thus the horizon component of diffuse, to be non-negative.  This restriction should not happen.  F2 and the horizon coefficient should be allowed to be negative and to reduce overall diffuse.\r\n\r\nAccording to the original paper at https://www.osti.gov/servlets/purl/7024029\r\nSection III.2 states this explicitly for the horizon component:\r\n\"(2) The horizon brightening coefficient, F2, is negative for overcast and low E occurrences -- indicative of brightening of the zenithal region of the sky for these conditions. This becomes positive past intermediate conditions and increases substantially with clearness.\"\r\n\r\nWe observed a higher than expected POAI, coming from poa diffuse, on cloudy days at certain sites.\r\nExpected:\r\nHorizon (burgundy) can be less than zero and sky diffuse (green) is less than isotropic (blue)\r\n![image](https://user-images.githubusercontent.com/81724637/119172295-9ebc7900-ba1a-11eb-8e1a-3a170e1f995a.png)\r\n\r\nObserved from PVLib:\r\nHorizon is prevented from being negative and sky diffuse ends up higher than isotropic.\r\n![image](https://user-images.githubusercontent.com/81724637/119172410-c4498280-ba1a-11eb-8e91-540db0ddc609.png)\r\n\r\nRepro'd on PVLib 0.8.1\r\n\r\nSee added test case in the PR for this repro case.\r\n\n",
    "input": "     F1 = np.maximum(F1, 0)\n \n     F2 = (F2c[ebin, 0] + F2c[ebin, 1] * delta + F2c[ebin, 2] * z)\n    F2 = np.maximum(F2, 0)\n \n     A = aoi_projection(surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth)",
    "output": "     F1 = np.maximum(F1, 0)\n \n     F2 = (F2c[ebin, 0] + F2c[ebin, 1] * delta + F2c[ebin, 2] * z)\n \n     A = aoi_projection(surface_tilt, surface_azimuth,\n                        solar_zenith, solar_azimuth)"
  },
  {
    "instruction": "Update get_cams protocol to https\nAccording to an email sent out by Transvalor on May 12th 2022, the SoDa websites and services will switch from using \"HTTP\" to only \"HTTPS\". \r\n\r\nThe existing HTTP endpoints will redirect to the correct HTTPS sites, hence without any changes the [`get_cams`](https://pvlib-python.readthedocs.io/en/stable/reference/generated/pvlib.iotools.get_cams.html) function will continue to work correctly (as the request package handles redirects automatically). However, for good practice and to avoid unnecessary redirects, updating the existing URLs and endpoints to HTTPS is surely a good idea:\r\nhttps://github.com/pvlib/pvlib-python/blob/a0812b12584cfd5e662fa5aeb8972090763a671f/pvlib/iotools/sodapro.py#L188\r\n\r\n<br>\r\nFor reference, here's a screen-shot of Transvalor email:\r\n\r\n![image](https://user-images.githubusercontent.com/39184289/168595497-095c17c1-3fec-43c9-b6fd-49c928b51d78.png)\r\n\r\nFYI: this is a good first issue to tackle to get familiar with contribution to pvlib :) \n",
    "input": "     References\n     ----------\n     .. [1] `CAMS Radiation Service Info\n       <http://www.soda-pro.com/web-services/radiation/cams-radiation-service/info>`_\n     .. [2] `CAMS McClear Service Info\n       <http://www.soda-pro.com/web-services/radiation/cams-mcclear/info>`_\n     .. [3] `CAMS McClear Automatic Access\n       <http://www.soda-pro.com/help/cams-services/cams-mcclear-service/automatic-access>`_\n     \"\"\"\n     try:\n         time_step_str = TIME_STEPS_MAP[time_step]\n     email = email.replace('@', '%2540')  # Format email address\n     identifier = 'get_{}'.format(identifier.lower())  # Format identifier str\n \n    base_url = f\"http://{server}/service/wps\"\n \n     data_inputs_dict = {\n         'latitude': latitude,\n     References\n     ----------\n     .. [1] `CAMS Radiation Service Info\n       <http://www.soda-pro.com/web-services/radiation/cams-radiation-service/info>`_\n     .. [2] `CAMS McClear Service Info\n       <http://www.soda-pro.com/web-services/radiation/cams-mcclear/info>`_\n     \"\"\"\n     metadata = {}\n     # Initial lines starting with # contain metadata\n     References\n     ----------\n     .. [1] `CAMS Radiation Service Info\n       <http://www.soda-pro.com/web-services/radiation/cams-radiation-service/info>`_\n     .. [2] `CAMS McClear Service Info\n       <http://www.soda-pro.com/web-services/radiation/cams-mcclear/info>`_\n     \"\"\"\n     with open(str(filename), 'r') as fbuf:\n         content = parse_cams(fbuf, integrated, label, map_variables)",
    "output": "     References\n     ----------\n     .. [1] `CAMS Radiation Service Info\n       <https://www.soda-pro.com/web-services/radiation/cams-radiation-service/info>`_\n     .. [2] `CAMS McClear Service Info\n       <https://www.soda-pro.com/web-services/radiation/cams-mcclear/info>`_\n     .. [3] `CAMS McClear Automatic Access\n       <https://www.soda-pro.com/help/cams-services/cams-mcclear-service/automatic-access>`_\n     \"\"\"\n     try:\n         time_step_str = TIME_STEPS_MAP[time_step]\n     email = email.replace('@', '%2540')  # Format email address\n     identifier = 'get_{}'.format(identifier.lower())  # Format identifier str\n \n    base_url = f\"https://{server}/service/wps\"\n \n     data_inputs_dict = {\n         'latitude': latitude,\n     References\n     ----------\n     .. [1] `CAMS Radiation Service Info\n       <https://www.soda-pro.com/web-services/radiation/cams-radiation-service/info>`_\n     .. [2] `CAMS McClear Service Info\n       <https://www.soda-pro.com/web-services/radiation/cams-mcclear/info>`_\n     \"\"\"\n     metadata = {}\n     # Initial lines starting with # contain metadata\n     References\n     ----------\n     .. [1] `CAMS Radiation Service Info\n       <https://www.soda-pro.com/web-services/radiation/cams-radiation-service/info>`_\n     .. [2] `CAMS McClear Service Info\n       <https://www.soda-pro.com/web-services/radiation/cams-mcclear/info>`_\n     \"\"\"\n     with open(str(filename), 'r') as fbuf:\n         content = parse_cams(fbuf, integrated, label, map_variables)"
  },
  {
    "instruction": "change eta_m to module_efficiency\n`temperature.noct_sam` uses `eta_m_ref` to describe the module efficiency at reference conditions and `temperature.pvsyst_cell` uses `eta_m` to describe the module efficiency generically.\r\n\r\nJust calling both of these `module_efficiency` would make the function signatures easily understandable by many more people. I'd be ok with `module_efficiency_ref` but I don't think that precision is very important.\r\n\r\nI skimmed [pvterms](https://duramat.github.io/pv-terms/) and didn't see a suggestion for this quantity.\r\n\r\n`temperature.noct_sam` has not yet been released and it's just a positional argument, so changing the name is trivial. `temperature.pvsyst_cell` would need a deprecation cycle.\r\n\r\nOriginally discussed in https://github.com/pvlib/pvlib-python/pull/1177#discussion_r589081257\r\n\r\nAssignment of milestone indicates that we will act on this or close it forever before 0.9 is released.\r\n\n",
    "input": "             return self.faiman_temp\n         elif {'noct_installed'} <= params:\n             return self.fuentes_temp\n        elif {'noct', 'eta_m_ref'} <= params:\n             return self.noct_sam_temp\n         else:\n             raise ValueError(f'could not infer temperature model from '\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         wind_speed = self._validate_per_array(wind_speed, system_wide=True)\n \n         def build_celltemp_kwargs(array):\n            return {**_build_kwargs(['eta_m', 'alpha_absorption'],\n                                     array.module_parameters),\n                     **_build_kwargs(['u_c', 'u_v'],\n                                     array.temperature_model_parameters)}\n                 # bundled with kwargs for simplicity\n                 temp_model_kwargs['noct'] = \\\n                     array.temperature_model_parameters['noct']\n                temp_model_kwargs['eta_m_ref'] = \\\n                    array.temperature_model_parameters['eta_m_ref']\n             except KeyError:\n                msg = ('Parameters noct and eta_m_ref are required.'\n                        ' Found {} in temperature_model_parameters.'\n                        .format(array.temperature_model_parameters))\n                 raise KeyError(msg)\ndiff --git a/pvlib/temperature.py b/pvlib/temperature.py\n import numpy as np\n import pandas as pd\n from pvlib.tools import sind\n \n TEMPERATURE_MODEL_PARAMETERS = {\n     'sapm': {\n \n \n def pvsyst_cell(poa_global, temp_air, wind_speed=1.0, u_c=29.0, u_v=0.0,\n                eta_m=0.1, alpha_absorption=0.9):\n     r\"\"\"\n     Calculate cell temperature using an empirical heat loss factor model\n     as implemented in PVsyst.\n     u_v : float, default 0.0\n         Combined heat loss factor influenced by wind. Parameter :math:`U_{v}`\n         in :eq:`pvsyst`.\n        :math:`\\left[ \\frac{\\text{W}/\\text{m}^2}{\\text{C}\\ \\left( \\text{m/s} \\right)} \\right]`\n \n    eta_m : numeric, default 0.1\n        Module external efficiency as a fraction, i.e.,\n        :math:`DC\\ power / (POA\\ irradiance \\times module\\ area)`.\n        Parameter :math:`\\eta_{m}` in :eq:`pvsyst`.\n \n     alpha_absorption : numeric, default 0.9\n         Absorption coefficient. Parameter :math:`\\alpha` in :eq:`pvsyst`.\n     37.93103448275862\n     \"\"\"\n \n     total_loss_factor = u_c + u_v * wind_speed\n    heat_input = poa_global * alpha_absorption * (1 - eta_m)\n     temp_difference = heat_input / total_loss_factor\n     return temp_air + temp_difference\n \n                         [0., 18., 11., 6., 2., 0.])\n \n \ndef noct_sam(poa_global, temp_air, wind_speed, noct, eta_m_ref,\n              effective_irradiance=None, transmittance_absorptance=0.9,\n              array_height=1, mount_standoff=4):\n     r'''\n         Nominal operating cell temperature [C], determined at conditions of\n         800 W/m^2 irradiance, 20 C ambient air temperature and 1 m/s wind.\n \n    eta_m_ref : float\n         Module external efficiency [unitless] at reference conditions of\n        1000 W/m^2 and 20C. Calculate as\n         :math:`\\eta_{m} = \\frac{V_{mp} I_{mp}}{A \\times 1000 W/m^2}`\n         where A is module area [m^2].\n \n     # [1] Eq. 10.37 isn't clear on exactly what \"G\" is. SAM SSC code uses\n     # poa_global where G appears\n     cell_temp_init = poa_global / 800. * (noct_adj - 20.)\n    heat_loss = 1 - eta_m_ref / tau_alpha\n     wind_loss = 9.5 / (5.7 + 3.8 * wind_adj)\n     return temp_air + cell_temp_init * heat_loss * wind_loss",
    "output": "             return self.faiman_temp\n         elif {'noct_installed'} <= params:\n             return self.fuentes_temp\n        elif {'noct', 'module_efficiency'} <= params:\n             return self.noct_sam_temp\n         else:\n             raise ValueError(f'could not infer temperature model from '\ndiff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py\n         wind_speed = self._validate_per_array(wind_speed, system_wide=True)\n \n         def build_celltemp_kwargs(array):\n            # TODO remove 'eta_m' after deprecation of this parameter\n            return {**_build_kwargs(['eta_m', 'module_efficiency',\n                                     'alpha_absorption'],\n                                     array.module_parameters),\n                     **_build_kwargs(['u_c', 'u_v'],\n                                     array.temperature_model_parameters)}\n                 # bundled with kwargs for simplicity\n                 temp_model_kwargs['noct'] = \\\n                     array.temperature_model_parameters['noct']\n                temp_model_kwargs['module_efficiency'] = \\\n                    array.temperature_model_parameters['module_efficiency']\n             except KeyError:\n                msg = ('Parameters noct and module_efficiency are required.'\n                        ' Found {} in temperature_model_parameters.'\n                        .format(array.temperature_model_parameters))\n                 raise KeyError(msg)\ndiff --git a/pvlib/temperature.py b/pvlib/temperature.py\n import numpy as np\n import pandas as pd\n from pvlib.tools import sind\nfrom pvlib._deprecation import warn_deprecated\n \n TEMPERATURE_MODEL_PARAMETERS = {\n     'sapm': {\n \n \n def pvsyst_cell(poa_global, temp_air, wind_speed=1.0, u_c=29.0, u_v=0.0,\n                eta_m=None, module_efficiency=0.1, alpha_absorption=0.9):\n     r\"\"\"\n     Calculate cell temperature using an empirical heat loss factor model\n     as implemented in PVsyst.\n     u_v : float, default 0.0\n         Combined heat loss factor influenced by wind. Parameter :math:`U_{v}`\n         in :eq:`pvsyst`.\n        :math:`\\left[ \\frac{\\text{W}/\\text{m}^2}{\\text{C}\\ \\left( \\text{m/s} \\right)} \\right]`  # noQA: E501\n\n    eta_m : numeric, default None (deprecated, use module_efficiency instead)\n \n    module_efficiency : numeric, default 0.1\n        Module external efficiency as a fraction. Parameter :math:`\\eta_{m}`\n        in :eq:`pvsyst`. Calculate as\n        :math:`\\eta_{m} = DC\\ power / (POA\\ irradiance \\times module\\ area)`.\n \n     alpha_absorption : numeric, default 0.9\n         Absorption coefficient. Parameter :math:`\\alpha` in :eq:`pvsyst`.\n     37.93103448275862\n     \"\"\"\n \n    if eta_m:\n        warn_deprecated(\n            since='v0.9', message='eta_m overwriting module_efficiency',\n            name='eta_m', alternative='module_efficiency', removal='v0.10')\n        module_efficiency = eta_m\n     total_loss_factor = u_c + u_v * wind_speed\n    heat_input = poa_global * alpha_absorption * (1 - module_efficiency)\n     temp_difference = heat_input / total_loss_factor\n     return temp_air + temp_difference\n \n                         [0., 18., 11., 6., 2., 0.])\n \n \ndef noct_sam(poa_global, temp_air, wind_speed, noct, module_efficiency,\n              effective_irradiance=None, transmittance_absorptance=0.9,\n              array_height=1, mount_standoff=4):\n     r'''\n         Nominal operating cell temperature [C], determined at conditions of\n         800 W/m^2 irradiance, 20 C ambient air temperature and 1 m/s wind.\n \n    module_efficiency : float\n         Module external efficiency [unitless] at reference conditions of\n        1000 W/m^2 and 20C. Denoted as :math:`eta_{m}` in [1]_. Calculate as\n         :math:`\\eta_{m} = \\frac{V_{mp} I_{mp}}{A \\times 1000 W/m^2}`\n         where A is module area [m^2].\n \n     # [1] Eq. 10.37 isn't clear on exactly what \"G\" is. SAM SSC code uses\n     # poa_global where G appears\n     cell_temp_init = poa_global / 800. * (noct_adj - 20.)\n    heat_loss = 1 - module_efficiency / tau_alpha\n     wind_loss = 9.5 / (5.7 + 3.8 * wind_adj)\n     return temp_air + cell_temp_init * heat_loss * wind_loss"
  },
  {
    "instruction": "pvlib.irradiance.reindl() model generates NaNs when GHI = 0\n**Describe the bug**\r\nThe reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \r\n\r\n**Expected behavior**\r\nThe reindl function should result in zero sky diffuse when GHI is zero.\r\n\r\n\npvlib.irradiance.reindl() model generates NaNs when GHI = 0\n**Describe the bug**\r\nThe reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \r\n\r\n**Expected behavior**\r\nThe reindl function should result in zero sky diffuse when GHI is zero.\r\n\r\n\n",
    "input": "     # these are the () and [] sub-terms of the second term of eqn 8\n     term1 = 1 - AI\n     term2 = 0.5 * (1 + tools.cosd(surface_tilt))\n    term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)\n\n     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)\n     sky_diffuse = np.maximum(sky_diffuse, 0)\n ",
    "output": "     # these are the () and [] sub-terms of the second term of eqn 8\n     term1 = 1 - AI\n     term2 = 0.5 * (1 + tools.cosd(surface_tilt))\n    with np.errstate(invalid='ignore', divide='ignore'):\n        hb_to_ghi = np.where(ghi == 0, 0, np.divide(HB, ghi))\n    term3 = 1 + np.sqrt(hb_to_ghi) * (tools.sind(0.5 * surface_tilt)**3)\n     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)\n     sky_diffuse = np.maximum(sky_diffuse, 0)\n "
  },
  {
    "instruction": "Consider creating a ``UninferableType`` or ``_Uninferable`` class\nI opened https://github.com/microsoft/pyright/issues/3641 as I wondered why `pyright` didn't recognise how we type `Uninferable`. Normally they are a little bit more up to date than `mypy` so I wondered if this was intentional.\r\n\r\nTurns out it is. According to them, the way we currently handle the typing of `Uninferable` is incorrect and should ideally be refactored.\r\nAs we're stille early days into the typing of `astroid` I think there is still chance to do this.\r\n\r\nTheir suggestion is to create a private class (`_Uninferable`) which `Uninferable` can then instantiate. One of the issues with this is that we tend to require `Uninferable` as a type in `pylint` as well and so we would need to import that private class as well.\r\nWe could also create a public class, perhaps suffixed with `Type`, which we document as only being useful for typing.\r\n\r\nLet me know if you guys thinks this is something we should do and what approach is best.\r\n\r\n/CC @cdce8p As you're likely interested in this.\n",
    "input": " from astroid.bases import Instance\n from astroid.context import CallContext, InferenceContext\n from astroid.exceptions import InferenceError, NoDefault\nfrom astroid.util import Uninferable\n \n \n class CallSite:\n         self._unpacked_kwargs = self._unpack_keywords(keywords, context=context)\n \n         self.positional_arguments = [\n            arg for arg in self._unpacked_args if arg is not Uninferable\n         ]\n         self.keyword_arguments = {\n             key: value\n             for key, value in self._unpacked_kwargs.items()\n            if value is not Uninferable\n         }\n \n     @classmethod\n                 except StopIteration:\n                     continue\n \n                if inferred is Uninferable:\n                     values.append(Uninferable)\n                     continue\n                 if not hasattr(inferred, \"elts\"):\ndiff --git a/astroid/bases.py b/astroid/bases.py\n     NameInferenceError,\n )\n from astroid.typing import InferBinaryOp, InferenceErrorInfo, InferenceResult\nfrom astroid.util import Uninferable, lazy_descriptor, lazy_import\n \n if sys.version_info >= (3, 8):\n     from typing import Literal\n     if PROPERTIES.intersection(decoratornames):\n         return True\n     stripped = {\n        name.split(\".\")[-1] for name in decoratornames if name is not Uninferable\n     }\n     if any(name in stripped for name in POSSIBLE_PROPERTIES):\n         return True\n         return False\n     for decorator in meth.decorators.nodes or ():\n         inferred = helpers.safe_infer(decorator, context=context)\n        if inferred is None or inferred is Uninferable:\n             continue\n         if inferred.__class__.__name__ == \"ClassDef\":\n             for base_class in inferred.bases:\n \n \n def _infer_stmts(\n    stmts: Sequence[nodes.NodeNG | type[Uninferable] | Instance],\n     context: InferenceContext | None,\n     frame: nodes.NodeNG | Instance | None = None,\n ) -> collections.abc.Generator[InferenceResult, None, None]:\n         context = InferenceContext()\n \n     for stmt in stmts:\n        if stmt is Uninferable:\n             yield stmt\n             inferred = True\n             continue\n             for constraint_stmt, potential_constraints in constraints.items():\n                 if not constraint_stmt.parent_of(stmt):\n                     stmt_constraints.update(potential_constraints)\n            # Mypy doesn't recognize that 'stmt' can't be Uninferable\n            for inf in stmt.infer(context=context):  # type: ignore[union-attr]\n                 if all(constraint.satisfied_by(inf) for constraint in stmt_constraints):\n                     yield inf\n                     inferred = True\n         try:\n             context.callcontext = CallContext(args=[], callee=meth)\n             for value in meth.infer_call_result(instance, context=context):\n                if value is Uninferable:\n                     return value\n                 try:\n                     inferred = next(value.infer(context=context))\n \n         # Otherwise we infer the call to the __call__ dunder normally\n         for node in self._proxied.igetattr(\"__call__\", context):\n            if node is Uninferable or not node.callable():\n                 continue\n             for res in node.infer_call_result(caller, context):\n                 inferred = True\n         caller: nodes.Call,\n         context: InferenceContext,\n     ) -> collections.abc.Generator[\n        nodes.Const | Instance | type[Uninferable], None, None\n     ]:\n         if not caller.args:\n             return\n \n         node_context = context.extra_context.get(caller.args[0])\n         for inferred in caller.args[0].infer(context=node_context):\n            if inferred is Uninferable:\n                 yield inferred\n             if isinstance(inferred, nodes.ClassDef):\n                 yield Instance(inferred)\ndiff --git a/astroid/brain/brain_builtin_inference.py b/astroid/brain/brain_builtin_inference.py\n             inferred = next(arg.infer(context=context))\n         except (InferenceError, StopIteration) as exc:\n             raise UseInferenceDefault from exc\n        if inferred is util.Uninferable:\n             raise UseInferenceDefault\n         transformed = transform(inferred)\n    if not transformed or transformed is util.Uninferable:\n         raise UseInferenceDefault\n     return transformed\n \n         except (InferenceError, StopIteration) as exc:\n             raise UseInferenceDefault from exc\n \n    if mro_pointer is util.Uninferable or mro_type is util.Uninferable:\n         # No way we could understand this.\n         raise UseInferenceDefault\n \n     except (InferenceError, StopIteration) as exc:\n         raise UseInferenceDefault from exc\n \n    if obj is util.Uninferable or attr is util.Uninferable:\n         # If one of the arguments is something we can't infer,\n         # then also make the result of the getattr call something\n         # which is unknown.\n     \"\"\"\n     obj, attr = _infer_getattr_args(node, context)\n     if (\n        obj is util.Uninferable\n        or attr is util.Uninferable\n         or not hasattr(obj, \"igetattr\")\n     ):\n         return util.Uninferable\n     try:\n         obj, attr = _infer_getattr_args(node, context)\n         if (\n            obj is util.Uninferable\n            or attr is util.Uninferable\n             or not hasattr(obj, \"getattr\")\n         ):\n             return util.Uninferable\n         inferred = next(argument.infer(context=context))\n     except (InferenceError, StopIteration):\n         return util.Uninferable\n    if inferred is util.Uninferable:\n         return util.Uninferable\n     return nodes.Const(inferred.callable())\n \n         inferred = next(argument.infer(context=context))\n     except (InferenceError, StopIteration):\n         return util.Uninferable\n    if inferred is util.Uninferable:\n         return util.Uninferable\n \n     bool_value = inferred.bool_value(context=context)\n    if bool_value is util.Uninferable:\n         return util.Uninferable\n     return nodes.Const(bool_value)\n \n     infer_func = partial(helpers.safe_infer, context=context)\n     args = [infer_func(arg) for arg in args]\n     for arg in args:\n        if not arg or arg is util.Uninferable:\n             raise UseInferenceDefault\n         if not isinstance(arg, nodes.Const):\n             raise UseInferenceDefault\n         raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n     except MroError as exc:\n         raise UseInferenceDefault from exc\n    if isinstance_bool is util.Uninferable:\n         raise UseInferenceDefault\n     return nodes.Const(isinstance_bool)\n \n         except (InferenceError, StopIteration) as exc:\n             raise UseInferenceDefault(str(exc)) from exc\n \n        if first_value is util.Uninferable:\n             raise UseInferenceDefault\n \n         if isinstance(first_value, nodes.Const) and isinstance(\n \n def _infer_str_format_call(\n     node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.Const | type[util.Uninferable]]:\n     \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n     call = arguments.CallSite.from_call(node, context=context)\n     if isinstance(node.func.expr, nodes.Name):\ndiff --git a/astroid/brain/brain_dataclasses.py b/astroid/brain/brain_dataclasses.py\n from astroid.inference_tip import inference_tip\n from astroid.manager import AstroidManager\n from astroid.typing import InferenceResult\nfrom astroid.util import Uninferable\n \n if sys.version_info >= (3, 8):\n     from typing import Literal\n     except (InferenceError, StopIteration):\n         inferred = Uninferable\n \n    if inferred is Uninferable:\n         if isinstance(node, nodes.Name):\n             return node.name in decorator_names\n         if isinstance(node, nodes.Attribute):\n \n def _infer_instance_from_annotation(\n     node: nodes.NodeNG, ctx: context.InferenceContext | None = None\n) -> Iterator[type[Uninferable] | bases.Instance]:\n     \"\"\"Infer an instance corresponding to the type annotation represented by node.\n \n     Currently has limited support for the typing module.\ndiff --git a/astroid/brain/brain_functools.py b/astroid/brain/brain_functools.py\n from astroid.manager import AstroidManager\n from astroid.nodes.node_classes import AssignName, Attribute, Call, Name\n from astroid.nodes.scoped_nodes import FunctionDef\nfrom astroid.util import Uninferable\n \n LRU_CACHE = \"functools.lru_cache\"\n \n         inferred_wrapped_function = next(partial_function.infer(context=context))\n     except (InferenceError, StopIteration) as exc:\n         raise UseInferenceDefault from exc\n    if inferred_wrapped_function is Uninferable:\n         raise UseInferenceDefault(\"Cannot infer the wrapped function\")\n     if not isinstance(inferred_wrapped_function, FunctionDef):\n         raise UseInferenceDefault(\"The wrapped function is not a function\")\ndiff --git a/astroid/brain/brain_namedtuple_enum.py b/astroid/brain/brain_namedtuple_enum.py\n \n \n def _infer_first(node, context):\n    if node is util.Uninferable:\n         raise UseInferenceDefault\n     try:\n         value = next(node.infer(context=context))\n     except StopIteration as exc:\n         raise InferenceError from exc\n    if value is util.Uninferable:\n         raise UseInferenceDefault()\n     return value\n \ndiff --git a/astroid/brain/brain_typing.py b/astroid/brain/brain_typing.py\n     Tuple,\n )\n from astroid.nodes.scoped_nodes import ClassDef, FunctionDef\nfrom astroid.util import Uninferable\n \n if sys.version_info >= (3, 8):\n     from typing import Final\n         col_offset=assign_name.col_offset,\n         parent=node.parent,\n     )\n    if res != Uninferable and isinstance(res, ClassDef):\n         # Only add `res` as base if it's a `ClassDef`\n         # This isn't the case for `typing.Pattern` and `typing.Match`\n         class_def.postinit(bases=[res], body=[], decorators=None)\ndiff --git a/astroid/builder.py b/astroid/builder.py\n         try:\n             frame = node.frame(future=True)\n             for inferred in node.expr.infer():\n                if inferred is util.Uninferable:\n                     continue\n                 try:\n                     # pylint: disable=unidiomatic-typecheck # We want a narrow check on the\n                         # Const, Tuple or other containers that inherit from\n                         # `Instance`\n                         continue\n                    elif (\n                        isinstance(inferred, bases.Proxy)\n                        or inferred is util.Uninferable\n                    ):\n                         continue\n                     elif inferred.is_function:\n                         iattrs = inferred.instance_attrs\ndiff --git a/astroid/constraint.py b/astroid/constraint.py\n     def satisfied_by(self, inferred: InferenceResult) -> bool:\n         \"\"\"Return True if this constraint is satisfied by the given inferred value.\"\"\"\n         # Assume true if uninferable\n        if inferred is util.Uninferable:\n             return True\n \n         # Return the XOR of self.negate and matches(inferred, self.CONST_NONE)\ndiff --git a/astroid/helpers.py b/astroid/helpers.py\n             yield _build_proxy_class(\"module\", builtins)\n         elif isinstance(inferred, nodes.Unknown):\n             raise InferenceError\n        elif inferred is util.Uninferable:\n             yield inferred\n         elif isinstance(inferred, (bases.Proxy, nodes.Slice)):\n             yield inferred._proxied\n     else:\n         class_seq = class_or_seq\n \n    if obj_type is util.Uninferable:\n         return util.Uninferable\n \n     # Instances are not types\n     # issubclass(type, (object, 1)) evaluates to true\n     # issubclass(object, (1, type)) raises TypeError\n     for klass in class_seq:\n        if klass is util.Uninferable:\n             raise AstroidTypeError(\"arg 2 must be a type or tuple of types\")\n \n         for obj_subclass in obj_type.mro():\n     :raises AstroidTypeError: if the given ``classes_or_seq`` are not types\n     \"\"\"\n     obj_type = object_type(node, context)\n    if obj_type is util.Uninferable:\n         return util.Uninferable\n     return _object_type_is_subclass(obj_type, class_or_seq, context=context)\n \n         )\n         raise InferenceError(message)\n \n    if inferred_node is None or inferred_node is util.Uninferable:\n         raise InferenceError(node=node)\n     if isinstance(inferred_node, nodes.Const) and isinstance(\n         inferred_node.value, (bytes, str)\n         ) from e\n \n     inferred = len_call.infer_call_result(node, context)\n    if inferred is util.Uninferable:\n         raise InferenceError(node=node, context=context)\n     result_of_len = next(inferred, None)\n     if (\ndiff --git a/astroid/inference.py b/astroid/inference.py\n         callcontext.extra_context = _populate_context_lookup(self, context.clone())\n \n     for callee in self.func.infer(context):\n        if callee is util.Uninferable:\n             yield callee\n             continue\n         try:\n ) -> Generator[InferenceResult, None, InferenceErrorInfo]:\n     \"\"\"Infer an Attribute node by using getattr on the associated object.\"\"\"\n     for owner in self.expr.infer(context):\n        if owner is util.Uninferable:\n             yield owner\n             continue\n \n \n     found_one = False\n     for value in self.value.infer(context):\n        if value is util.Uninferable:\n             yield util.Uninferable\n             return None\n         for index in self.slice.infer(context):\n            if index is util.Uninferable:\n                 yield util.Uninferable\n                 return None\n \n \n             # Prevent inferring if the inferred subscript\n             # is the same as the original subscripted object.\n            if self is assigned or assigned is util.Uninferable:\n                 yield util.Uninferable\n                 return None\n             yield from assigned.infer(context)\n         return None\n \n     for pair in itertools.product(*inferred_values):\n        if any(item is util.Uninferable for item in pair):\n             # Can't infer the final result, just yield Uninferable.\n             yield util.Uninferable\n             continue\n \n         bool_values = [item.bool_value() for item in pair]\n        if any(item is util.Uninferable for item in bool_values):\n             # Can't infer the final result, just yield Uninferable.\n             yield util.Uninferable\n             continue\n                 # value and negate its result, unless it is\n                 # Uninferable, which will be returned as is.\n                 bool_value = operand.bool_value()\n                if bool_value is not util.Uninferable:\n                     yield nodes.const_factory(not bool_value)\n                 else:\n                     yield util.Uninferable\n \n                     meth = methods[0]\n                     inferred = next(meth.infer(context=context), None)\n                    if inferred is util.Uninferable or not inferred.callable():\n                         continue\n \n                     context = copy_context(context)\n \n def _infer_old_style_string_formatting(\n     instance: nodes.Const, other: nodes.NodeNG, context: InferenceContext\n) -> tuple[type[util.Uninferable] | nodes.Const]:\n     \"\"\"Infer the result of '\"string\" % ...'.\n \n     TODO: Instead of returning Uninferable we should rely\n         inferred = next(method.infer(context=context))\n     except StopIteration as e:\n         raise InferenceError(node=method, context=context) from e\n    if inferred is util.Uninferable:\n         raise InferenceError\n     if not isinstance(\n         instance, (nodes.Const, nodes.Tuple, nodes.List, nodes.ClassDef, bases.Instance)\n             yield util.Uninferable\n             return\n         else:\n            if any(result is util.Uninferable for result in results):\n                 yield util.Uninferable\n                 return\n \n     lhs_iter = left.infer(context=lhs_context)\n     rhs_iter = right.infer(context=rhs_context)\n     for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any(value is util.Uninferable for value in (rhs, lhs)):\n             # Don't know how to process this.\n             yield util.Uninferable\n             return\n \n def _do_compare(\n     left_iter: Iterable[nodes.NodeNG], op: str, right_iter: Iterable[nodes.NodeNG]\n) -> bool | type[util.Uninferable]:\n     \"\"\"\n     If all possible combinations are either True or False, return that:\n     >>> _do_compare([1, 2], '<=', [3, 4])\n     op_func = COMPARE_OPS[op]\n \n     for left, right in itertools.product(left_iter, right_iter):\n        if left is util.Uninferable or right is util.Uninferable:\n             return util.Uninferable\n \n         try:\n \n def _infer_compare(\n     self: nodes.Compare, context: InferenceContext | None = None, **kwargs: Any\n) -> Generator[nodes.Const | type[util.Uninferable], None, None]:\n     \"\"\"Chained comparison inference logic.\"\"\"\n    retval: bool | type[util.Uninferable] = True\n \n     ops = self.ops\n     left_node = self.left\n     lhs_iter = self.target.infer_lhs(context=context)\n     rhs_iter = self.value.infer(context=rhs_context)\n     for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any(value is util.Uninferable for value in (rhs, lhs)):\n             # Don't know how to process this.\n             yield util.Uninferable\n             return\n     except (InferenceError, StopIteration):\n         both_branches = True\n     else:\n        if test is not util.Uninferable:\n             if test.bool_value():\n                 yield from self.body.infer(context=lhs_context)\n             else:\ndiff --git a/astroid/inference_tip.py b/astroid/inference_tip.py\n \n import wrapt\n \nfrom astroid import bases, util\n from astroid.exceptions import InferenceOverwriteError, UseInferenceDefault\n from astroid.nodes import NodeNG\nfrom astroid.typing import InferFn\n \nInferOptions = typing.Union[\n    NodeNG, bases.Instance, bases.UnboundMethod, typing.Type[util.Uninferable]\n]\n\n_cache: dict[tuple[InferFn, NodeNG], list[InferOptions] | None] = {}\n \n \n def clear_inference_tip_cache() -> None:\n @wrapt.decorator\n def _inference_tip_cached(\n     func: InferFn, instance: None, args: typing.Any, kwargs: typing.Any\n) -> Iterator[InferOptions]:\n     \"\"\"Cache decorator used for inference tips.\"\"\"\n     node = args[0]\n     try:\ndiff --git a/astroid/interpreter/objectmodel.py b/astroid/interpreter/objectmodel.py\n                 except StopIteration as e:\n                     raise InferenceError(context=context, node=caller.args[0]) from e\n \n                if cls is astroid.Uninferable:\n                     raise InferenceError(\n                         \"Invalid class inferred\", target=self, context=context\n                     )\ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n         return {\"node\": stmt, \"context\": context}\n     # else, infer recursively, except Uninferable object that should be returned as is\n     for inferred in stmt.infer(context):\n        if inferred is util.Uninferable:\n             yield inferred\n         else:\n             yield from unpack_infer(inferred, context)\n                     continue\n \n             for inferredkey in key.infer(context):\n                if inferredkey is util.Uninferable:\n                     continue\n                 if isinstance(inferredkey, Const) and isinstance(index, Const):\n                     if inferredkey.value == index.value:\n     _astroid_fields = (\"original\",)\n     _other_fields = (\"value\",)\n \n    def __init__(\n        self, original: NodeNG, value: NodeNG | type[util.Uninferable]\n    ) -> None:\n         self.original: NodeNG = original\n         \"\"\"The original node that has already been evaluated\"\"\"\n \n        self.value: NodeNG | type[util.Uninferable] = value\n         \"\"\"The inferred value\"\"\"\n \n         super().__init__(\n \n     def _infer(\n         self, context: InferenceContext | None = None, **kwargs: Any\n    ) -> Generator[NodeNG | type[util.Uninferable], None, None]:\n         yield self.value\n \n \ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n                 new_class.hide = True\n                 new_class.parent = self\n                 new_class.postinit(\n                    bases=[base for base in class_bases if base != util.Uninferable],\n                     body=[],\n                     decorators=[],\n                     metaclass=metaclass,\n                 if isinstance(baseobj, bases.Instance):\n                     # not abstract\n                     return False\n                if baseobj is util.Uninferable:\n                    continue\n                 if baseobj is klass:\n                     continue\n                 if not isinstance(baseobj, ClassDef):\n                 return next(\n                     node\n                     for node in self._metaclass.infer(context=context)\n                    if node is not util.Uninferable\n                 )\n             except (InferenceError, StopIteration):\n                 return None\n                 values = [item[0] for item in slots.items]\n             else:\n                 values = slots.itered()\n            if values is util.Uninferable:\n                 continue\n             if not values:\n                 # Stop the iteration, because the class\n             for elt in values:\n                 try:\n                     for inferred in elt.infer():\n                        if inferred is util.Uninferable:\n                            continue\n                         if not isinstance(\n                             inferred, node_classes.Const\n                         ) or not isinstance(inferred.value, str):\ndiff --git a/astroid/protocols.py b/astroid/protocols.py\n     other: InferenceResult,\n     context: InferenceContext,\n     _: SuccessfulInferenceResult,\n) -> Generator[ConstFactoryResult | type[util.Uninferable], None, None]:\n     not_implemented = nodes.Const(NotImplemented)\n     if isinstance(other, nodes.Const):\n         if (\n     filtered_elts = (\n         helpers.safe_infer(elt, context) or util.Uninferable\n         for elt in self.elts\n        if elt is not util.Uninferable\n     )\n     node.elts = list(filtered_elts) * other.value\n     return node\n     elts: Sequence[InferenceResult], context: InferenceContext\n ) -> Iterator[SuccessfulInferenceResult]:\n     for elt in elts:\n        if elt is util.Uninferable:\n             yield nodes.Unknown()\n         else:\n             for inferred in elt.infer(context):\n                if inferred is not util.Uninferable:\n                     yield inferred\n                 else:\n                     yield nodes.Unknown()\n     other: InferenceResult,\n     context: InferenceContext,\n     method: SuccessfulInferenceResult,\n) -> Generator[_TupleListNodeT | nodes.Const | type[util.Uninferable], None, None]:\n     \"\"\"Infer a binary operation on a tuple or list.\n \n     The instance on which the binary operation is performed is a tuple\n     assign_path = assign_path[:]\n     index = assign_path.pop(0)\n     for part in parts:\n        if part is util.Uninferable:\n             continue\n         if not hasattr(part, \"itered\"):\n             continue\n                 # we achieved to resolved the assignment path,\n                 # don't infer the last part\n                 yield assigned\n            elif assigned is util.Uninferable:\n                 break\n             else:\n                 # we are not yet on the last part of the path\n             # we achieved to resolved the assignment path, don't infer the\n             # last part\n             yield assigned\n        elif assigned is util.Uninferable:\n             return\n         else:\n             # we are not yet on the last part of the path search on each\n         except (InferenceError, StopIteration):\n             yield util.Uninferable\n             return\n        if rhs is util.Uninferable or not hasattr(rhs, \"itered\"):\n             yield util.Uninferable\n             return\n \n         except (InferenceError, StopIteration):\n             yield util.Uninferable\n             return\n        if inferred_iterable is util.Uninferable or not hasattr(\n             inferred_iterable, \"itered\"\n         ):\n             yield util.Uninferable\ndiff --git a/astroid/typing.py b/astroid/typing.py\n     _transform: transforms.TransformVisitor\n \n \nInferenceResult = Union[\"nodes.NodeNG\", \"type[util.Uninferable]\", \"bases.Proxy\"]\n SuccessfulInferenceResult = Union[\"nodes.NodeNG\", \"bases.Proxy\"]\n \n ConstFactoryResult = Union[\ndiff --git a/astroid/util.py b/astroid/util.py\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n import importlib\n import sys\n import warnings\n import lazy_object_proxy\n \n if sys.version_info >= (3, 8):\n    from typing import Literal\n else:\n    from typing_extensions import Literal\n \n \n def lazy_descriptor(obj):\n     )\n \n \n@object.__new__\nclass Uninferable:\n    \"\"\"Special inference object, which is returned when inference fails.\"\"\"\n \n    def __repr__(self) -> str:\n         return \"Uninferable\"\n \n     __str__ = __repr__\n             return object.__getattribute__(self, name)\n         return self\n \n    def __call__(self, *args, **kwargs):\n         return self\n \n     def __bool__(self) -> Literal[False]:\n         return visitor.visit_uninferable(self)\n \n \n class BadOperationMessage:\n     \"\"\"Object which describes a TypeError occurred somewhere in the inference chain.\n \n \n     def _object_type(self, obj):\n         objtype = self._object_type_helper(obj)\n        if objtype is Uninferable:\n             return None\n \n         return objtype",
    "output": " from astroid.bases import Instance\n from astroid.context import CallContext, InferenceContext\n from astroid.exceptions import InferenceError, NoDefault\nfrom astroid.util import Uninferable, UninferableBase\n \n \n class CallSite:\n         self._unpacked_kwargs = self._unpack_keywords(keywords, context=context)\n \n         self.positional_arguments = [\n            arg for arg in self._unpacked_args if not isinstance(arg, UninferableBase)\n         ]\n         self.keyword_arguments = {\n             key: value\n             for key, value in self._unpacked_kwargs.items()\n            if not isinstance(value, UninferableBase)\n         }\n \n     @classmethod\n                 except StopIteration:\n                     continue\n \n                if isinstance(inferred, UninferableBase):\n                     values.append(Uninferable)\n                     continue\n                 if not hasattr(inferred, \"elts\"):\ndiff --git a/astroid/bases.py b/astroid/bases.py\n     NameInferenceError,\n )\n from astroid.typing import InferBinaryOp, InferenceErrorInfo, InferenceResult\nfrom astroid.util import Uninferable, UninferableBase, lazy_descriptor, lazy_import\n \n if sys.version_info >= (3, 8):\n     from typing import Literal\n     if PROPERTIES.intersection(decoratornames):\n         return True\n     stripped = {\n        name.split(\".\")[-1]\n        for name in decoratornames\n        if not isinstance(name, UninferableBase)\n     }\n     if any(name in stripped for name in POSSIBLE_PROPERTIES):\n         return True\n         return False\n     for decorator in meth.decorators.nodes or ():\n         inferred = helpers.safe_infer(decorator, context=context)\n        if inferred is None or isinstance(inferred, UninferableBase):\n             continue\n         if inferred.__class__.__name__ == \"ClassDef\":\n             for base_class in inferred.bases:\n \n \n def _infer_stmts(\n    stmts: Sequence[nodes.NodeNG | UninferableBase | Instance],\n     context: InferenceContext | None,\n     frame: nodes.NodeNG | Instance | None = None,\n ) -> collections.abc.Generator[InferenceResult, None, None]:\n         context = InferenceContext()\n \n     for stmt in stmts:\n        if isinstance(stmt, UninferableBase):\n             yield stmt\n             inferred = True\n             continue\n             for constraint_stmt, potential_constraints in constraints.items():\n                 if not constraint_stmt.parent_of(stmt):\n                     stmt_constraints.update(potential_constraints)\n            for inf in stmt.infer(context=context):\n                 if all(constraint.satisfied_by(inf) for constraint in stmt_constraints):\n                     yield inf\n                     inferred = True\n         try:\n             context.callcontext = CallContext(args=[], callee=meth)\n             for value in meth.infer_call_result(instance, context=context):\n                if isinstance(value, UninferableBase):\n                     return value\n                 try:\n                     inferred = next(value.infer(context=context))\n \n         # Otherwise we infer the call to the __call__ dunder normally\n         for node in self._proxied.igetattr(\"__call__\", context):\n            if isinstance(node, UninferableBase) or not node.callable():\n                 continue\n             for res in node.infer_call_result(caller, context):\n                 inferred = True\n         caller: nodes.Call,\n         context: InferenceContext,\n     ) -> collections.abc.Generator[\n        nodes.Const | Instance | UninferableBase, None, None\n     ]:\n         if not caller.args:\n             return\n \n         node_context = context.extra_context.get(caller.args[0])\n         for inferred in caller.args[0].infer(context=node_context):\n            if isinstance(inferred, UninferableBase):\n                 yield inferred\n             if isinstance(inferred, nodes.ClassDef):\n                 yield Instance(inferred)\ndiff --git a/astroid/brain/brain_builtin_inference.py b/astroid/brain/brain_builtin_inference.py\n             inferred = next(arg.infer(context=context))\n         except (InferenceError, StopIteration) as exc:\n             raise UseInferenceDefault from exc\n        if isinstance(inferred, util.UninferableBase):\n             raise UseInferenceDefault\n         transformed = transform(inferred)\n    if not transformed or isinstance(transformed, util.UninferableBase):\n         raise UseInferenceDefault\n     return transformed\n \n         except (InferenceError, StopIteration) as exc:\n             raise UseInferenceDefault from exc\n \n    if isinstance(mro_pointer, util.UninferableBase) or isinstance(\n        mro_type, util.UninferableBase\n    ):\n         # No way we could understand this.\n         raise UseInferenceDefault\n \n     except (InferenceError, StopIteration) as exc:\n         raise UseInferenceDefault from exc\n \n    if isinstance(obj, util.UninferableBase) or isinstance(attr, util.UninferableBase):\n         # If one of the arguments is something we can't infer,\n         # then also make the result of the getattr call something\n         # which is unknown.\n     \"\"\"\n     obj, attr = _infer_getattr_args(node, context)\n     if (\n        isinstance(obj, util.UninferableBase)\n        or isinstance(attr, util.UninferableBase)\n         or not hasattr(obj, \"igetattr\")\n     ):\n         return util.Uninferable\n     try:\n         obj, attr = _infer_getattr_args(node, context)\n         if (\n            isinstance(obj, util.UninferableBase)\n            or isinstance(attr, util.UninferableBase)\n             or not hasattr(obj, \"getattr\")\n         ):\n             return util.Uninferable\n         inferred = next(argument.infer(context=context))\n     except (InferenceError, StopIteration):\n         return util.Uninferable\n    if isinstance(inferred, util.UninferableBase):\n         return util.Uninferable\n     return nodes.Const(inferred.callable())\n \n         inferred = next(argument.infer(context=context))\n     except (InferenceError, StopIteration):\n         return util.Uninferable\n    if isinstance(inferred, util.UninferableBase):\n         return util.Uninferable\n \n     bool_value = inferred.bool_value(context=context)\n    if isinstance(bool_value, util.UninferableBase):\n         return util.Uninferable\n     return nodes.Const(bool_value)\n \n     infer_func = partial(helpers.safe_infer, context=context)\n     args = [infer_func(arg) for arg in args]\n     for arg in args:\n        if not arg or isinstance(arg, util.UninferableBase):\n             raise UseInferenceDefault\n         if not isinstance(arg, nodes.Const):\n             raise UseInferenceDefault\n         raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n     except MroError as exc:\n         raise UseInferenceDefault from exc\n    if isinstance(isinstance_bool, util.UninferableBase):\n         raise UseInferenceDefault\n     return nodes.Const(isinstance_bool)\n \n         except (InferenceError, StopIteration) as exc:\n             raise UseInferenceDefault(str(exc)) from exc\n \n        if isinstance(first_value, util.UninferableBase):\n             raise UseInferenceDefault\n \n         if isinstance(first_value, nodes.Const) and isinstance(\n \n def _infer_str_format_call(\n     node: nodes.Call, context: InferenceContext | None = None\n) -> Iterator[nodes.Const | util.UninferableBase]:\n     \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n     call = arguments.CallSite.from_call(node, context=context)\n     if isinstance(node.func.expr, nodes.Name):\ndiff --git a/astroid/brain/brain_dataclasses.py b/astroid/brain/brain_dataclasses.py\n from astroid.inference_tip import inference_tip\n from astroid.manager import AstroidManager\n from astroid.typing import InferenceResult\nfrom astroid.util import Uninferable, UninferableBase\n \n if sys.version_info >= (3, 8):\n     from typing import Literal\n     except (InferenceError, StopIteration):\n         inferred = Uninferable\n \n    if isinstance(inferred, UninferableBase):\n         if isinstance(node, nodes.Name):\n             return node.name in decorator_names\n         if isinstance(node, nodes.Attribute):\n \n def _infer_instance_from_annotation(\n     node: nodes.NodeNG, ctx: context.InferenceContext | None = None\n) -> Iterator[UninferableBase | bases.Instance]:\n     \"\"\"Infer an instance corresponding to the type annotation represented by node.\n \n     Currently has limited support for the typing module.\ndiff --git a/astroid/brain/brain_functools.py b/astroid/brain/brain_functools.py\n from astroid.manager import AstroidManager\n from astroid.nodes.node_classes import AssignName, Attribute, Call, Name\n from astroid.nodes.scoped_nodes import FunctionDef\nfrom astroid.util import UninferableBase\n \n LRU_CACHE = \"functools.lru_cache\"\n \n         inferred_wrapped_function = next(partial_function.infer(context=context))\n     except (InferenceError, StopIteration) as exc:\n         raise UseInferenceDefault from exc\n    if isinstance(inferred_wrapped_function, UninferableBase):\n         raise UseInferenceDefault(\"Cannot infer the wrapped function\")\n     if not isinstance(inferred_wrapped_function, FunctionDef):\n         raise UseInferenceDefault(\"The wrapped function is not a function\")\ndiff --git a/astroid/brain/brain_namedtuple_enum.py b/astroid/brain/brain_namedtuple_enum.py\n \n \n def _infer_first(node, context):\n    if isinstance(node, util.UninferableBase):\n         raise UseInferenceDefault\n     try:\n         value = next(node.infer(context=context))\n     except StopIteration as exc:\n         raise InferenceError from exc\n    if isinstance(value, util.UninferableBase):\n         raise UseInferenceDefault()\n     return value\n \ndiff --git a/astroid/brain/brain_typing.py b/astroid/brain/brain_typing.py\n     Tuple,\n )\n from astroid.nodes.scoped_nodes import ClassDef, FunctionDef\n \n if sys.version_info >= (3, 8):\n     from typing import Final\n         col_offset=assign_name.col_offset,\n         parent=node.parent,\n     )\n    if isinstance(res, ClassDef):\n         # Only add `res` as base if it's a `ClassDef`\n         # This isn't the case for `typing.Pattern` and `typing.Match`\n         class_def.postinit(bases=[res], body=[], decorators=None)\ndiff --git a/astroid/builder.py b/astroid/builder.py\n         try:\n             frame = node.frame(future=True)\n             for inferred in node.expr.infer():\n                if isinstance(inferred, util.UninferableBase):\n                     continue\n                 try:\n                     # pylint: disable=unidiomatic-typecheck # We want a narrow check on the\n                         # Const, Tuple or other containers that inherit from\n                         # `Instance`\n                         continue\n                    elif isinstance(inferred, (bases.Proxy, util.UninferableBase)):\n                         continue\n                     elif inferred.is_function:\n                         iattrs = inferred.instance_attrs\ndiff --git a/astroid/constraint.py b/astroid/constraint.py\n     def satisfied_by(self, inferred: InferenceResult) -> bool:\n         \"\"\"Return True if this constraint is satisfied by the given inferred value.\"\"\"\n         # Assume true if uninferable\n        if isinstance(inferred, util.UninferableBase):\n             return True\n \n         # Return the XOR of self.negate and matches(inferred, self.CONST_NONE)\ndiff --git a/astroid/helpers.py b/astroid/helpers.py\n             yield _build_proxy_class(\"module\", builtins)\n         elif isinstance(inferred, nodes.Unknown):\n             raise InferenceError\n        elif isinstance(inferred, util.UninferableBase):\n             yield inferred\n         elif isinstance(inferred, (bases.Proxy, nodes.Slice)):\n             yield inferred._proxied\n     else:\n         class_seq = class_or_seq\n \n    if isinstance(obj_type, util.UninferableBase):\n         return util.Uninferable\n \n     # Instances are not types\n     # issubclass(type, (object, 1)) evaluates to true\n     # issubclass(object, (1, type)) raises TypeError\n     for klass in class_seq:\n        if isinstance(klass, util.UninferableBase):\n             raise AstroidTypeError(\"arg 2 must be a type or tuple of types\")\n \n         for obj_subclass in obj_type.mro():\n     :raises AstroidTypeError: if the given ``classes_or_seq`` are not types\n     \"\"\"\n     obj_type = object_type(node, context)\n    if isinstance(obj_type, util.UninferableBase):\n         return util.Uninferable\n     return _object_type_is_subclass(obj_type, class_or_seq, context=context)\n \n         )\n         raise InferenceError(message)\n \n    if inferred_node is None or isinstance(inferred_node, util.UninferableBase):\n         raise InferenceError(node=node)\n     if isinstance(inferred_node, nodes.Const) and isinstance(\n         inferred_node.value, (bytes, str)\n         ) from e\n \n     inferred = len_call.infer_call_result(node, context)\n    if isinstance(inferred, util.UninferableBase):\n         raise InferenceError(node=node, context=context)\n     result_of_len = next(inferred, None)\n     if (\ndiff --git a/astroid/inference.py b/astroid/inference.py\n         callcontext.extra_context = _populate_context_lookup(self, context.clone())\n \n     for callee in self.func.infer(context):\n        if isinstance(callee, util.UninferableBase):\n             yield callee\n             continue\n         try:\n ) -> Generator[InferenceResult, None, InferenceErrorInfo]:\n     \"\"\"Infer an Attribute node by using getattr on the associated object.\"\"\"\n     for owner in self.expr.infer(context):\n        if isinstance(owner, util.UninferableBase):\n             yield owner\n             continue\n \n \n     found_one = False\n     for value in self.value.infer(context):\n        if isinstance(value, util.UninferableBase):\n             yield util.Uninferable\n             return None\n         for index in self.slice.infer(context):\n            if isinstance(index, util.UninferableBase):\n                 yield util.Uninferable\n                 return None\n \n \n             # Prevent inferring if the inferred subscript\n             # is the same as the original subscripted object.\n            if self is assigned or isinstance(assigned, util.UninferableBase):\n                 yield util.Uninferable\n                 return None\n             yield from assigned.infer(context)\n         return None\n \n     for pair in itertools.product(*inferred_values):\n        if any(isinstance(item, util.UninferableBase) for item in pair):\n             # Can't infer the final result, just yield Uninferable.\n             yield util.Uninferable\n             continue\n \n         bool_values = [item.bool_value() for item in pair]\n        if any(isinstance(item, util.UninferableBase) for item in bool_values):\n             # Can't infer the final result, just yield Uninferable.\n             yield util.Uninferable\n             continue\n                 # value and negate its result, unless it is\n                 # Uninferable, which will be returned as is.\n                 bool_value = operand.bool_value()\n                if not isinstance(bool_value, util.UninferableBase):\n                     yield nodes.const_factory(not bool_value)\n                 else:\n                     yield util.Uninferable\n \n                     meth = methods[0]\n                     inferred = next(meth.infer(context=context), None)\n                    if (\n                        isinstance(inferred, util.UninferableBase)\n                        or not inferred.callable()\n                    ):\n                         continue\n \n                     context = copy_context(context)\n \n def _infer_old_style_string_formatting(\n     instance: nodes.Const, other: nodes.NodeNG, context: InferenceContext\n) -> tuple[util.UninferableBase | nodes.Const]:\n     \"\"\"Infer the result of '\"string\" % ...'.\n \n     TODO: Instead of returning Uninferable we should rely\n         inferred = next(method.infer(context=context))\n     except StopIteration as e:\n         raise InferenceError(node=method, context=context) from e\n    if isinstance(inferred, util.UninferableBase):\n         raise InferenceError\n     if not isinstance(\n         instance, (nodes.Const, nodes.Tuple, nodes.List, nodes.ClassDef, bases.Instance)\n             yield util.Uninferable\n             return\n         else:\n            if any(isinstance(result, util.UninferableBase) for result in results):\n                 yield util.Uninferable\n                 return\n \n     lhs_iter = left.infer(context=lhs_context)\n     rhs_iter = right.infer(context=rhs_context)\n     for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any(isinstance(value, util.UninferableBase) for value in (rhs, lhs)):\n             # Don't know how to process this.\n             yield util.Uninferable\n             return\n \n def _do_compare(\n     left_iter: Iterable[nodes.NodeNG], op: str, right_iter: Iterable[nodes.NodeNG]\n) -> bool | util.UninferableBase:\n     \"\"\"\n     If all possible combinations are either True or False, return that:\n     >>> _do_compare([1, 2], '<=', [3, 4])\n     op_func = COMPARE_OPS[op]\n \n     for left, right in itertools.product(left_iter, right_iter):\n        if isinstance(left, util.UninferableBase) or isinstance(\n            right, util.UninferableBase\n        ):\n             return util.Uninferable\n \n         try:\n \n def _infer_compare(\n     self: nodes.Compare, context: InferenceContext | None = None, **kwargs: Any\n) -> Generator[nodes.Const | util.UninferableBase, None, None]:\n     \"\"\"Chained comparison inference logic.\"\"\"\n    retval: bool | util.UninferableBase = True\n \n     ops = self.ops\n     left_node = self.left\n     lhs_iter = self.target.infer_lhs(context=context)\n     rhs_iter = self.value.infer(context=rhs_context)\n     for lhs, rhs in itertools.product(lhs_iter, rhs_iter):\n        if any(isinstance(value, util.UninferableBase) for value in (rhs, lhs)):\n             # Don't know how to process this.\n             yield util.Uninferable\n             return\n     except (InferenceError, StopIteration):\n         both_branches = True\n     else:\n        if not isinstance(test, util.UninferableBase):\n             if test.bool_value():\n                 yield from self.body.infer(context=lhs_context)\n             else:\ndiff --git a/astroid/inference_tip.py b/astroid/inference_tip.py\n \n import wrapt\n \n from astroid.exceptions import InferenceOverwriteError, UseInferenceDefault\n from astroid.nodes import NodeNG\nfrom astroid.typing import InferenceResult, InferFn\n \n_cache: dict[tuple[InferFn, NodeNG], list[InferenceResult] | None] = {}\n \n \n def clear_inference_tip_cache() -> None:\n @wrapt.decorator\n def _inference_tip_cached(\n     func: InferFn, instance: None, args: typing.Any, kwargs: typing.Any\n) -> Iterator[InferenceResult]:\n     \"\"\"Cache decorator used for inference tips.\"\"\"\n     node = args[0]\n     try:\ndiff --git a/astroid/interpreter/objectmodel.py b/astroid/interpreter/objectmodel.py\n                 except StopIteration as e:\n                     raise InferenceError(context=context, node=caller.args[0]) from e\n \n                if isinstance(cls, util.UninferableBase):\n                     raise InferenceError(\n                         \"Invalid class inferred\", target=self, context=context\n                     )\ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n         return {\"node\": stmt, \"context\": context}\n     # else, infer recursively, except Uninferable object that should be returned as is\n     for inferred in stmt.infer(context):\n        if isinstance(inferred, util.UninferableBase):\n             yield inferred\n         else:\n             yield from unpack_infer(inferred, context)\n                     continue\n \n             for inferredkey in key.infer(context):\n                if isinstance(inferredkey, util.UninferableBase):\n                     continue\n                 if isinstance(inferredkey, Const) and isinstance(index, Const):\n                     if inferredkey.value == index.value:\n     _astroid_fields = (\"original\",)\n     _other_fields = (\"value\",)\n \n    def __init__(self, original: NodeNG, value: NodeNG | util.UninferableBase) -> None:\n         self.original: NodeNG = original\n         \"\"\"The original node that has already been evaluated\"\"\"\n \n        self.value: NodeNG | util.UninferableBase = value\n         \"\"\"The inferred value\"\"\"\n \n         super().__init__(\n \n     def _infer(\n         self, context: InferenceContext | None = None, **kwargs: Any\n    ) -> Generator[NodeNG | util.UninferableBase, None, None]:\n         yield self.value\n \n \ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n                 new_class.hide = True\n                 new_class.parent = self\n                 new_class.postinit(\n                    bases=[\n                        base\n                        for base in class_bases\n                        if not isinstance(base, util.UninferableBase)\n                    ],\n                     body=[],\n                     decorators=[],\n                     metaclass=metaclass,\n                 if isinstance(baseobj, bases.Instance):\n                     # not abstract\n                     return False\n                 if baseobj is klass:\n                     continue\n                 if not isinstance(baseobj, ClassDef):\n                 return next(\n                     node\n                     for node in self._metaclass.infer(context=context)\n                    if not isinstance(node, util.UninferableBase)\n                 )\n             except (InferenceError, StopIteration):\n                 return None\n                 values = [item[0] for item in slots.items]\n             else:\n                 values = slots.itered()\n            if isinstance(values, util.UninferableBase):\n                 continue\n             if not values:\n                 # Stop the iteration, because the class\n             for elt in values:\n                 try:\n                     for inferred in elt.infer():\n                         if not isinstance(\n                             inferred, node_classes.Const\n                         ) or not isinstance(inferred.value, str):\ndiff --git a/astroid/protocols.py b/astroid/protocols.py\n     other: InferenceResult,\n     context: InferenceContext,\n     _: SuccessfulInferenceResult,\n) -> Generator[ConstFactoryResult | util.UninferableBase, None, None]:\n     not_implemented = nodes.Const(NotImplemented)\n     if isinstance(other, nodes.Const):\n         if (\n     filtered_elts = (\n         helpers.safe_infer(elt, context) or util.Uninferable\n         for elt in self.elts\n        if not isinstance(elt, util.UninferableBase)\n     )\n     node.elts = list(filtered_elts) * other.value\n     return node\n     elts: Sequence[InferenceResult], context: InferenceContext\n ) -> Iterator[SuccessfulInferenceResult]:\n     for elt in elts:\n        if isinstance(elt, util.UninferableBase):\n             yield nodes.Unknown()\n         else:\n             for inferred in elt.infer(context):\n                if not isinstance(inferred, util.UninferableBase):\n                     yield inferred\n                 else:\n                     yield nodes.Unknown()\n     other: InferenceResult,\n     context: InferenceContext,\n     method: SuccessfulInferenceResult,\n) -> Generator[_TupleListNodeT | nodes.Const | util.UninferableBase, None, None]:\n     \"\"\"Infer a binary operation on a tuple or list.\n \n     The instance on which the binary operation is performed is a tuple\n     assign_path = assign_path[:]\n     index = assign_path.pop(0)\n     for part in parts:\n        if isinstance(part, util.UninferableBase):\n             continue\n         if not hasattr(part, \"itered\"):\n             continue\n                 # we achieved to resolved the assignment path,\n                 # don't infer the last part\n                 yield assigned\n            elif isinstance(assigned, util.UninferableBase):\n                 break\n             else:\n                 # we are not yet on the last part of the path\n             # we achieved to resolved the assignment path, don't infer the\n             # last part\n             yield assigned\n        elif isinstance(assigned, util.UninferableBase):\n             return\n         else:\n             # we are not yet on the last part of the path search on each\n         except (InferenceError, StopIteration):\n             yield util.Uninferable\n             return\n        if isinstance(rhs, util.UninferableBase) or not hasattr(rhs, \"itered\"):\n             yield util.Uninferable\n             return\n \n         except (InferenceError, StopIteration):\n             yield util.Uninferable\n             return\n        if isinstance(inferred_iterable, util.UninferableBase) or not hasattr(\n             inferred_iterable, \"itered\"\n         ):\n             yield util.Uninferable\ndiff --git a/astroid/typing.py b/astroid/typing.py\n     _transform: transforms.TransformVisitor\n \n \nInferenceResult = Union[\"nodes.NodeNG\", \"util.UninferableBase\", \"bases.Proxy\"]\n SuccessfulInferenceResult = Union[\"nodes.NodeNG\", \"bases.Proxy\"]\n \n ConstFactoryResult = Union[\ndiff --git a/astroid/util.py b/astroid/util.py\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n\nfrom __future__ import annotations\n\n import importlib\n import sys\n import warnings\n import lazy_object_proxy\n \n if sys.version_info >= (3, 8):\n    from typing import Final, Literal\n else:\n    from typing_extensions import Final, Literal\n \n \n def lazy_descriptor(obj):\n     )\n \n \nclass UninferableBase:\n    \"\"\"Special inference object, which is returned when inference fails.\n\n    This is meant to be used as a singleton. Use astroid.util.Uninferable to access it.\n    \"\"\"\n \n    def __repr__(self) -> Literal[\"Uninferable\"]:\n         return \"Uninferable\"\n \n     __str__ = __repr__\n             return object.__getattribute__(self, name)\n         return self\n \n    def __call__(self, *args: Any, **kwargs: Any) -> UninferableBase:\n         return self\n \n     def __bool__(self) -> Literal[False]:\n         return visitor.visit_uninferable(self)\n \n \nUninferable: Final = UninferableBase()\n\n\n class BadOperationMessage:\n     \"\"\"Object which describes a TypeError occurred somewhere in the inference chain.\n \n \n     def _object_type(self, obj):\n         objtype = self._object_type_helper(obj)\n        if isinstance(objtype, UninferableBase):\n             return None\n \n         return objtype"
  },
  {
    "instruction": "Infer calls to str.format() on names\nFuture enhancement could infer this value instead of giving an empty string:\r\n\r\n```python\r\nfrom astroid import extract_node\r\ncall = extract_node(\"\"\"\r\nx = 'python is {}'\r\nx.format('helpful sometimes')\r\n\"\"\")\r\ncall.inferred()[0].value  # gives \"\"\r\n```\r\n\r\n_Originally posted by @jacobtylerwalls in https://github.com/PyCQA/astroid/pull/1602#discussion_r893423433_\n",
    "input": " \n def _is_str_format_call(node: nodes.Call) -> bool:\n     \"\"\"Catch calls to str.format().\"\"\"\n    return (\n        isinstance(node.func, nodes.Attribute)\n        and node.func.attrname == \"format\"\n        and isinstance(node.func.expr, nodes.Const)\n        and isinstance(node.func.expr.value, str)\n    )\n \n \n def _infer_str_format_call(\n ) -> Iterator[nodes.Const | type[util.Uninferable]]:\n     \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n     call = arguments.CallSite.from_call(node, context=context)\n    format_template: str = node.func.expr.value\n \n     # Get the positional arguments passed\n     inferred_positional = [",
    "output": " \n def _is_str_format_call(node: nodes.Call) -> bool:\n     \"\"\"Catch calls to str.format().\"\"\"\n    if not isinstance(node.func, nodes.Attribute) or not node.func.attrname == \"format\":\n        return False\n\n    if isinstance(node.func.expr, nodes.Name):\n        value = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n\n    return isinstance(value, nodes.Const) and isinstance(value.value, str)\n \n \n def _infer_str_format_call(\n ) -> Iterator[nodes.Const | type[util.Uninferable]]:\n     \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n     call = arguments.CallSite.from_call(node, context=context)\n    if isinstance(node.func.expr, nodes.Name):\n        value: nodes.Const = helpers.safe_infer(node.func.expr)\n    else:\n        value = node.func.expr\n\n    format_template = value.value\n \n     # Get the positional arguments passed\n     inferred_positional = ["
  },
  {
    "instruction": "Deprecation warnings from numpy\n### Steps to reproduce\r\n\r\n1. Run pylint over the following test case:\r\n\r\n```\r\n\"\"\"Test case\"\"\"\r\n\r\nimport numpy as np\r\nvalue = np.random.seed(1234)\r\n```\r\n\r\n### Current behavior\r\n```\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n```\r\n\r\n### Expected behavior\r\nThere should be no future warnings.\r\n\r\n### python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\" output\r\n2.12.13\n",
    "input": " \n import builtins\n import inspect\n import os\n import sys\n import types\n import warnings\n from collections.abc import Iterable\n from typing import Any, Union\n \n from astroid import bases, nodes\n from astroid.manager import AstroidManager\n from astroid.nodes import node_classes\n \n _FunctionTypes = Union[\n     types.FunctionType,\n     types.MethodType,\n             # check if it sounds valid and then add an import node, else use a\n             # dummy node\n             try:\n                getattr(sys.modules[modname], name)\n             except (KeyError, AttributeError):\n                 attach_dummy_node(node, name, member)\n             else:",
    "output": " \n import builtins\n import inspect\nimport io\nimport logging\n import os\n import sys\n import types\n import warnings\n from collections.abc import Iterable\nfrom contextlib import redirect_stderr, redirect_stdout\n from typing import Any, Union\n \n from astroid import bases, nodes\n from astroid.manager import AstroidManager\n from astroid.nodes import node_classes\n \nlogger = logging.getLogger(__name__)\n\n\n _FunctionTypes = Union[\n     types.FunctionType,\n     types.MethodType,\n             # check if it sounds valid and then add an import node, else use a\n             # dummy node\n             try:\n                with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(\n                    io.StringIO()\n                ) as stdout:\n                    getattr(sys.modules[modname], name)\n                    stderr_value = stderr.getvalue()\n                    if stderr_value:\n                        logger.error(\n                            \"Captured stderr while getting %s from %s:\\n%s\",\n                            name,\n                            sys.modules[modname],\n                            stderr_value,\n                        )\n                    stdout_value = stdout.getvalue()\n                    if stdout_value:\n                        logger.info(\n                            \"Captured stdout while getting %s from %s:\\n%s\",\n                            name,\n                            sys.modules[modname],\n                            stdout_value,\n                        )\n             except (KeyError, AttributeError):\n                 attach_dummy_node(node, name, member)\n             else:"
  },
  {
    "instruction": "error during inference of class inheriting from another with `mod.Type` format\nConsider package a `level` with a class `Model` defined in `level`'s `__init__.py` file.\r\n\r\n```\r\nclass Model:\r\n    data: int = 1\r\n```\r\n\r\nIf a class `Test` inherits from `Model` as `class Test(Model)`, and `Model` comes from `from level import Model`,  then inferring `Test.data` works fine (below, A is an alias for astroid).\r\n\r\n<img width=\"248\" alt=\"Screen Shot 2021-02-19 at 09 41 09\" src=\"https://user-images.githubusercontent.com/2905588/108505730-9b3c1900-7296-11eb-8bb8-5b66b7253cf4.png\">\r\n\r\nHowever, if a `Test` inherits from `Model` as `class Test(level.Model)` and `level` comes from `import level`, then inference triggers an exception.\r\n\r\n<img width=\"784\" alt=\"Screen Shot 2021-02-19 at 09 42 09\" src=\"https://user-images.githubusercontent.com/2905588/108505815-beff5f00-7296-11eb-92a2-641be827e1f0.png\">\r\n\r\n\r\n\r\n\r\n\n",
    "input": "         elif not context:\n             context = contextmod.InferenceContext()\n \n         try:\n             context.boundnode = owner\n             yield from owner.igetattr(self.attrname, context)\n         ):\n             pass\n         finally:\n            context.boundnode = None\n     return dict(node=self, context=context)\n \n ",
    "output": "         elif not context:\n             context = contextmod.InferenceContext()\n \n        old_boundnode = context.boundnode\n         try:\n             context.boundnode = owner\n             yield from owner.igetattr(self.attrname, context)\n         ):\n             pass\n         finally:\n            context.boundnode = old_boundnode\n     return dict(node=self, context=context)\n \n "
  },
  {
    "instruction": "v2.13.x regression: Crash when inspecting `PyQt5.QtWidgets` due to `RuntimeError` during `hasattr`\n### Steps to reproduce\r\n\r\nInstall PyQt5, run `pylint --extension-pkg-whitelist=PyQt5 x.py` over a file containing `from PyQt5 import QtWidgets`\r\n\r\n### Current behavior\r\n\r\nWith astroid 2.12.13 and pylint 2.15.10, this works fine. With astroid 2.13.2, this happens:\r\n\r\n```pytb\r\nException on node <ImportFrom l.1 at 0x7fc5a3c47d00> in file '/home/florian/tmp/pylintbug/x.py'\r\nTraceback (most recent call last):\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\r\n    self._check_module_attrs(node, module, name.split(\".\"))\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\r\n    module = next(module.getattr(name)[0].infer())\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\r\n    result = [self.import_module(name, relative_only=True)]\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\r\n    return self.ast_from_module(named_module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\r\n    elif hasattr(member, \"__all__\"):\r\nRuntimeError: wrapped C/C++ object of type QApplication has been deleted\r\nx.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-11-06-17.txt'. (astroid-error)\r\n```\r\n\r\nIt looks like it happens when `member` is `QtWidgets.qApp`, which is a kind of \"magic\" object referring to the QApplication singleton. Since none exists, it looks like PyQt doesn't like trying to access an attribute on that.\r\n\r\nBisected to:\r\n\r\n- #1885 \r\n\r\nIt looks like 974f26f75eb3eccb4bcd8ea143901baf60a685ff is the exact culprit.\r\n\r\ncc @nickdrozd \r\n\r\n(took the freedom to add appropriate labels already, hope that's fine)\r\n\n",
    "input": "         object_build_function(node, member, name)\n \n \n class InspectBuilder:\n     \"\"\"class for building nodes from living object\n \n                 # This should be called for Jython, where some builtin\n                 # methods aren't caught by isbuiltin branch.\n                 _build_from_function(node, name, member, self._module)\n            elif hasattr(member, \"__all__\"):\n                 module = build_module(name)\n                 _attach_local_node(node, module, name)\n                 # recursion",
    "output": "         object_build_function(node, member, name)\n \n \ndef _safe_has_attribute(obj, member: str) -> bool:\n    \"\"\"Required because unexpected RunTimeError can be raised.\n\n    See https://github.com/PyCQA/astroid/issues/1958\n    \"\"\"\n    try:\n        return hasattr(obj, member)\n    except Exception:  # pylint: disable=broad-except\n        return False\n\n\n class InspectBuilder:\n     \"\"\"class for building nodes from living object\n \n                 # This should be called for Jython, where some builtin\n                 # methods aren't caught by isbuiltin branch.\n                 _build_from_function(node, name, member, self._module)\n            elif _safe_has_attribute(member, \"__all__\"):\n                 module = build_module(name)\n                 _attach_local_node(node, module, name)\n                 # recursion"
  },
  {
    "instruction": "`.arguments` property ignores keyword-only args, *args, and **kwargs\n```python\r\n>>> from astroid import extract_node\r\n>>> node = extract_node(\"\"\"def a(*args, b=None, c=None, **kwargs): ...\"\"\")\r\n>>> node.args.arguments\r\n[]\r\n```\r\n\r\nExpected to find all the arguments from the function signature.\r\n\r\nThe wanted data can be found here:\r\n\r\n```python\r\n>>> node.args.vararg\r\n'args'\r\n>>> node.args.kwarg\r\n'kwargs'\r\n>>> node.args.kwonlyargs\r\n[<AssignName.b l.1 at 0x1048189b0>, <AssignName.c l.1 at 0x104818830>]\r\n```\r\n\r\nDiscussed at https://github.com/pylint-dev/pylint/pull/7577#discussion_r989000829.\r\n\r\nNotice that positional-only args are found for some reason \ud83e\udd37 \n",
    "input": " \n         positional = self.positional_arguments[: len(funcnode.args.args)]\n         vararg = self.positional_arguments[len(funcnode.args.args) :]\n        argindex = funcnode.args.find_argname(name)[0]\n         kwonlyargs = {arg.name for arg in funcnode.args.kwonlyargs}\n         kwargs = {\n             key: value\ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n DEPRECATED_ARGUMENT_DEFAULT = \"DEPRECATED_ARGUMENT_DEFAULT\"\n \n \nclass Arguments(_base_nodes.AssignTypeNode):\n     \"\"\"Class representing an :class:`ast.arguments` node.\n \n     An :class:`Arguments` node represents that arguments in a\n     kwargannotation: NodeNG | None\n     \"\"\"The type annotation for the variable length keyword arguments.\"\"\"\n \n    def __init__(self, vararg: str | None, kwarg: str | None, parent: NodeNG) -> None:\n         \"\"\"Almost all attributes can be None for living objects where introspection failed.\"\"\"\n         super().__init__(\n             parent=parent,\n         self.kwarg = kwarg\n         \"\"\"The name of the variable length keyword arguments.\"\"\"\n \n     # pylint: disable=too-many-arguments\n     def postinit(\n         self,\n \n     @cached_property\n     def arguments(self):\n        \"\"\"Get all the arguments for this node, including positional only and positional and keyword\"\"\"\n        return list(itertools.chain((self.posonlyargs or ()), self.args or ()))\n \n     def format_args(self, *, skippable_names: set[str] | None = None) -> str:\n         \"\"\"Get the arguments formatted as string.\n         :raises NoDefault: If there is no default value defined for the\n             given argument.\n         \"\"\"\n        args = self.arguments\n         index = _find_arg(argname, args)[0]\n         if index is not None:\n            idx = index - (len(args) - len(self.defaults))\n             if idx >= 0:\n                 return self.defaults[idx]\n        index = _find_arg(argname, self.kwonlyargs)[0]\n        if index is not None and self.kw_defaults[index] is not None:\n            return self.kw_defaults[index]\n         raise NoDefault(func=self.parent, name=argname)\n \n     def is_argument(self, name) -> bool:\n             return True\n         if name == self.kwarg:\n             return True\n        return (\n            self.find_argname(name)[1] is not None\n            or self.kwonlyargs\n            and _find_arg(name, self.kwonlyargs)[1] is not None\n        )\n \n     def find_argname(self, argname, rec=DEPRECATED_ARGUMENT_DEFAULT):\n         \"\"\"Get the index and :class:`AssignName` node for given name.\n                 stacklevel=2,\n             )\n         if self.arguments:\n            return _find_arg(argname, self.arguments)\n         return None, None\n \n     def get_children(self):\ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n             names = [elt.name for elt in self.args.arguments]\n         else:\n             names = []\n        if self.args.vararg:\n            names.append(self.args.vararg)\n        names += [elt.name for elt in self.args.kwonlyargs]\n        if self.args.kwarg:\n            names.append(self.args.kwarg)\n         return names\n \n     def infer_call_result(\n             names = [elt.name for elt in self.args.arguments]\n         else:\n             names = []\n        if self.args.vararg:\n            names.append(self.args.vararg)\n        names += [elt.name for elt in self.args.kwonlyargs]\n        if self.args.kwarg:\n            names.append(self.args.kwarg)\n         return names\n \n     def getattr(\ndiff --git a/astroid/protocols.py b/astroid/protocols.py\n     # more\n     from astroid import arguments  # pylint: disable=import-outside-toplevel\n \n    if not (self.arguments or self.vararg or self.kwarg):\n         yield util.Uninferable\n         return\n \n     functype = self.parent.type\n     # first argument of instance/class method\n     if (\n        self.arguments\n         and getattr(self.arguments[0], \"name\", None) == name\n         and functype != \"staticmethod\"\n     ):\n     if name == self.vararg:\n         vararg = nodes.const_factory(())\n         vararg.parent = self\n        if not self.arguments and self.parent.name == \"__init__\":\n             cls = self.parent.parent.scope()\n             vararg.elts = [cls.instantiate_class()]\n         yield vararg\ndiff --git a/astroid/rebuilder.py b/astroid/rebuilder.py\n from astroid.const import IS_PYPY, PY38, PY39_PLUS, PY312_PLUS, Context\n from astroid.manager import AstroidManager\n from astroid.nodes import NodeNG\n from astroid.nodes.utils import Position\n from astroid.typing import InferenceResult\n \n         \"\"\"Visit an Arguments node by returning a fresh instance of it.\"\"\"\n         vararg: str | None = None\n         kwarg: str | None = None\n         newnode = nodes.Arguments(\n             node.vararg.arg if node.vararg else None,\n             node.kwarg.arg if node.kwarg else None,\n             parent,\n         )\n         args = [self.visit(child, newnode) for child in node.args]\n         defaults = [self.visit(child, newnode) for child in node.defaults]",
    "output": " \n         positional = self.positional_arguments[: len(funcnode.args.args)]\n         vararg = self.positional_arguments[len(funcnode.args.args) :]\n\n        # preserving previous behavior, when vararg and kwarg were not included in find_argname results\n        if name in [funcnode.args.vararg, funcnode.args.kwarg]:\n            argindex = None\n        else:\n            argindex = funcnode.args.find_argname(name)[0]\n\n         kwonlyargs = {arg.name for arg in funcnode.args.kwonlyargs}\n         kwargs = {\n             key: value\ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n DEPRECATED_ARGUMENT_DEFAULT = \"DEPRECATED_ARGUMENT_DEFAULT\"\n \n \nclass Arguments(\n    _base_nodes.AssignTypeNode\n):  # pylint: disable=too-many-instance-attributes\n     \"\"\"Class representing an :class:`ast.arguments` node.\n \n     An :class:`Arguments` node represents that arguments in a\n     kwargannotation: NodeNG | None\n     \"\"\"The type annotation for the variable length keyword arguments.\"\"\"\n \n    vararg_node: AssignName | None\n    \"\"\"The node for variable length arguments\"\"\"\n\n    kwarg_node: AssignName | None\n    \"\"\"The node for variable keyword arguments\"\"\"\n\n    def __init__(\n        self,\n        vararg: str | None,\n        kwarg: str | None,\n        parent: NodeNG,\n        vararg_node: AssignName | None = None,\n        kwarg_node: AssignName | None = None,\n    ) -> None:\n         \"\"\"Almost all attributes can be None for living objects where introspection failed.\"\"\"\n         super().__init__(\n             parent=parent,\n         self.kwarg = kwarg\n         \"\"\"The name of the variable length keyword arguments.\"\"\"\n \n        self.vararg_node = vararg_node\n        self.kwarg_node = kwarg_node\n\n     # pylint: disable=too-many-arguments\n     def postinit(\n         self,\n \n     @cached_property\n     def arguments(self):\n        \"\"\"Get all the arguments for this node. This includes:\n        * Positional only arguments\n        * Positional arguments\n        * Keyword arguments\n        * Variable arguments (.e.g *args)\n        * Variable keyword arguments (e.g **kwargs)\n        \"\"\"\n        retval = list(itertools.chain((self.posonlyargs or ()), (self.args or ())))\n        if self.vararg_node:\n            retval.append(self.vararg_node)\n        retval += self.kwonlyargs or ()\n        if self.kwarg_node:\n            retval.append(self.kwarg_node)\n\n        return retval\n \n     def format_args(self, *, skippable_names: set[str] | None = None) -> str:\n         \"\"\"Get the arguments formatted as string.\n         :raises NoDefault: If there is no default value defined for the\n             given argument.\n         \"\"\"\n        args = [\n            arg for arg in self.arguments if arg.name not in [self.vararg, self.kwarg]\n        ]\n\n        index = _find_arg(argname, self.kwonlyargs)[0]\n        if index is not None and self.kw_defaults[index] is not None:\n            return self.kw_defaults[index]\n\n         index = _find_arg(argname, args)[0]\n         if index is not None:\n            idx = index - (len(args) - len(self.defaults) - len(self.kw_defaults))\n             if idx >= 0:\n                 return self.defaults[idx]\n\n         raise NoDefault(func=self.parent, name=argname)\n \n     def is_argument(self, name) -> bool:\n             return True\n         if name == self.kwarg:\n             return True\n        return self.find_argname(name)[1] is not None\n \n     def find_argname(self, argname, rec=DEPRECATED_ARGUMENT_DEFAULT):\n         \"\"\"Get the index and :class:`AssignName` node for given name.\n                 stacklevel=2,\n             )\n         if self.arguments:\n            index, argument = _find_arg(argname, self.arguments)\n            if argument:\n                return index, argument\n         return None, None\n \n     def get_children(self):\ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n             names = [elt.name for elt in self.args.arguments]\n         else:\n             names = []\n\n         return names\n \n     def infer_call_result(\n             names = [elt.name for elt in self.args.arguments]\n         else:\n             names = []\n\n         return names\n \n     def getattr(\ndiff --git a/astroid/protocols.py b/astroid/protocols.py\n     # more\n     from astroid import arguments  # pylint: disable=import-outside-toplevel\n \n    if not self.arguments:\n         yield util.Uninferable\n         return\n \n    args = [arg for arg in self.arguments if arg.name not in [self.vararg, self.kwarg]]\n     functype = self.parent.type\n     # first argument of instance/class method\n     if (\n        args\n         and getattr(self.arguments[0], \"name\", None) == name\n         and functype != \"staticmethod\"\n     ):\n     if name == self.vararg:\n         vararg = nodes.const_factory(())\n         vararg.parent = self\n        if not args and self.parent.name == \"__init__\":\n             cls = self.parent.parent.scope()\n             vararg.elts = [cls.instantiate_class()]\n         yield vararg\ndiff --git a/astroid/rebuilder.py b/astroid/rebuilder.py\n from astroid.const import IS_PYPY, PY38, PY39_PLUS, PY312_PLUS, Context\n from astroid.manager import AstroidManager\n from astroid.nodes import NodeNG\nfrom astroid.nodes.node_classes import AssignName\n from astroid.nodes.utils import Position\n from astroid.typing import InferenceResult\n \n         \"\"\"Visit an Arguments node by returning a fresh instance of it.\"\"\"\n         vararg: str | None = None\n         kwarg: str | None = None\n        vararg_node = node.vararg\n        kwarg_node = node.kwarg\n\n         newnode = nodes.Arguments(\n             node.vararg.arg if node.vararg else None,\n             node.kwarg.arg if node.kwarg else None,\n             parent,\n            AssignName(\n                vararg_node.arg,\n                vararg_node.lineno,\n                vararg_node.col_offset,\n                parent,\n                end_lineno=vararg_node.end_lineno,\n                end_col_offset=vararg_node.end_col_offset,\n            )\n            if vararg_node\n            else None,\n            AssignName(\n                kwarg_node.arg,\n                kwarg_node.lineno,\n                kwarg_node.col_offset,\n                parent,\n                end_lineno=kwarg_node.end_lineno,\n                end_col_offset=kwarg_node.end_col_offset,\n            )\n            if kwarg_node\n            else None,\n         )\n         args = [self.visit(child, newnode) for child in node.args]\n         defaults = [self.visit(child, newnode) for child in node.defaults]"
  },
  {
    "instruction": "@property members defined in metaclasses of a base class are not correctly inferred\nRef https://github.com/PyCQA/astroid/issues/927#issuecomment-817244963\r\n\r\nInference works on the parent class but not the child in the following example:\r\n\r\n```python\r\nclass BaseMeta(type):\r\n    @property\r\n    def __members__(cls):\r\n        return ['a', 'property']\r\nclass Parent(metaclass=BaseMeta):\r\n    pass\r\nclass Derived(Parent):\r\n    pass\r\nParent.__members__  # [<Set.set l.10 at 0x...>]\r\nDerived.__members__  # [<Property.__members__ l.8 at 0x...>]\r\n```\n",
    "input": "         if node.root().name == \"enum\":\n             # Skip if the class is directly from enum module.\n             break\n         for local, values in node.locals.items():\n             if any(not isinstance(value, nodes.AssignName) for value in values):\n                 continue\n                 for method in node.mymethods():\n                     fake.locals[method.name] = [method]\n                 new_targets.append(fake.instantiate_class())\n             node.locals[local] = new_targets\n         break\n     return node\n \ndiff --git a/astroid/scoped_nodes.py b/astroid/scoped_nodes.py\n         context = contextmod.copy_context(context)\n         context.lookupname = name\n \n        metaclass = self.declared_metaclass(context=context)\n         try:\n             attributes = self.getattr(name, context, class_context=class_context)\n             # If we have more than one attribute, make sure that those starting from\n                         yield from function.infer_call_result(\n                             caller=self, context=context\n                         )\n                    # If we have a metaclass, we're accessing this attribute through\n                    # the class itself, which means we can solve the property\n                    elif metaclass:\n                         # Resolve a property as long as it is not accessed through\n                         # the class itself.\n                         yield from function.infer_call_result(",
    "output": "         if node.root().name == \"enum\":\n             # Skip if the class is directly from enum module.\n             break\n        dunder_members = {}\n         for local, values in node.locals.items():\n             if any(not isinstance(value, nodes.AssignName) for value in values):\n                 continue\n                 for method in node.mymethods():\n                     fake.locals[method.name] = [method]\n                 new_targets.append(fake.instantiate_class())\n                dunder_members[local] = fake\n             node.locals[local] = new_targets\n        members = nodes.Dict(parent=node)\n        members.postinit(\n            [\n                (nodes.Const(k, parent=members), nodes.Name(v.name, parent=members))\n                for k, v in dunder_members.items()\n            ]\n        )\n        node.locals[\"__members__\"] = [members]\n         break\n     return node\n \ndiff --git a/astroid/scoped_nodes.py b/astroid/scoped_nodes.py\n         context = contextmod.copy_context(context)\n         context.lookupname = name\n \n        metaclass = self.metaclass(context=context)\n         try:\n             attributes = self.getattr(name, context, class_context=class_context)\n             # If we have more than one attribute, make sure that those starting from\n                         yield from function.infer_call_result(\n                             caller=self, context=context\n                         )\n                    # If we're in a class context, we need to determine if the property\n                    # was defined in the metaclass (a derived class must be a subclass of\n                    # the metaclass of all its bases), in which case we can resolve the\n                    # property. If not, i.e. the property is defined in some base class\n                    # instead, then we return the property object\n                    elif metaclass and function.parent.scope() is metaclass:\n                         # Resolve a property as long as it is not accessed through\n                         # the class itself.\n                         yield from function.infer_call_result("
  },
  {
    "instruction": "Replace `cachedproperty` with `functools.cached_property` (>= 3.8)\nI thought about this PR recently again. Typing `cachedproperty` might not work, but it can be replaced with `functools.cached_property`. We only need to `sys` guard it for `< 3.8`. This should work\r\n```py\r\nif sys.version_info >= (3, 8):\r\n    from functools import cached_property\r\nelse:\r\n    from astroid.decorators import cachedproperty as cached_property\r\n```\r\n\r\nAdditionally, the deprecation warning can be limited to `>= 3.8`.\r\n\r\n_Originally posted by @cdce8p in https://github.com/PyCQA/astroid/issues/1243#issuecomment-1052834322_\n",
    "input": "         return result\n \n \n class cachedproperty:\n     \"\"\"Provides a cached property equivalent to the stacking of\n     @cached and @property, but more efficient.\n     __slots__ = (\"wrapped\",)\n \n     def __init__(self, wrapped):\n         try:\n             wrapped.__name__\n         except AttributeError as exc:\ndiff --git a/astroid/mixins.py b/astroid/mixins.py\n \"\"\"This module contains some mixins for the different nodes.\n \"\"\"\n import itertools\n from typing import TYPE_CHECKING, Optional\n \n from astroid import decorators\n if TYPE_CHECKING:\n     from astroid import nodes\n \n \n class BlockRangeMixIn:\n     \"\"\"override block range\"\"\"\n \n    @decorators.cachedproperty\n     def blockstart_tolineno(self):\n         return self.lineno\n \n     Assign nodes, etc.\n     \"\"\"\n \n    @decorators.cachedproperty\n     def _multi_line_blocks(self):\n         return tuple(getattr(self, field) for field in self._multi_line_block_fields)\n \ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n     from astroid import nodes\n     from astroid.nodes import LocalsDictNodeNG\n \n \n def _is_const(value):\n     return isinstance(value, tuple(CONST_CLS))\n             return name\n         return None\n \n    @decorators.cachedproperty\n     def fromlineno(self):\n         \"\"\"The first line that this node appears on in the source code.\n \n         lineno = super().fromlineno\n         return max(lineno, self.parent.fromlineno or 0)\n \n    @decorators.cachedproperty\n     def arguments(self):\n         \"\"\"Get all the arguments for this node, including positional only and positional and keyword\"\"\"\n         return list(itertools.chain((self.posonlyargs or ()), self.args or ()))\n         if body is not None:\n             self.body = body\n \n    @decorators.cachedproperty\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n     See astroid/protocols.py for actual implementation.\n     \"\"\"\n \n    @decorators.cachedproperty\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n         if isinstance(self.parent, If) and self in self.parent.orelse:\n             self.is_orelse = True\n \n    @decorators.cachedproperty\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n             return const\n         return attr\n \n    @decorators.cachedproperty\n     def _proxied(self):\n         builtins = AstroidManager().builtins_module\n         return builtins.getattr(\"slice\")[0]\n         if orelse is not None:\n             self.orelse = orelse\n \n    @decorators.cachedproperty\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n     See astroid/protocols.py for actual implementation.\n     \"\"\"\n \n    @decorators.cachedproperty\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \ndiff --git a/astroid/nodes/node_ng.py b/astroid/nodes/node_ng.py\n else:\n     from typing_extensions import Literal\n \n \n # Types for 'NodeNG.nodes_of_class()'\n T_Nodes = TypeVar(\"T_Nodes\", bound=\"NodeNG\")\n     # these are lazy because they're relatively expensive to compute for every\n     # single node, and they rarely get looked at\n \n    @decorators.cachedproperty\n     def fromlineno(self) -> Optional[int]:\n         \"\"\"The first line that this node appears on in the source code.\"\"\"\n         if self.lineno is None:\n             return self._fixed_source_line()\n         return self.lineno\n \n    @decorators.cachedproperty\n     def tolineno(self) -> Optional[int]:\n         \"\"\"The last line that this node appears on in the source code.\"\"\"\n         if self.end_lineno is not None:\ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n import sys\n import typing\n import warnings\nfrom typing import Dict, List, Optional, Set, TypeVar, Union, overload\n \n from astroid import bases\n from astroid import decorators as decorators_mod\n else:\n     from typing_extensions import Literal\n \n \n ITER_METHODS = (\"__iter__\", \"__getitem__\")\n EXCEPTION_BASE_CLASSES = frozenset({\"Exception\", \"BaseException\"})\n         self.position = position\n         self.doc_node = doc_node\n \n    @decorators_mod.cachedproperty\n     def extra_decorators(self) -> List[node_classes.Call]:\n         \"\"\"The extra decorators that this function can have.\n \n                             decorators.append(assign.value)\n         return decorators\n \n    @decorators_mod.cachedproperty\n     def type(\n         self,\n     ):  # pylint: disable=invalid-overridden-method,too-many-return-statements\n                 pass\n         return type_name\n \n    @decorators_mod.cachedproperty\n     def fromlineno(self) -> Optional[int]:\n         \"\"\"The first line that this node appears on in the source code.\"\"\"\n         # lineno is the line number of the first decorator, we want the def\n \n         return lineno\n \n    @decorators_mod.cachedproperty\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n         doc=(\"Whether this is a new style class or not\\n\\n\" \":type: bool or None\"),\n     )\n \n    @decorators_mod.cachedproperty\n     def fromlineno(self) -> Optional[int]:\n         \"\"\"The first line that this node appears on in the source code.\"\"\"\n         if not PY38_PLUS:\n             return lineno\n         return super().fromlineno\n \n    @decorators_mod.cachedproperty\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \ndiff --git a/astroid/objects.py b/astroid/objects.py\n     Call(func=Name('frozenset'), args=Tuple(...))\n \"\"\"\n \n \nfrom astroid import bases, decorators, util\n from astroid.exceptions import (\n     AttributeInferenceError,\n     InferenceError,\n \n objectmodel = util.lazy_import(\"interpreter.objectmodel\")\n \n \n class FrozenSet(node_classes.BaseContainer):\n     \"\"\"class representing a FrozenSet composite node\"\"\"\n     def _infer(self, context=None):\n         yield self\n \n    @decorators.cachedproperty\n     def _proxied(self):  # pylint: disable=method-hidden\n         ast_builtins = AstroidManager().builtins_module\n         return ast_builtins.getattr(\"frozenset\")[0]\n         index = mro.index(self.mro_pointer)\n         return mro[index + 1 :]\n \n    @decorators.cachedproperty\n     def _proxied(self):\n         ast_builtins = AstroidManager().builtins_module\n         return ast_builtins.getattr(\"super\")[0]\n     the case of .args.\n     \"\"\"\n \n    @decorators.cachedproperty\n     def special_attributes(self):\n         qname = self.qname()\n         instance = objectmodel.BUILTIN_EXCEPTIONS.get(",
    "output": "         return result\n \n \n# TODO: Remove when support for 3.7 is dropped\n# TODO: astroid 3.0 -> move class behind sys.version_info < (3, 8) guard\n class cachedproperty:\n     \"\"\"Provides a cached property equivalent to the stacking of\n     @cached and @property, but more efficient.\n     __slots__ = (\"wrapped\",)\n \n     def __init__(self, wrapped):\n        if sys.version_info >= (3, 8):\n            warnings.warn(\n                \"cachedproperty has been deprecated and will be removed in astroid 3.0 for Python 3.8+. \"\n                \"Use functools.cached_property instead.\",\n                DeprecationWarning,\n            )\n         try:\n             wrapped.__name__\n         except AttributeError as exc:\ndiff --git a/astroid/mixins.py b/astroid/mixins.py\n \"\"\"This module contains some mixins for the different nodes.\n \"\"\"\n import itertools\nimport sys\n from typing import TYPE_CHECKING, Optional\n \n from astroid import decorators\n if TYPE_CHECKING:\n     from astroid import nodes\n \nif sys.version_info >= (3, 8) or TYPE_CHECKING:\n    from functools import cached_property\nelse:\n    from astroid.decorators import cachedproperty as cached_property\n\n \n class BlockRangeMixIn:\n     \"\"\"override block range\"\"\"\n \n    @cached_property\n     def blockstart_tolineno(self):\n         return self.lineno\n \n     Assign nodes, etc.\n     \"\"\"\n \n    @cached_property\n     def _multi_line_blocks(self):\n         return tuple(getattr(self, field) for field in self._multi_line_block_fields)\n \ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n     from astroid import nodes\n     from astroid.nodes import LocalsDictNodeNG\n \nif sys.version_info >= (3, 8) or TYPE_CHECKING:\n    # pylint: disable-next=ungrouped-imports\n    from functools import cached_property\nelse:\n    from astroid.decorators import cachedproperty as cached_property\n\n \n def _is_const(value):\n     return isinstance(value, tuple(CONST_CLS))\n             return name\n         return None\n \n    @cached_property\n     def fromlineno(self):\n         \"\"\"The first line that this node appears on in the source code.\n \n         lineno = super().fromlineno\n         return max(lineno, self.parent.fromlineno or 0)\n \n    @cached_property\n     def arguments(self):\n         \"\"\"Get all the arguments for this node, including positional only and positional and keyword\"\"\"\n         return list(itertools.chain((self.posonlyargs or ()), self.args or ()))\n         if body is not None:\n             self.body = body\n \n    @cached_property\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n     See astroid/protocols.py for actual implementation.\n     \"\"\"\n \n    @cached_property\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n         if isinstance(self.parent, If) and self in self.parent.orelse:\n             self.is_orelse = True\n \n    @cached_property\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n             return const\n         return attr\n \n    @cached_property\n     def _proxied(self):\n         builtins = AstroidManager().builtins_module\n         return builtins.getattr(\"slice\")[0]\n         if orelse is not None:\n             self.orelse = orelse\n \n    @cached_property\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n     See astroid/protocols.py for actual implementation.\n     \"\"\"\n \n    @cached_property\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \ndiff --git a/astroid/nodes/node_ng.py b/astroid/nodes/node_ng.py\n else:\n     from typing_extensions import Literal\n \nif sys.version_info >= (3, 8) or TYPE_CHECKING:\n    # pylint: disable-next=ungrouped-imports\n    from functools import cached_property\nelse:\n    # pylint: disable-next=ungrouped-imports\n    from astroid.decorators import cachedproperty as cached_property\n \n # Types for 'NodeNG.nodes_of_class()'\n T_Nodes = TypeVar(\"T_Nodes\", bound=\"NodeNG\")\n     # these are lazy because they're relatively expensive to compute for every\n     # single node, and they rarely get looked at\n \n    @cached_property\n     def fromlineno(self) -> Optional[int]:\n         \"\"\"The first line that this node appears on in the source code.\"\"\"\n         if self.lineno is None:\n             return self._fixed_source_line()\n         return self.lineno\n \n    @cached_property\n     def tolineno(self) -> Optional[int]:\n         \"\"\"The last line that this node appears on in the source code.\"\"\"\n         if self.end_lineno is not None:\ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n import sys\n import typing\n import warnings\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Set, TypeVar, Union, overload\n \n from astroid import bases\n from astroid import decorators as decorators_mod\n else:\n     from typing_extensions import Literal\n \nif sys.version_info >= (3, 8) or TYPE_CHECKING:\n    from functools import cached_property\nelse:\n    # pylint: disable-next=ungrouped-imports\n    from astroid.decorators import cachedproperty as cached_property\n\n \n ITER_METHODS = (\"__iter__\", \"__getitem__\")\n EXCEPTION_BASE_CLASSES = frozenset({\"Exception\", \"BaseException\"})\n         self.position = position\n         self.doc_node = doc_node\n \n    @cached_property\n     def extra_decorators(self) -> List[node_classes.Call]:\n         \"\"\"The extra decorators that this function can have.\n \n                             decorators.append(assign.value)\n         return decorators\n \n    @cached_property\n     def type(\n         self,\n     ):  # pylint: disable=invalid-overridden-method,too-many-return-statements\n                 pass\n         return type_name\n \n    @cached_property\n     def fromlineno(self) -> Optional[int]:\n         \"\"\"The first line that this node appears on in the source code.\"\"\"\n         # lineno is the line number of the first decorator, we want the def\n \n         return lineno\n \n    @cached_property\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \n         doc=(\"Whether this is a new style class or not\\n\\n\" \":type: bool or None\"),\n     )\n \n    @cached_property\n     def fromlineno(self) -> Optional[int]:\n         \"\"\"The first line that this node appears on in the source code.\"\"\"\n         if not PY38_PLUS:\n             return lineno\n         return super().fromlineno\n \n    @cached_property\n     def blockstart_tolineno(self):\n         \"\"\"The line on which the beginning of this block ends.\n \ndiff --git a/astroid/objects.py b/astroid/objects.py\n     Call(func=Name('frozenset'), args=Tuple(...))\n \"\"\"\n \nimport sys\nfrom typing import TYPE_CHECKING\n \nfrom astroid import bases, util\n from astroid.exceptions import (\n     AttributeInferenceError,\n     InferenceError,\n \n objectmodel = util.lazy_import(\"interpreter.objectmodel\")\n \nif sys.version_info >= (3, 8) or TYPE_CHECKING:\n    from functools import cached_property\nelse:\n    from astroid.decorators import cachedproperty as cached_property\n\n \n class FrozenSet(node_classes.BaseContainer):\n     \"\"\"class representing a FrozenSet composite node\"\"\"\n     def _infer(self, context=None):\n         yield self\n \n    @cached_property\n     def _proxied(self):  # pylint: disable=method-hidden\n         ast_builtins = AstroidManager().builtins_module\n         return ast_builtins.getattr(\"frozenset\")[0]\n         index = mro.index(self.mro_pointer)\n         return mro[index + 1 :]\n \n    @cached_property\n     def _proxied(self):\n         ast_builtins = AstroidManager().builtins_module\n         return ast_builtins.getattr(\"super\")[0]\n     the case of .args.\n     \"\"\"\n \n    @cached_property\n     def special_attributes(self):\n         qname = self.qname()\n         instance = objectmodel.BUILTIN_EXCEPTIONS.get("
  },
  {
    "instruction": "MRO failure on Python 3.7 with typing_extensions\n### Steps to reproduce\r\n\r\nRun the following script on Python 3.7:\r\n\r\n```python\r\nfrom astroid import parse\r\nmodule = parse(\"\"\"\r\nimport abc\r\nimport typing\r\nimport dataclasses\r\n\r\nimport typing_extensions\r\n\r\nT = typing.TypeVar(\"T\")\r\n\r\nclass MyProtocol(typing_extensions.Protocol): pass\r\nclass EarlyBase(typing.Generic[T], MyProtocol): pass\r\nclass Base(EarlyBase[T], abc.ABC): pass\r\nclass Final(Base[object]): pass\r\n\"\"\")\r\n\r\n#                    typing.Protocol\r\n#                          |\r\n# typing.Generic[T]    MyProtocol\r\n#              \\       /\r\n#              EarlyBase     abc.ABC\r\n#                       \\    /\r\n#                        Base\r\n#                         |\r\n#                        Final\r\n\r\nfinal_def = module.body[-1]\r\nfinal_def.mro()\r\n```\r\n\r\n### Current behavior\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"xxx.py\", line 31, in <module>\r\n    print(\"mro:\", final_def.mro())\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 3009, in mro\r\n    return self._compute_mro(context=context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2985, in _compute_mro\r\n    mro = base._compute_mro(context=context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 2999, in _compute_mro\r\n    return _c3_merge(unmerged_mro, self, context)\r\n  File \"/home/rturner/astroid/astroid/nodes/scoped_nodes.py\", line 103, in _c3_merge\r\n    context=context,\r\nastroid.exceptions.InconsistentMroError: Cannot create a consistent method resolution order for MROs (tuple, object), (EarlyBase, tuple, Generic, object, MyProtocol), (ABC, object), (tuple, EarlyBase, ABC) of class <ClassDef.Base l.1347 at 0x7fa0efd52590>.\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo MRO error is raised; Python 3.7 doesn't raise an error.\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.6.7-dev0; the test case fails in pylint 2.9.6 and on the main branch at commit 6e8699cef0888631bd827b096533fc6e894d2fb2.\n",
    "input": " PY310_PLUS = sys.version_info >= (3, 10)\n BUILTINS = \"builtins\"  # TODO Remove in 2.8\n \n \n class Context(enum.Enum):\n     Load = 1",
    "output": " PY310_PLUS = sys.version_info >= (3, 10)\n BUILTINS = \"builtins\"  # TODO Remove in 2.8\n \nWIN32 = sys.platform == \"win32\"\n\n \n class Context(enum.Enum):\n     Load = 1"
  },
  {
    "instruction": "Invalid variable lookup when walrus operator is used\n### Steps to reproduce\r\n1. Consider following code in `loop_error.py`:\r\n\t```\r\n    \"\"\"Test module\"\"\"\r\n\r\n\r\n\tdef walrus_in_comprehension_test_2(some_path, module_namespace):\r\n\t    \"\"\"Suspected error\"\"\"\r\n\t    for mod in some_path.iterdir():\r\n\t        print(mod)\r\n\t\r\n\t    for org_mod in some_path.iterdir():\r\n\t        if org_mod.is_dir():\r\n\t            if mod := module_namespace.get_mod_from_alias(org_mod.name):\r\n\t                new_name = mod.name\r\n\t            else:\r\n\t                new_name = org_mod.name\r\n\t\r\n\t            print(new_name)\r\n\t```\r\n2. Run `pylint ./loop_error.py`\r\n\r\n### Current behavior\r\nA warning appears: ```W0631: Using possibly undefined loop variable 'mod' (undefined-loop-variable)```\r\n\r\n### Expected behavior\r\nNo warning, because the variable `mod` is always defined.\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n- 2.14.1\r\n- 2.15.0-dev0 on 56a65daf1ba391cc85d1a32a8802cfd0c7b7b2ab with Python 3.10.6\n",
    "input": "             # if the common parent is a If or TryExcept statement, look if\n             # nodes are in exclusive branches\n             if isinstance(node, If) and exceptions is None:\n                if (\n                    node.locate_child(previous)[1]\n                    is not node.locate_child(children[node])[1]\n                ):\n                     return True\n             elif isinstance(node, TryExcept):\n                 c2attr, c2node = node.locate_child(previous)",
    "output": "             # if the common parent is a If or TryExcept statement, look if\n             # nodes are in exclusive branches\n             if isinstance(node, If) and exceptions is None:\n                c2attr, c2node = node.locate_child(previous)\n                c1attr, c1node = node.locate_child(children[node])\n                if \"test\" in (c1attr, c2attr):\n                    # If any node is `If.test`, then it must be inclusive with\n                    # the other node (`If.body` and `If.orelse`)\n                    return False\n                if c1attr != c2attr:\n                    # different `If` branches (`If.body` and `If.orelse`)\n                     return True\n             elif isinstance(node, TryExcept):\n                 c2attr, c2node = node.locate_child(previous)"
  },
  {
    "instruction": "Cython module with import triggers deep introspection for pandas, raises unhandled FutureWarning\nThis is a somewhat complicated situation to reproduce, but basically `pandas` throws `FutureWarning`s for certain attributes, and when you import it into a Cython module (triggering astroid's deep module inspection), these future warnings are not handled by astroid and bubble up as `AstroidError`s through to pylint. Here is a full repro:\r\n\r\n\r\n### Cython module `pyx.pyx`\r\n\r\n```python\r\n# distutils: language = c++\r\nimport pandas as pd\r\n\r\ncdef class Test:\r\n    def __cinit__(self):\r\n        ...\r\n```\r\n\r\n\r\n### Python module `test.py`\r\n\r\n```python\r\nimport pyx\r\n\r\npyx.Test()\r\n```\r\n\r\n\r\n\r\n### Commands\r\n```\r\ncythonize -a -i pyx.pyx\r\npylint --extension-pkg-allow-list=pyx,pandas test.py\r\n```\r\n\r\n\r\n### Exception\r\n```\r\nException on node <Import l.1 at 0x106b23ca0> in file '/Users/timkpaine/Programs/projects/other/astroid/test.py'\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 765, in _get_imported_module\r\n    return importnode.do_import_module(modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/mixins.py\", line 102, in do_import_module\r\n    return mymodule.import_module(\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(absmodname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 168, in ast_from_module_name\r\n    return self.ast_from_module(module, modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 265, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/builder.py\", line 91, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 311, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 367, in object_build\r\n    self.object_build(module, member)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 325, in object_build\r\n    member = getattr(obj, name)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pandas/__init__.py\", line 198, in __getattr__\r\n    warnings.warn(\r\nFutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 472, in visit_import\r\n    imported_module = self._get_imported_module(node, name)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 788, in _get_imported_module\r\n    raise astroid.AstroidError from e\r\nastroid.exceptions.AstroidError\r\n************* Module test\r\ntest.py:1:0: F0002: test.py: Fatal error while checking 'test.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/Users/timkpaine/Library/Caches/pylint/pylint-crash-2022-07-19-17.txt'. (astroid-error)\r\n```\r\n\r\n\r\n\r\n\r\n### Standalone (Non Cython) repro for convenience\r\n\r\n```python\r\nimport types\r\nimport pandas as pd\r\nfrom astroid.builder import AstroidBuilder\r\n\r\n\r\nm = types.ModuleType(\"test\")\r\nm.pd = pd\r\n\r\nAstroidBuilder().module_build(m, \"test\")\r\n```\r\n\r\n\r\nxref: https://github.com/PyCQA/pylint/issues/7205\r\nxref: https://github.com/PyCQA/astroid/pull/1719\n",
    "input": "             pypy__class_getitem__ = IS_PYPY and name == \"__class_getitem__\"\n             try:\n                 with warnings.catch_warnings():\n                    warnings.simplefilter(\"error\")\n                     member = getattr(obj, name)\n            except (AttributeError, DeprecationWarning):\n                 # damned ExtensionClass.Base, I know you're there !\n                 attach_dummy_node(node, name)\n                 continue",
    "output": "             pypy__class_getitem__ = IS_PYPY and name == \"__class_getitem__\"\n             try:\n                 with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                     member = getattr(obj, name)\n            except (AttributeError):\n                 # damned ExtensionClass.Base, I know you're there !\n                 attach_dummy_node(node, name)\n                 continue"
  },
  {
    "instruction": "Crash when inferring `str.format` call involving unpacking kwargs\nWhen parsing the following file:\r\n\r\n<!--\r\n If sharing the code is not an option, please state so,\r\n but providing only the stacktrace would still be helpful.\r\n -->\r\n\r\n```python\r\nclass A:\r\n    def render(self, audit_log_entry: AuditLogEntry):\r\n        return \"joined team {team_slug}\".format(**audit_log_entry.data)\r\n\r\n\r\n\r\n```\r\n\r\npylint crashed with a ``AstroidError`` and with the following stacktrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/.../astroid/astroid/inference_tip.py\", line 38, in _inference_tip_cached\r\n    result = _cache[func, node]\r\nKeyError: (<function _infer_str_format_call at 0x1064a96c0>, <Call l.3 at 0x106c452d0>)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 731, in _check_file\r\n    check_astroid_module(ast_node)\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 950, in check_astroid_module\r\n    retval = self._check_astroid_module(\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 1000, in _check_astroid_module\r\n    walker.walk(node)\r\n  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 93, in walk\r\n    self.walk(child)\r\n  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 93, in walk\r\n    self.walk(child)\r\n  File \"/Users/.../pylint/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/Users/.../pylint/pylint/checkers/classes/special_methods_checker.py\", line 170, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/Users/.../pylint/pylint/checkers/classes/special_methods_checker.py\", line 31, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/Users/.../astroid/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1752, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/Users/.../astroid/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/Users/.../astroid/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/Users/.../astroid/astroid/brain/brain_builtin_inference.py\", line 948, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nKeyError: 'team_slug'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 688, in _check_files\r\n    self._check_file(get_ast, check_astroid_module, file)\r\n  File \"/Users/.../pylint/pylint/lint/pylinter.py\", line 733, in _check_file\r\n    raise astroid.AstroidError from e\r\nastroid.exceptions.AstroidError\r\n```\r\n***\r\ncc @DanielNoord in #1602 \r\nfound by pylint primer \ud83d\ude80 \n",
    "input": " \n     try:\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n    except IndexError:\n         # If there is an IndexError there are too few arguments to interpolate\n         return iter([util.Uninferable])\n ",
    "output": " \n     try:\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (IndexError, KeyError):\n         # If there is an IndexError there are too few arguments to interpolate\n         return iter([util.Uninferable])\n "
  },
  {
    "instruction": "Pyreverse regression after #857 (astroid 2.5)\n### Steps to reproduce\r\n1. Checkout pylint's source (which contains pyreverse)\r\n1. cd `<pylint checkout>` \r\n2. Run `source .tox/py39/bin/activate` or similar (you may need to run a tox session first)\r\n3. Ensure you have `astroid` ac2b173bc8acd2d08f6b6ffe29dd8cda0b2c8814 or later\r\n4. Ensure you have installed `astroid` (`python3 -m pip install -e <path-to-astroid>`) as dependencies may be different\r\n4. Run `pyreverse --output png --project test tests/data`\r\n\r\n### Current behaviour\r\nA `ModuleNotFoundError` exception is raised.\r\n\r\n```\r\n$ pyreverse --output png --project test tests/data\r\nparsing tests/data/__init__.py...\r\nparsing /opt/contrib/pylint/pylint/tests/data/suppliermodule_test.py...\r\nparsing /opt/contrib/pylint/pylint/tests/data/__init__.py...\r\nparsing /opt/contrib/pylint/pylint/tests/data/clientmodule_test.py...\r\nTraceback (most recent call last):\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/bin/pyreverse\", line 8, in <module>\r\n    sys.exit(run_pyreverse())\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/__init__.py\", line 39, in run_pyreverse\r\n    PyreverseRun(sys.argv[1:])\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/main.py\", line 201, in __init__\r\n    sys.exit(self.run(args))\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/main.py\", line 219, in run\r\n    diadefs = handler.get_diadefs(project, linker)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/diadefslib.py\", line 236, in get_diadefs\r\n    diagrams = DefaultDiadefGenerator(linker, self).visit(project)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 210, in visit\r\n    self.visit(local_node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 207, in visit\r\n    methods[0](node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/diadefslib.py\", line 162, in visit_module\r\n    self.linker.visit(node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 210, in visit\r\n    self.visit(local_node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/utils.py\", line 207, in visit\r\n    methods[0](node)\r\n  File \"/opt/contrib/pylint/pylint/.tox/py39/lib/python3.9/site-packages/pylint/pyreverse/inspector.py\", line 257, in visit_importfrom\r\n    relative = astroid.modutils.is_relative(basename, context_file)\r\n  File \"/opt/contrib/pylint/astroid/astroid/modutils.py\", line 581, in is_relative\r\n    parent_spec = importlib.util.find_spec(name, from_file)\r\n  File \"/usr/local/Cellar/python@3.9/3.9.2_4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/util.py\", line 94, in find_spec\r\n    parent = __import__(parent_name, fromlist=['__path__'])\r\nModuleNotFoundError: No module named 'pylint.tests'\r\n```\r\n\r\n### Expected behaviour\r\nNo exception should be raised. Prior to #857 no exception was raised.\r\n\r\n```\r\n$ pyreverse --output png --project test tests/data\r\nparsing tests/data/__init__.py...\r\nparsing /opt/contributing/pylint/tests/data/suppliermodule_test.py...\r\nparsing /opt/contributing/pylint/tests/data/__init__.py...\r\nparsing /opt/contributing/pylint/tests/data/clientmodule_test.py...\r\n```\r\n\r\n### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\r\n`2.6.0-dev0` (cab9b08737ed7aad2a08ce90718c67155fa5c4a0)\r\n\n",
    "input": "     new_paths = _cached_set_diff(req_paths, cached_paths)\n     for entry_path in new_paths:\n         try:\n            pic[entry_path] = zipimport.zipimporter(  # pylint: disable=no-member\n                entry_path\n            )\n        except zipimport.ZipImportError:  # pylint: disable=no-member\n             continue\n     return {\n         key: value\n         for key, value in pic.items()\n        if isinstance(value, zipimport.zipimporter)  # pylint: disable=no-member\n     }\n \n \ndiff --git a/astroid/manager.py b/astroid/manager.py\n             except ValueError:\n                 continue\n             try:\n                importer = zipimport.zipimporter(  # pylint: disable=no-member\n                    eggpath + ext\n                )\n                 zmodname = resource.replace(os.path.sep, \".\")\n                 if importer.is_package(resource):\n                     zmodname = zmodname + \".__init__\"\ndiff --git a/astroid/modutils.py b/astroid/modutils.py\n # Copyright (c) 2020 hippo91 <guillaume.peillex@gmail.com>\n # Copyright (c) 2020 Peter Kolbus <peter.kolbus@gmail.com>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\n # We disable the import-error so pylint can work without distutils installed.\n # pylint: disable=no-name-in-module,useless-suppression\n \n import importlib.util\n import itertools\n import os\n         from_file = os.path.dirname(from_file)\n     if from_file in sys.path:\n         return False\n    name = os.path.basename(from_file)\n    file_path = os.path.dirname(from_file)\n    parent_spec = importlib.util.find_spec(name, from_file)\n    while parent_spec is None and len(file_path) > 0:\n        name = os.path.basename(file_path) + \".\" + name\n        file_path = os.path.dirname(file_path)\n        parent_spec = importlib.util.find_spec(name, from_file)\n\n    if parent_spec is None:\n        return False\n\n    submodule_spec = importlib.util.find_spec(\n        name + \".\" + modname.split(\".\")[0], parent_spec.submodule_search_locations\n     )\n    return submodule_spec is not None\n \n \n # internal only functions #####################################################",
    "output": "     new_paths = _cached_set_diff(req_paths, cached_paths)\n     for entry_path in new_paths:\n         try:\n            pic[entry_path] = zipimport.zipimporter(entry_path)\n        except zipimport.ZipImportError:\n             continue\n     return {\n         key: value\n         for key, value in pic.items()\n        if isinstance(value, zipimport.zipimporter)\n     }\n \n \ndiff --git a/astroid/manager.py b/astroid/manager.py\n             except ValueError:\n                 continue\n             try:\n                importer = zipimport.zipimporter(eggpath + ext)\n                 zmodname = resource.replace(os.path.sep, \".\")\n                 if importer.is_package(resource):\n                     zmodname = zmodname + \".__init__\"\ndiff --git a/astroid/modutils.py b/astroid/modutils.py\n # Copyright (c) 2020 hippo91 <guillaume.peillex@gmail.com>\n # Copyright (c) 2020 Peter Kolbus <peter.kolbus@gmail.com>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Andreas Finkler <andi.finkler@gmail.com>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\n # We disable the import-error so pylint can work without distutils installed.\n # pylint: disable=no-name-in-module,useless-suppression\n \nimport importlib\nimport importlib.machinery\n import importlib.util\n import itertools\n import os\n         from_file = os.path.dirname(from_file)\n     if from_file in sys.path:\n         return False\n    return bool(\n        importlib.machinery.PathFinder.find_spec(\n            modname.split(\".\", maxsplit=1)[0], [from_file]\n        )\n     )\n \n \n # internal only functions #####################################################"
  },
  {
    "instruction": "ImportError: cannot import name 'Statement' from 'astroid.node_classes' \n### Steps to reproduce\r\n\r\n1. run pylint <some_file>\r\n\r\n\r\n### Current behavior\r\n\r\n```python\r\nexception: Traceback (most recent call last):\r\n  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/__main__.py\", line 9, in <module>\r\n    pylint.run_pylint()\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/__init__.py\", line 24, in run_pylint\r\n    PylintRun(sys.argv[1:])\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/lint/run.py\", line 331, in __init__\r\n    linter.load_plugin_modules(plugins)\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/pylint/lint/pylinter.py\", line 551, in load_plugin_modules\r\n    module = astroid.modutils.load_module_from_name(modname)\r\n  File \"/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/astroid/modutils.py\", line 218, in load_module_from_name\r\n    return importlib.import_module(dotted_name)\r\n  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 855, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\r\n  File \"/home/user/folder/check_mk/tests/testlib/pylint_checker_cmk_module_layers.py\", line 14, in <module>\r\n    from astroid.node_classes import Import, ImportFrom, Statement  # type: ignore[import]\r\nImportError: cannot import name 'Statement' from 'astroid.node_classes' (/home/user/folder/check_mk/.venv/lib/python3.9/site-packages/astroid/node_classes.py)\r\n```\r\n\r\n### Expected behavior\r\nNo exception\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.7.3\r\npylint 2.10.2\r\nastroid 2.7.3\r\nPython 3.9.5 (default, May 11 2021, 08:20:37) \n",
    "input": "     Set,\n     Slice,\n     Starred,\n     Subscript,\n     TryExcept,\n     TryFinally,\n     SetComp,\n     builtin_lookup,\n     function_to_method,\n )\n \n _BaseContainer = BaseContainer  # TODO Remove for astroid 3.0\n     \"FunctionDef\",\n     \"function_to_method\",\n     \"GeneratorExp\",\n     \"Global\",\n     \"If\",\n     \"IfExp\",\n     \"SetComp\",\n     \"Slice\",\n     \"Starred\",\n     \"Subscript\",\n     \"TryExcept\",\n     \"TryFinally\",",
    "output": "     Set,\n     Slice,\n     Starred,\n    Statement,\n     Subscript,\n     TryExcept,\n     TryFinally,\n     SetComp,\n     builtin_lookup,\n     function_to_method,\n    get_wrapping_class,\n )\n \n _BaseContainer = BaseContainer  # TODO Remove for astroid 3.0\n     \"FunctionDef\",\n     \"function_to_method\",\n     \"GeneratorExp\",\n    \"get_wrapping_class\",\n     \"Global\",\n     \"If\",\n     \"IfExp\",\n     \"SetComp\",\n     \"Slice\",\n     \"Starred\",\n    \"Statement\",\n     \"Subscript\",\n     \"TryExcept\",\n     \"TryFinally\","
  },
  {
    "instruction": "astroid has an undeclared dependency on setuptools.\nThe dependency is here: https://github.com/PyCQA/astroid/blob/1342591e2beb955a377e4486e5595478f79789e8/astroid/__pkginfo__.py#L29\n\nThe lack of declaration is here: https://github.com/PyCQA/astroid/blob/1342591e2beb955a377e4486e5595478f79789e8/setup.cfg#L37-L41\n",
    "input": " # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\n \n\"\"\"astroid packaging information\"\"\"\n\nfrom pkg_resources import DistributionNotFound, get_distribution\n\ntry:\n    __version__ = get_distribution(\"astroid\").version\nexcept DistributionNotFound:\n    __version__ = \"2.5.7+\"\n\n version = __version__\ndiff --git a/astroid/helpers.py b/astroid/helpers.py\n # Copyright (c) 2020 Simon Hewitt <si@sjhewitt.co.uk>\n # Copyright (c) 2020 Bryce Guinta <bryce.guinta@protonmail.com>\n # Copyright (c) 2020 Ram Rachum <ram@rachum.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\ndiff --git a/astroid/node_classes.py b/astroid/node_classes.py\n # Copyright (c) 2019 kavins14 <kavinsingh@hotmail.com>\n # Copyright (c) 2020 Raphael Gaschignard <raphael@rtpg.co>\n # Copyright (c) 2020 Bryce Guinta <bryce.guinta@protonmail.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n # Copyright (c) 2021 Federico Bond <federicobond@gmail.com>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\ndiff --git a/astroid/transforms.py b/astroid/transforms.py\n # Copyright (c) 2015-2016, 2018 Claudiu Popa <pcmanticore@gmail.com>\n # Copyright (c) 2016 Ceridwen <ceridwenv@gmail.com>\n # Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\ndiff --git a/script/bump_changelog.py b/script/bump_changelog.py\nnew file mode 100644\ndiff --git a/setup.py b/setup.py\n from setuptools import setup\n \nsetup(use_scm_version=True)",
    "output": " # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\n \n__version__ = \"2.6.0-dev0\"\n version = __version__\ndiff --git a/astroid/helpers.py b/astroid/helpers.py\n # Copyright (c) 2020 Simon Hewitt <si@sjhewitt.co.uk>\n # Copyright (c) 2020 Bryce Guinta <bryce.guinta@protonmail.com>\n # Copyright (c) 2020 Ram Rachum <ram@rachum.com>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\ndiff --git a/astroid/node_classes.py b/astroid/node_classes.py\n # Copyright (c) 2019 kavins14 <kavinsingh@hotmail.com>\n # Copyright (c) 2020 Raphael Gaschignard <raphael@rtpg.co>\n # Copyright (c) 2020 Bryce Guinta <bryce.guinta@protonmail.com>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n # Copyright (c) 2021 Federico Bond <federicobond@gmail.com>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\ndiff --git a/astroid/transforms.py b/astroid/transforms.py\n # Copyright (c) 2015-2016, 2018 Claudiu Popa <pcmanticore@gmail.com>\n # Copyright (c) 2016 Ceridwen <ceridwenv@gmail.com>\n # Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n# Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n \n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/master/LICENSE\ndiff --git a/script/bump_changelog.py b/script/bump_changelog.py\nnew file mode 100644\n\"\"\"\nThis script permits to upgrade the changelog in astroid or pylint when releasing a version.\n\"\"\"\nimport argparse\nfrom datetime import datetime\nfrom pathlib import Path\n\nDEFAULT_CHANGELOG_PATH = Path(\"ChangeLog\")\nerr = \"in the changelog, fix that first!\"\nTBA_ERROR_MSG = \"More than one release date 'TBA' %s\" % err\nNEW_VERSION_ERROR_MSG = \"The text for this version '{version}' did not exists %s\" % err\nNEXT_VERSION_ERROR_MSG = (\n    \"The text for the next version '{version}' already exists %s\" % err\n)\n\nTODAY = datetime.now()\nWHATS_NEW_TEXT = \"What's New in astroid\"\nFULL_WHATS_NEW_TEXT = WHATS_NEW_TEXT + \" {version}?\"\nRELEASE_DATE_TEXT = \"Release Date: TBA\"\nNEW_RELEASE_DATE_MESSAGE = \"Release Date: {}\".format(TODAY.strftime(\"%Y-%m-%d\"))\n\n\ndef main() -> None:\n    parser = argparse.ArgumentParser(add_help=__doc__)\n    parser.add_argument(\"version\", help=\"The version we want to release\")\n    args = parser.parse_args()\n    if \"dev\" not in args.version:\n        version = args.version\n        next_version = get_next_version(version)\n        run(version, next_version)\n\n\ndef get_next_version(version: str) -> str:\n    new_version = version.split(\".\")\n    patch = new_version[2]\n    reminder = None\n    if \"-\" in patch:\n        patch, reminder = patch.split(\"-\")\n    patch = str(int(patch) + 1)\n    new_version[2] = patch if reminder is None else f\"{patch}-{reminder}\"\n    return \".\".join(new_version)\n\n\ndef run(version: str, next_version: str) -> None:\n    with open(DEFAULT_CHANGELOG_PATH) as f:\n        content = f.read()\n    content = transform_content(content, version, next_version)\n    with open(DEFAULT_CHANGELOG_PATH, \"w\") as f:\n        f.write(content)\n\n\ndef transform_content(content: str, version: str, next_version: str) -> str:\n    wn_new_version = FULL_WHATS_NEW_TEXT.format(version=version)\n    wn_next_version = FULL_WHATS_NEW_TEXT.format(version=next_version)\n    # There is only one field where the release date is TBA\n    assert content.count(RELEASE_DATE_TEXT) == 1, TBA_ERROR_MSG\n    # There is already a release note for the version we want to release\n    assert content.count(wn_new_version) == 1, NEW_VERSION_ERROR_MSG.format(\n        version=version\n    )\n    # There is no release notes for the next version\n    assert content.count(wn_next_version) == 0, NEXT_VERSION_ERROR_MSG.format(\n        version=next_version\n    )\n    index = content.find(WHATS_NEW_TEXT)\n    content = content.replace(RELEASE_DATE_TEXT, NEW_RELEASE_DATE_MESSAGE)\n    end_content = content[index:]\n    content = content[:index]\n    content += wn_next_version + \"\\n\"\n    content += \"=\" * len(wn_next_version) + \"\\n\"\n    content += RELEASE_DATE_TEXT + \"\\n\" * 4\n    content += end_content\n    return content\n\n\nif __name__ == \"__main__\":\n    main()\ndiff --git a/setup.py b/setup.py\n from setuptools import setup\n \nsetup()"
  },
  {
    "instruction": "astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py\n### Steps to reproduce\r\n> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04\r\n\r\n> Update 2022-01-04: Corrected repro steps and added more environment details\r\n\r\n1. Set up simple repo with following structure (all files can be empty):\r\n```\r\nroot_dir/\r\n|--src/\r\n|----project/ # Notice the missing __init__.py\r\n|------file.py # It can be empty, but I added `import os` at the top\r\n|----__init__.py\r\n```\r\n2. Open a command prompt\r\n3. `cd root_dir`\r\n4. `python -m venv venv`\r\n5. `venv/Scripts/activate`\r\n6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2\r\n7. `pylint src/project` # Updated from `pylint src`\r\n8. Observe failure:\r\n```\r\nsrc\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:\r\n```\r\n\r\n### Current behavior\r\nFails with `src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:`\r\n\r\n### Expected behavior\r\nDoes not fail with error.\r\n> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.9.1\r\n\r\n`python 3.9.1`\r\n`pylint 2.12.2 `\r\n\r\n\r\n\r\nThis issue has been observed with astroid `2.9.1` and `2.9.2`\n",
    "input": "     if os.path.normcase(real_filename).startswith(path_to_check):\n         importable_path = real_filename\n \n     if importable_path:\n         base_path = os.path.splitext(importable_path)[0]\n         relative_base_path = base_path[len(path_to_check) :]\n \n def modpath_from_file_with_callback(filename, path=None, is_package_cb=None):\n     filename = os.path.expanduser(_path_from_filename(filename))\n     for pathname in itertools.chain(\n        path or [], map(_cache_normalize_path, sys.path), sys.path\n     ):\n         if not pathname:\n             continue",
    "output": "     if os.path.normcase(real_filename).startswith(path_to_check):\n         importable_path = real_filename\n \n    # if \"var\" in path_to_check:\n    #     breakpoint()\n\n     if importable_path:\n         base_path = os.path.splitext(importable_path)[0]\n         relative_base_path = base_path[len(path_to_check) :]\n \n def modpath_from_file_with_callback(filename, path=None, is_package_cb=None):\n     filename = os.path.expanduser(_path_from_filename(filename))\n    paths_to_check = sys.path.copy()\n    if path:\n        paths_to_check += path\n     for pathname in itertools.chain(\n        paths_to_check, map(_cache_normalize_path, paths_to_check)\n     ):\n         if not pathname:\n             continue"
  },
  {
    "instruction": "Unhandled AttributeError during str.format template evaluation\n### Steps to reproduce\r\n\r\n1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\r\n\r\n```py\r\ndaniel_age = 12\r\n\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\r\n```\r\n\r\n### Current behavior\r\n\r\n1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\r\n\r\n### Expected behavior\r\n\r\n1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n`2.13.0-dev0`\r\n\r\nRelates to pycqa/pylint#7939.\nUnhandled AttributeError during str.format template evaluation\n### Steps to reproduce\r\n\r\n1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\r\n\r\n```py\r\ndaniel_age = 12\r\n\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\r\n```\r\n\r\n### Current behavior\r\n\r\n1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\r\n\r\n### Expected behavior\r\n\r\n1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n`2.13.0-dev0`\r\n\r\nRelates to pycqa/pylint#7939.\n",
    "input": " \n     try:\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (IndexError, KeyError, TypeError, ValueError):\n         # IndexError: there are too few arguments to interpolate\n         # TypeError: Unsupported format string\n         # ValueError: Unknown format code",
    "output": " \n     try:\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (AttributeError, IndexError, KeyError, TypeError, ValueError):\n        # AttributeError: processing a replacement field using the arguments failed\n         # IndexError: there are too few arguments to interpolate\n         # TypeError: Unsupported format string\n         # ValueError: Unknown format code"
  },
  {
    "instruction": "getitem does not infer the actual unpacked value\nWhen trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:\r\n\r\n- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. \r\n- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.\r\n\r\n\r\nHere is a short reproducer;\r\n\r\n```py\r\nfrom astroid import parse\r\n\r\n\r\nsource = \"\"\"\r\nX = {\r\n    'A': 'B'\r\n}\r\n\r\nY = {\r\n    **X\r\n}\r\n\r\nKEY = 'A'\r\n\"\"\"\r\n\r\ntree = parse(source)\r\n\r\nfirst_dict = tree.body[0].value\r\nsecond_dict = tree.body[1].value\r\nkey = tree.body[2].value\r\n\r\nprint(f'{first_dict.getitem(key).value = }')\r\nprint(f'{second_dict.getitem(key).value = }')\r\n\r\n\r\n```\r\n\r\nThe current output;\r\n\r\n```\r\n $ python t1.py                                                                                                 3ms\r\nfirst_dict.getitem(key).value = 'B'\r\nTraceback (most recent call last):\r\n  File \"/home/isidentical/projects/astroid/t1.py\", line 23, in <module>\r\n    print(f'{second_dict.getitem(key).value = }')\r\n  File \"/home/isidentical/projects/astroid/astroid/nodes/node_classes.py\", line 2254, in getitem\r\n    return value.getitem(index, context)\r\nAttributeError: 'Name' object has no attribute 'getitem'\r\n```\r\n\r\nExpeceted output;\r\n```\r\n $ python t1.py                                                                                                 4ms\r\nfirst_dict.getitem(key).value = 'B'\r\nsecond_dict.getitem(key).value = 'B'\r\n\r\n```\r\n\n",
    "input": "         \"\"\"\n         return [key for (key, _) in self.items]\n \n    def getitem(self, index, context=None):\n         \"\"\"Get an item from this node.\n \n         :param index: The node to use as a subscript index.\n        :type index: Const or Slice\n \n         :raises AstroidTypeError: When the given index cannot be used as a\n             subscript index, or if this node is not subscriptable.\n         :raises AstroidIndexError: If the given index does not exist in the\n             dictionary.\n         \"\"\"\n         for key, value in self.items:\n             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n             if isinstance(key, DictUnpack):\n                 try:\n                    return value.getitem(index, context)\n                 except (AstroidTypeError, AstroidIndexError):\n                     continue\n             for inferredkey in key.infer(context):\n                 if inferredkey is util.Uninferable:\n                     continue",
    "output": "         \"\"\"\n         return [key for (key, _) in self.items]\n \n    def getitem(\n        self, index: Const | Slice, context: InferenceContext | None = None\n    ) -> NodeNG:\n         \"\"\"Get an item from this node.\n \n         :param index: The node to use as a subscript index.\n \n         :raises AstroidTypeError: When the given index cannot be used as a\n             subscript index, or if this node is not subscriptable.\n         :raises AstroidIndexError: If the given index does not exist in the\n             dictionary.\n         \"\"\"\n        # pylint: disable-next=import-outside-toplevel; circular import\n        from astroid.helpers import safe_infer\n\n         for key, value in self.items:\n             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.\n             if isinstance(key, DictUnpack):\n                inferred_value = safe_infer(value, context)\n                if not isinstance(inferred_value, Dict):\n                    continue\n\n                 try:\n                    return inferred_value.getitem(index, context)\n                 except (AstroidTypeError, AstroidIndexError):\n                     continue\n\n             for inferredkey in key.infer(context):\n                 if inferredkey is util.Uninferable:\n                     continue"
  },
  {
    "instruction": "Regression in Astroid version 2.15.7 in handling subscriptable type parameters\nAstroid version 2.15.7 fails to correctly handle a subscriptable type parameter  most likely due to the change in this [PR](https://github.com/pylint-dev/astroid/pull/2239). \r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nfrom collections.abc import Mapping\r\nfrom typing import Generic, TypeVar, TypedDict\r\nfrom dataclasses import dataclass\r\n\r\nclass Identity(TypedDict):\r\n    \"\"\"It's the identity.\"\"\"\r\n\r\n    name: str\r\n\r\nT = TypeVar(\"T\", bound=Mapping)\r\n\r\n@dataclass\r\nclass Animal(Generic[T]):\r\n    \"\"\"It's an animal.\"\"\"\r\n\r\n    identity: T\r\n\r\nclass Dog(Animal[Identity]):\r\n    \"\"\"It's a Dog.\"\"\"\r\n\r\ndog = Dog(identity=Identity(name=\"Dog\"))\r\nprint(dog.identity[\"name\"])\r\n```\r\n\r\n### Current behavior\r\nPylint (running Astroid 2.15.7) gives the following error for the example above:\r\n```\r\nE1136: Value 'dog.identity' is unsubscriptable (unsubscriptable-object)\r\n```\r\n### Expected behavior\r\nAstroid should correctly handle a subscriptable type parameter.\r\n\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.15.7\n",
    "input": " from astroid.builder import AstroidBuilder, _extract_single_node\n from astroid.const import PY39_PLUS, PY312_PLUS\n from astroid.exceptions import (\n     AttributeInferenceError,\n     InferenceError,\n     UseInferenceDefault,\n         raise UseInferenceDefault\n \n     typename = node.args[0].as_string().strip(\"'\")\n    node = ClassDef(\n        name=typename,\n        lineno=node.lineno,\n        col_offset=node.col_offset,\n        parent=node.parent,\n        end_lineno=node.end_lineno,\n        end_col_offset=node.end_col_offset,\n    )\n     return node.infer(context=context_itton)\n \n ",
    "output": " from astroid.builder import AstroidBuilder, _extract_single_node\n from astroid.const import PY39_PLUS, PY312_PLUS\n from astroid.exceptions import (\n    AstroidSyntaxError,\n     AttributeInferenceError,\n     InferenceError,\n     UseInferenceDefault,\n         raise UseInferenceDefault\n \n     typename = node.args[0].as_string().strip(\"'\")\n    try:\n        node = extract_node(TYPING_TYPE_TEMPLATE.format(typename))\n    except AstroidSyntaxError as exc:\n        raise InferenceError from exc\n     return node.infer(context=context_itton)\n \n "
  },
  {
    "instruction": "infer_stmts cannot infer multiple uses of the same AssignName\nGiven multiple assignments to the same target which both reference the same AssignName, infer_stmts fails for subsequent attempts after the first.\r\n\r\n### Steps to reproduce\r\n\r\nThis appears to be a minimum working example, removing any part removes the effect:\r\n\r\n```python\r\nfails = astroid.extract_node(\"\"\"\r\n    pair = [1, 2]\r\n    ex = pair[0]\r\n    if 1 + 1 == 2:\r\n        ex = pair[1]\r\n    ex\r\n\"\"\")\r\nprint(list(fails.infer()))\r\n# [<Const.int l.2 at 0x...>, Uninferable]\r\n```\r\n\r\nFor some context, I originally saw this with attributes on an imported module, i.e.\r\n\r\n```python\r\nimport mod\r\nex = mod.One()\r\n# later ... or in some branch\r\nex = mod.Two()\r\n```\r\n\r\n### Current behavior\r\n\r\nSee above.\r\n\r\n### Expected behavior\r\n\r\nInlining the variable or switching to a different name works fine:\r\n\r\n```python\r\nworks = astroid.extract_node(\"\"\"\r\n    # pair = [1, 2]\r\n    ex = [1, 2][0]\r\n    if 1 + 1 == 2:\r\n        ex = [1, 2][1]\r\n    ex\r\n\"\"\")\r\nprint(list(works.infer()))\r\n# [<Const.int l.3 at 0x...>, <Const.int l.5 at 0x...>]\r\n\r\nworks = astroid.extract_node(\"\"\"\r\n    first = [1, 2]\r\n    second = [1, 2]\r\n    ex = first[0]\r\n    if 1 + 1 == 2:\r\n        ex = second[1]\r\n    ex\r\n\"\"\")\r\nprint(list(works.infer()))\r\n# [<Const.int l.2 at 0x...>, <Const.int l.3 at 0x...>]\r\n```\r\n\r\nI would expect that the first failing example would work similarly. This (only) worked\r\nin astroid 2.5 and appears to have been \"broken\" by the revert of cc3bfc5 in 03d15b0 (astroid 2.5.1 and above).\r\n\r\n### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\r\n\r\n```\r\n$ python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"\r\n2.5-dev\r\n$ git rev-parse HEAD\r\n03d15b0f32f7d7c9b2cb062b9321e531bd954344\r\n```\r\n\n",
    "input": "         starts with the same context but diverge as each side is inferred\n         so the InferenceContext will need be cloned\"\"\"\n         # XXX copy lookupname/callcontext ?\n        clone = InferenceContext(self.path, inferred=self.inferred)\n         clone.callcontext = self.callcontext\n         clone.boundnode = self.boundnode\n         clone.extra_context = self.extra_context",
    "output": "         starts with the same context but diverge as each side is inferred\n         so the InferenceContext will need be cloned\"\"\"\n         # XXX copy lookupname/callcontext ?\n        clone = InferenceContext(self.path.copy(), inferred=self.inferred.copy())\n         clone.callcontext = self.callcontext\n         clone.boundnode = self.boundnode\n         clone.extra_context = self.extra_context"
  },
  {
    "instruction": "\"TypeError: unsupported format string passed to NoneType.__format__\" while running type inference in version 2.12.x\n### Steps to reproduce\r\n\r\nI have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is \"valid\", it runs in production here.\r\n\r\n### Current behavior\r\n\r\nWhen running pylint on some code, I get this exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 183, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 42, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1749, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py\", line 956, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nTypeError: unsupported format string passed to NoneType.__format__\r\n```\r\n\r\n### Expected behavior\r\n\r\nTypeError exception should not happen\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.12.10,\r\n2.12.12\n",
    "input": " \n     try:\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (IndexError, KeyError):\n        # If there is an IndexError there are too few arguments to interpolate\n         return iter([util.Uninferable])\n \n     return iter([nodes.const_factory(formatted_string)])",
    "output": " \n     try:\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n    except (IndexError, KeyError, TypeError, ValueError):\n        # IndexError: there are too few arguments to interpolate\n        # TypeError: Unsupported format string\n        # ValueError: Unknown format code\n         return iter([util.Uninferable])\n \n     return iter([nodes.const_factory(formatted_string)])"
  },
  {
    "instruction": "Yield self is inferred to be of a mistaken type \n### Steps to reproduce\r\n\r\n1. Run the following\r\n```\r\nimport astroid\r\n\r\n\r\nprint(list(astroid.parse('''\r\nimport contextlib\r\n\r\nclass A:\r\n    @contextlib.contextmanager\r\n    def get(self):\r\n        yield self\r\n\r\nclass B(A):\r\n    def play():\r\n        pass\r\n\r\nwith B().get() as b:\r\n    b.play()\r\n''').ilookup('b')))\r\n```\r\n\r\n### Current behavior\r\n```Prints [<Instance of .A at 0x...>]```\r\n\r\n### Expected behavior\r\n```Prints [<Instance of .B at 0x...>]```\r\n\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.6.2\n",
    "input": " import collections\n \n from astroid import context as contextmod\nfrom astroid import util\n from astroid.const import BUILTINS, PY310_PLUS\n from astroid.exceptions import (\n     AstroidTypeError,\n \n     special_attributes = util.lazy_descriptor(objectmodel.GeneratorModel)\n \n    def __init__(self, parent=None):\n         super().__init__()\n         self.parent = parent\n \n     def callable(self):\n         return False\ndiff --git a/astroid/protocols.py b/astroid/protocols.py\n             # It doesn't interest us.\n             raise InferenceError(node=func)\n \n        # Get the first yield point. If it has multiple yields,\n        # then a RuntimeError will be raised.\n \n        possible_yield_points = func.nodes_of_class(nodes.Yield)\n        # Ignore yields in nested functions\n        yield_point = next(\n            (node for node in possible_yield_points if node.scope() == func), None\n        )\n        if yield_point:\n            if not yield_point.value:\n                const = nodes.Const(None)\n                const.parent = yield_point\n                const.lineno = yield_point.lineno\n                yield const\n            else:\n                yield from yield_point.value.infer(context=context)\n     elif isinstance(inferred, bases.Instance):\n         try:\n             enter = next(inferred.igetattr(\"__enter__\", context=context))\ndiff --git a/astroid/scoped_nodes.py b/astroid/scoped_nodes.py\n         \"\"\"\n         return bool(next(self._get_yield_nodes_skip_lambdas(), False))\n \n     def infer_call_result(self, caller=None, context=None):\n         \"\"\"Infer what the function returns when called.\n \n                 generator_cls = bases.AsyncGenerator\n             else:\n                 generator_cls = bases.Generator\n            result = generator_cls(self)\n             yield result\n             return\n         # This is really a gigantic hack to work around metaclass generators",
    "output": " import collections\n \n from astroid import context as contextmod\nfrom astroid import decorators, util\n from astroid.const import BUILTINS, PY310_PLUS\n from astroid.exceptions import (\n     AstroidTypeError,\n \n     special_attributes = util.lazy_descriptor(objectmodel.GeneratorModel)\n \n    def __init__(self, parent=None, generator_initial_context=None):\n         super().__init__()\n         self.parent = parent\n        self._call_context = contextmod.copy_context(generator_initial_context)\n\n    @decorators.cached\n    def infer_yield_types(self):\n        yield from self.parent.infer_yield_result(self._call_context)\n \n     def callable(self):\n         return False\ndiff --git a/astroid/protocols.py b/astroid/protocols.py\n             # It doesn't interest us.\n             raise InferenceError(node=func)\n \n        yield next(inferred.infer_yield_types())\n \n     elif isinstance(inferred, bases.Instance):\n         try:\n             enter = next(inferred.igetattr(\"__enter__\", context=context))\ndiff --git a/astroid/scoped_nodes.py b/astroid/scoped_nodes.py\n         \"\"\"\n         return bool(next(self._get_yield_nodes_skip_lambdas(), False))\n \n    def infer_yield_result(self, context=None):\n        \"\"\"Infer what the function yields when called\n\n        :returns: What the function yields\n        :rtype: iterable(NodeNG or Uninferable) or None\n        \"\"\"\n        for yield_ in self.nodes_of_class(node_classes.Yield):\n            if yield_.value is None:\n                const = node_classes.Const(None)\n                const.parent = yield_\n                const.lineno = yield_.lineno\n                yield const\n            elif yield_.scope() == self:\n                yield from yield_.value.infer(context=context)\n\n     def infer_call_result(self, caller=None, context=None):\n         \"\"\"Infer what the function returns when called.\n \n                 generator_cls = bases.AsyncGenerator\n             else:\n                 generator_cls = bases.Generator\n            result = generator_cls(self, generator_initial_context=context)\n             yield result\n             return\n         # This is really a gigantic hack to work around metaclass generators"
  },
  {
    "instruction": "Regression in Astroid version 2.15.7 in handling subscriptable type parameters\nAstroid version 2.15.7 fails to correctly handle a subscriptable type parameter  most likely due to the change in this [PR](https://github.com/pylint-dev/astroid/pull/2239). \r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nfrom collections.abc import Mapping\r\nfrom typing import Generic, TypeVar, TypedDict\r\nfrom dataclasses import dataclass\r\n\r\nclass Identity(TypedDict):\r\n    \"\"\"It's the identity.\"\"\"\r\n\r\n    name: str\r\n\r\nT = TypeVar(\"T\", bound=Mapping)\r\n\r\n@dataclass\r\nclass Animal(Generic[T]):\r\n    \"\"\"It's an animal.\"\"\"\r\n\r\n    identity: T\r\n\r\nclass Dog(Animal[Identity]):\r\n    \"\"\"It's a Dog.\"\"\"\r\n\r\ndog = Dog(identity=Identity(name=\"Dog\"))\r\nprint(dog.identity[\"name\"])\r\n```\r\n\r\n### Current behavior\r\nPylint (running Astroid 2.15.7) gives the following error for the example above:\r\n```\r\nE1136: Value 'dog.identity' is unsubscriptable (unsubscriptable-object)\r\n```\r\n### Expected behavior\r\nAstroid should correctly handle a subscriptable type parameter.\r\n\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.15.7\n",
    "input": " from astroid.builder import _extract_single_node\n from astroid.const import PY38_PLUS, PY39_PLUS\n from astroid.exceptions import (\n     AttributeInferenceError,\n     InferenceError,\n     UseInferenceDefault,\n         raise UseInferenceDefault\n \n     typename = node.args[0].as_string().strip(\"'\")\n    node = ClassDef(\n        name=typename,\n        lineno=node.lineno,\n        col_offset=node.col_offset,\n        parent=node.parent,\n        end_lineno=node.end_lineno,\n        end_col_offset=node.end_col_offset,\n    )\n     return node.infer(context=context_itton)\n \n ",
    "output": " from astroid.builder import _extract_single_node\n from astroid.const import PY38_PLUS, PY39_PLUS\n from astroid.exceptions import (\n    AstroidSyntaxError,\n     AttributeInferenceError,\n     InferenceError,\n     UseInferenceDefault,\n         raise UseInferenceDefault\n \n     typename = node.args[0].as_string().strip(\"'\")\n    try:\n        node = extract_node(TYPING_TYPE_TEMPLATE.format(typename))\n    except AstroidSyntaxError as exc:\n        raise InferenceError from exc\n     return node.infer(context=context_itton)\n \n "
  },
  {
    "instruction": "Implement new nodes for PEP 695: Type Parameter Syntax\nThere's a new syntax in python 3.12, we need to handle it before claiming we support 3.12, see https://docs.python.org/3.12/whatsnew/3.12.html#pep-695-type-parameter-syntax\n",
    "input": "diff --git a/astroid/brain/brain_typing.py b/astroid/brain/brain_typing.py\n \n from __future__ import annotations\n \n import typing\n from collections.abc import Iterator\n from functools import partial\n from typing import Final\n \n from astroid import context, extract_node, inference_tip\nfrom astroid.builder import _extract_single_node\nfrom astroid.const import PY39_PLUS\n from astroid.exceptions import (\n     AttributeInferenceError,\n     InferenceError,\n     \"\"\"\n     return (\n         isinstance(node.func, Name)\n        and node.func.name == \"_alias\"\n         and (\n             # _alias function works also for builtins object such as list and dict\n             isinstance(node.args[0], (Attribute, Name))\n \n     :param node: call node\n     :param context: inference context\n     \"\"\"\n     if (\n         not isinstance(node.parent, Assign)\n     return node.args[1].infer(context=ctx)\n \n \n AstroidManager().register_transform(\n     Call,\n     inference_tip(infer_typing_typevar_or_newtype),\n AstroidManager().register_transform(\n     Call, inference_tip(infer_special_alias), _looks_like_special_alias\n )\ndiff --git a/astroid/const.py b/astroid/const.py\n PY39_PLUS = sys.version_info >= (3, 9)\n PY310_PLUS = sys.version_info >= (3, 10)\n PY311_PLUS = sys.version_info >= (3, 11)\n \n WIN32 = sys.platform == \"win32\"\n \ndiff --git a/astroid/inference.py b/astroid/inference.py\n nodes.Lambda._infer = infer_end  # type: ignore[assignment]\n nodes.Const._infer = infer_end  # type: ignore[assignment]\n nodes.Slice._infer = infer_end  # type: ignore[assignment]\n \n \n def _infer_sequence_helper(\ndiff --git a/astroid/nodes/__init__.py b/astroid/nodes/__init__.py\n     NamedExpr,\n     NodeNG,\n     Nonlocal,\n     Pass,\n     Pattern,\n     Raise,\n     TryFinally,\n     TryStar,\n     Tuple,\n     UnaryOp,\n     Unknown,\n     While,\n     NamedExpr,\n     NodeNG,\n     Nonlocal,\n     Pass,\n     Pattern,\n     Raise,\n     TryFinally,\n     TryStar,\n     Tuple,\n     UnaryOp,\n     Unknown,\n     While,\n     \"NamedExpr\",\n     \"NodeNG\",\n     \"Nonlocal\",\n     \"Pass\",\n     \"Position\",\n     \"Raise\",\n     \"TryFinally\",\n     \"TryStar\",\n     \"Tuple\",\n     \"UnaryOp\",\n     \"Unknown\",\n     \"unpack_infer\",\ndiff --git a/astroid/nodes/as_string.py b/astroid/nodes/as_string.py\n         args += [n.accept(self) for n in node.keywords]\n         args_str = f\"({', '.join(args)})\" if args else \"\"\n         docs = self._docs_dedent(node.doc_node)\n         return \"\\n\\n{}class {}{}:{}\\n{}\\n\".format(\n             decorate, node.name, args_str, docs, self._stmt_list(node.body)\n         )\n         if node.returns:\n             return_annotation = \" -> \" + node.returns.as_string()\n             trailer = return_annotation + \":\"\n         def_format = \"\\n%s%s %s(%s)%s%s\\n%s\"\n         return def_format % (\n             decorate,\n         \"\"\"return an astroid.Nonlocal node as string\"\"\"\n         return f\"nonlocal {', '.join(node.names)}\"\n \n     def visit_pass(self, node) -> str:\n         \"\"\"return an astroid.Pass node as string\"\"\"\n         return \"pass\"\n             return f\"({node.elts[0].accept(self)}, )\"\n         return f\"({', '.join(child.accept(self) for child in node.elts)})\"\n \n     def visit_unaryop(self, node) -> str:\n         \"\"\"return an astroid.UnaryOp node as string\"\"\"\n         if node.op == \"not\":\ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n     ClassVar,\n     Literal,\n     Optional,\n    TypeVar,\n     Union,\n )\n \n     return isinstance(value, tuple(CONST_CLS))\n \n \n_NodesT = TypeVar(\"_NodesT\", bound=NodeNG)\n_BadOpMessageT = TypeVar(\"_BadOpMessageT\", bound=util.BadOperationMessage)\n \n AssignedStmtsPossibleNode = Union[\"List\", \"Tuple\", \"AssignName\", \"AssignAttr\", None]\n AssignedStmtsCall = Callable[\n         return name\n \n \n class Pass(_base_nodes.NoChildrenNode, _base_nodes.Statement):\n     \"\"\"Class representing an :class:`ast.Pass` node.\n \n         return _container_getitem(self, self.elts, index, context=context)\n \n \n class UnaryOp(NodeNG):\n     \"\"\"Class representing an :class:`ast.UnaryOp` node.\n \ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n     <FunctionDef.my_func l.2 at 0x7f23b2e71e10>\n     \"\"\"\n \n    _astroid_fields = (\"decorators\", \"args\", \"returns\", \"doc_node\", \"body\")\n     _multi_line_block_fields = (\"body\",)\n     returns = None\n \n         self.body: list[NodeNG] = []\n         \"\"\"The contents of the function body.\"\"\"\n \n         self.instance_attrs: dict[str, list[NodeNG]] = {}\n \n         super().__init__(\n         *,\n         position: Position | None = None,\n         doc_node: Const | None = None,\n     ):\n         \"\"\"Do some setup after initialisation.\n \n             Position of function keyword(s) and name.\n         :param doc_node:\n             The doc node associated with this node.\n         \"\"\"\n         self.args = args\n         self.body = body\n         self.type_comment_args = type_comment_args\n         self.position = position\n         self.doc_node = doc_node\n \n     @cached_property\n     def extra_decorators(self) -> list[node_classes.Call]:\n     return klass\n \n \nclass ClassDef(\n     _base_nodes.FilterStmtsBaseNode, LocalsDictNodeNG, _base_nodes.Statement\n ):\n     \"\"\"Class representing an :class:`ast.ClassDef` node.\n     # by a raw factories\n \n     # a dictionary of class instances attributes\n    _astroid_fields = (\"decorators\", \"bases\", \"keywords\", \"doc_node\", \"body\")  # name\n \n     decorators = None\n     \"\"\"The decorators that are applied to this class.\n         self.is_dataclass: bool = False\n         \"\"\"Whether this class is a dataclass.\"\"\"\n \n         super().__init__(\n             lineno=lineno,\n             col_offset=col_offset,\n         *,\n         position: Position | None = None,\n         doc_node: Const | None = None,\n     ) -> None:\n         if keywords is not None:\n             self.keywords = keywords\n         self._metaclass = metaclass\n         self.position = position\n         self.doc_node = doc_node\n \n     def _newstyle_impl(self, context: InferenceContext | None = None):\n         if context is None:\ndiff --git a/astroid/rebuilder.py b/astroid/rebuilder.py\n \n from astroid import nodes\n from astroid._ast import ParserModule, get_parser_module, parse_function_type_comment\nfrom astroid.const import IS_PYPY, PY38, PY39_PLUS, Context\n from astroid.manager import AstroidManager\n from astroid.nodes import NodeNG\n from astroid.nodes.utils import Position\n         def visit(self, node: ast.Constant, parent: NodeNG) -> nodes.Const:\n             ...\n \n         @overload\n         def visit(self, node: ast.Pass, parent: NodeNG) -> nodes.Pass:\n             ...\n         def visit(self, node: ast.Tuple, parent: NodeNG) -> nodes.Tuple:\n             ...\n \n         @overload\n         def visit(self, node: ast.UnaryOp, parent: NodeNG) -> nodes.UnaryOp:\n             ...\n             ],\n             position=self._get_position_info(node, newnode),\n             doc_node=self.visit(doc_ast_node, newnode),\n         )\n         return newnode\n \n             type_comment_args=type_comment_args,\n             position=self._get_position_info(node, newnode),\n             doc_node=self.visit(doc_ast_node, newnode),\n         )\n         self._global_names.pop()\n         return newnode\n             parent=parent,\n         )\n \n     def visit_pass(self, node: ast.Pass, parent: NodeNG) -> nodes.Pass:\n         \"\"\"Visit a Pass node by returning a fresh instance of it.\"\"\"\n         return nodes.Pass(\n         newnode.postinit([self.visit(child, newnode) for child in node.elts])\n         return newnode\n \n     def visit_unaryop(self, node: ast.UnaryOp, parent: NodeNG) -> nodes.UnaryOp:\n         \"\"\"Visit a UnaryOp node by returning a fresh instance of it.\"\"\"\n         newnode = nodes.UnaryOp(",
    "output": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/pylint-dev/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/pylint-dev/astroid/blob/main/CONTRIBUTORS.txt\n\nimport textwrap\n\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import AstroidBuilder\nfrom astroid.const import PY312_PLUS\nfrom astroid.manager import AstroidManager\n\n\ndef datetime_transform():\n    \"\"\"The datetime module was C-accelerated in Python 3.12, so we\n    lack a Python source.\"\"\"\n    return AstroidBuilder(AstroidManager()).string_build(\n        textwrap.dedent(\n            \"\"\"\n    class date: ...\n    class time: ...\n    class datetime(date): ...\n    class timedelta: ...\n    class tzinfo: ...\n    class timezone(tzinfo): ...\n    \"\"\"\n        )\n    )\n\n\nif PY312_PLUS:\n    register_module_extender(AstroidManager(), \"datetime\", datetime_transform)\ndiff --git a/astroid/brain/brain_typing.py b/astroid/brain/brain_typing.py\n \n from __future__ import annotations\n \nimport textwrap\n import typing\n from collections.abc import Iterator\n from functools import partial\n from typing import Final\n \n from astroid import context, extract_node, inference_tip\nfrom astroid.brain.helpers import register_module_extender\nfrom astroid.builder import AstroidBuilder, _extract_single_node\nfrom astroid.const import PY39_PLUS, PY312_PLUS\n from astroid.exceptions import (\n     AttributeInferenceError,\n     InferenceError,\n     \"\"\"\n     return (\n         isinstance(node.func, Name)\n        # TODO: remove _DeprecatedGenericAlias when Py3.14 min\n        and node.func.name in {\"_alias\", \"_DeprecatedGenericAlias\"}\n         and (\n             # _alias function works also for builtins object such as list and dict\n             isinstance(node.args[0], (Attribute, Name))\n \n     :param node: call node\n     :param context: inference context\n\n    # TODO: evaluate if still necessary when Py3.12 is minimum\n     \"\"\"\n     if (\n         not isinstance(node.parent, Assign)\n     return node.args[1].infer(context=ctx)\n \n \ndef _typing_transform():\n    return AstroidBuilder(AstroidManager()).string_build(\n        textwrap.dedent(\n            \"\"\"\n    class Generic:\n        @classmethod\n        def __class_getitem__(cls, item):  return cls\n    class ParamSpec: ...\n    class ParamSpecArgs: ...\n    class ParamSpecKwargs: ...\n    class TypeAlias: ...\n    class Type:\n        @classmethod\n        def __class_getitem__(cls, item):  return cls\n    class TypeVar:\n        @classmethod\n        def __class_getitem__(cls, item):  return cls\n    class TypeVarTuple: ...\n    \"\"\"\n        )\n    )\n\n\n AstroidManager().register_transform(\n     Call,\n     inference_tip(infer_typing_typevar_or_newtype),\n AstroidManager().register_transform(\n     Call, inference_tip(infer_special_alias), _looks_like_special_alias\n )\n\nif PY312_PLUS:\n    register_module_extender(AstroidManager(), \"typing\", _typing_transform)\ndiff --git a/astroid/const.py b/astroid/const.py\n PY39_PLUS = sys.version_info >= (3, 9)\n PY310_PLUS = sys.version_info >= (3, 10)\n PY311_PLUS = sys.version_info >= (3, 11)\nPY312_PLUS = sys.version_info >= (3, 12)\n \n WIN32 = sys.platform == \"win32\"\n \ndiff --git a/astroid/inference.py b/astroid/inference.py\n nodes.Lambda._infer = infer_end  # type: ignore[assignment]\n nodes.Const._infer = infer_end  # type: ignore[assignment]\n nodes.Slice._infer = infer_end  # type: ignore[assignment]\nnodes.TypeAlias._infer = infer_end  # type: ignore[assignment]\nnodes.TypeVar._infer = infer_end  # type: ignore[assignment]\nnodes.ParamSpec._infer = infer_end  # type: ignore[assignment]\nnodes.TypeVarTuple._infer = infer_end  # type: ignore[assignment]\n \n \n def _infer_sequence_helper(\ndiff --git a/astroid/nodes/__init__.py b/astroid/nodes/__init__.py\n     NamedExpr,\n     NodeNG,\n     Nonlocal,\n    ParamSpec,\n     Pass,\n     Pattern,\n     Raise,\n     TryFinally,\n     TryStar,\n     Tuple,\n    TypeAlias,\n    TypeVar,\n    TypeVarTuple,\n     UnaryOp,\n     Unknown,\n     While,\n     NamedExpr,\n     NodeNG,\n     Nonlocal,\n    ParamSpec,\n    TypeVarTuple,\n     Pass,\n     Pattern,\n     Raise,\n     TryFinally,\n     TryStar,\n     Tuple,\n    TypeAlias,\n    TypeVar,\n     UnaryOp,\n     Unknown,\n     While,\n     \"NamedExpr\",\n     \"NodeNG\",\n     \"Nonlocal\",\n    \"ParamSpec\",\n     \"Pass\",\n     \"Position\",\n     \"Raise\",\n     \"TryFinally\",\n     \"TryStar\",\n     \"Tuple\",\n    \"TypeAlias\",\n    \"TypeVar\",\n    \"TypeVarTuple\",\n     \"UnaryOp\",\n     \"Unknown\",\n     \"unpack_infer\",\ndiff --git a/astroid/nodes/as_string.py b/astroid/nodes/as_string.py\n         args += [n.accept(self) for n in node.keywords]\n         args_str = f\"({', '.join(args)})\" if args else \"\"\n         docs = self._docs_dedent(node.doc_node)\n        # TODO: handle type_params\n         return \"\\n\\n{}class {}{}:{}\\n{}\\n\".format(\n             decorate, node.name, args_str, docs, self._stmt_list(node.body)\n         )\n         if node.returns:\n             return_annotation = \" -> \" + node.returns.as_string()\n             trailer = return_annotation + \":\"\n        # TODO: handle type_params\n         def_format = \"\\n%s%s %s(%s)%s%s\\n%s\"\n         return def_format % (\n             decorate,\n         \"\"\"return an astroid.Nonlocal node as string\"\"\"\n         return f\"nonlocal {', '.join(node.names)}\"\n \n    def visit_paramspec(self, node: nodes.ParamSpec) -> str:\n        \"\"\"return an astroid.ParamSpec node as string\"\"\"\n        return node.name.accept(self)\n\n     def visit_pass(self, node) -> str:\n         \"\"\"return an astroid.Pass node as string\"\"\"\n         return \"pass\"\n             return f\"({node.elts[0].accept(self)}, )\"\n         return f\"({', '.join(child.accept(self) for child in node.elts)})\"\n \n    def visit_typealias(self, node: nodes.TypeAlias) -> str:\n        \"\"\"return an astroid.TypeAlias node as string\"\"\"\n        return node.name.accept(self) if node.name else \"_\"\n\n    def visit_typevar(self, node: nodes.TypeVar) -> str:\n        \"\"\"return an astroid.TypeVar node as string\"\"\"\n        return node.name.accept(self) if node.name else \"_\"\n\n    def visit_typevartuple(self, node: nodes.TypeVarTuple) -> str:\n        \"\"\"return an astroid.TypeVarTuple node as string\"\"\"\n        return \"*\" + node.name.accept(self) if node.name else \"\"\n\n     def visit_unaryop(self, node) -> str:\n         \"\"\"return an astroid.UnaryOp node as string\"\"\"\n         if node.op == \"not\":\ndiff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py\n     ClassVar,\n     Literal,\n     Optional,\n     Union,\n )\n \n     return isinstance(value, tuple(CONST_CLS))\n \n \n_NodesT = typing.TypeVar(\"_NodesT\", bound=NodeNG)\n_BadOpMessageT = typing.TypeVar(\"_BadOpMessageT\", bound=util.BadOperationMessage)\n \n AssignedStmtsPossibleNode = Union[\"List\", \"Tuple\", \"AssignName\", \"AssignAttr\", None]\n AssignedStmtsCall = Callable[\n         return name\n \n \nclass ParamSpec(_base_nodes.AssignTypeNode):\n    \"\"\"Class representing a :class:`ast.ParamSpec` node.\n\n    >>> import astroid\n    >>> node = astroid.extract_node('type Alias[**P] = Callable[P, int]')\n    >>> node.type_params[0]\n    <ParamSpec l.1 at 0x7f23b2e4e198>\n    \"\"\"\n\n    def __init__(\n        self,\n        lineno: int,\n        col_offset: int,\n        parent: NodeNG,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        self.name: AssignName | None\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n\n    def postinit(self, *, name: AssignName | None) -> None:\n        self.name = name\n\n\n class Pass(_base_nodes.NoChildrenNode, _base_nodes.Statement):\n     \"\"\"Class representing an :class:`ast.Pass` node.\n \n         return _container_getitem(self, self.elts, index, context=context)\n \n \nclass TypeAlias(_base_nodes.AssignTypeNode):\n    \"\"\"Class representing a :class:`ast.TypeAlias` node.\n\n    >>> import astroid\n    >>> node = astroid.extract_node('type Point = tuple[float, float]')\n    >>> node\n    <TypeAlias l.1 at 0x7f23b2e4e198>\n    \"\"\"\n\n    _astroid_fields = (\"type_params\", \"value\")\n\n    def __init__(\n        self,\n        lineno: int,\n        col_offset: int,\n        parent: NodeNG,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        self.name: AssignName | None\n        self.type_params: list[TypeVar, ParamSpec, TypeVarTuple]\n        self.value: NodeNG\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n\n    def postinit(\n        self,\n        *,\n        name: AssignName | None,\n        type_params: list[TypeVar, ParamSpec, TypeVarTuple],\n        value: NodeNG,\n    ) -> None:\n        self.name = name\n        self.type_params = type_params\n        self.value = value\n\n\nclass TypeVar(_base_nodes.AssignTypeNode):\n    \"\"\"Class representing a :class:`ast.TypeVar` node.\n\n    >>> import astroid\n    >>> node = astroid.extract_node('type Point[T] = tuple[float, float]')\n    >>> node.type_params[0]\n    <TypeVar l.1 at 0x7f23b2e4e198>\n    \"\"\"\n\n    _astroid_fields = (\"bound\",)\n\n    def __init__(\n        self,\n        lineno: int,\n        col_offset: int,\n        parent: NodeNG,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        self.name: AssignName | None\n        self.bound: NodeNG | None\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n\n    def postinit(self, *, name: AssignName | None, bound: NodeNG | None) -> None:\n        self.name = name\n        self.bound = bound\n\n\nclass TypeVarTuple(_base_nodes.AssignTypeNode):\n    \"\"\"Class representing a :class:`ast.TypeVarTuple` node.\n\n    >>> import astroid\n    >>> node = astroid.extract_node('type Alias[*Ts] = tuple[*Ts]')\n    >>> node.type_params[0]\n    <TypeVarTuple l.1 at 0x7f23b2e4e198>\n    \"\"\"\n\n    def __init__(\n        self,\n        lineno: int,\n        col_offset: int,\n        parent: NodeNG,\n        *,\n        end_lineno: int | None = None,\n        end_col_offset: int | None = None,\n    ) -> None:\n        self.name: AssignName | None\n        super().__init__(\n            lineno=lineno,\n            col_offset=col_offset,\n            end_lineno=end_lineno,\n            end_col_offset=end_col_offset,\n            parent=parent,\n        )\n\n    def postinit(self, *, name: AssignName | None) -> None:\n        self.name = name\n\n\n class UnaryOp(NodeNG):\n     \"\"\"Class representing an :class:`ast.UnaryOp` node.\n \ndiff --git a/astroid/nodes/scoped_nodes/scoped_nodes.py b/astroid/nodes/scoped_nodes/scoped_nodes.py\n     <FunctionDef.my_func l.2 at 0x7f23b2e71e10>\n     \"\"\"\n \n    _astroid_fields = (\n        \"decorators\",\n        \"args\",\n        \"returns\",\n        \"type_params\",\n        \"doc_node\",\n        \"body\",\n    )\n     _multi_line_block_fields = (\"body\",)\n     returns = None\n \n         self.body: list[NodeNG] = []\n         \"\"\"The contents of the function body.\"\"\"\n \n        self.type_params: list[nodes.TypeVar, nodes.ParamSpec, nodes.TypeVarTuple] = []\n        \"\"\"PEP 695 (Python 3.12+) type params, e.g. first 'T' in def func[T]() -> T: ...\"\"\"\n\n         self.instance_attrs: dict[str, list[NodeNG]] = {}\n \n         super().__init__(\n         *,\n         position: Position | None = None,\n         doc_node: Const | None = None,\n        type_params: list[nodes.TypeVar] | None = None,\n     ):\n         \"\"\"Do some setup after initialisation.\n \n             Position of function keyword(s) and name.\n         :param doc_node:\n             The doc node associated with this node.\n        :param type_params:\n            The type_params associated with this node.\n         \"\"\"\n         self.args = args\n         self.body = body\n         self.type_comment_args = type_comment_args\n         self.position = position\n         self.doc_node = doc_node\n        self.type_params = type_params or []\n \n     @cached_property\n     def extra_decorators(self) -> list[node_classes.Call]:\n     return klass\n \n \nclass ClassDef(  # pylint: disable=too-many-instance-attributes\n     _base_nodes.FilterStmtsBaseNode, LocalsDictNodeNG, _base_nodes.Statement\n ):\n     \"\"\"Class representing an :class:`ast.ClassDef` node.\n     # by a raw factories\n \n     # a dictionary of class instances attributes\n    _astroid_fields = (\n        \"decorators\",\n        \"bases\",\n        \"keywords\",\n        \"doc_node\",\n        \"body\",\n        \"type_params\",\n    )  # name\n \n     decorators = None\n     \"\"\"The decorators that are applied to this class.\n         self.is_dataclass: bool = False\n         \"\"\"Whether this class is a dataclass.\"\"\"\n \n        self.type_params: list[nodes.TypeVar, nodes.ParamSpec, nodes.TypeVarTuple] = []\n        \"\"\"PEP 695 (Python 3.12+) type params, e.g. class MyClass[T]: ...\"\"\"\n\n         super().__init__(\n             lineno=lineno,\n             col_offset=col_offset,\n         *,\n         position: Position | None = None,\n         doc_node: Const | None = None,\n        type_params: list[nodes.TypeVar] | None = None,\n     ) -> None:\n         if keywords is not None:\n             self.keywords = keywords\n         self._metaclass = metaclass\n         self.position = position\n         self.doc_node = doc_node\n        self.type_params = type_params or []\n \n     def _newstyle_impl(self, context: InferenceContext | None = None):\n         if context is None:\ndiff --git a/astroid/rebuilder.py b/astroid/rebuilder.py\n \n from astroid import nodes\n from astroid._ast import ParserModule, get_parser_module, parse_function_type_comment\nfrom astroid.const import IS_PYPY, PY38, PY39_PLUS, PY312_PLUS, Context\n from astroid.manager import AstroidManager\n from astroid.nodes import NodeNG\n from astroid.nodes.utils import Position\n         def visit(self, node: ast.Constant, parent: NodeNG) -> nodes.Const:\n             ...\n \n        if sys.version_info >= (3, 12):\n\n            @overload\n            def visit(self, node: ast.ParamSpec, parent: NodeNG) -> nodes.ParamSpec:\n                ...\n\n         @overload\n         def visit(self, node: ast.Pass, parent: NodeNG) -> nodes.Pass:\n             ...\n         def visit(self, node: ast.Tuple, parent: NodeNG) -> nodes.Tuple:\n             ...\n \n        if sys.version_info >= (3, 12):\n\n            @overload\n            def visit(self, node: ast.TypeAlias, parent: NodeNG) -> nodes.TypeAlias:\n                ...\n\n            @overload\n            def visit(self, node: ast.TypeVar, parent: NodeNG) -> nodes.TypeVar:\n                ...\n\n            @overload\n            def visit(\n                self, node: ast.TypeVarTuple, parent: NodeNG\n            ) -> nodes.TypeVarTuple:\n                ...\n\n         @overload\n         def visit(self, node: ast.UnaryOp, parent: NodeNG) -> nodes.UnaryOp:\n             ...\n             ],\n             position=self._get_position_info(node, newnode),\n             doc_node=self.visit(doc_ast_node, newnode),\n            type_params=[self.visit(param, newnode) for param in node.type_params]\n            if PY312_PLUS\n            else [],\n         )\n         return newnode\n \n             type_comment_args=type_comment_args,\n             position=self._get_position_info(node, newnode),\n             doc_node=self.visit(doc_ast_node, newnode),\n            type_params=[self.visit(param, newnode) for param in node.type_params]\n            if PY312_PLUS\n            else [],\n         )\n         self._global_names.pop()\n         return newnode\n             parent=parent,\n         )\n \n    def visit_paramspec(self, node: ast.ParamSpec, parent: NodeNG) -> nodes.ParamSpec:\n        \"\"\"Visit a ParamSpec node by returning a fresh instance of it.\"\"\"\n        newnode = nodes.ParamSpec(\n            lineno=node.lineno,\n            col_offset=node.col_offset,\n            end_lineno=node.end_lineno,\n            end_col_offset=node.end_col_offset,\n            parent=parent,\n        )\n        # Add AssignName node for 'node.name'\n        # https://bugs.python.org/issue43994\n        newnode.postinit(name=self.visit_assignname(node, newnode, node.name))\n        return newnode\n\n     def visit_pass(self, node: ast.Pass, parent: NodeNG) -> nodes.Pass:\n         \"\"\"Visit a Pass node by returning a fresh instance of it.\"\"\"\n         return nodes.Pass(\n         newnode.postinit([self.visit(child, newnode) for child in node.elts])\n         return newnode\n \n    def visit_typealias(self, node: ast.TypeAlias, parent: NodeNG) -> nodes.TypeAlias:\n        \"\"\"Visit a TypeAlias node by returning a fresh instance of it.\"\"\"\n        newnode = nodes.TypeAlias(\n            lineno=node.lineno,\n            col_offset=node.col_offset,\n            end_lineno=node.end_lineno,\n            end_col_offset=node.end_col_offset,\n            parent=parent,\n        )\n        newnode.postinit(\n            name=self.visit(node.name, newnode),\n            type_params=[self.visit(p, newnode) for p in node.type_params],\n            value=self.visit(node.value, newnode),\n        )\n        return newnode\n\n    def visit_typevar(self, node: ast.TypeVar, parent: NodeNG) -> nodes.TypeVar:\n        \"\"\"Visit a TypeVar node by returning a fresh instance of it.\"\"\"\n        newnode = nodes.TypeVar(\n            lineno=node.lineno,\n            col_offset=node.col_offset,\n            end_lineno=node.end_lineno,\n            end_col_offset=node.end_col_offset,\n            parent=parent,\n        )\n        # Add AssignName node for 'node.name'\n        # https://bugs.python.org/issue43994\n        newnode.postinit(\n            name=self.visit_assignname(node, newnode, node.name),\n            bound=self.visit(node.bound, newnode),\n        )\n        return newnode\n\n    def visit_typevartuple(\n        self, node: ast.TypeVarTuple, parent: NodeNG\n    ) -> nodes.TypeVarTuple:\n        \"\"\"Visit a TypeVarTuple node by returning a fresh instance of it.\"\"\"\n        newnode = nodes.TypeVarTuple(\n            lineno=node.lineno,\n            col_offset=node.col_offset,\n            end_lineno=node.end_lineno,\n            end_col_offset=node.end_col_offset,\n            parent=parent,\n        )\n        # Add AssignName node for 'node.name'\n        # https://bugs.python.org/issue43994\n        newnode.postinit(name=self.visit_assignname(node, newnode, node.name))\n        return newnode\n\n     def visit_unaryop(self, node: ast.UnaryOp, parent: NodeNG) -> nodes.UnaryOp:\n         \"\"\"Visit a UnaryOp node by returning a fresh instance of it.\"\"\"\n         newnode = nodes.UnaryOp("
  },
  {
    "instruction": "Cannot infer empty functions\n### Steps to reproduce\r\n```python\r\nimport astroid\r\nastroid.extract_node(\"\"\"\r\ndef f():\r\n    pass\r\nf()\r\n\"\"\").inferred()\r\n```\r\n### Current behavior\r\nraises `StopIteration`\r\n\r\n### Expected behavior\r\nReturns `[const.NoneType]`\r\n\r\n### ``python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"`` output\r\n\r\n2.0.0\r\n\r\nThis also applies to procedural functions which don't explicitly return any values.\n",
    "input": "             raise exceptions.InferenceError(\n                 \"Could not find __getitem__ for {node!r}.\", node=self, context=context\n             )\n         return next(method.infer_call_result(self, new_context))\n \n \ndiff --git a/astroid/scoped_nodes.py b/astroid/scoped_nodes.py\n         \"\"\"\n         return self.type == \"classmethod\"\n \n    def is_abstract(self, pass_is_abstract=True):\n         \"\"\"Check if the method is abstract.\n \n         A method is considered abstract if any of the following is true:\n         * The only statement is 'raise NotImplementedError'\n         * The only statement is 'pass' and pass_is_abstract is True\n         * The method is annotated with abc.astractproperty/abc.abstractmethod\n \n \n         for child_node in self.body:\n             if isinstance(child_node, node_classes.Raise):\n                 if child_node.raises_not_implemented():\n                     return True\n             return pass_is_abstract and isinstance(child_node, node_classes.Pass)\n \n         first_return = next(returns, None)\n         if not first_return:\n            if self.body and isinstance(self.body[-1], node_classes.Assert):\n                yield node_classes.Const(None)\n                 return\n \n             raise exceptions.InferenceError(",
    "output": "             raise exceptions.InferenceError(\n                 \"Could not find __getitem__ for {node!r}.\", node=self, context=context\n             )\n        if len(method.args.arguments) != 2:  # (self, index)\n            raise exceptions.AstroidTypeError(\n                \"__getitem__ for {node!r} does not have correct signature\",\n                node=self,\n                context=context,\n            )\n         return next(method.infer_call_result(self, new_context))\n \n \ndiff --git a/astroid/scoped_nodes.py b/astroid/scoped_nodes.py\n         \"\"\"\n         return self.type == \"classmethod\"\n \n    def is_abstract(self, pass_is_abstract=True, any_raise_is_abstract=False):\n         \"\"\"Check if the method is abstract.\n \n         A method is considered abstract if any of the following is true:\n         * The only statement is 'raise NotImplementedError'\n        * The only statement is 'raise <SomeException>' and any_raise_is_abstract is True\n         * The only statement is 'pass' and pass_is_abstract is True\n         * The method is annotated with abc.astractproperty/abc.abstractmethod\n \n \n         for child_node in self.body:\n             if isinstance(child_node, node_classes.Raise):\n                if any_raise_is_abstract:\n                    return True\n                 if child_node.raises_not_implemented():\n                     return True\n             return pass_is_abstract and isinstance(child_node, node_classes.Pass)\n \n         first_return = next(returns, None)\n         if not first_return:\n            if self.body:\n                if self.is_abstract(pass_is_abstract=True, any_raise_is_abstract=True):\n                    yield util.Uninferable\n                else:\n                    yield node_classes.Const(None)\n                 return\n \n             raise exceptions.InferenceError("
  },
  {
    "instruction": "'AsStringVisitor' object has no attribute 'visit_unknown'\n```python\r\n>>> import astroid\r\n>>> astroid.nodes.Unknown().as_string()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 609, in as_string\r\n    return AsStringVisitor()(self)\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py\", line 56, in __call__\r\n    return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 220, in accept\r\n    func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\r\nAttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'\r\n>>> \r\n```\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.8.6-dev0\n",
    "input": "         MatchSingleton,\n         MatchStar,\n         MatchValue,\n     )\n \n # pylint: disable=unused-argument\n     def visit_evaluatedobject(self, node):\n         return node.original.accept(self)\n \n \n def _import_string(names):\n     \"\"\"return a list of (name, asname) formatted as a string\"\"\"",
    "output": "         MatchSingleton,\n         MatchStar,\n         MatchValue,\n        Unknown,\n     )\n \n # pylint: disable=unused-argument\n     def visit_evaluatedobject(self, node):\n         return node.original.accept(self)\n \n    def visit_unknown(self, node: \"Unknown\") -> str:\n        return str(node)\n\n \n def _import_string(names):\n     \"\"\"return a list of (name, asname) formatted as a string\"\"\""
  },
  {
    "instruction": "Delayed attribute assignment to object() may cause incorrect inference of instance attributes\n@cdce8p: `aiohttp` and `VLCTelnet` turned out to be red herrings. This case fails on current stable versions:\r\n\r\n```python\r\nclass Example:\r\n    def prev(self):\r\n        pass\r\n    def next(self):\r\n        pass\r\n    def other(self):\r\n        pass\r\n\r\n\r\nex = Example()\r\nex.other()  # no warning\r\nex.prev()   # no warning\r\nex.next()   # no warning\r\n\r\nimport typing\r\n\r\nex.other()  # no warning\r\nex.prev()   # false-positive: not-callable\r\nex.next()   # false-positive: not-callable\r\n```\r\n\r\n_Originally posted by @nelfin in https://github.com/PyCQA/astroid/issues/927#issuecomment-818626368_\r\n\r\nI've bisected this down to 78d5537. Pylint 2.3.1 passes this case with 20a7ae5 and fails with 78d5537\n",
    "input": "     else:\n         if slots and attrname not in {slot.value for slot in slots}:\n             return False\n    return True\n \n \n class AstroidBuilder(raw_building.InspectBuilder):",
    "output": "     else:\n         if slots and attrname not in {slot.value for slot in slots}:\n             return False\n    return node.qname() != \"builtins.object\"\n \n \n class AstroidBuilder(raw_building.InspectBuilder):"
  },
  {
    "instruction": "Replace modutils.is_standard_module() logic with sys.stdlib_module_names\n\r\nThis extends from the conversation in https://github.com/PyCQA/pylint/pull/8190.\r\n\r\nThe logic in `modutils.is_standard_module()` should largely be able to be replaced with [sys.stdlib_module_names](https://docs.python.org/3/library/sys.html#sys.stdlib_module_names), which was introduced in 3.10. The advantages are it will be faster (no imports, no filesystem traversal), it's not dependent on the local environment,  and it's maintained upstream, generated from source. For the referenced PR, I backported the generating code in CPython to generate sets for a shim to support 3.7 - 3.9.\r\n\r\nI started working on a PR for Astroid, but it seems `modutils.is_standard_module()` actually does two different things depending on how it's called.\r\n1. If no path is specified, it tries to determine if a module is part of the standard library (or a builtin, or compiled in) by inspecting the path of module after importing it.\r\n2. If a path is specified, it does the same logic, but ultimately is determining if the module is in the path specified.\r\n\r\nFor the second case, I could only find one case in the wild, in pyreverse.\r\n\r\nhttps://github.com/PyCQA/pylint/blob/5bc4cd9a4b4c240227a41786823a6f226864dc4b/pylint/pyreverse/inspector.py#L308\r\n\r\nThese seem like different behaviors to me. I'm unsure how to proceed with PR. Here are some options I've considered.\r\n\r\n- Option 1:\r\n  - Introduce a new function, basically a wrapper for sys.stdlib_module_names and the shim\r\n  - Old behavior is preserved\r\n  - Advantage of a function, even though it's very simple, is it provides a space to add overriding logic if needed down the road\r\n   \r\n- Option 2:\r\n   - Only introduce the shim, so the code is in a common place for Astroid and Pylint\r\n   - Can be dropped with 3.9\r\n   - Old behavior is preserved\r\n\r\n- Option 3:\r\n  - Fall back to old behavior if a path is given, check sys.stdlib_module_names otherwise\r\n\r\n- Option 4:\r\n  - Deprecate `is_standard_module()`\r\n  - Introduce new functions more specific to how they are used\r\n\r\n- Option 5:\r\n  - Do Nothing\r\n\r\nI'm sure there are more options, but this is what comes to mind now. Would appreciate your thoughts and ideas.\r\n\n",
    "input": "diff --git a/astroid/manager.py b/astroid/manager.py\n     get_source_file,\n     is_module_name_part_of_extension_package_whitelist,\n     is_python_source,\n    is_standard_module,\n     load_module_from_name,\n     modpath_from_file,\n )\n     def _can_load_extension(self, modname: str) -> bool:\n         if self.always_load_extensions:\n             return True\n        if is_standard_module(modname):\n             return True\n         return is_module_name_part_of_extension_package_whitelist(\n             modname, self.extension_package_whitelist\ndiff --git a/astroid/modutils.py b/astroid/modutils.py\n import sys\n import sysconfig\n import types\n from collections.abc import Callable, Iterable, Sequence\n from contextlib import redirect_stderr, redirect_stdout\n from functools import lru_cache\n from pathlib import Path\n \nfrom astroid.const import IS_JYTHON, IS_PYPY\n from astroid.interpreter._import import spec, util\n \n logger = logging.getLogger(__name__)\n \n \n     return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS\n \n \n def is_standard_module(modname: str, std_path: Iterable[str] | None = None) -> bool:\n     \"\"\"Try to guess if a module is a standard python module (by default,\n     see `std_path` parameter's description).\n       - is located on the path listed in one of the directory in `std_path`\n       - is a built-in module\n     \"\"\"\n     modname = modname.split(\".\")[0]\n     try:\n         filename = file_from_modpath([modname])",
    "output": "# Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n# For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n# Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n\n\"\"\"\nShim to support Python versions < 3.10 that don't have sys.stdlib_module_names\n\nThese values were created by cherry-picking the commits from\nhttps://bugs.python.org/issue42955 into each version, but may be updated\nmanually if changes are needed.\n\"\"\"\n\nimport sys\n\n# TODO: Remove this file when Python 3.9 is no longer supported\n\nPY_3_7 = frozenset(\n    {\n        \"__future__\",\n        \"_abc\",\n        \"_ast\",\n        \"_asyncio\",\n        \"_bisect\",\n        \"_blake2\",\n        \"_bootlocale\",\n        \"_bz2\",\n        \"_codecs\",\n        \"_codecs_cn\",\n        \"_codecs_hk\",\n        \"_codecs_iso2022\",\n        \"_codecs_jp\",\n        \"_codecs_kr\",\n        \"_codecs_tw\",\n        \"_collections\",\n        \"_collections_abc\",\n        \"_compat_pickle\",\n        \"_compression\",\n        \"_contextvars\",\n        \"_crypt\",\n        \"_csv\",\n        \"_ctypes\",\n        \"_curses\",\n        \"_curses_panel\",\n        \"_datetime\",\n        \"_dbm\",\n        \"_decimal\",\n        \"_dummy_thread\",\n        \"_elementtree\",\n        \"_functools\",\n        \"_gdbm\",\n        \"_hashlib\",\n        \"_heapq\",\n        \"_imp\",\n        \"_io\",\n        \"_json\",\n        \"_locale\",\n        \"_lsprof\",\n        \"_lzma\",\n        \"_markupbase\",\n        \"_md5\",\n        \"_msi\",\n        \"_multibytecodec\",\n        \"_multiprocessing\",\n        \"_opcode\",\n        \"_operator\",\n        \"_osx_support\",\n        \"_pickle\",\n        \"_posixsubprocess\",\n        \"_py_abc\",\n        \"_pydecimal\",\n        \"_pyio\",\n        \"_queue\",\n        \"_random\",\n        \"_sha1\",\n        \"_sha256\",\n        \"_sha3\",\n        \"_sha512\",\n        \"_signal\",\n        \"_sitebuiltins\",\n        \"_socket\",\n        \"_sqlite3\",\n        \"_sre\",\n        \"_ssl\",\n        \"_stat\",\n        \"_string\",\n        \"_strptime\",\n        \"_struct\",\n        \"_symtable\",\n        \"_thread\",\n        \"_threading_local\",\n        \"_tkinter\",\n        \"_tracemalloc\",\n        \"_uuid\",\n        \"_warnings\",\n        \"_weakref\",\n        \"_weakrefset\",\n        \"_winapi\",\n        \"abc\",\n        \"aifc\",\n        \"antigravity\",\n        \"argparse\",\n        \"array\",\n        \"ast\",\n        \"asynchat\",\n        \"asyncio\",\n        \"asyncore\",\n        \"atexit\",\n        \"audioop\",\n        \"base64\",\n        \"bdb\",\n        \"binascii\",\n        \"binhex\",\n        \"bisect\",\n        \"builtins\",\n        \"bz2\",\n        \"cProfile\",\n        \"calendar\",\n        \"cgi\",\n        \"cgitb\",\n        \"chunk\",\n        \"cmath\",\n        \"cmd\",\n        \"code\",\n        \"codecs\",\n        \"codeop\",\n        \"collections\",\n        \"colorsys\",\n        \"compileall\",\n        \"concurrent\",\n        \"configparser\",\n        \"contextlib\",\n        \"contextvars\",\n        \"copy\",\n        \"copyreg\",\n        \"crypt\",\n        \"csv\",\n        \"ctypes\",\n        \"curses\",\n        \"dataclasses\",\n        \"datetime\",\n        \"dbm\",\n        \"decimal\",\n        \"difflib\",\n        \"dis\",\n        \"distutils\",\n        \"doctest\",\n        \"dummy_threading\",\n        \"email\",\n        \"encodings\",\n        \"ensurepip\",\n        \"enum\",\n        \"errno\",\n        \"faulthandler\",\n        \"fcntl\",\n        \"filecmp\",\n        \"fileinput\",\n        \"fnmatch\",\n        \"formatter\",\n        \"fractions\",\n        \"ftplib\",\n        \"functools\",\n        \"gc\",\n        \"genericpath\",\n        \"getopt\",\n        \"getpass\",\n        \"gettext\",\n        \"glob\",\n        \"grp\",\n        \"gzip\",\n        \"hashlib\",\n        \"heapq\",\n        \"hmac\",\n        \"html\",\n        \"http\",\n        \"idlelib\",\n        \"imaplib\",\n        \"imghdr\",\n        \"imp\",\n        \"importlib\",\n        \"inspect\",\n        \"io\",\n        \"ipaddress\",\n        \"itertools\",\n        \"json\",\n        \"keyword\",\n        \"lib2to3\",\n        \"linecache\",\n        \"locale\",\n        \"logging\",\n        \"lzma\",\n        \"macpath\",\n        \"mailbox\",\n        \"mailcap\",\n        \"marshal\",\n        \"math\",\n        \"mimetypes\",\n        \"mmap\",\n        \"modulefinder\",\n        \"msilib\",\n        \"msvcrt\",\n        \"multiprocessing\",\n        \"netrc\",\n        \"nis\",\n        \"nntplib\",\n        \"nt\",\n        \"ntpath\",\n        \"nturl2path\",\n        \"numbers\",\n        \"opcode\",\n        \"operator\",\n        \"optparse\",\n        \"os\",\n        \"ossaudiodev\",\n        \"parser\",\n        \"pathlib\",\n        \"pdb\",\n        \"pickle\",\n        \"pickletools\",\n        \"pipes\",\n        \"pkgutil\",\n        \"platform\",\n        \"plistlib\",\n        \"poplib\",\n        \"posix\",\n        \"posixpath\",\n        \"pprint\",\n        \"profile\",\n        \"pstats\",\n        \"pty\",\n        \"pwd\",\n        \"py_compile\",\n        \"pyclbr\",\n        \"pydoc\",\n        \"pydoc_data\",\n        \"pyexpat\",\n        \"queue\",\n        \"quopri\",\n        \"random\",\n        \"re\",\n        \"readline\",\n        \"reprlib\",\n        \"resource\",\n        \"rlcompleter\",\n        \"runpy\",\n        \"sched\",\n        \"secrets\",\n        \"select\",\n        \"selectors\",\n        \"shelve\",\n        \"shlex\",\n        \"shutil\",\n        \"signal\",\n        \"site\",\n        \"smtpd\",\n        \"smtplib\",\n        \"sndhdr\",\n        \"socket\",\n        \"socketserver\",\n        \"spwd\",\n        \"sqlite3\",\n        \"sre_compile\",\n        \"sre_constants\",\n        \"sre_parse\",\n        \"ssl\",\n        \"stat\",\n        \"statistics\",\n        \"string\",\n        \"stringprep\",\n        \"struct\",\n        \"subprocess\",\n        \"sunau\",\n        \"symbol\",\n        \"symtable\",\n        \"sys\",\n        \"sysconfig\",\n        \"syslog\",\n        \"tabnanny\",\n        \"tarfile\",\n        \"telnetlib\",\n        \"tempfile\",\n        \"termios\",\n        \"textwrap\",\n        \"this\",\n        \"threading\",\n        \"time\",\n        \"timeit\",\n        \"tkinter\",\n        \"token\",\n        \"tokenize\",\n        \"trace\",\n        \"traceback\",\n        \"tracemalloc\",\n        \"tty\",\n        \"turtle\",\n        \"turtledemo\",\n        \"types\",\n        \"typing\",\n        \"unicodedata\",\n        \"unittest\",\n        \"urllib\",\n        \"uu\",\n        \"uuid\",\n        \"venv\",\n        \"warnings\",\n        \"wave\",\n        \"weakref\",\n        \"webbrowser\",\n        \"winreg\",\n        \"winsound\",\n        \"wsgiref\",\n        \"xdrlib\",\n        \"xml\",\n        \"xmlrpc\",\n        \"zipapp\",\n        \"zipfile\",\n        \"zipimport\",\n        \"zlib\",\n    }\n)\n\nPY_3_8 = frozenset(\n    PY_3_7\n    - {\n        \"macpath\",\n    }\n    | {\n        \"_posixshmem\",\n        \"_statistics\",\n        \"_xxsubinterpreters\",\n    }\n)\n\nPY_3_9 = frozenset(\n    PY_3_8\n    - {\n        \"_dummy_thread\",\n        \"dummy_threading\",\n    }\n    | {\n        \"_aix_support\",\n        \"_bootsubprocess\",\n        \"_peg_parser\",\n        \"_zoneinfo\",\n        \"graphlib\",\n        \"zoneinfo\",\n    }\n)\n\nif sys.version_info[:2] == (3, 7):\n    stdlib_module_names = PY_3_7\nelif sys.version_info[:2] == (3, 8):\n    stdlib_module_names = PY_3_8\nelif sys.version_info[:2] == (3, 9):\n    stdlib_module_names = PY_3_9\nelse:\n    raise AssertionError(\"This module is only intended as a backport for Python <= 3.9\")\ndiff --git a/astroid/manager.py b/astroid/manager.py\n     get_source_file,\n     is_module_name_part_of_extension_package_whitelist,\n     is_python_source,\n    is_stdlib_module,\n     load_module_from_name,\n     modpath_from_file,\n )\n     def _can_load_extension(self, modname: str) -> bool:\n         if self.always_load_extensions:\n             return True\n        if is_stdlib_module(modname):\n             return True\n         return is_module_name_part_of_extension_package_whitelist(\n             modname, self.extension_package_whitelist\ndiff --git a/astroid/modutils.py b/astroid/modutils.py\n import sys\n import sysconfig\n import types\nimport warnings\n from collections.abc import Callable, Iterable, Sequence\n from contextlib import redirect_stderr, redirect_stdout\n from functools import lru_cache\n from pathlib import Path\n \nfrom astroid.const import IS_JYTHON, IS_PYPY, PY310_PLUS\n from astroid.interpreter._import import spec, util\n \nif PY310_PLUS:\n    from sys import stdlib_module_names\nelse:\n    from astroid._backport_stdlib_names import stdlib_module_names\n\n logger = logging.getLogger(__name__)\n \n \n     return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS\n \n \ndef is_stdlib_module(modname: str) -> bool:\n    \"\"\"Return: True if the modname is in the standard library\"\"\"\n    return modname.split(\".\")[0] in stdlib_module_names\n\n\ndef module_in_path(modname: str, path: str | Iterable[str]) -> bool:\n    \"\"\"Try to determine if a module is imported from one of the specified paths\n\n    :param modname: name of the module\n\n    :param path: paths to consider\n\n    :return:\n      true if the module:\n      - is located on the path listed in one of the directory in `paths`\n    \"\"\"\n\n    modname = modname.split(\".\")[0]\n    try:\n        filename = file_from_modpath([modname])\n    except ImportError:\n        # Import failed, we can't check path if we don't know it\n        return False\n\n    if filename is None:\n        # No filename likely means it's compiled in, or potentially a namespace\n        return False\n    filename = _normalize_path(filename)\n\n    if isinstance(path, str):\n        return filename.startswith(_cache_normalize_path(path))\n\n    return any(filename.startswith(_cache_normalize_path(entry)) for entry in path)\n\n\n def is_standard_module(modname: str, std_path: Iterable[str] | None = None) -> bool:\n     \"\"\"Try to guess if a module is a standard python module (by default,\n     see `std_path` parameter's description).\n       - is located on the path listed in one of the directory in `std_path`\n       - is a built-in module\n     \"\"\"\n    warnings.warn(\n        \"is_standard_module() is deprecated. Use, is_stdlib_module() or module_in_path() instead\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n\n     modname = modname.split(\".\")[0]\n     try:\n         filename = file_from_modpath([modname])"
  },
  {
    "instruction": "``nodes.Module`` don't have a ``end_lineno`` and ``end_col_offset``\n### Steps to reproduce\r\n\r\n```python\r\nimport astroid\r\n\r\ncode = \"\"\"\r\n    print(\"a module\")\r\n    \"\"\"\r\n\r\nmodule = astroid.parse(code)\r\nprint(module.end_lineno)\r\nprint(module.end_col_offset)\r\n```\r\n\r\n### Current behavior\r\n\r\n`AttributeError` on both of the last lines.\r\n\r\n### Expected behavior\r\n\r\n@cdce8p Let me know if I misunderstood you, but I thought we wanted these to be accessible on all nodes, just initialised as `None`.\r\nIf that was not the case, I would make the case to do so as it allows you to do `node.end_lineno` without running in to `AttributeError`'s.\r\n\r\n### Version\r\n\r\nLatest `main`.\r\n\n",
    "input": " \n     :type: int or None\n     \"\"\"\n    lineno = 0\n     \"\"\"The line that this node appears on in the source code.\n\n    :type: int or None\n     \"\"\"\n \n     # attributes below are set by the builder module or by raw factories\n     )\n     _other_other_fields = (\"locals\", \"globals\")\n \n    lineno: None\n     col_offset: None\n     end_lineno: None\n     end_col_offset: None\n         self.file = file\n         self.path = path\n         self.package = package\n        self.parent = parent\n         self.pure_python = pure_python\n         self.locals = self.globals = {}\n         \"\"\"A map of the name of a local variable to the node defining the local.\n         \"\"\"\n         self.future_imports = set()\n \n     # pylint: enable=redefined-builtin\n \n     def postinit(self, body=None):",
    "output": " \n     :type: int or None\n     \"\"\"\n    lineno: Literal[0] = 0\n     \"\"\"The line that this node appears on in the source code.\n     \"\"\"\n \n     # attributes below are set by the builder module or by raw factories\n     )\n     _other_other_fields = (\"locals\", \"globals\")\n \n     col_offset: None\n     end_lineno: None\n     end_col_offset: None\n         self.file = file\n         self.path = path\n         self.package = package\n         self.pure_python = pure_python\n         self.locals = self.globals = {}\n         \"\"\"A map of the name of a local variable to the node defining the local.\n         \"\"\"\n         self.future_imports = set()\n \n        super().__init__(lineno=0, parent=parent)\n\n     # pylint: enable=redefined-builtin\n \n     def postinit(self, body=None):"
  },
  {
    "instruction": "Decorator.toline is off by 1\n### Steps to reproduce\r\n\r\nI came across this inconsistency while debugging why pylint reports `missing-docstring` on the wrong line for the `g2` function in the example. As it turns out, the `toline` of the decorator seems to point to `b=3,` instead of `)`.\r\n\r\n```python\r\nimport ast\r\nimport astroid\r\n\r\nsource = \"\"\"\\\r\n@f(a=2,\r\n   b=3,\r\n)\r\ndef g2():\r\n    pass\r\n\"\"\"\r\n\r\n[f] = ast.parse(source).body\r\n[deco] = f.decorator_list\r\nprint(\"ast\", deco.lineno, deco.end_lineno)\r\n\r\n[f] = astroid.parse(source).body\r\n[deco] = f.decorators.nodes\r\nprint(\"astroid\", deco.fromlineno, deco.tolineno)\r\n```\r\n\r\n### Current behavior\r\n\r\n```\r\nast 1 3\r\nastroid 1 2\r\n```\r\n\r\n### Expected behavior\r\n\r\n```\r\nast 1 3\r\nastroid 1 3\r\n```\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.9.3\n",
    "input": "     @decorators.cachedproperty\n     def tolineno(self) -> Optional[int]:\n         \"\"\"The last line that this node appears on in the source code.\"\"\"\n         if not self._astroid_fields:\n             # can't have children\n             last_child = None\ndiff --git a/astroid/rebuilder.py b/astroid/rebuilder.py\n     def visit_tryexcept(self, node: \"ast.Try\", parent: NodeNG) -> nodes.TryExcept:\n         \"\"\"visit a TryExcept node by returning a fresh instance of it\"\"\"\n         if sys.version_info >= (3, 8):\n             newnode = nodes.TryExcept(\n                 lineno=node.lineno,\n                 col_offset=node.col_offset,\n                end_lineno=node.end_lineno,\n                end_col_offset=node.end_col_offset,\n                 parent=parent,\n             )\n         else:",
    "output": "     @decorators.cachedproperty\n     def tolineno(self) -> Optional[int]:\n         \"\"\"The last line that this node appears on in the source code.\"\"\"\n        if self.end_lineno is not None:\n            return self.end_lineno\n         if not self._astroid_fields:\n             # can't have children\n             last_child = None\ndiff --git a/astroid/rebuilder.py b/astroid/rebuilder.py\n     def visit_tryexcept(self, node: \"ast.Try\", parent: NodeNG) -> nodes.TryExcept:\n         \"\"\"visit a TryExcept node by returning a fresh instance of it\"\"\"\n         if sys.version_info >= (3, 8):\n            # TryExcept excludes the 'finally' but that will be included in the\n            # end_lineno from 'node'. Therefore, we check all non 'finally'\n            # children to find the correct end_lineno and column.\n            end_lineno = node.end_lineno\n            end_col_offset = node.end_col_offset\n            all_children: List[\"ast.AST\"] = [*node.body, *node.handlers, *node.orelse]\n            for child in reversed(all_children):\n                end_lineno = child.end_lineno\n                end_col_offset = child.end_col_offset\n                break\n             newnode = nodes.TryExcept(\n                 lineno=node.lineno,\n                 col_offset=node.col_offset,\n                end_lineno=end_lineno,\n                end_col_offset=end_col_offset,\n                 parent=parent,\n             )\n         else:"
  },
  {
    "instruction": "v2.13.x regression: Crash when inspecting `PyQt5.QtWidgets` due to `RuntimeError` during `hasattr`\n### Steps to reproduce\r\n\r\nInstall PyQt5, run `pylint --extension-pkg-whitelist=PyQt5 x.py` over a file containing `from PyQt5 import QtWidgets`\r\n\r\n### Current behavior\r\n\r\nWith astroid 2.12.13 and pylint 2.15.10, this works fine. With astroid 2.13.2, this happens:\r\n\r\n```pytb\r\nException on node <ImportFrom l.1 at 0x7fc5a3c47d00> in file '/home/florian/tmp/pylintbug/x.py'\r\nTraceback (most recent call last):\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\r\n    self._check_module_attrs(node, module, name.split(\".\"))\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\r\n    module = next(module.getattr(name)[0].infer())\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\r\n    result = [self.import_module(name, relative_only=True)]\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\r\n    return self.ast_from_module(named_module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\r\n    elif hasattr(member, \"__all__\"):\r\nRuntimeError: wrapped C/C++ object of type QApplication has been deleted\r\nx.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-11-06-17.txt'. (astroid-error)\r\n```\r\n\r\nIt looks like it happens when `member` is `QtWidgets.qApp`, which is a kind of \"magic\" object referring to the QApplication singleton. Since none exists, it looks like PyQt doesn't like trying to access an attribute on that.\r\n\r\nBisected to:\r\n\r\n- #1885 \r\n\r\nIt looks like 974f26f75eb3eccb4bcd8ea143901baf60a685ff is the exact culprit.\r\n\r\ncc @nickdrozd \r\n\r\n(took the freedom to add appropriate labels already, hope that's fine)\r\n\n",
    "input": "         object_build_function(node, member, name)\n \n \n class InspectBuilder:\n     \"\"\"class for building nodes from living object\n \n                 # This should be called for Jython, where some builtin\n                 # methods aren't caught by isbuiltin branch.\n                 _build_from_function(node, name, member, self._module)\n            elif hasattr(member, \"__all__\"):\n                 module = build_module(name)\n                 _attach_local_node(node, module, name)\n                 # recursion",
    "output": "         object_build_function(node, member, name)\n \n \ndef _safe_has_attribute(obj, member: str) -> bool:\n    \"\"\"Required because unexpected RunTimeError can be raised.\n\n    See https://github.com/PyCQA/astroid/issues/1958\n    \"\"\"\n    try:\n        return hasattr(obj, member)\n    except Exception:  # pylint: disable=broad-except\n        return False\n\n\n class InspectBuilder:\n     \"\"\"class for building nodes from living object\n \n                 # This should be called for Jython, where some builtin\n                 # methods aren't caught by isbuiltin branch.\n                 _build_from_function(node, name, member, self._module)\n            elif _safe_has_attribute(member, \"__all__\"):\n                 module = build_module(name)\n                 _attach_local_node(node, module, name)\n                 # recursion"
  },
  {
    "instruction": "Confusing behaviour of ParametricEllipsoid\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nWhen creating a ParametricEllispoid using a direction of [0, 1, 0], the ellipsoid is rotated along the y axis.  \r\nFor example if setting the direction to [1e-5, 1, 0], which corresponds to approximately similar direction, the ellipsoid displays then the correct behaviour.\r\n\r\n### Steps to reproduce the bug.\r\n\r\n```python\r\nimport pyvista as pv\r\n\r\nellipsoid = pv.ParametricEllipsoid(300, 100, 10, direction=[0, 1, 0])\r\n```\r\n\r\n### System Information\r\n\r\n```shell\r\nDate: Wed Sep 06 14:07:38 2023 CEST\r\n\r\n                OS : Linux\r\n            CPU(s) : 8\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n               RAM : 31.2 GiB\r\n       Environment : Jupyter\r\n       File system : ext4\r\n        GPU Vendor : Intel\r\n      GPU Renderer : Mesa Intel(R) UHD Graphics (CML GT2)\r\n       GPU Version : 4.6 (Core Profile) Mesa 22.0.1\r\n  MathText Support : False\r\n\r\n  Python 3.8.13 (default, Apr 19 2022, 02:32:06)  [GCC 11.2.0]\r\n\r\n           pyvista : 0.41.1\r\n               vtk : 9.2.6\r\n             numpy : 1.24.4\r\n        matplotlib : 3.3.4\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n           imageio : 2.31.1\r\n           IPython : 8.12.2\r\n        ipywidgets : 8.0.7\r\n             scipy : 1.10.1\r\n              tqdm : 4.65.0\r\n        jupyterlab : 3.6.5\r\n             trame : 2.5.2\r\n      trame_client : 2.10.0\r\n      trame_server : 2.11.7\r\n         trame_vtk : 2.5.8\r\n      nest_asyncio : 1.5.6\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\nHere is the given ellipsoid\r\n![confusing_ellipsoid](https://github.com/pyvista/pyvista/assets/57682091/f0e1b5f7-eca1-4224-a020-df44385ed68b)\r\nHere is what is expected\r\n![expected_ellipsoid](https://github.com/pyvista/pyvista/assets/57682091/d4f67ead-9928-4af3-9c3a-b6121180b780)\r\n\n",
    "input": "     if np.array_equal(normz, (0.0, 0.0, 0.0)):\n         # the assumed normy axis is parallel to normx, so shift its\n         # axis and recalculate normz\n        norm_y_temp = np.roll(norm_y_temp, 1)\n         normz = np.cross(normx, norm_y_temp)\n     normz /= np.linalg.norm(normz)\n     normy = np.cross(normz, normx)",
    "output": "     if np.array_equal(normz, (0.0, 0.0, 0.0)):\n         # the assumed normy axis is parallel to normx, so shift its\n         # axis and recalculate normz\n        norm_y_temp = [-1.0, 0.0, 0.0]\n         normz = np.cross(normx, norm_y_temp)\n     normz /= np.linalg.norm(normz)\n     normy = np.cross(normz, normx)"
  },
  {
    "instruction": "Clean up and clarify sampling-like filters\n### Describe what maintenance you would like added.\n\nThere was a discussion on slack on the use of sampling-like filters, i.e. `sample`, `probe`, and `interpolate`.  One issue is that it is hard to figure out when to use which filter.  The other issue is that `probe` has the opposite behavior of `sample` and `interpolate` in regards to order of operation (see below).\n\n### Links to source code.\n\n_No response_\n\n### Pseudocode or Screenshots\n\n```python\r\nimport pyvista as pv\r\n\r\nsmall = pv.ImageData(dimensions=(5, 5, 5))\r\nlarge = pv.ImageData(dimensions=(10, 10, 10))\r\nprint(small.n_points)\r\nprint(large.n_points)\r\nprint(small.probe(large).n_points)  # gives different result\r\nprint(small.sample(large).n_points)\r\nprint(small.interpolate(large).n_points)\r\n```\r\n\r\n\r\nThis  gives\r\n\r\n```txt\r\n125\r\n1000\r\n1000\r\n125\r\n125\r\n```\n",
    "input": " Interpolating\n ~~~~~~~~~~~~~\n \n Interpolate one mesh's point/cell arrays onto another mesh's nodes using a\n Gaussian Kernel.\n \"\"\"\ndiff --git a/examples/01-filter/resample.py b/examples/01-filter/resample.py\n Resampling\n ~~~~~~~~~~\n \n Resample one mesh's point/cell arrays onto another mesh's nodes.\n \"\"\"\n ###############################################################################\n# This example will resample a volumetric mesh's  scalar data onto the surface\n # of a sphere contained in that volume.\n \n # sphinx_gallery_thumbnail_number = 3\n ###############################################################################\n # Simple Resample\n # +++++++++++++++\n# Query a grids points onto a sphere\n mesh = pv.Sphere(center=(4.5, 4.5, 4.5), radius=4.5)\n data_to_probe = examples.load_uniform()\n \ndiff --git a/pyvista/core/filters/data_set.py b/pyvista/core/filters/data_set.py\n \n import pyvista\n import pyvista.core._vtk_core as _vtk\nfrom pyvista.core.errors import AmbiguousDataError, MissingDataError, VTKVersionError\n from pyvista.core.filters import _get_output, _update_alg\n from pyvista.core.utilities import transformations\n from pyvista.core.utilities.arrays import (\n     ):\n         \"\"\"Sample data values at specified point locations.\n \n        This uses :class:`vtk.vtkProbeFilter`.\n \n         Parameters\n         ----------\n         >>> from pyvista import examples\n         >>> mesh = pv.Sphere(center=(4.5, 4.5, 4.5), radius=4.5)\n         >>> grid = examples.load_uniform()\n        >>> result = grid.probe(mesh)\n        >>> 'Spatial Point Data' in result.point_data\n         True\n \n         \"\"\"\n         if not pyvista.is_pyvista_dataset(points):\n             points = wrap(points)\n         alg = _vtk.vtkProbeFilter()\n         pass_point_data=True,\n         categorical=False,\n         progress_bar=False,\n     ):\n         \"\"\"Resample array data from a passed mesh onto this mesh.\n \n         This uses :class:`vtk.vtkResampleWithDataSet`.\n \n         Parameters\n         progress_bar : bool, default: False\n             Display a progress bar to indicate progress.\n \n         Returns\n         -------\n         pyvista.DataSet\n             Dataset containing resampled data.\n \n         Examples\n         --------\n         Resample data from another dataset onto a sphere.\n \n        >>> import pyvista\n         >>> from pyvista import examples\n        >>> mesh = pyvista.Sphere(center=(4.5, 4.5, 4.5), radius=4.5)\n         >>> data_to_probe = examples.load_uniform()\n         >>> result = mesh.sample(data_to_probe)\n         >>> result.plot(scalars=\"Spatial Point Data\")\n \n         See :ref:`resampling_example` for more examples using this filter.\n \n         \"\"\"\n         if tolerance is not None:\n             alg.SetComputeTolerance(False)\n             alg.SetTolerance(tolerance)\n         _update_alg(alg, progress_bar, 'Resampling array Data from a Passed Mesh onto Mesh')\n         return _get_output(alg)\n \n     ):\n         \"\"\"Interpolate values onto this mesh from a given dataset.\n \n        The input dataset is typically a point cloud. Only point data from\n        the source mesh will be interpolated onto points of this mesh. Whether\n         preexisting point and cell data of this mesh are preserved in the\n         output can be customized with the ``pass_point_data`` and\n         ``pass_cell_data`` parameters.\n         ``radius`` parameters to adjust this kernel. You can also switch this\n         kernel to use an N closest points approach.\n \n         Parameters\n         ----------\n         target : pyvista.DataSet\n         pyvista.DataSet\n             Interpolated dataset.  Return type matches input.\n \n         Examples\n         --------\n         Interpolate the values of 5 points onto a sample plane.",
    "output": " Interpolating\n ~~~~~~~~~~~~~\n \nThere are two main methods of interpolating or sampling data from a target mesh\nin PyVista. :func:`pyvista.DataSetFilters.interpolate` uses a distance weighting\nkernel to interpolate point data from nearby points of the target mesh onto\nthe desired points.\n:func:`pyvista.DataSetFilters.sample` interpolates data using the\ninterpolation scheme of the enclosing cell from the target mesh.\n\nIf the target mesh is a point cloud, i.e. there is no connectivity in the cell\nstructure, then :func:`pyvista.DataSetFilters.interpolate` is typically\npreferred.  If interpolation is desired within the cells of the target mesh, then\n:func:`pyvista.DataSetFilters.sample` is typically desired.\n\n\nThis example uses :func:`pyvista.DataSetFilters.interpolate`.\nFor :func:`pyvista.DataSetFilters.sample`, see :ref:`resampling_example`.\n\n Interpolate one mesh's point/cell arrays onto another mesh's nodes using a\n Gaussian Kernel.\n \"\"\"\ndiff --git a/examples/01-filter/resample.py b/examples/01-filter/resample.py\n Resampling\n ~~~~~~~~~~\n \nThere are two main methods of interpolating or sampling data from a target mesh\nin PyVista. :func:`pyvista.DataSetFilters.interpolate` uses a distance weighting\nkernel to interpolate point data from nearby points of the target mesh onto\nthe desired points.\n:func:`pyvista.DataSetFilters.sample` interpolates data using the\ninterpolation scheme of the enclosing cell from the target mesh.\n\nIf the target mesh is a point cloud, i.e. there is no connectivity in the cell\nstructure, then :func:`pyvista.DataSetFilters.interpolate` is typically\npreferred.  If interpolation is desired within the cells of the target mesh, then\n:func:`pyvista.DataSetFilters.sample` is typically desired.\n\n\nThis example uses :func:`pyvista.DataSetFilters.sample`.\nFor :func:`pyvista.DataSetFilters.interpolate`, see :ref:`interpolate_example`.\n\n\n Resample one mesh's point/cell arrays onto another mesh's nodes.\n \"\"\"\n ###############################################################################\n# This example will resample a volumetric mesh's scalar data onto the surface\n # of a sphere contained in that volume.\n \n # sphinx_gallery_thumbnail_number = 3\n ###############################################################################\n # Simple Resample\n # +++++++++++++++\n# Query a grid's points onto a sphere\n mesh = pv.Sphere(center=(4.5, 4.5, 4.5), radius=4.5)\n data_to_probe = examples.load_uniform()\n \ndiff --git a/pyvista/core/filters/data_set.py b/pyvista/core/filters/data_set.py\n \n import pyvista\n import pyvista.core._vtk_core as _vtk\nfrom pyvista.core.errors import (\n    AmbiguousDataError,\n    MissingDataError,\n    PyVistaDeprecationWarning,\n    VTKVersionError,\n)\n from pyvista.core.filters import _get_output, _update_alg\n from pyvista.core.utilities import transformations\n from pyvista.core.utilities.arrays import (\n     ):\n         \"\"\"Sample data values at specified point locations.\n \n        .. deprecated:: 0.41.0\n          `probe` will be removed in a future version. Use\n          :func:`pyvista.DataSetFilters.sample` instead.\n          If using `mesh1.probe(mesh2)`, use `mesh2.sample(mesh1)`.\n\n        This uses :class:`vtkProbeFilter`.\n \n         Parameters\n         ----------\n         >>> from pyvista import examples\n         >>> mesh = pv.Sphere(center=(4.5, 4.5, 4.5), radius=4.5)\n         >>> grid = examples.load_uniform()\n        >>> result = grid.probe(mesh)  # doctest:+SKIP\n        >>> 'Spatial Point Data' in result.point_data  # doctest:+SKIP\n         True\n \n         \"\"\"\n        # deprecated in v0.41.0\n        # remove in v0.44.0\n        warnings.warn(\n            \"\"\"probe filter is deprecated and will be removed in a future version.\n            Use sample filter instead.\n            If using `mesh1.probe(mesh2)`, use `mesh2.sample(mesh1)`.\n            \"\"\",\n            PyVistaDeprecationWarning,\n        )\n\n         if not pyvista.is_pyvista_dataset(points):\n             points = wrap(points)\n         alg = _vtk.vtkProbeFilter()\n         pass_point_data=True,\n         categorical=False,\n         progress_bar=False,\n        locator=None,\n     ):\n         \"\"\"Resample array data from a passed mesh onto this mesh.\n \n        For `mesh1.sample(mesh2)`, the arrays from `mesh2` are sampled onto\n        the points of `mesh1`.  This function interpolates within an\n        enclosing cell.  This contrasts with\n        :function`pyvista.DataSetFilters.interpolate` that uses a distance\n        weighting for nearby points.  If there is cell topology, `sample` is\n        usually preferred.\n\n         This uses :class:`vtk.vtkResampleWithDataSet`.\n \n         Parameters\n         progress_bar : bool, default: False\n             Display a progress bar to indicate progress.\n \n        locator : vtkAbstractCellLocator, optional\n            Prototype cell locator to perform the ``FindCell()``\n            operation.  Default uses the DataSet ``FindCell`` method.\n\n         Returns\n         -------\n         pyvista.DataSet\n             Dataset containing resampled data.\n \n        See Also\n        --------\n        pyvista.DataSetFilters.interpolate\n\n         Examples\n         --------\n         Resample data from another dataset onto a sphere.\n \n        >>> import pyvista as pv\n         >>> from pyvista import examples\n        >>> mesh = pv.Sphere(center=(4.5, 4.5, 4.5), radius=4.5)\n         >>> data_to_probe = examples.load_uniform()\n         >>> result = mesh.sample(data_to_probe)\n         >>> result.plot(scalars=\"Spatial Point Data\")\n \n        If sampling from a set of points represented by a ``(n, 3)``\n        shaped ``numpy.ndarray``, they need to be converted to a\n        PyVista DataSet, e.g. :class:`pyvista.PolyData`, first.\n\n        >>> import numpy as np\n        >>> points = np.array([[1.5, 5.0, 6.2], [6.7, 4.2, 8.0]])\n        >>> mesh = pv.PolyData(points)\n        >>> result = mesh.sample(data_to_probe)\n        >>> result[\"Spatial Point Data\"]\n        pyvista_ndarray([ 46.5 , 225.12])\n\n         See :ref:`resampling_example` for more examples using this filter.\n \n         \"\"\"\n         if tolerance is not None:\n             alg.SetComputeTolerance(False)\n             alg.SetTolerance(tolerance)\n        if locator:\n            alg.SetCellLocatorPrototype(locator)\n\n         _update_alg(alg, progress_bar, 'Resampling array Data from a Passed Mesh onto Mesh')\n         return _get_output(alg)\n \n     ):\n         \"\"\"Interpolate values onto this mesh from a given dataset.\n \n        The ``target`` dataset is typically a point cloud. Only point data from\n        the ``target`` mesh will be interpolated onto points of this mesh. Whether\n         preexisting point and cell data of this mesh are preserved in the\n         output can be customized with the ``pass_point_data`` and\n         ``pass_cell_data`` parameters.\n         ``radius`` parameters to adjust this kernel. You can also switch this\n         kernel to use an N closest points approach.\n \n        If the cell topology is more useful for interpolating, e.g. from a\n        discretized FEM or CFD simulation, use\n        :func:`pyvista.DataSetFilters.sample` instead.\n\n         Parameters\n         ----------\n         target : pyvista.DataSet\n         pyvista.DataSet\n             Interpolated dataset.  Return type matches input.\n \n        See Also\n        --------\n        pyvista.DataSetFilters.sample\n\n         Examples\n         --------\n         Interpolate the values of 5 points onto a sample plane."
  },
  {
    "instruction": "Boolean Operation freezes/crashes \n### Describe the bug, what's wrong, and what you expected.\n\nApparently, if two polyData have the exact same shape, their boolean operation freezes/crashes the application!\r\n\n\n### Steps to reproduce the bug.\n\n```python\r\np1 = pv.Sphere().triangulate()\r\np2 = pv.Sphere().triangulate()\r\n\r\np1.boolean_intersection(p2)\r\n``````\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Tue Aug 22 12:17:01 2023 EEST\r\n\r\n                OS : Darwin\r\n            CPU(s) : 12\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n               RAM : 16.0 GiB\r\n       Environment : Jupyter\r\n       File system : apfs\r\n        GPU Vendor : ATI Technologies Inc.\r\n      GPU Renderer : AMD Radeon Pro 5300M OpenGL Engine\r\n       GPU Version : 4.1 ATI-4.14.1\r\n  MathText Support : False\r\n\r\n  Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0\r\n  (clang-1300.0.29.30)]\r\n\r\n           pyvista : 0.41.1\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n           IPython : 8.14.0\r\n             scipy : 1.10.1\r\n        jupyterlab : 4.0.5\r\n      nest_asyncio : 1.5.7\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n",
    "input": " \n     def _boolean(self, btype, other_mesh, tolerance, progress_bar=False):\n         \"\"\"Perform boolean operation.\"\"\"\n         if not isinstance(other_mesh, pyvista.PolyData):\n             raise TypeError(\"Input mesh must be PolyData.\")\n         if not self.is_all_triangles or not other_mesh.is_all_triangles:",
    "output": " \n     def _boolean(self, btype, other_mesh, tolerance, progress_bar=False):\n         \"\"\"Perform boolean operation.\"\"\"\n        if self.n_points == other_mesh.n_points:\n            if np.allclose(self.points, other_mesh.points):\n                raise ValueError(\n                    \"The input mesh contains identical points to the surface being operated on. Unable to perform boolean operations on an identical surface.\"\n                )\n         if not isinstance(other_mesh, pyvista.PolyData):\n             raise TypeError(\"Input mesh must be PolyData.\")\n         if not self.is_all_triangles or not other_mesh.is_all_triangles:"
  },
  {
    "instruction": "Allow passing through cell data in `to_tetrahedra` method in RectilinearGrid\n### Describe the feature you would like to be added.\n\nNo cell data is passed through when converting to a tetrahedra.  The user can currently request to pass through the original cell id, but it requires one more step to regenerate the cell data on the tetrahedralized mesh.\n\n### Links to VTK Documentation, Examples, or Class Definitions.\n\n_No response_\n\n### Pseudocode or Screenshots\n\nCurrently we have to do\r\n\r\n```python\r\nmesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\r\ntetra_mesh = mesh.to_tetrahedra(pass_cell_ids=True)\r\ntetra_mesh[\"cell_data\"] = mesh[\"cell_data\"][tetra_mesh.cell_data.active_scalars]\r\n```\r\n\r\nIt would be better to do something like\r\n\r\n```python\r\nmesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\r\ntetra_mesh = mesh.to_tetrahedra(pass_cell_data=True)  # the prior code would occur inside the method\r\n```\n",
    "input": " \n         \"\"\"\n         if self.GetScalars() is not None:\n            return str(self.GetScalars().GetName())\n         return None\n \n     @active_scalars_name.setter\ndiff --git a/pyvista/core/filters/rectilinear_grid.py b/pyvista/core/filters/rectilinear_grid.py\n         self,\n         tetra_per_cell: int = 5,\n         mixed: Union[Sequence[int], bool] = False,\n        pass_cell_ids: bool = False,\n         progress_bar: bool = False,\n     ):\n         \"\"\"Create a tetrahedral mesh structured grid.\n             string uses a cell array rather than the active array to determine\n             the number of tetrahedra to generate per cell.\n \n        pass_cell_ids : bool, default: False\n             Set to ``True`` to make the tetrahedra have scalar data indicating\n             which cell they came from in the original\n            :class:`pyvista.RectilinearGrid`.\n \n         progress_bar : bool, default: False\n             Display a progress bar to indicate progress.\n \n         \"\"\"\n         alg = _vtk.vtkRectilinearGridToTetrahedra()\n        alg.SetRememberVoxelId(pass_cell_ids)\n         if mixed is not False:\n             if isinstance(mixed, str):\n                 self.cell_data.active_scalars_name = mixed\n \n         alg.SetInputData(self)\n         _update_alg(alg, progress_bar, 'Converting to tetrahedra')\n        return _get_output(alg)",
    "output": " \n         \"\"\"\n         if self.GetScalars() is not None:\n            name = self.GetScalars().GetName()\n            if name is None:\n                # Getting the keys has the side effect of naming \"unnamed\" arrays\n                self.keys()\n                name = self.GetScalars().GetName()\n            return str(name)\n         return None\n \n     @active_scalars_name.setter\ndiff --git a/pyvista/core/filters/rectilinear_grid.py b/pyvista/core/filters/rectilinear_grid.py\n         self,\n         tetra_per_cell: int = 5,\n         mixed: Union[Sequence[int], bool] = False,\n        pass_cell_ids: bool = True,\n        pass_cell_data: bool = True,\n         progress_bar: bool = False,\n     ):\n         \"\"\"Create a tetrahedral mesh structured grid.\n             string uses a cell array rather than the active array to determine\n             the number of tetrahedra to generate per cell.\n \n        pass_cell_ids : bool, default: True\n             Set to ``True`` to make the tetrahedra have scalar data indicating\n             which cell they came from in the original\n            :class:`pyvista.RectilinearGrid`. The name of this array is\n            ``'vtkOriginalCellIds'`` within the ``cell_data``.\n\n        pass_cell_data : bool, default: True\n            Set to ``True`` to make the tetradera mesh have the cell data from\n            the original :class:`pyvista.RectilinearGrid`.  This uses\n            ``pass_cell_ids=True`` internally. If ``True``, ``pass_cell_ids``\n            will also be set to ``True``.\n \n         progress_bar : bool, default: False\n             Display a progress bar to indicate progress.\n \n         \"\"\"\n         alg = _vtk.vtkRectilinearGridToTetrahedra()\n        alg.SetRememberVoxelId(pass_cell_ids or pass_cell_data)\n         if mixed is not False:\n             if isinstance(mixed, str):\n                 self.cell_data.active_scalars_name = mixed\n \n         alg.SetInputData(self)\n         _update_alg(alg, progress_bar, 'Converting to tetrahedra')\n        out = _get_output(alg)\n        if pass_cell_data:\n            # algorithm stores original cell ids in active scalars\n            for name in self.cell_data:  # type: ignore\n                if name != out.cell_data.active_scalars_name:\n                    out[name] = self.cell_data[name][out.cell_data.active_scalars]  # type: ignore\n\n        if alg.GetRememberVoxelId():\n            # original cell_ids are not named and are the active scalars\n            out.cell_data.set_array(\n                out.cell_data.pop(out.cell_data.active_scalars_name), 'vtkOriginalCellIds'\n            )\n\n        return out"
  },
  {
    "instruction": "Adding ``CircularArc``s together does not provide a line\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nDon't know if it can be considered a bug or not but...\r\n\r\nIf you define two consecutive ``pv.CircularArc`` and you plot them, weird things start to appear with the new PyVista 0.39 version. Run the following code snippet using ``pyvista==0.38.6`` and ``pyvista==0.39.0``\r\n\r\n### Steps to reproduce the bug.\r\n\r\n```python\r\nimport pyvista as pv\r\n\r\n\r\n# Define your arcs\r\n#\r\n#              Y       (s2)\r\n#              ^   ____(e1)____\r\n#              |  /             \\\r\n#              | /               \\\r\n#              |/                 \\\r\n#          (s1)O ------(c)-------(e2)----> X\r\n#\r\n#   Let's imagine the above is an arc from (0,0) to (10,10) and origin\r\n#   at (10,0); and another consecutive arc from (10,10) to (20,0) and\r\n#   origin at (10,0)\r\n#\r\narc_1 = pv.CircularArc([0, 0, 0], [10, 10, 0], [10, 0, 0], negative=False)\r\narc_2 = pv.CircularArc([10, 10, 0], [20, 0, 0], [10, 0, 0], negative=False)\r\n\r\n# ========== CRITICAL BEHAVIOR ==========\r\n# I add them together\r\narc = arc_1 + arc_2\r\n# ========== CRITICAL BEHAVIOR ==========\r\n\r\n# Instantiate plotter\r\npl = pv.Plotter()\r\n\r\n# Add the polydata\r\npl.add_mesh(arc)\r\n\r\n# Plotter config: view from the top\r\npl.view_vector(vector=[0, 0, 1], viewup=[0, 1, 0])\r\n\r\n# Plot\r\npl.show()\r\n\r\n```\r\n\r\n### System Information\r\n\r\n```shell\r\nFor PyVista 0.38.6\r\n\r\n--------------------------------------------------------------------------------\r\n  Date: Thu May 11 13:49:09 2023 Romance Daylight Time\r\n\r\n                OS : Windows\r\n            CPU(s) : 16\r\n           Machine : AMD64\r\n      Architecture : 64bit\r\n       Environment : Python\r\n        GPU Vendor : Intel\r\n      GPU Renderer : Intel(R) UHD Graphics\r\n       GPU Version : 4.5.0 - Build 30.0.100.9955\r\n\r\n  Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64\r\n  bit (AMD64)]\r\n\r\n           pyvista : 0.38.6\r\n               vtk : 9.2.6\r\n             numpy : 1.24.3\r\n           imageio : 2.28.1\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n--------------------------------------------------------------------------------\r\n\r\nFor PyVista 0.39.0\r\n\r\n--------------------------------------------------------------------------------\r\n  Date: Thu May 11 13:50:00 2023 Romance Daylight Time\r\n\r\n                OS : Windows\r\n            CPU(s) : 16\r\n           Machine : AMD64\r\n      Architecture : 64bit\r\n       Environment : Python\r\n        GPU Vendor : Intel\r\n      GPU Renderer : Intel(R) UHD Graphics\r\n       GPU Version : 4.5.0 - Build 30.0.100.9955\r\n  MathText Support : False\r\n\r\n  Python 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64\r\n  bit (AMD64)]\r\n\r\n           pyvista : 0.39.0\r\n               vtk : 9.2.6\r\n             numpy : 1.24.3\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.2\r\n             pooch : v1.7.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\nPyVista 0.39\r\n\r\n![PyVista 0.39](https://github.com/pyvista/pyvista/assets/37798125/87bda0a2-2eb7-4171-8005-239f5a4f27c2)\r\n\r\nPyVista 0.38.6\r\n\r\n![PyVista 0.38.6](https://github.com/pyvista/pyvista/assets/37798125/b6159e12-97bc-4768-9691-91005fadfb26)\r\n\n",
    "input": "         \"\"\"\n         # check if dataset or datasets are not polydata\n         if isinstance(dataset, (list, tuple, pyvista.MultiBlock)):\n            is_pd = all(isinstance(data, pyvista.PolyData) for data in dataset)\n         else:\n            is_pd = isinstance(dataset, pyvista.PolyData)\n \n        if inplace and not is_pd:\n             raise TypeError(\"In-place merge requires both input datasets to be PolyData.\")\n \n         merged = DataSetFilters.merge(\n         )\n \n         # convert back to a polydata if both inputs were polydata\n        if is_pd:\n            pd_merged = pyvista.PolyData(merged.points, faces=merged.cells, n_faces=merged.n_cells)\n            pd_merged.point_data.update(merged.point_data)\n            pd_merged.cell_data.update(merged.cell_data)\n            pd_merged.field_data.update(merged.field_data)\n            merged = pd_merged\n \n         if inplace:\n             self.deep_copy(merged)",
    "output": "         \"\"\"\n         # check if dataset or datasets are not polydata\n         if isinstance(dataset, (list, tuple, pyvista.MultiBlock)):\n            is_polydata = all(isinstance(data, pyvista.PolyData) for data in dataset)\n         else:\n            is_polydata = isinstance(dataset, pyvista.PolyData)\n \n        if inplace and not is_polydata:\n             raise TypeError(\"In-place merge requires both input datasets to be PolyData.\")\n \n         merged = DataSetFilters.merge(\n         )\n \n         # convert back to a polydata if both inputs were polydata\n        if is_polydata:\n            # if either of the input datasets contained lines or strips, we\n            # must use extract_geometry to ensure they get converted back\n            # correctly. This incurrs a performance penalty, but is needed to\n            # maintain data consistency.\n            if isinstance(dataset, (list, tuple, pyvista.MultiBlock)):\n                dataset_has_lines_strips = any(\n                    [ds.n_lines or ds.n_strips or ds.n_verts for ds in dataset]\n                )\n            else:\n                dataset_has_lines_strips = dataset.n_lines or dataset.n_strips or dataset.n_verts\n\n            if self.n_lines or self.n_strips or self.n_verts or dataset_has_lines_strips:\n                merged = merged.extract_geometry()\n            else:\n                polydata_merged = pyvista.PolyData(\n                    merged.points, faces=merged.cells, n_faces=merged.n_cells, deep=False\n                )\n                polydata_merged.point_data.update(merged.point_data)\n                polydata_merged.cell_data.update(merged.cell_data)\n                polydata_merged.field_data.update(merged.field_data)\n                merged = polydata_merged\n \n         if inplace:\n             self.deep_copy(merged)"
  },
  {
    "instruction": "``Multiblock``.plot does not work when using ``PointSet``\n### Describe the bug, what's wrong, and what you expected.\n\nIt seems ``MultiBlock`` entities made of ``PointSet`` plot nothing when using ``plot`` method.\n\n### Steps to reproduce the bug.\n\n```python\r\nimport pyvista as pv\r\nimport numpy as np\r\n\r\npoints_arr = np.array(\r\n    [\r\n        [0.0, 1.0, 0.0],\r\n        [0.0, 0.0, 0.0],\r\n        [1.0, 1.0, 0.0],\r\n        [1.0, 0.0, 0.0],\r\n        [0.0, 0.0, 1.0],\r\n        [1.0, 0.0, 1.0],\r\n        [1.0, 1.0, 1.0],\r\n        [0.0, 1.0, 1.0],\r\n    ]\r\n)\r\n\r\npoints = pv.MultiBlock()\r\nfor each_kp in points_arr:\r\n    points.append(pv.PointSet(each_kp))\r\n\r\npoints.plot()\r\n```\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Wed May 10 18:07:18 2023 CEST\r\n\r\n                OS : Darwin\r\n            CPU(s) : 8\r\n           Machine : arm64\r\n      Architecture : 64bit\r\n               RAM : 16.0 GiB\r\n       Environment : IPython\r\n       File system : apfs\r\n        GPU Vendor : Apple\r\n      GPU Renderer : Apple M2\r\n       GPU Version : 4.1 Metal - 83.1\r\n  MathText Support : False\r\n\r\n  Python 3.11.1 (main, Dec 23 2022, 09:28:24) [Clang 14.0.0\r\n  (clang-1400.0.29.202)]\r\n\r\n           pyvista : 0.39.0\r\n               vtk : 9.2.6\r\n             numpy : 1.24.3\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n           imageio : 2.28.0\r\n           IPython : 8.12.1\r\n        ipywidgets : 8.0.6\r\n             scipy : 1.10.1\r\n              tqdm : 4.65.0\r\n        jupyterlab : 3.6.3\r\n         pythreejs : 2.4.2\r\n      nest_asyncio : 1.5.6\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n<img width=\"624\" alt=\"image\" src=\"https://github.com/pyvista/pyvista/assets/28149841/a1b0999f-2d35-4911-a216-eb6503955860\">\r\n\n",
    "input": "             if block is not None:\n                 if isinstance(block, MultiBlock):\n                     dataset.replace(i, block.as_polydata_blocks(copy=copy))\n                 elif not isinstance(block, pyvista.PolyData):\n                     dataset.replace(i, block.extract_surface())\n                 elif copy:",
    "output": "             if block is not None:\n                 if isinstance(block, MultiBlock):\n                     dataset.replace(i, block.as_polydata_blocks(copy=copy))\n                elif isinstance(block, pyvista.PointSet):\n                    dataset.replace(i, block.cast_to_polydata(deep=True))\n                 elif not isinstance(block, pyvista.PolyData):\n                     dataset.replace(i, block.extract_surface())\n                 elif copy:"
  },
  {
    "instruction": "to_tetrahedra active scalars\n### Describe the bug, what's wrong, and what you expected.\n\n#4311 passes cell data through the `to_tetrahedra` call. However, after these changes.  The active scalars information is lost.\r\n\r\ncc @akaszynski who implemented these changes in that PR.\n\n### Steps to reproduce the bug.\n\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\nmesh = pv.UniformGrid(dimensions=(10, 10, 10))\r\nmesh[\"a\"] = np.zeros(mesh.n_cells)\r\nmesh[\"b\"] = np.ones(mesh.n_cells)\r\nprint(mesh.cell_data)\r\ntet = mesh.to_tetrahedra()\r\nprint(tet.cell_data)\r\n```\r\n\r\n```txt\r\npyvista DataSetAttributes\r\nAssociation     : CELL\r\nActive Scalars  : a\r\nActive Vectors  : None\r\nActive Texture  : None\r\nActive Normals  : None\r\nContains arrays :\r\n    a                       float64    (729,)               SCALARS\r\n    b                       float64    (729,)\r\npyvista DataSetAttributes\r\nAssociation     : CELL\r\nActive Scalars  : None\r\nActive Vectors  : None\r\nActive Texture  : None\r\nActive Normals  : None\r\nContains arrays :\r\n    a                       float64    (3645,)\r\n    b                       float64    (3645,)\r\n    vtkOriginalCellIds      int32      (3645,)\r\n```\n\n### System Information\n\n```shell\nPython 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.39.0\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n        matplotlib : 3.7.1\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n           imageio : 2.27.0\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n",
    "input": " \n import collections\n from typing import Sequence, Union\n \n import numpy as np\n \n from pyvista import _vtk, abstract_class\n from pyvista.core.filters import _get_output, _update_alg\n \n \n @abstract_class\n class RectilinearGridFilters:\n     \"\"\"An internal class to manage filters/algorithms for rectilinear grid datasets.\"\"\"\n \n     def to_tetrahedra(\n         self,\n         tetra_per_cell: int = 5,\n         mixed: Union[Sequence[int], bool] = False,\n         pass_cell_ids: bool = True,\n        pass_cell_data: bool = True,\n         progress_bar: bool = False,\n     ):\n         \"\"\"Create a tetrahedral mesh structured grid.\n \n             :class:`pyvista.RectilinearGrid`. The name of this array is\n             ``'vtkOriginalCellIds'`` within the ``cell_data``.\n \n        pass_cell_data : bool, default: True\n             Set to ``True`` to make the tetrahedra mesh have the cell data from\n             the original :class:`pyvista.RectilinearGrid`.  This uses\n             ``pass_cell_ids=True`` internally. If ``True``, ``pass_cell_ids``\n         progress_bar : bool, default: False\n             Display a progress bar to indicate progress.\n \n         Returns\n         -------\n         pyvista.UnstructuredGrid\n         >>> tet_grid.explode(factor=0.5).plot(show_edges=True)\n \n         \"\"\"\n         alg = _vtk.vtkRectilinearGridToTetrahedra()\n        alg.SetRememberVoxelId(pass_cell_ids or pass_cell_data)\n         if mixed is not False:\n             if isinstance(mixed, str):\n                 self.cell_data.active_scalars_name = mixed\n         alg.SetInputData(self)\n         _update_alg(alg, progress_bar, 'Converting to tetrahedra')\n         out = _get_output(alg)\n        if pass_cell_data:\n             # algorithm stores original cell ids in active scalars\n             for name in self.cell_data:  # type: ignore\n                 if name != out.cell_data.active_scalars_name:\n                     out[name] = self.cell_data[name][out.cell_data.active_scalars]  # type: ignore\n \n         if alg.GetRememberVoxelId():\n             # original cell_ids are not named and are the active scalars\n             out.cell_data.set_array(\n                 out.cell_data.pop(out.cell_data.active_scalars_name), 'vtkOriginalCellIds'\n             )\n \n         return out",
    "output": " \n import collections\n from typing import Sequence, Union\nimport warnings\n \n import numpy as np\n \n from pyvista import _vtk, abstract_class\n from pyvista.core.filters import _get_output, _update_alg\nfrom pyvista.utilities import assert_empty_kwargs\nfrom pyvista.utilities.misc import PyVistaDeprecationWarning\n \n \n @abstract_class\n class RectilinearGridFilters:\n     \"\"\"An internal class to manage filters/algorithms for rectilinear grid datasets.\"\"\"\n \n    # Note remove kwargs when removing deprecation for pass_cell_data\n     def to_tetrahedra(\n         self,\n         tetra_per_cell: int = 5,\n         mixed: Union[Sequence[int], bool] = False,\n         pass_cell_ids: bool = True,\n        pass_data: bool = True,\n         progress_bar: bool = False,\n        **kwargs,\n     ):\n         \"\"\"Create a tetrahedral mesh structured grid.\n \n             :class:`pyvista.RectilinearGrid`. The name of this array is\n             ``'vtkOriginalCellIds'`` within the ``cell_data``.\n \n        pass_data : bool, default: True\n             Set to ``True`` to make the tetrahedra mesh have the cell data from\n             the original :class:`pyvista.RectilinearGrid`.  This uses\n             ``pass_cell_ids=True`` internally. If ``True``, ``pass_cell_ids``\n         progress_bar : bool, default: False\n             Display a progress bar to indicate progress.\n \n        **kwargs : dict, optional\n            Deprecated keyword argument ``pass_cell_data``.\n\n         Returns\n         -------\n         pyvista.UnstructuredGrid\n         >>> tet_grid.explode(factor=0.5).plot(show_edges=True)\n \n         \"\"\"\n        # Note remove this section when deprecation is done\n        pass_cell_data = kwargs.pop(\"pass_cell_data\", None)\n        assert_empty_kwargs(**kwargs)\n        if pass_cell_data is not None:\n            warnings.warn(\n                \"pass_cell_data is a deprecated option, use pass_data\", PyVistaDeprecationWarning\n            )\n            pass_data = pass_cell_data\n\n         alg = _vtk.vtkRectilinearGridToTetrahedra()\n        alg.SetRememberVoxelId(pass_cell_ids or pass_data)\n         if mixed is not False:\n             if isinstance(mixed, str):\n                 self.cell_data.active_scalars_name = mixed\n         alg.SetInputData(self)\n         _update_alg(alg, progress_bar, 'Converting to tetrahedra')\n         out = _get_output(alg)\n\n        if pass_data:\n             # algorithm stores original cell ids in active scalars\n            # this does not preserve active scalars, but we need to\n            # keep active scalars until they are renamed\n             for name in self.cell_data:  # type: ignore\n                 if name != out.cell_data.active_scalars_name:\n                     out[name] = self.cell_data[name][out.cell_data.active_scalars]  # type: ignore\n \n            for name in self.point_data:  # type: ignore\n                out[name] = self.point_data[name]  # type: ignore\n\n         if alg.GetRememberVoxelId():\n             # original cell_ids are not named and are the active scalars\n             out.cell_data.set_array(\n                 out.cell_data.pop(out.cell_data.active_scalars_name), 'vtkOriginalCellIds'\n             )\n \n        if pass_data:\n            # Now reset active scalars in cast the original mesh had data with active scalars\n            association, name = self.active_scalars_info  # type: ignore\n            out.set_active_scalars(name, preference=association)\n\n         return out"
  },
  {
    "instruction": "Diffuse and Specular setters silently ignore invalid values\n### Describe the bug, what's wrong, and what you expected.\n\nWhile working on #3870, I noticed that `diffuse` and `specular` do not always get set on `pyvista.Property`. This happens if an invalid value is used. For example, diffuse should be between 0-1, but if you pass a value of 2.0, `vtkProperty` corrects it to 1.0:\r\n\r\n```py\r\n>>> import vtk\r\n>>> prop = vtk.vtkProperty()\r\n>>> prop.SetDiffuse(2.0)\r\n>>> prop.GetDiffuse()\r\n1.0\r\n```\r\n\r\nThis similarly happens for specular, which should also have a valid range of 0-1.\r\n\r\nShould we have `pyvista.Property`'s setters for these methods error out when an invalid value is passed? I ask because I definitely wasted time trying to figure out why a diffuse value of 1.0 looks the same as 2.0 before thinking it should be between 0 and 1.\r\n\r\nPerhaps this at a minimum should be documented in the setters and docstring for `add_mesh()`?\n\n### Steps to reproduce the bug.\n\n```python\r\nimport pyvista as pv\r\n\r\npl = pv.Plotter()\r\na = pl.add_mesh(pv.Sphere(), diffuse=3.0, specular=10)\r\n# Expected to error for invalid values\r\n```\n\n### System Information\n\n```shell\nmain branch\n```\n\n\n### Screenshots\n\n_No response_\n",
    "input": " import pyvista as pv\n from pyvista import _vtk\n from pyvista.plotting.opts import InterpolationType\nfrom pyvista.utilities.misc import no_new_attr\n \n from .colors import Color\n \n         The specular lighting coefficient.\n \n     specular_power : float, default: :attr:`pyvista.themes._LightingConfig.specular_power`\n        The specular power. Between 0.0 and 128.0.\n \n     show_edges : bool, default: :attr:`pyvista.themes.DefaultTheme.show_edges`\n         Shows the edges.  Does not apply to a wireframe representation.\n \n     @opacity.setter\n     def opacity(self, value: float):\n         self.SetOpacity(value)\n \n     @property\n     def ambient(self) -> float:\n         \"\"\"Return or set ambient.\n \n        When lighting is enabled, this is the amount of light in\n        the range of 0 to 1 (default 0.0) that reaches the actor\n        when not directed at the light source emitted from the\n        viewer.\n \n         Examples\n         --------\n \n     @ambient.setter\n     def ambient(self, value: float):\n         self.SetAmbient(value)\n \n     @property\n     def diffuse(self) -> float:\n         \"\"\"Return or set the diffuse lighting coefficient.\n \n        Default 1.0.\n \n        This is the scattering of light by reflection or transmission. Diffuse\n        reflection results when light strikes an irregular surface such as a\n        frosted window or the surface of a frosted or coated light bulb.\n \n         Examples\n         --------\n \n     @diffuse.setter\n     def diffuse(self, value: float):\n         self.SetDiffuse(value)\n \n     @property\n     def specular(self) -> float:\n         \"\"\"Return or set specular.\n \n        Default 0.0\n \n        Specular lighting simulates the bright spot of a light that appears on\n        shiny objects.\n \n         Examples\n         --------\n \n     @specular.setter\n     def specular(self, value: float):\n         self.SetSpecular(value)\n \n     @property\n     def specular_power(self) -> float:\n         \"\"\"Return or set specular power.\n \n        The specular power. Between 0.0 and 128.0. Default 1.0\n \n         Examples\n         --------\n \n     @specular_power.setter\n     def specular_power(self, value: float):\n         self.SetSpecularPower(value)\n \n     @property\n     def metallic(self) -> float:\n         \"\"\"Return or set metallic.\n \n         This requires that the interpolation be set to ``'Physically based\n        rendering'``\n \n         Examples\n         --------\n \n     @metallic.setter\n     def metallic(self, value: float):\n         self.SetMetallic(value)\n \n     @property\n     def roughness(self) -> float:\n         \"\"\"Return or set roughness.\n \n         This requires that the interpolation be set to ``'Physically based\n        rendering'``\n \n         Examples\n         --------\n \n     @roughness.setter\n     def roughness(self, value: bool):\n         self.SetRoughness(value)\n \n     @property\n     def interpolation(self) -> InterpolationType:\n         \"\"\"Return or set the method of shading.\n \n         One of the following options.\n \n         * ``'Physically based rendering'`` - Physically based rendering.\n     def render_points_as_spheres(self) -> bool:\n         \"\"\"Return or set rendering points as spheres.\n \n         Requires representation style be set to ``'points'``.\n \n         Examples\n         --------\n        Enable rendering points as spheres\n \n         >>> import pyvista as pv\n         >>> prop = pv.Property()\n     def render_lines_as_tubes(self) -> bool:\n         \"\"\"Return or set rendering lines as tubes.\n \n         Requires representation style be set to ``'wireframe'``.\n \n         Examples\n     def line_width(self) -> float:\n         \"\"\"Return or set the line width.\n \n         Examples\n         --------\n         Change the line width to ``10``.\n     def point_size(self):\n         \"\"\"Return or set the point size.\n \n         Examples\n         --------\n         Change the point size to ``10.0``.\n             from pyvista.core.errors import VTKVersionError\n \n             raise VTKVersionError('Anisotropy requires VTK v9.1.0 or newer.')\n         self.SetAnisotropy(value)\n \n     def plot(self, **kwargs) -> None:\ndiff --git a/pyvista/plotting/composite_mapper.py b/pyvista/plotting/composite_mapper.py\n from pyvista import _vtk\n from pyvista.utilities import convert_array, convert_string_array\n \nfrom ..utilities.misc import vtk_version_info\n from .colors import Color\n from .mapper import _BaseMapper\n \n             self._attr.Modified()\n             return\n \n         self._attr.SetBlockOpacity(self._block, new_opacity)\n \n     @property\ndiff --git a/pyvista/themes.py b/pyvista/themes.py\n from .plotting.opts import InterpolationType\n from .plotting.plotting import Plotter\n from .plotting.tools import parse_font_family\nfrom .utilities.misc import PyVistaDeprecationWarning\n \n \n class _rcParams(dict):  # pragma: no cover\n         return repr(pyvista.global_theme)\n \n \ndef _check_between_zero_and_one(value: float, value_name: str = 'value'):\n    \"\"\"Check if a value is between zero and one.\"\"\"\n    if value < 0 or value > 1:\n        raise ValueError(f'{value_name} must be between 0 and 1.')\n\n\n def load_theme(filename):\n     \"\"\"Load a theme from a file.\n \n     def metallic(self) -> float:\n         \"\"\"Return or set the metallic value.\n \n        Usually this value is either 0 or 1 for a real material but any\n        value in between is valid. This parameter is only used by PBR\n        interpolation.\n \n         Examples\n         --------\n \n     @metallic.setter\n     def metallic(self, metallic: float):\n         self._metallic = metallic\n \n     @property\n \n     @roughness.setter\n     def roughness(self, roughness: float):\n         self._roughness = roughness\n \n     @property\n \n     @ambient.setter\n     def ambient(self, ambient: float):\n         self._ambient = ambient\n \n     @property\n     def diffuse(self) -> float:\n         \"\"\"Return or set the diffuse value.\n \n         Examples\n         --------\n         Set the global diffuse lighting value to ``0.5``.\n \n     @diffuse.setter\n     def diffuse(self, diffuse: float):\n         self._diffuse = diffuse\n \n     @property\n     def specular(self) -> float:\n         \"\"\"Return or set the specular value.\n \n        Should be between 0 and 1.\n \n         Examples\n         --------\n \n     @specular.setter\n     def specular(self, specular: float):\n         self._specular = specular\n \n     @property\n     def specular_power(self) -> float:\n         \"\"\"Return or set the specular power value.\n \n         Examples\n         --------\n         Set the global specular power value to ``50``.\n \n     @specular_power.setter\n     def specular_power(self, specular_power: float):\n         self._specular_power = specular_power\n \n     @property\n \n     @opacity.setter\n     def opacity(self, opacity: float):\n        _check_between_zero_and_one(opacity, 'opacity')\n         self._opacity = float(opacity)\n \n     @property\n         if decimate is None:\n             self._decimate = None\n         else:\n            _check_between_zero_and_one(decimate, 'decimate')\n             self._decimate = float(decimate)\n \n     def __repr__(self):\n \n     @cap_opacity.setter\n     def cap_opacity(self, cap_opacity: float):\n         self._cap_opacity = float(cap_opacity)\n \n     @property\n \n     @opacity.setter\n     def opacity(self, opacity: float):\n        _check_between_zero_and_one(opacity, 'opacity')\n         self._opacity = float(opacity)\n \n     @property\ndiff --git a/pyvista/utilities/misc.py b/pyvista/utilities/misc.py\n             )\n \n \n @lru_cache(maxsize=None)\n def has_module(module_name):\n     \"\"\"Return if a module can be imported.\"\"\"",
    "output": " import pyvista as pv\n from pyvista import _vtk\n from pyvista.plotting.opts import InterpolationType\nfrom pyvista.utilities.misc import _check_range, no_new_attr\n \n from .colors import Color\n \n         The specular lighting coefficient.\n \n     specular_power : float, default: :attr:`pyvista.themes._LightingConfig.specular_power`\n        The specular power. Must be between 0.0 and 128.0.\n \n     show_edges : bool, default: :attr:`pyvista.themes.DefaultTheme.show_edges`\n         Shows the edges.  Does not apply to a wireframe representation.\n \n     @opacity.setter\n     def opacity(self, value: float):\n        _check_range(value, (0, 1), 'opacity')\n         self.SetOpacity(value)\n \n     @property\n     def ambient(self) -> float:\n         \"\"\"Return or set ambient.\n \n        Default Default :attr:`pyvista.themes._LightingConfig.ambient`.\n\n        When lighting is enabled, this is the amount of light in the range\n        of 0 to 1 that reaches the actor when not directed at the light\n        source emitted from the viewer.\n \n         Examples\n         --------\n \n     @ambient.setter\n     def ambient(self, value: float):\n        _check_range(value, (0, 1), 'ambient')\n         self.SetAmbient(value)\n \n     @property\n     def diffuse(self) -> float:\n         \"\"\"Return or set the diffuse lighting coefficient.\n \n        Default :attr:`pyvista.themes._LightingConfig.diffuse`.\n \n        This is the scattering of light by reflection or\n        transmission. Diffuse reflection results when light strikes an\n        irregular surface such as a frosted window or the surface of a\n        frosted or coated light bulb. Must be between 0 and 1.\n \n         Examples\n         --------\n \n     @diffuse.setter\n     def diffuse(self, value: float):\n        _check_range(value, (0, 1), 'diffuse')\n         self.SetDiffuse(value)\n \n     @property\n     def specular(self) -> float:\n         \"\"\"Return or set specular.\n \n        Default Default :attr:`pyvista.themes._LightingConfig.specular`.\n \n        Specular lighting simulates the bright spot of a light that appears\n        on shiny objects. Must be between 0 and 1.\n \n         Examples\n         --------\n \n     @specular.setter\n     def specular(self, value: float):\n        _check_range(value, (0, 1), 'specular')\n         self.SetSpecular(value)\n \n     @property\n     def specular_power(self) -> float:\n         \"\"\"Return or set specular power.\n \n        Default :attr:`pyvista.themes._LightingConfig.specular_power`.\n\n        The specular power. Must be between 0.0 and 128.0.\n \n         Examples\n         --------\n \n     @specular_power.setter\n     def specular_power(self, value: float):\n        _check_range(value, (0, 128), 'specular_power')\n         self.SetSpecularPower(value)\n \n     @property\n     def metallic(self) -> float:\n         \"\"\"Return or set metallic.\n \n        Default :attr:`pyvista.themes._LightingConfig.metallic`.\n\n         This requires that the interpolation be set to ``'Physically based\n        rendering'``. Must be between 0 and 1.\n \n         Examples\n         --------\n \n     @metallic.setter\n     def metallic(self, value: float):\n        _check_range(value, (0, 1), 'metallic')\n         self.SetMetallic(value)\n \n     @property\n     def roughness(self) -> float:\n         \"\"\"Return or set roughness.\n \n        Default :attr:`pyvista.themes._LightingConfig.roughness`.\n\n         This requires that the interpolation be set to ``'Physically based\n        rendering'``. Must be between 0 and 1.\n \n         Examples\n         --------\n \n     @roughness.setter\n     def roughness(self, value: bool):\n        _check_range(value, (0, 1), 'roughness')\n         self.SetRoughness(value)\n \n     @property\n     def interpolation(self) -> InterpolationType:\n         \"\"\"Return or set the method of shading.\n \n        Defaults to :attr:`pyvista.themes._LightingConfig.interpolation`.\n\n         One of the following options.\n \n         * ``'Physically based rendering'`` - Physically based rendering.\n     def render_points_as_spheres(self) -> bool:\n         \"\"\"Return or set rendering points as spheres.\n \n        Defaults to :attr:`pyvista.themes.DefaultTheme.render_points_as_spheres`.\n\n         Requires representation style be set to ``'points'``.\n \n         Examples\n         --------\n        Enable rendering points as spheres.\n \n         >>> import pyvista as pv\n         >>> prop = pv.Property()\n     def render_lines_as_tubes(self) -> bool:\n         \"\"\"Return or set rendering lines as tubes.\n \n        Defaults to :attr:`pyvista.themes.DefaultTheme.render_lines_as_tubes`.\n\n         Requires representation style be set to ``'wireframe'``.\n \n         Examples\n     def line_width(self) -> float:\n         \"\"\"Return or set the line width.\n \n        Defaults to :attr:`pyvista.themes.DefaultTheme.line_width`.\n\n         Examples\n         --------\n         Change the line width to ``10``.\n     def point_size(self):\n         \"\"\"Return or set the point size.\n \n        Defaults to :attr:`pyvista.themes.DefaultTheme.point_size`.\n\n         Examples\n         --------\n         Change the point size to ``10.0``.\n             from pyvista.core.errors import VTKVersionError\n \n             raise VTKVersionError('Anisotropy requires VTK v9.1.0 or newer.')\n        _check_range(value, (0, 1), 'anisotropy')\n         self.SetAnisotropy(value)\n \n     def plot(self, **kwargs) -> None:\ndiff --git a/pyvista/plotting/composite_mapper.py b/pyvista/plotting/composite_mapper.py\n from pyvista import _vtk\n from pyvista.utilities import convert_array, convert_string_array\n \nfrom ..utilities.misc import _check_range, vtk_version_info\n from .colors import Color\n from .mapper import _BaseMapper\n \n             self._attr.Modified()\n             return\n \n        _check_range(new_opacity, (0, 1), 'opacity')\n         self._attr.SetBlockOpacity(self._block, new_opacity)\n \n     @property\ndiff --git a/pyvista/themes.py b/pyvista/themes.py\n from .plotting.opts import InterpolationType\n from .plotting.plotting import Plotter\n from .plotting.tools import parse_font_family\nfrom .utilities.misc import PyVistaDeprecationWarning, _check_range\n \n \n class _rcParams(dict):  # pragma: no cover\n         return repr(pyvista.global_theme)\n \n \n def load_theme(filename):\n     \"\"\"Load a theme from a file.\n \n     def metallic(self) -> float:\n         \"\"\"Return or set the metallic value.\n \n        This requires that the interpolation be set to ``'Physically based\n        rendering'``. Must be between 0 and 1.\n \n         Examples\n         --------\n \n     @metallic.setter\n     def metallic(self, metallic: float):\n        _check_range(metallic, (0, 1), 'metallic')\n         self._metallic = metallic\n \n     @property\n \n     @roughness.setter\n     def roughness(self, roughness: float):\n        _check_range(roughness, (0, 1), 'roughness')\n         self._roughness = roughness\n \n     @property\n \n     @ambient.setter\n     def ambient(self, ambient: float):\n        _check_range(ambient, (0, 1), 'ambient')\n         self._ambient = ambient\n \n     @property\n     def diffuse(self) -> float:\n         \"\"\"Return or set the diffuse value.\n \n        This is the scattering of light by reflection or\n        transmission. Diffuse reflection results when light strikes an\n        irregular surface such as a frosted window or the surface of a\n        frosted or coated light bulb. Must be between 0 and 1.\n\n         Examples\n         --------\n         Set the global diffuse lighting value to ``0.5``.\n \n     @diffuse.setter\n     def diffuse(self, diffuse: float):\n        _check_range(diffuse, (0, 1), 'diffuse')\n         self._diffuse = diffuse\n \n     @property\n     def specular(self) -> float:\n         \"\"\"Return or set the specular value.\n \n        Specular lighting simulates the bright spot of a light that appears\n        on shiny objects. Must be between 0 and 1.\n \n         Examples\n         --------\n \n     @specular.setter\n     def specular(self, specular: float):\n        _check_range(specular, (0, 1), 'specular')\n         self._specular = specular\n \n     @property\n     def specular_power(self) -> float:\n         \"\"\"Return or set the specular power value.\n \n        Must be between 0.0 and 128.0.\n\n         Examples\n         --------\n         Set the global specular power value to ``50``.\n \n     @specular_power.setter\n     def specular_power(self, specular_power: float):\n        _check_range(specular_power, (0, 128), 'specular_power')\n         self._specular_power = specular_power\n \n     @property\n \n     @opacity.setter\n     def opacity(self, opacity: float):\n        _check_range(opacity, (0, 1), 'opacity')\n         self._opacity = float(opacity)\n \n     @property\n         if decimate is None:\n             self._decimate = None\n         else:\n            _check_range(decimate, (0, 1), 'decimate')\n             self._decimate = float(decimate)\n \n     def __repr__(self):\n \n     @cap_opacity.setter\n     def cap_opacity(self, cap_opacity: float):\n        _check_range(cap_opacity, (0, 1), 'cap_opacity')\n         self._cap_opacity = float(cap_opacity)\n \n     @property\n \n     @opacity.setter\n     def opacity(self, opacity: float):\n        _check_range(opacity, (0, 1), 'opacity')\n         self._opacity = float(opacity)\n \n     @property\ndiff --git a/pyvista/utilities/misc.py b/pyvista/utilities/misc.py\n             )\n \n \ndef _check_range(value, rng, parm_name):\n    \"\"\"Check if a parameter is within a range.\"\"\"\n    if value < rng[0] or value > rng[1]:\n        raise ValueError(\n            f'The value {float(value)} for `{parm_name}` is outside the acceptable range {tuple(rng)}.'\n        )\n\n\n @lru_cache(maxsize=None)\n def has_module(module_name):\n     \"\"\"Return if a module can be imported.\"\"\""
  },
  {
    "instruction": "Unexpected threshold behavior\n### Describe the bug, what's wrong, and what you expected.\n\nI'm using simple structed grids of cells, and need to filter-out some \"nodata\" cells. To do this, I'm setting scalar values to the cell data, then using [threshold](https://docs.pyvista.org/api/core/_autosummary/pyvista.DataSetFilters.threshold.html) with the nodata value with `invert=True`. However, I'm getting confusing and inconsistent results compared to ParaView.\n\n### Steps to reproduce the bug.\n\n```python\r\nimport numpy as np\r\nimport pyvista\r\n\r\nx = np.arange(5, dtype=float)\r\ny = np.arange(6, dtype=float)\r\nz = np.arange(2, dtype=float)\r\nxx, yy, zz = np.meshgrid(x, y, z)\r\nmesh = pyvista.StructuredGrid(xx, yy, zz)\r\nmesh.cell_data.set_scalars(np.repeat(range(5), 4))\r\n\r\n# All data\r\nmesh.plot(show_edges=True)\r\n# output is normal\r\n\r\n# Filtering out nodata (zero) values\r\nmesh.threshold(0, invert=True).plot(show_edges=True)\r\n# output does not look normal, only 0-value cells are shown\r\n```\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Thu Nov 17 15:23:57 2022 New Zealand Daylight Time\r\n\r\n                OS : Windows\r\n            CPU(s) : 12\r\n           Machine : AMD64\r\n      Architecture : 64bit\r\n               RAM : 31.7 GiB\r\n       Environment : IPython\r\n        GPU Vendor : NVIDIA Corporation\r\n      GPU Renderer : NVIDIA RTX A4000/PCIe/SSE2\r\n       GPU Version : 4.5.0 NVIDIA 472.39\r\n\r\n  Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC\r\n  v.1929 64 bit (AMD64)]\r\n\r\n           pyvista : 0.37.0\r\n               vtk : 9.1.0\r\n             numpy : 1.22.3\r\n           imageio : 2.22.0\r\n            scooby : 0.7.0\r\n             pooch : v1.6.0\r\n        matplotlib : 3.6.2\r\n             PyQt5 : 5.12.3\r\n           IPython : 8.6.0\r\n          colorcet : 3.0.1\r\n             scipy : 1.8.0\r\n              tqdm : 4.63.0\r\n            meshio : 5.3.4\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\nNormal looking whole grid:\r\n![image](https://user-images.githubusercontent.com/895458/202339692-5046b23f-c3c8-4b2c-aaa7-4aa06afbae9f.png)\r\n\r\nOdd-looking threshold attempt with pyvista, showing only 0-values:\r\n![image](https://user-images.githubusercontent.com/895458/202339879-b2270e4c-a71b-4d43-86f8-4f67445b7b69.png)\r\n\r\nExpected result with ParaView theshold filter with upper/lower set to 0 and invert selected:\r\n![image](https://user-images.githubusercontent.com/895458/202340379-fea26838-b0f4-4828-b510-825f53522e87.png)\r\n\r\nApologies for any \"user error\", as I'm new to this package.\n",
    "input": "         continuous=False,\n         preference='cell',\n         all_scalars=False,\n        progress_bar=False,\n        component_mode=\"all\",\n         component=0,\n     ):\n         \"\"\"Apply a ``vtkThreshold`` filter to the input dataset.\n \n            thresholding depends on whether that point is part of a cell that\n            is kept after thresholding.\n \n         Parameters\n         ----------\n         value : float or sequence, optional\n            Single value or (min, max) to be used for the data threshold.  If\n             a sequence, then length must be 2. If no value is specified, the\n             non-NaN data range will be used to remove any NaN values.\n \n         scalars : str, optional\n             Name of scalars to threshold on. Defaults to currently active scalars.\n \n        invert : bool, optional\n            If value is a single value, when invert is ``True`` cells\n            are kept when their values are below parameter ``\"value\"``.\n            When invert is ``False`` cells are kept when their value is\n            above the threshold ``\"value\"``.  Default is ``False``:\n            yielding above the threshold ``\"value\"``.\n \n        continuous : bool, optional\n             When True, the continuous interval [minimum cell scalar,\n             maximum cell scalar] will be used to intersect the threshold bound,\n             rather than the set of discrete scalar values from the vertices.\n \n        preference : str, optional\n             When ``scalars`` is specified, this is the preferred array\n             type to search for in the dataset.  Must be either\n            ``'point'`` or ``'cell'``.\n \n        all_scalars : bool, optional\n             If using scalars from point data, all\n             points in a cell must satisfy the threshold when this\n             value is ``True``.  When ``False``, any point of the cell\n             with a scalar value satisfying the threshold criterion\n             will extract the cell. Has no effect when using cell data.\n \n        progress_bar : bool, optional\n            Display a progress bar to indicate progress.\n\n         component_mode : {'selected', 'all', 'any'}\n             The method to satisfy the criteria for the threshold of\n             multicomponent scalars.  'selected' (default)\n             When using ``component_mode='selected'``, this sets\n             which component to threshold on.\n \n         Returns\n         -------\n         pyvista.UnstructuredGrid\n \n         field = get_array_association(self, scalars, preference=preference)\n \n        # If using an inverted range, merge the result of two filters:\n        if isinstance(value, (np.ndarray, collections.abc.Sequence)) and invert:\n            valid_range = [np.nanmin(arr), np.nanmax(arr)]\n            # Create two thresholds\n            t1 = self.threshold(\n                [valid_range[0], value[0]],\n                scalars=scalars,\n                continuous=continuous,\n                preference=preference,\n                invert=False,\n            )\n            t2 = self.threshold(\n                [value[1], valid_range[1]],\n                scalars=scalars,\n                continuous=continuous,\n                preference=preference,\n                invert=False,\n            )\n            # Use an AppendFilter to merge the two results\n            appender = _vtk.vtkAppendFilter()\n            appender.AddInputData(t1)\n            appender.AddInputData(t2)\n            _update_alg(appender, progress_bar, 'Thresholding')\n            return _get_output(appender)\n\n         # Run a standard threshold algorithm\n         alg = _vtk.vtkThreshold()\n         alg.SetAllScalars(all_scalars)\n         # use valid range if no value given\n         if value is None:\n             value = self.get_data_range(scalars)\n        # check if value is a sequence (if so threshold by min max range like ParaView)\n        if isinstance(value, (np.ndarray, collections.abc.Sequence)):\n            if len(value) != 2:\n                raise ValueError(\n                    f'Value range must be length one for a float value or two for min/max; not ({value}).'\n                )\n            alg.ThresholdBetween(value[0], value[1])\n        elif isinstance(value, collections.abc.Iterable):\n            raise TypeError('Value must either be a single scalar or a sequence.')\n        else:\n            # just a single value\n            _set_threshold_limit(alg, value, invert)\n         if component_mode == \"component\":\n             alg.SetComponentModeToUseSelected()\n             dim = arr.shape[1]\n             raise ValueError(\n                 f\"component_mode must be 'component', 'all', or 'any' got: {component_mode}\"\n             )\n         # Run the threshold\n         _update_alg(alg, progress_bar, 'Thresholding')\n         return _get_output(alg)\n         invert=False,\n         continuous=False,\n         preference='cell',\n         progress_bar=False,\n     ):\n         \"\"\"Threshold the dataset by a percentage of its range on the active scalars array.\n         scalars : str, optional\n             Name of scalars to threshold on. Defaults to currently active scalars.\n \n        invert : bool, optional\n            When invert is ``True`` cells are kept when their values are\n            below the percentage of the range.  When invert is\n            ``False``, cells are kept when their value is above the\n            percentage of the range. Default is ``False``: yielding\n            above the threshold ``\"value\"``.\n\n        continuous : bool, optional\n            When ``True``, the continuous interval [minimum cell scalar,\n            maximum cell scalar] will be used to intersect the threshold\n            bound, rather than the set of discrete scalar values from\n            the vertices.\n \n        preference : str, optional\n             When ``scalars`` is specified, this is the preferred array\n             type to search for in the dataset.  Must be either\n            ``'point'`` or ``'cell'``.\n \n        progress_bar : bool, optional\n             Display a progress bar to indicate progress.\n \n         Returns\n             invert=invert,\n             continuous=continuous,\n             preference=preference,\n             progress_bar=progress_bar,\n         )\n \n         return self.shrink(1.0)\n \n \ndef _set_threshold_limit(alg, value, invert):\n    \"\"\"Set vtkThreshold limit.\n \n    Addresses VTK API deprecation as pointed out in\n    https://github.com/pyvista/pyvista/issues/2850\n \n     \"\"\"\n    if invert:\n        if pyvista.vtk_version_info >= (9, 1):\n            alg.SetUpperThreshold(value)\n        else:  # pragma: no cover\n            alg.ThresholdByLower(value)\n    else:\n        if pyvista.vtk_version_info >= (9, 1):\n            alg.SetLowerThreshold(value)\n        else:  # pragma: no cover\n            alg.ThresholdByUpper(value)\ndiff --git a/pyvista/plotting/widgets.py b/pyvista/plotting/widgets.py\n         pointa=(0.4, 0.9),\n         pointb=(0.9, 0.9),\n         continuous=False,\n         **kwargs,\n     ):\n         \"\"\"Apply a threshold on a mesh with a slider.\n         scalars : str, optional\n             The string name of the scalars on the mesh to threshold and display.\n \n        invert : bool, optional\n            Invert (flip) the threshold.\n \n         widget_color : color_like, optional\n             Color of the widget.  Either a string, RGB sequence, or\n             * ``color=[1.0, 1.0, 1.0]``\n             * ``color='#FFFFFF'``\n \n        preference : str, optional\n             When ``mesh.n_points == mesh.n_cells`` and setting\n             scalars, this parameter sets how the scalars will be\n             mapped to the mesh.  Default ``'cell'``, causes the\n         title : str, optional\n             The string label of the slider widget.\n \n        pointa : sequence, optional\n             The relative coordinates of the left point of the slider\n             on the display port.\n \n        pointb : sequence, optional\n             The relative coordinates of the right point of the slider\n             on the display port.\n \n        continuous : bool, optional\n             If this is enabled (default is ``False``), use the continuous\n             interval ``[minimum cell scalar, maximum cell scalar]``\n             to intersect the threshold bound, rather than the set of\n             discrete scalar values from the vertices.\n \n         **kwargs : dict, optional\n             All additional keyword arguments are passed to ``add_mesh`` to\n             control how the mesh is displayed.\n             0, 0, 0, field.value, scalars\n         )  # args: (idx, port, connection, field, name)\n         alg.SetUseContinuousCellRange(continuous)\n \n         threshold_mesh = pyvista.wrap(alg.GetOutput())\n         self.threshold_meshes.append(threshold_mesh)\n \n         def callback(value):\n            _set_threshold_limit(alg, value, invert)\n             alg.Update()\n             threshold_mesh.shallow_copy(alg.GetOutput())\n ",
    "output": "         continuous=False,\n         preference='cell',\n         all_scalars=False,\n        component_mode='all',\n         component=0,\n        method='upper',\n        progress_bar=False,\n     ):\n         \"\"\"Apply a ``vtkThreshold`` filter to the input dataset.\n \n            thresholding depends on whether that point is part of a cell that\n            is kept after thresholding.\n \n           Please also note the default ``preference`` choice for CELL data\n           over POINT data. This is contrary to most other places in PyVista's\n           API where the preference typically defaults to POINT data. We chose\n           to prefer CELL data here so that if thresholding by a named array\n           that exists for both the POINT and CELL data, this filter will\n           default to the CELL data array while performing the CELL-wise\n           operation.\n\n         Parameters\n         ----------\n         value : float or sequence, optional\n            Single value or (min, max) to be used for the data threshold. If\n             a sequence, then length must be 2. If no value is specified, the\n             non-NaN data range will be used to remove any NaN values.\n            Please reference the ``method`` parameter for how single values\n            are handled.\n \n         scalars : str, optional\n             Name of scalars to threshold on. Defaults to currently active scalars.\n \n        invert : bool, default: False\n            Invert the threshold results. That is, cells that would have been\n            in the output with this option off are excluded, while cells that\n            would have been excluded from the output are included.\n \n            .. warning::\n                This option is only supported for VTK version 9+\n\n        continuous : bool, default: False\n             When True, the continuous interval [minimum cell scalar,\n             maximum cell scalar] will be used to intersect the threshold bound,\n             rather than the set of discrete scalar values from the vertices.\n \n        preference : str, default: 'cell'\n             When ``scalars`` is specified, this is the preferred array\n             type to search for in the dataset.  Must be either\n            ``'point'`` or ``'cell'``. Throughout PyVista, the preference\n            is typically ``'point'`` but since the threshold filter is a\n            cell-wise operation, we prefer cell data for thresholding\n            operations.\n \n        all_scalars : bool, default: False\n             If using scalars from point data, all\n             points in a cell must satisfy the threshold when this\n             value is ``True``.  When ``False``, any point of the cell\n             with a scalar value satisfying the threshold criterion\n             will extract the cell. Has no effect when using cell data.\n \n         component_mode : {'selected', 'all', 'any'}\n             The method to satisfy the criteria for the threshold of\n             multicomponent scalars.  'selected' (default)\n             When using ``component_mode='selected'``, this sets\n             which component to threshold on.\n \n        method : str, default: 'upper'\n            Set the threshold method for single-values, defining which\n            threshold bounds to use. If the ``value`` is a range, this\n            parameter will be ignored, extracting data between the two\n            values. For single values, ``'lower'`` will extract data\n            lower than the  ``value``. ``'upper'`` will extract data\n            larger than the ``value``.\n\n        progress_bar : bool, default: False\n            Display a progress bar to indicate progress.\n\n         Returns\n         -------\n         pyvista.UnstructuredGrid\n \n         field = get_array_association(self, scalars, preference=preference)\n \n         # Run a standard threshold algorithm\n         alg = _vtk.vtkThreshold()\n         alg.SetAllScalars(all_scalars)\n         # use valid range if no value given\n         if value is None:\n             value = self.get_data_range(scalars)\n\n        _set_threshold_limit(alg, value, method, invert)\n\n         if component_mode == \"component\":\n             alg.SetComponentModeToUseSelected()\n             dim = arr.shape[1]\n             raise ValueError(\n                 f\"component_mode must be 'component', 'all', or 'any' got: {component_mode}\"\n             )\n\n         # Run the threshold\n         _update_alg(alg, progress_bar, 'Thresholding')\n         return _get_output(alg)\n         invert=False,\n         continuous=False,\n         preference='cell',\n        method='upper',\n         progress_bar=False,\n     ):\n         \"\"\"Threshold the dataset by a percentage of its range on the active scalars array.\n         scalars : str, optional\n             Name of scalars to threshold on. Defaults to currently active scalars.\n \n        invert : bool, default: False\n            Invert the threshold results. That is, cells that would have been\n            in the output with this option off are excluded, while cells that\n            would have been excluded from the output are included.\n \n            .. warning::\n                This option is only supported for VTK version 9+\n\n        continuous : bool, default: False\n            When True, the continuous interval [minimum cell scalar,\n            maximum cell scalar] will be used to intersect the threshold bound,\n            rather than the set of discrete scalar values from the vertices.\n\n        preference : str, default: 'cell'\n             When ``scalars`` is specified, this is the preferred array\n             type to search for in the dataset.  Must be either\n            ``'point'`` or ``'cell'``. Throughout PyVista, the preference\n            is typically ``'point'`` but since the threshold filter is a\n            cell-wise operation, we prefer cell data for thresholding\n            operations.\n\n        method : str, default: 'upper'\n            Set the threshold method for single-values, defining which\n            threshold bounds to use. If the ``value`` is a range, this\n            parameter will be ignored, extracting data between the two\n            values. For single values, ``'lower'`` will extract data\n            lower than the  ``value``. ``'upper'`` will extract data\n            larger than the ``value``.\n \n        progress_bar : bool, default: False\n             Display a progress bar to indicate progress.\n \n         Returns\n             invert=invert,\n             continuous=continuous,\n             preference=preference,\n            method=method,\n             progress_bar=progress_bar,\n         )\n \n         return self.shrink(1.0)\n \n \ndef _set_threshold_limit(alg, value, method, invert):\n    \"\"\"Set vtkThreshold limits and function.\n\n    Addresses VTK API deprecations and previous PyVista inconsistencies with ParaView. Reference:\n \n    * https://github.com/pyvista/pyvista/issues/2850\n    * https://github.com/pyvista/pyvista/issues/3610\n    * https://discourse.vtk.org/t/unnecessary-vtk-api-change/9929\n \n     \"\"\"\n    # Check value\n    if isinstance(value, (np.ndarray, collections.abc.Sequence)):\n        if len(value) != 2:\n            raise ValueError(\n                f'Value range must be length one for a float value or two for min/max; not ({value}).'\n            )\n        # Check range\n        if value[0] > value[1]:\n            raise ValueError(\n                'Value sequence is invalid, please use (min, max). The provided first value is greater than the second.'\n            )\n    elif isinstance(value, collections.abc.Iterable):\n        raise TypeError('Value must either be a single scalar or a sequence.')\n    if pyvista.vtk_version_info >= (9,):\n        alg.SetInvert(invert)\n    elif invert:  # pragma: no cover\n        raise ValueError('PyVista no longer supports inverted thresholds for VTK<9.')\n    # Set values and function\n    if pyvista.vtk_version_info >= (9, 1):\n        if isinstance(value, (np.ndarray, collections.abc.Sequence)):\n            alg.SetThresholdFunction(_vtk.vtkThreshold.THRESHOLD_BETWEEN)\n            alg.SetLowerThreshold(value[0])\n            alg.SetUpperThreshold(value[1])\n        else:\n            # Single value\n            if method.lower() == 'lower':\n                alg.SetLowerThreshold(value)\n                alg.SetThresholdFunction(_vtk.vtkThreshold.THRESHOLD_LOWER)\n            elif method.lower() == 'upper':\n                alg.SetUpperThreshold(value)\n                alg.SetThresholdFunction(_vtk.vtkThreshold.THRESHOLD_UPPER)\n            else:\n                raise ValueError('Invalid method choice. Either `lower` or `upper`')\n    else:  # pragma: no cover\n        # ThresholdByLower, ThresholdByUpper, ThresholdBetween\n        if isinstance(value, (np.ndarray, collections.abc.Sequence)):\n            alg.ThresholdBetween(value[0], value[1])\n        else:\n            # Single value\n            if method.lower() == 'lower':\n                alg.ThresholdByLower(value)\n            elif method.lower() == 'upper':\n                alg.ThresholdByUpper(value)\n            else:\n                raise ValueError('Invalid method choice. Either `lower` or `upper`')\ndiff --git a/pyvista/plotting/widgets.py b/pyvista/plotting/widgets.py\n         pointa=(0.4, 0.9),\n         pointb=(0.9, 0.9),\n         continuous=False,\n        all_scalars=False,\n        method='upper',\n         **kwargs,\n     ):\n         \"\"\"Apply a threshold on a mesh with a slider.\n         scalars : str, optional\n             The string name of the scalars on the mesh to threshold and display.\n \n        invert : bool, default: False\n            Invert the threshold results. That is, cells that would have been\n            in the output with this option off are excluded, while cells that\n            would have been excluded from the output are included.\n\n            .. warning::\n                This option is only supported for VTK version 9+\n \n         widget_color : color_like, optional\n             Color of the widget.  Either a string, RGB sequence, or\n             * ``color=[1.0, 1.0, 1.0]``\n             * ``color='#FFFFFF'``\n \n        preference : str, default: 'cell'\n             When ``mesh.n_points == mesh.n_cells`` and setting\n             scalars, this parameter sets how the scalars will be\n             mapped to the mesh.  Default ``'cell'``, causes the\n         title : str, optional\n             The string label of the slider widget.\n \n        pointa : sequence, default: (0.4, 0.9)\n             The relative coordinates of the left point of the slider\n             on the display port.\n \n        pointb : sequence, default: (0.9, 0.9)\n             The relative coordinates of the right point of the slider\n             on the display port.\n \n        continuous : bool, default: False\n             If this is enabled (default is ``False``), use the continuous\n             interval ``[minimum cell scalar, maximum cell scalar]``\n             to intersect the threshold bound, rather than the set of\n             discrete scalar values from the vertices.\n \n        all_scalars : bool, default: False\n            If using scalars from point data, all\n            points in a cell must satisfy the threshold when this\n            value is ``True``.  When ``False``, any point of the cell\n            with a scalar value satisfying the threshold criterion\n            will extract the cell. Has no effect when using cell data.\n\n        method : str, default: 'upper'\n            Set the threshold method for single-values, defining which\n            threshold bounds to use. If the ``value`` is a range, this\n            parameter will be ignored, extracting data between the two\n            values. For single values, ``'lower'`` will extract data\n            lower than the  ``value``. ``'upper'`` will extract data\n            larger than the ``value``.\n\n         **kwargs : dict, optional\n             All additional keyword arguments are passed to ``add_mesh`` to\n             control how the mesh is displayed.\n             0, 0, 0, field.value, scalars\n         )  # args: (idx, port, connection, field, name)\n         alg.SetUseContinuousCellRange(continuous)\n        alg.SetAllScalars(all_scalars)\n \n         threshold_mesh = pyvista.wrap(alg.GetOutput())\n         self.threshold_meshes.append(threshold_mesh)\n \n         def callback(value):\n            _set_threshold_limit(alg, value, method, invert)\n             alg.Update()\n             threshold_mesh.shallow_copy(alg.GetOutput())\n "
  },
  {
    "instruction": "`bounds` property return type is mutable from `MultiBlock`\nThe `bounds` property has a different return type for meshes and `MultiBlock` objects:\r\n\r\n```\r\n>>> import pyvista as pv\r\n>>> slices = pv.Sphere().slice_orthogonal()\r\n# MultiBlock returns list (mutable)\r\n>>> slices.bounds\r\n[-0.49926671385765076, 0.49926671385765076, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5]\r\n# Mesh returns tuple (immutable)\r\n>>> slices[0].bounds\r\n(-6.162975822039155e-33, 0.0, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5)\r\n```\r\n\r\nIMO, the return value should be immutable and the `bounds` property should  be cast to a tuple before returning.\n",
    "input": "                 self.SetBlock(i, pyvista.wrap(block))\n \n     @property\n    def bounds(self) -> List[float]:\n         \"\"\"Find min/max for bounds across blocks.\n \n         Returns\n         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n         >>> blocks = pv.MultiBlock(data)\n         >>> blocks.bounds\n        [-0.5, 2.5, -0.5, 2.5, -0.5, 0.5]\n \n         \"\"\"\n         # apply reduction of min and max over each block\n         all_bounds = [block.bounds for block in self if block]\n         # edge case where block has no bounds\n         if not all_bounds:  # pragma: no cover\n            minima = np.array([0, 0, 0])\n            maxima = np.array([0, 0, 0])\n         else:\n             minima = np.minimum.reduce(all_bounds)[::2]\n             maxima = np.maximum.reduce(all_bounds)[1::2]\n \n         # interleave minima and maxima for bounds\n        return np.stack([minima, maxima]).ravel('F').tolist()\n \n     @property\n     def center(self) -> Any:\ndiff --git a/pyvista/plotting/actor.py b/pyvista/plotting/actor.py\n \"\"\"Wrap vtkActor.\"\"\"\n \nfrom typing import Optional, Union\n import weakref\n \n import numpy as np\n         self.SetUserMatrix(value)\n \n     @property\n    def bounds(self) -> tuple:\n         \"\"\"Return the bounds of the actor.\n \n         Bounds are ``(-X, +X, -Y, +Y, -Z, +Z)``\ndiff --git a/pyvista/plotting/plotting.py b/pyvista/plotting/plotting.py\n import textwrap\n from threading import Thread\n import time\nfrom typing import Dict\n import warnings\n import weakref\n \n         self.renderer.camera_set = is_set\n \n     @property\n    def bounds(self):\n         \"\"\"Return the bounds of the active renderer.\n \n         Returns\n         >>> pl = pyvista.Plotter()\n         >>> _ = pl.add_mesh(pyvista.Cube())\n         >>> pl.bounds\n        [-0.5, 0.5, -0.5, 0.5, -0.5, 0.5]\n \n         \"\"\"\n         return self.renderer.bounds\ndiff --git a/pyvista/plotting/renderer.py b/pyvista/plotting/renderer.py\n \n import collections.abc\n from functools import partial\nfrom typing import Sequence\n import warnings\n from weakref import proxy\n \n         self.camera_set = True\n \n     @property\n    def bounds(self):\n         \"\"\"Return the bounds of all actors present in the rendering window.\"\"\"\n         the_bounds = np.array([np.inf, -np.inf, np.inf, -np.inf, np.inf, -np.inf])\n \n             the_bounds[the_bounds == np.inf] = -1.0\n             the_bounds[the_bounds == -np.inf] = 1.0\n \n        return the_bounds.tolist()\n \n     @property\n     def length(self):",
    "output": "                 self.SetBlock(i, pyvista.wrap(block))\n \n     @property\n    def bounds(self) -> Tuple[float, float, float, float, float, float]:\n         \"\"\"Find min/max for bounds across blocks.\n \n         Returns\n         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n         >>> blocks = pv.MultiBlock(data)\n         >>> blocks.bounds\n        (-0.5, 2.5, -0.5, 2.5, -0.5, 0.5)\n \n         \"\"\"\n         # apply reduction of min and max over each block\n         all_bounds = [block.bounds for block in self if block]\n         # edge case where block has no bounds\n         if not all_bounds:  # pragma: no cover\n            minima = np.array([0.0, 0.0, 0.0])\n            maxima = np.array([0.0, 0.0, 0.0])\n         else:\n             minima = np.minimum.reduce(all_bounds)[::2]\n             maxima = np.maximum.reduce(all_bounds)[1::2]\n \n         # interleave minima and maxima for bounds\n        the_bounds = np.stack([minima, maxima]).ravel('F')\n\n        return cast(Tuple[float, float, float, float, float, float], tuple(the_bounds))\n \n     @property\n     def center(self) -> Any:\ndiff --git a/pyvista/plotting/actor.py b/pyvista/plotting/actor.py\n \"\"\"Wrap vtkActor.\"\"\"\n \nfrom typing import Optional, Tuple, Union\n import weakref\n \n import numpy as np\n         self.SetUserMatrix(value)\n \n     @property\n    def bounds(self) -> Tuple[float, float, float, float, float, float]:\n         \"\"\"Return the bounds of the actor.\n \n         Bounds are ``(-X, +X, -Y, +Y, -Z, +Z)``\ndiff --git a/pyvista/plotting/plotting.py b/pyvista/plotting/plotting.py\n import textwrap\n from threading import Thread\n import time\nfrom typing import Dict, Tuple\n import warnings\n import weakref\n \n         self.renderer.camera_set = is_set\n \n     @property\n    def bounds(self) -> Tuple[float, float, float, float, float, float]:\n         \"\"\"Return the bounds of the active renderer.\n \n         Returns\n         >>> pl = pyvista.Plotter()\n         >>> _ = pl.add_mesh(pyvista.Cube())\n         >>> pl.bounds\n        (-0.5, 0.5, -0.5, 0.5, -0.5, 0.5)\n \n         \"\"\"\n         return self.renderer.bounds\ndiff --git a/pyvista/plotting/renderer.py b/pyvista/plotting/renderer.py\n \n import collections.abc\n from functools import partial\nfrom typing import Sequence, Tuple, cast\n import warnings\n from weakref import proxy\n \n         self.camera_set = True\n \n     @property\n    def bounds(self) -> Tuple[float, float, float, float, float, float]:\n         \"\"\"Return the bounds of all actors present in the rendering window.\"\"\"\n         the_bounds = np.array([np.inf, -np.inf, np.inf, -np.inf, np.inf, -np.inf])\n \n             the_bounds[the_bounds == np.inf] = -1.0\n             the_bounds[the_bounds == -np.inf] = 1.0\n \n        return cast(Tuple[float, float, float, float, float, float], tuple(the_bounds))\n \n     @property\n     def length(self):"
  },
  {
    "instruction": "PolyData faces array is not updatable in-place and has unexpected behavior\n### Describe the bug, what's wrong, and what you expected.\n\nWhen accessing `PolyData.faces` (and likely other cell data), we cannot update the array in place. Further, there is some unexpected behavior where accessing `PolyData.faces` will override existing, modified views of the array.\n\n### Steps to reproduce the bug.\n\n```python \r\n>>> import pyvista as pv\r\n>>> mesh = pv.Sphere()\r\n>>> f = mesh.faces\r\n>>> f\r\narray([  3,   2,  30, ..., 840,  29,  28])\r\n>>> a = f[1:4]\r\n>>> a\r\narray([ 2, 30,  0])\r\n>>> b = f[5:8]\r\n>>> b\r\narray([30, 58,  0])\r\n>>> f[1:4] = b\r\n>>> f[5:8] = a\r\n>>> f\r\narray([  3,  30,  58, ..., 840,  29,  28])\r\n>>> assert all(f[1:4] == b) and all(f[5:8] == a)\r\n>>> mesh.faces  # access overwrites `f` in place which is unexpected and causes the check above to now fail\r\n>>> assert all(f[1:4] == b) and all(f[5:8] == a)\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-82-08205e08097f> in <cell line: 13>()\r\n     11 assert all(f[1:4] == b) and all(f[5:8] == a)\r\n     12 mesh.faces  # access overwrites `f` in place\r\n---> 13 assert all(f[1:4] == b) and all(f[5:8] == a)\r\n\r\nAssertionError: \r\n ```\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Thu May 26 11:45:54 2022 MDT\r\n\r\n                OS : Darwin\r\n            CPU(s) : 16\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n               RAM : 64.0 GiB\r\n       Environment : Jupyter\r\n       File system : apfs\r\n        GPU Vendor : ATI Technologies Inc.\r\n      GPU Renderer : AMD Radeon Pro 5500M OpenGL Engine\r\n       GPU Version : 4.1 ATI-4.8.13\r\n\r\n  Python 3.8.8 | packaged by conda-forge | (default, Feb 20 2021, 16:12:38)\r\n  [Clang 11.0.1 ]\r\n\r\n           pyvista : 0.35.dev0\r\n               vtk : 9.1.0\r\n             numpy : 1.22.1\r\n           imageio : 2.9.0\r\n           appdirs : 1.4.4\r\n            scooby : 0.5.12\r\n        matplotlib : 3.5.2\r\n           IPython : 7.32.0\r\n          colorcet : 3.0.0\r\n           cmocean : 2.0\r\n        ipyvtklink : 0.2.2\r\n             scipy : 1.8.0\r\n        itkwidgets : 0.32.1\r\n              tqdm : 4.60.0\r\n            meshio : 5.3.4\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's Code of Conduct\n",
    "input": " import pyvista\n from pyvista import _vtk\n from pyvista.utilities import abstract_class\nfrom pyvista.utilities.cells import CellArray, create_mixed_cells, get_mixed_cells\n \n from .._typing import BoundsLike\n from ..utilities.fileio import get_ext\n \n     @property\n     def faces(self) -> np.ndarray:\n        \"\"\"Return a pointer to the faces as a numpy array.\n \n         Returns\n         -------\n         numpy.ndarray\n            Array of face indices.\n \n         Examples\n         --------\n                [4, 1, 2, 5, 4],\n                [4, 3, 4, 7, 6],\n                [4, 4, 5, 8, 7]])\n         \"\"\"\n        return _vtk.vtk_to_numpy(self.GetPolys().GetData())\n \n     @faces.setter\n     def faces(self, faces):\n \n     @property\n     def cells(self) -> np.ndarray:\n        \"\"\"Return a pointer to the cells as a numpy object.\n \n         See Also\n         --------\n         pyvista.DataSet.get_cell\n \n         Examples\n         --------\n \n         >>> import pyvista\n         >>> from pyvista import examples\n        >>> hex_beam = pyvista.read(examples.hexbeamfile)\n        >>> hex_beam.cells[:18]  # doctest:+SKIP\n        array([ 8,  0,  2,  8,  7, 27, 36, 90, 81,  8,  2,  1,  4,\n                8, 36, 18, 54, 90])\n \n         \"\"\"\n        return _vtk.vtk_to_numpy(self.GetCells().GetData())\n \n     @property\n     def cells_dict(self) -> dict:\ndiff --git a/pyvista/demos/logo.py b/pyvista/demos/logo.py\n     v_grid_atom = atomize(v_grid)\n     v_grid_atom['scalars'] = v_grid_atom.points[:, 0]\n     v_grid_atom_surf = v_grid_atom.extract_surface()\n    faces = v_grid_atom_surf.faces.reshape(-1, 5)\n     faces[:, 1:] = faces[:, 1:][:, ::-1]\n     v_grid_atom_surf.faces = faces\n     plotter.add_mesh(",
    "output": " import pyvista\n from pyvista import _vtk\n from pyvista.utilities import abstract_class\nfrom pyvista.utilities.cells import CellArray, create_mixed_cells, get_mixed_cells, numpy_to_idarr\n \n from .._typing import BoundsLike\n from ..utilities.fileio import get_ext\n \n     @property\n     def faces(self) -> np.ndarray:\n        \"\"\"Return the connectivity array of the faces of this PolyData.\n\n        The faces array is organized as::\n\n           [n0, p0_0, p0_1, ..., p0_n, n1, p1_0, p1_1, ..., p1_n, ...]\n\n        where ``n0`` is the number of points in face 0, and ``pX_Y`` is the\n        Y'th point in face X.\n\n        For example, a triangle and a quadrilateral might be represented as::\n\n           [3, 0, 1, 2, 4, 0, 1, 3, 4]\n\n        Where the two individual faces would be ``[3, 0, 1, 2]`` and ``[4, 0, 1, 3, 4]``.\n \n         Returns\n         -------\n         numpy.ndarray\n            Array of face connectivity.\n\n        Notes\n        -----\n        The array returned cannot be modified in place and will raise a\n        ``ValueError`` if attempted.\n\n        You can, however, set the faces directly. See the example.\n \n         Examples\n         --------\n                [4, 1, 2, 5, 4],\n                [4, 3, 4, 7, 6],\n                [4, 4, 5, 8, 7]])\n\n        Set the faces directly. The following example creates a simple plane\n        with a single square faces and modifies it to have two triangles\n        instead.\n\n        >>> mesh = pv.Plane(i_resolution=1, j_resolution=1)\n        >>> mesh.faces = [3, 0, 1, 2, 3, 3, 2, 1]\n        >>> mesh.faces\n        array([3, 0, 1, 2, 3, 3, 2, 1])\n\n         \"\"\"\n        array = _vtk.vtk_to_numpy(self.GetPolys().GetData())\n        # Flag this array as read only to ensure users do not attempt to write to it.\n        array.flags['WRITEABLE'] = False\n        return array\n \n     @faces.setter\n     def faces(self, faces):\n \n     @property\n     def cells(self) -> np.ndarray:\n        \"\"\"Return the cell data as a numpy object.\n\n        This is the old style VTK data layout::\n\n           [n0, p0_0, p0_1, ..., p0_n, n1, p1_0, p1_1, ..., p1_n, ...]\n\n        where ``n0`` is the number of points in cell 0, and ``pX_Y`` is the\n        Y'th point in cell X.\n\n        For example, a triangle and a line might be represented as::\n\n           [3, 0, 1, 2, 2, 0, 1]\n\n        Where the two individual cells would be ``[3, 0, 1, 2]`` and ``[2, 0, 1]``.\n \n         See Also\n         --------\n         pyvista.DataSet.get_cell\n        pyvista.UnstructuredGrid.cell_connectivity\n        pyvista.UnstructuredGrid.offset\n\n        Notes\n        -----\n        The array returned cannot be modified in place and will raise a\n        ``ValueError`` if attempted.\n\n        You can, however, set the cells directly. See the example.\n \n         Examples\n         --------\n \n         >>> import pyvista\n         >>> from pyvista import examples\n        >>> grid = examples.load_hexbeam()\n        >>> grid.cells[:18]\n        array([ 8,  0,  2,  8,  7, 27, 36, 90, 81,  8,  2,  1,  4,  8, 36, 18, 54,\n               90])\n\n        While you cannot change the array inplace, you can overwrite it. For example:\n\n        >>> grid.cells = [8, 0, 1, 2, 3, 4, 5, 6, 7]\n \n         \"\"\"\n        # Flag this array as read only to ensure users do not attempt to write to it.\n        array = _vtk.vtk_to_numpy(self.GetCells().GetData())\n        array.flags['WRITEABLE'] = False\n        return array\n\n    @cells.setter\n    def cells(self, cells):\n        vtk_idarr = numpy_to_idarr(cells, deep=False, return_ind=False)\n        self.GetCells().ImportLegacyFormat(vtk_idarr)\n \n     @property\n     def cells_dict(self) -> dict:\ndiff --git a/pyvista/demos/logo.py b/pyvista/demos/logo.py\n     v_grid_atom = atomize(v_grid)\n     v_grid_atom['scalars'] = v_grid_atom.points[:, 0]\n     v_grid_atom_surf = v_grid_atom.extract_surface()\n    faces = v_grid_atom_surf.faces.reshape(-1, 5).copy()\n     faces[:, 1:] = faces[:, 1:][:, ::-1]\n     v_grid_atom_surf.faces = faces\n     plotter.add_mesh("
  },
  {
    "instruction": "Cell wise and dimension reducing filters do not work for PointSet\n### Describe the bug, what's wrong, and what you expected.\n\n`PointSet` is an odd data type as it has no cells and simply represents a 0-dimensional geometry: point clouds.\r\n\r\nThis means that two common types of operations are not possible on this data type:\r\n\r\n1. Cell-wise operations like thresholding\r\n2. Dimension-reducing operations like contouring\r\n\r\nCell wise operations can easily be fixed by adding cells. This can be done with `cast_to_polydata()`\r\n\r\nDimension-reducing operations, on the other hand, have no solution and should not be allowed on `PointSet`.\r\n\r\nHow can we properly error out or convert to `PolyData` when calling dataset filters like `threshold()` and `contour()`? Should these types of filters be overridden on the `PointSet` class?\n\n### Steps to reproduce the bug.\n\n### Cell wise operation produces *invalid* output\r\n\r\n```python\r\nimport pyvista as pv\r\nimport numpy as np\r\npc = pv.PointSet(np.random.random((100, 3)))\r\npc['foo'] = np.arange(pc.n_points)\r\npc.threshold()\r\n```\r\n\r\n<img width=\"575\" alt=\"Screen Shot 2023-01-11 at 5 47 02 PM 1\" src=\"https://user-images.githubusercontent.com/22067021/211949301-8d10e9ac-172e-4f27-ad81-c3ec2d335263.png\">\r\n\r\n\r\n### Dimension reducing operation produces *no* output\r\n\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npc = pv.PointSet(np.random.random((100, 3)))\r\npc['foo'] = np.arange(pc.n_points)\r\npc.contour()\r\n```\r\n<img width=\"417\" alt=\"Screen Shot 2023-01-11 at 5 47 57 PM\" src=\"https://user-images.githubusercontent.com/22067021/211949430-a3e77292-6b1e-4d2d-b2e3-b2a640ed65fc.png\">\r\n\r\n\n\n### System Information\n\n```shell\nn/a\n```\n\n\n### Screenshots\n\nn/a\n",
    "input": "     # Type alias\n     r'\\.ColorLike$',\n     r'\\.Chart$',\n     # Mixin methods from collections.abc\n     r'\\.MultiBlock\\.clear$',\n     r'\\.MultiBlock\\.count$',\ndiff --git a/pyvista/core/errors.py b/pyvista/core/errors.py\n     ):\n         \"\"\"Empty init.\"\"\"\n         RuntimeError.__init__(self, message)\ndiff --git a/pyvista/core/pointset.py b/pyvista/core/pointset.py\n from ..utilities.fileio import get_ext\n from .celltype import CellType\n from .dataset import DataSet\nfrom .errors import DeprecationError, VTKVersionError\n from .filters import PolyDataFilters, StructuredGridFilters, UnstructuredGridFilters, _get_output\n \n DEFAULT_INPLACE_WARNING = (\n         kwargs.setdefault('style', 'points')\n         return pdata.plot(*args, **kwargs)\n \n \n class PolyData(_vtk.vtkPolyData, _PointSet, PolyDataFilters):\n     \"\"\"Dataset consisting of surface geometry (e.g. vertices, lines, and polygons).",
    "output": "     # Type alias\n     r'\\.ColorLike$',\n     r'\\.Chart$',\n    # PointSet *args and **kwargs for wrapped parameters\n    r'\\.PointSet(\\.|$)',\n     # Mixin methods from collections.abc\n     r'\\.MultiBlock\\.clear$',\n     r'\\.MultiBlock\\.count$',\ndiff --git a/pyvista/core/errors.py b/pyvista/core/errors.py\n     ):\n         \"\"\"Empty init.\"\"\"\n         RuntimeError.__init__(self, message)\n\n\nclass PointSetNotSupported(TypeError):\n    \"\"\"Requested filter or property is not supported by the PointSet class.\"\"\"\n\n    def __init__(self, message='The requested operation is not supported for PointSets.'):\n        \"\"\"Empty init.\"\"\"\n        TypeError.__init__(self, message)\n\n\nclass PointSetCellOperationError(PointSetNotSupported):\n    \"\"\"Requested filter or property is not supported by the PointSet class.\"\"\"\n\n    def __init__(self, message='Cell operations are not supported. PointSets contain no cells.'):\n        \"\"\"Empty init.\"\"\"\n        PointSetNotSupported.__init__(self, message)\n\n\nclass PointSetDimensionReductionError(PointSetNotSupported):\n    \"\"\"Requested filter or property is not supported by the PointSet class.\"\"\"\n\n    def __init__(\n        self, message='Slice and other dimension reducing filters are not supported on PointSets.'\n    ):\n        \"\"\"Empty init.\"\"\"\n        PointSetNotSupported.__init__(self, message)\ndiff --git a/pyvista/core/pointset.py b/pyvista/core/pointset.py\n from ..utilities.fileio import get_ext\n from .celltype import CellType\n from .dataset import DataSet\nfrom .errors import (\n    DeprecationError,\n    PointSetCellOperationError,\n    PointSetDimensionReductionError,\n    PointSetNotSupported,\n    VTKVersionError,\n)\n from .filters import PolyDataFilters, StructuredGridFilters, UnstructuredGridFilters, _get_output\n \n DEFAULT_INPLACE_WARNING = (\n         kwargs.setdefault('style', 'points')\n         return pdata.plot(*args, **kwargs)\n \n    @wraps(PolyDataFilters.threshold)\n    def threshold(self, *args, **kwargs):\n        \"\"\"Cast to PolyData and threshold.\n\n        Need this because cell-wise operations fail for PointSets.\n        \"\"\"\n        return self.cast_to_polydata(False).threshold(*args, **kwargs).cast_to_pointset()\n\n    @wraps(PolyDataFilters.threshold_percent)\n    def threshold_percent(self, *args, **kwargs):\n        \"\"\"Cast to PolyData and threshold.\n\n        Need this because cell-wise operations fail for PointSets.\n        \"\"\"\n        return self.cast_to_polydata(False).threshold_percent(*args, **kwargs).cast_to_pointset()\n\n    @wraps(PolyDataFilters.explode)\n    def explode(self, *args, **kwargs):\n        \"\"\"Cast to PolyData and explode.\n\n        The explode filter relies on cells.\n\n        \"\"\"\n        return self.cast_to_polydata(False).explode(*args, **kwargs).cast_to_pointset()\n\n    @wraps(PolyDataFilters.delaunay_3d)\n    def delaunay_3d(self, *args, **kwargs):\n        \"\"\"Cast to PolyData and run delaunay_3d.\"\"\"\n        return self.cast_to_polydata(False).delaunay_3d(*args, **kwargs)\n\n    @property\n    def area(self) -> float:\n        \"\"\"Return 0.0 since a PointSet has no area.\"\"\"\n        return 0.0\n\n    @property\n    def volume(self) -> float:\n        \"\"\"Return 0.0 since a PointSet has no volume.\"\"\"\n        return 0.0\n\n    def contour(self, *args, **kwargs):\n        \"\"\"Raise dimension reducing operations are not supported.\"\"\"\n        raise PointSetNotSupported(\n            'Contour and other dimension reducing filters are not supported on PointSets'\n        )\n\n    def cell_data_to_point_data(self, *args, **kwargs):\n        \"\"\"Raise PointSets do not have cells.\"\"\"\n        raise PointSetNotSupported('PointSets contain no cells or cell data.')\n\n    def point_data_to_cell_data(self, *args, **kwargs):\n        \"\"\"Raise PointSets do not have cells.\"\"\"\n        raise PointSetNotSupported('PointSets contain no cells or cell data.')\n\n    def triangulate(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n    def decimate_boundary(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n    def find_cells_along_line(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n    def tessellate(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n    def slice(self, *args, **kwargs):\n        \"\"\"Raise dimension reducing operations are not supported.\"\"\"\n        raise PointSetDimensionReductionError\n\n    def slice_along_axis(self, *args, **kwargs):\n        \"\"\"Raise dimension reducing operations are not supported.\"\"\"\n        raise PointSetDimensionReductionError\n\n    def slice_along_line(self, *args, **kwargs):\n        \"\"\"Raise dimension reducing operations are not supported.\"\"\"\n        raise PointSetDimensionReductionError\n\n    def slice_implicit(self, *args, **kwargs):\n        \"\"\"Raise dimension reducing operations are not supported.\"\"\"\n        raise PointSetDimensionReductionError\n\n    def slice_orthogonal(self, *args, **kwargs):\n        \"\"\"Raise dimension reducing operations are not supported.\"\"\"\n        raise PointSetDimensionReductionError\n\n    def shrink(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n    def separate_cells(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n    def remove_cells(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n    def point_is_inside_cell(self, *args, **kwargs):\n        \"\"\"Raise cell operations are not supported.\"\"\"\n        raise PointSetCellOperationError\n\n \n class PolyData(_vtk.vtkPolyData, _PointSet, PolyDataFilters):\n     \"\"\"Dataset consisting of surface geometry (e.g. vertices, lines, and polygons)."
  },
  {
    "instruction": "Strictly enforce keyword arguments\nI see folks quite often forget the s in the `scalars` argument for the `BasePlotter.add_mesh()` method. We should allow `scalar` as an alias much like how we allow `rng` and `clim` for the colorbar range/limits\n",
    "input": " \n # Define a camera potion the shows this mesh properly\n cpos = [(0.4, -0.07, -0.31), (0.05, -0.13, -0.06), (-0.1, 1, 0.08)]\ndargs = dict(cpos=cpos, show_edges=True, color=True)\n \n # Preview the mesh\nmesh.plot(**dargs)\n \n ###############################################################################\n #  Now let's define a target reduction and compare the\n ###############################################################################\n decimated = mesh.decimate(target_reduction)\n \ndecimated.plot(**dargs)\n \n \n ###############################################################################\n pro_decimated = mesh.decimate_pro(target_reduction, preserve_topology=True)\n \npro_decimated.plot(**dargs)\n \n \n ###############################################################################\ndiff --git a/examples/01-filter/streamlines.py b/examples/01-filter/streamlines.py\n boundary = mesh.decimate_boundary().wireframe()\n \n p = pv.Plotter()\np.add_mesh(streamlines.tube(radius=0.2), ligthing=False)\n p.add_mesh(src)\n p.add_mesh(boundary, color=\"grey\", opacity=0.25)\n p.camera_position = [(10, 9.5, -43), (87.0, 73.5, 123.0), (-0.5, -0.7, 0.5)]\ndiff --git a/pyvista/core/filters.py b/pyvista/core/filters.py\n \n import pyvista\n from pyvista.utilities import (CELL_DATA_FIELD, POINT_DATA_FIELD, NORMALS,\n                               generate_plane, get_array, wrap)\n \n \n def _get_output(algorithm, iport=0, iconnection=0, oport=0, active_scalar=None,\n         inplace : bool\n             If True, the points of the give dataset will be updated.\n         \"\"\"\n         if scalars is None:\n             field, scalars = dataset.active_scalar_info\n         arr, field = get_array(dataset, scalars, preference='point', info=True)\n         if field != pyvista.POINT_DATA_FIELD:\n             raise AssertionError('Dataset can only by warped by a point data array.')\n        scale_factor = kwargs.get('scale_factor', None)\n        if scale_factor is not None:\n            factor = scale_factor\n         # Run the algorithm\n         alg = vtk.vtkWarpScalar()\n         alg.SetInputDataObject(dataset)\n             - Minimum\n \n         **kwargs : optional\n            See help(pyvista.plot)\n \n         Returns\n         -------\n         \"\"\"\n         if tolerance is None:\n             tolerance = kwargs.pop('merge_tol', None)\n         clean = vtk.vtkCleanPolyData()\n         clean.SetPointMerging(point_merging)\n         clean.SetConvertLinesToPoints(lines_to_points)\n         return intersection_points, intersection_cells\n \n \n    def plot_boundaries(poly_data, **kwargs):\n        \"\"\" Plots boundaries of a mesh \"\"\"\n         edges = DataSetFilters.extract_edges(poly_data)\n \n         plotter = pyvista.Plotter(off_screen=kwargs.pop('off_screen', False),\n                                   notebook=kwargs.pop('notebook', None))\n        plotter.add_mesh(edges, 'r', style='wireframe', legend='Edges')\n        plotter.add_mesh(poly_data, legend='Mesh', **kwargs)\n         return plotter.show()\n \n \ndiff --git a/pyvista/core/objects.py b/pyvista/core/objects.py\n import vtk\n \n import pyvista\nfrom pyvista.utilities import (ROW_DATA_FIELD, convert_array, get_array,\n                               parse_field_choice, row_array, vtk_bit_array_to_char)\n \n from .common import DataObject, _ScalarsDict\n \n class Texture(vtk.vtkTexture):\n     \"\"\"A helper class for vtkTextures\"\"\"\n     def __init__(self, *args, **kwargs):\n         if len(args) == 1:\n             if isinstance(args[0], vtk.vtkTexture):\n                 self._from_texture(args[0])\ndiff --git a/pyvista/plotting/helpers.py b/pyvista/plotting/helpers.py\n     if notebook is None:\n         notebook = scooby.in_ipykernel()\n \n     if notebook:\n         off_screen = notebook\n     plotter = Plotter(off_screen=off_screen, notebook=notebook)\n     if text:\n         plotter.add_text(text)\n \n    if show_bounds or kwargs.get('show_grid', False):\n        if kwargs.get('show_grid', False):\n            plotter.show_grid()\n        else:\n            plotter.show_bounds()\n \n     if cpos is None:\n         cpos = plotter.get_default_cam_pos()\n     else:\n         plotter.camera_position = cpos\n \n    eye_dome_lighting = kwargs.pop(\"edl\", eye_dome_lighting)\n     if eye_dome_lighting:\n         plotter.enable_eye_dome_lighting()\n \n                           screenshot=screenshot,\n                           return_img=return_img,\n                           use_panel=use_panel,\n                          height=kwargs.get('height', 400))\n \n     # close and return camera position and maybe image\n    if kwargs.get('auto_close', rcParams['auto_close']):\n         plotter.close()\n \n     # Result will be handled by plotter.show(): cpos or [cpos, img]\ndiff --git a/pyvista/plotting/plotting.py b/pyvista/plotting/plotting.py\n import warnings\n \n import pyvista\nfrom pyvista.utilities import (convert_array, convert_string_array,\n                               get_array, is_pyvista_dataset, numpy_to_texture,\n                                raise_not_matching, try_callback, wrap)\n \n from .colors import get_cmap_safe\n                  specular_power=100.0, nan_color=None, nan_opacity=1.0,\n                  loc=None, culling=None, rgb=False, categories=False,\n                  use_transparency=False, below_color=None, above_color=None,\n                 annotations=None, pickable=True, **kwargs):\n         \"\"\"\n         Adds any PyVista/VTK mesh or dataset that PyVista can wrap to the\n         scene. This method using a mesh representation to view the surfaces\n         if lighting is None:\n             lighting = rcParams['lighting']\n \n        if clim is None:\n            clim = kwargs.get('rng', None)\n \n         if render_points_as_spheres is None:\n             render_points_as_spheres = rcParams['render_points_as_spheres']\n \n         if nan_color is None:\n             nan_color = rcParams['nan_color']\n        nanr, nanb, nang = parse_color(nan_color)\n        nan_color = nanr, nanb, nang, nan_opacity\n         if color is True:\n             color = rcParams['color']\n \n         if texture is False:\n             texture = None\n \n        if culling is None:\n            culling = kwargs.get(\"backface_culling\", False)\n            if culling is True:\n                culling = 'backface'\n \n         ##### Handle composite datasets #####\n \n                     raise RuntimeError('Scalar array must be given as a string name for multiblock datasets.')\n \n             the_arguments = locals()\n            the_arguments.update(kwargs)\n             the_arguments.pop('self')\n             the_arguments.pop('mesh')\n \n             if multi_colors:\n                 # Compute unique colors for each index of the block\n             self.mapper.SetArrayName(scalars)\n             original_scalar_name = scalars\n             scalars = get_array(mesh, scalars,\n                                preference=kwargs.get('preference', 'cell'), err=True)\n             if stitle is None:\n                 stitle = original_scalar_name\n \n             try:\n                 # Get array from mesh\n                 opacity = get_array(mesh, opacity,\n                                    preference=kwargs.get('preference', 'cell'), err=True)\n                 opacity = normalize(opacity)\n                 _custom_opac = True\n             except:\n             opacity = 255 - opacity\n \n         # Scalar formatting ===================================================\n        if cmap is None: # grab alias for cmaps: colormap\n            cmap = kwargs.get('colormap', None)\n         if cmap is None: # Set default map if matplotlib is avaialble\n             if has_matplotlib:\n                 cmap = rcParams['cmap']\n                 scalar_bar_args.setdefault('n_labels', 0)\n                 _using_labels = True\n \n            if rgb is False or rgb is None:\n                rgb = kwargs.get('rgba', False)\n             if rgb:\n                 if scalars.ndim != 2 or scalars.shape[1] < 3 or scalars.shape[1] > 4:\n                     raise ValueError('RGB array must be n_points/n_cells by 3/4 in shape.')\n                    loc=None, culling=False, multi_colors=False,\n                    blending='composite', mapper='fixed_point',\n                    stitle=None, scalar_bar_args=None, show_scalar_bar=None,\n                   annotations=None, pickable=True, **kwargs):\n         \"\"\"\n         Adds a volume, rendered using a fixed point ray cast mapper by default.\n \n         if name is None:\n             name = '{}({})'.format(type(volume).__name__, str(hex(id(volume))))\n \n        if clim is None:\n            clim = kwargs.get('rng', None)\n \n         if scalar_bar_args is None:\n             scalar_bar_args = {}\n         if show_scalar_bar is None:\n             show_scalar_bar = rcParams['show_scalar_bar']\n \n        if culling is None:\n            culling = kwargs.get(\"backface_culling\", False)\n            if culling is True:\n                culling = 'backface'\n \n         # Convert the VTK data object to a pyvista wrapped object if neccessary\n         if not is_pyvista_dataset(volume):\n                                     reset_camera=reset_camera, name=next_name,\n                                     ambient=ambient, categories=categories, loc=loc,\n                                     culling=culling, clim=clim,\n                                    mapper=mapper, pickable=pickable, **kwargs)\n \n                 actors.append(a)\n             return actors\n         if isinstance(scalars, str):\n             title = scalars\n             scalars = get_array(volume, scalars,\n                                preference=kwargs.get('preference', 'point'), err=True)\n             if stitle is None:\n                 stitle = title\n         else:\n             for val, anno in annotations.items():\n                 table.SetAnnotation(float(val), str(anno))\n \n        if cmap is None: # grab alias for cmaps: colormap\n            cmap = kwargs.get('colormap', None)\n            if cmap is None: # Set default map if matplotlib is avaialble\n                if has_matplotlib:\n                    cmap = rcParams['cmap']\n \n         if cmap is not None:\n             if not has_matplotlib:\n                          name=None, shape_color='grey', shape='rounded_rect',\n                          fill_shape=True, margin=3, shape_opacity=1.0,\n                          pickable=False, render_points_as_spheres=False,\n                         tolerance=0.001, **kwargs):\n         \"\"\"\n         Creates a point actor with one label from list labels assigned to\n         each point.\n             font_family = rcParams['font']['family']\n         if font_size is None:\n             font_size = rcParams['font']['size']\n        if point_color is None and text_color is None and kwargs.get('color', None) is not None:\n            point_color = kwargs.get('color', None)\n            text_color = kwargs.get('color', None)\n         if point_color is None:\n             point_color = rcParams['color']\n         if text_color is None:\ndiff --git a/pyvista/plotting/qt_plotting.py b/pyvista/plotting/qt_plotting.py\n                  border=None, border_color='k', border_width=2.0,\n                  multi_samples=None, line_smoothing=False,\n                  point_smoothing=False, polygon_smoothing=False,\n                 splitting_position=None, **kwargs):\n         \"\"\" Initialize Qt interactor \"\"\"\n         if not has_pyqt:\n             raise AssertionError('Requires PyQt5')\ndiff --git a/pyvista/plotting/widgets.py b/pyvista/plotting/widgets.py\n \n     def add_box_widget(self, callback, bounds=None, factor=1.25,\n                        rotation_enabled=True, color=None, use_planes=False,\n                       outline_translation=True, pass_widget=False, **kwargs):\n         \"\"\"Add a box widget to the scene. This is useless without a callback\n         function. You can pass a callable function that takes a single\n         argument, the PolyData box output from this widget, and performs a\n                          assign_to_axis=None, tubing=False,\n                          outline_translation=False,\n                          origin_translation=True, implicit=True,\n                         pass_widget=False, test_callback=True, **kwargs):\n         \"\"\"Add a plane widget to the scene. This is useless without a callback\n         function. You can pass a callable function that takes two\n         arguments, the normal and origin of the plane in that order output\n \n     def add_line_widget(self, callback, bounds=None, factor=1.25,\n                         resolution=100, color=None, use_vertices=False,\n                        pass_widget=False, **kwargs):\n         \"\"\"Add a line widget to the scene. This is useless without a callback\n         function. You can pass a callable function that takes a single\n         argument, the PolyData line output from this widget, and performs a\n     def add_spline_widget(self, callback, bounds=None, factor=1.25,\n                           n_hanldes=5, resolution=25, color=\"yellow\",\n                           show_ribbon=False, ribbon_color=\"pink\",\n                          ribbon_opacity=0.5, pass_widget=False, **kwargs):\n         \"\"\"Create and add a spline widget to the scene. Use the bounds\n         argument to place this widget. Several \"handles\" are used to control a\n         parametric function for building this spline. Click directly on the\ndiff --git a/pyvista/utilities/__init__.py b/pyvista/utilities/__init__.py\nfrom .errors import (Observer, Report, send_errors_to_logging,\n                     set_error_output_file)\n from .features import *\n from .fileio import *\n from .geometric_objects import *\n from .parametric_objects import *\n from .sphinx_gallery import Scraper, _get_sg_image_scraper\nfrom .utilities import *\ndiff --git a/pyvista/utilities/errors.py b/pyvista/utilities/errors.py\n import os\n import re\n import scooby\n \n import vtk\n \n         scooby.Report.__init__(self, additional=additional, core=core,\n                                optional=optional, ncol=ncol,\n                                text_width=text_width, sort=sort)\ndiff --git a/pyvista/utilities/geometric_objects.py b/pyvista/utilities/geometric_objects.py\n import vtk\n \n import pyvista\n \n \n NORMALS = {\n \n \n def Cylinder(center=(0.,0.,0.), direction=(1.,0.,0.), radius=0.5, height=1.0,\n             resolution=100, **kwargs):\n \n     \"\"\"\n     Create the surface of a cylinder.\n     >>> cylinder.plot() # doctest:+SKIP\n \n     \"\"\"\n    capping = kwargs.get('capping', kwargs.get('cap_ends', True))\n     cylinderSource = vtk.vtkCylinderSource()\n     cylinderSource.SetRadius(radius)\n     cylinderSource.SetHeight(height)\n     return pyvista.wrap(tri_filter.GetOutput())\n \n \ndef SuperToroid(**kwargs):\n     \"\"\"DEPRECATED: use :func:`pyvista.ParametricSuperToroid`\"\"\"\n     raise RuntimeError('use `pyvista.ParametricSuperToroid` instead')\n \n \ndef Ellipsoid(**kwargs):\n     \"\"\"DEPRECATED: use :func:`pyvista.ParametricEllipsoid`\"\"\"\n     raise RuntimeError('use `pyvista.ParametricEllipsoid` instead')\n \ndiff --git a/pyvista/utilities/utilities.py b/pyvista/utilities/helpers.py\nsimilarity index 100%\nrename from pyvista/utilities/utilities.py\nrename to pyvista/utilities/helpers.py\ndiff --git a/pyvista/utilities/parametric_objects.py b/pyvista/utilities/parametric_objects.py\n import vtk\n \n import pyvista\n \n from .geometric_objects import translate\n \n     >>> mesh.plot(color='w', smooth_shading=True)  # doctest:+SKIP\n     \"\"\"\n     parametric_function = vtk.vtkParametricEllipsoid()\n    parametric_keywords(parametric_function, **kwargs)\n \n     if xradius is not None:\n         parametric_function.SetXRadius(xradius)\n \n def parametric_keywords(parametric_function, min_u=0, max_u=2*pi,\n                         min_v=0.0, max_v=2*pi, join_u=False, join_v=False,\n                        twist_u=False, twist_v=False, clockwise=True, **kwargs):\n     \"\"\"Applys keyword arguments to a parametric function.\n \n     Parameters\n \n \n def surface_from_para(parametric_function, u_res=100, v_res=100,\n                      w_res=100, **kwargs):\n     \"\"\"Construct a mesh from a parametric function.\n \n     Parameters",
    "output": " \n # Define a camera potion the shows this mesh properly\n cpos = [(0.4, -0.07, -0.31), (0.05, -0.13, -0.06), (-0.1, 1, 0.08)]\ndargs = dict(show_edges=True, color=True)\n \n # Preview the mesh\nmesh.plot(cpos=cpos, **dargs)\n \n ###############################################################################\n #  Now let's define a target reduction and compare the\n ###############################################################################\n decimated = mesh.decimate(target_reduction)\n \ndecimated.plot(cpos=cpos, **dargs)\n \n \n ###############################################################################\n pro_decimated = mesh.decimate_pro(target_reduction, preserve_topology=True)\n \npro_decimated.plot(cpos=cpos, **dargs)\n \n \n ###############################################################################\ndiff --git a/examples/01-filter/streamlines.py b/examples/01-filter/streamlines.py\n boundary = mesh.decimate_boundary().wireframe()\n \n p = pv.Plotter()\np.add_mesh(streamlines.tube(radius=0.2), lighting=False)\n p.add_mesh(src)\n p.add_mesh(boundary, color=\"grey\", opacity=0.25)\n p.camera_position = [(10, 9.5, -43), (87.0, 73.5, 123.0), (-0.5, -0.7, 0.5)]\ndiff --git a/pyvista/core/filters.py b/pyvista/core/filters.py\n \n import pyvista\n from pyvista.utilities import (CELL_DATA_FIELD, POINT_DATA_FIELD, NORMALS,\n                               assert_empty_kwargs, generate_plane, get_array,\n                               wrap)\n \n \n def _get_output(algorithm, iport=0, iconnection=0, oport=0, active_scalar=None,\n         inplace : bool\n             If True, the points of the give dataset will be updated.\n         \"\"\"\n        factor = kwargs.pop('scale_factor', factor)\n        assert_empty_kwargs(**kwargs)\n         if scalars is None:\n             field, scalars = dataset.active_scalar_info\n         arr, field = get_array(dataset, scalars, preference='point', info=True)\n         if field != pyvista.POINT_DATA_FIELD:\n             raise AssertionError('Dataset can only by warped by a point data array.')\n         # Run the algorithm\n         alg = vtk.vtkWarpScalar()\n         alg.SetInputDataObject(dataset)\n             - Minimum\n \n         **kwargs : optional\n            See :func:`pyvista.plot`\n \n         Returns\n         -------\n         \"\"\"\n         if tolerance is None:\n             tolerance = kwargs.pop('merge_tol', None)\n        assert_empty_kwargs(**kwargs)\n         clean = vtk.vtkCleanPolyData()\n         clean.SetPointMerging(point_merging)\n         clean.SetConvertLinesToPoints(lines_to_points)\n         return intersection_points, intersection_cells\n \n \n    def plot_boundaries(poly_data, edge_color=\"red\", **kwargs):\n        \"\"\" Plots boundaries of a mesh\n\n        Parameters\n        ----------\n        edge_color : str, etc.\n            The color of the edges when they are added to the plotter.\n\n        kwargs : optional\n            All additional keyword arguments will be passed to\n            :func:`pyvista.BasePlotter.add_mesh`\n\n        \"\"\"\n         edges = DataSetFilters.extract_edges(poly_data)\n \n         plotter = pyvista.Plotter(off_screen=kwargs.pop('off_screen', False),\n                                   notebook=kwargs.pop('notebook', None))\n        plotter.add_mesh(edges, color=edge_color, style='wireframe', label='Edges')\n        plotter.add_mesh(poly_data, label='Mesh', **kwargs)\n        plotter.add_legend()\n         return plotter.show()\n \n \ndiff --git a/pyvista/core/objects.py b/pyvista/core/objects.py\n import vtk\n \n import pyvista\nfrom pyvista.utilities import (ROW_DATA_FIELD, assert_empty_kwargs,\n                               convert_array, get_array, parse_field_choice,\n                               row_array, vtk_bit_array_to_char)\n \n from .common import DataObject, _ScalarsDict\n \n class Texture(vtk.vtkTexture):\n     \"\"\"A helper class for vtkTextures\"\"\"\n     def __init__(self, *args, **kwargs):\n        assert_empty_kwargs(**kwargs)\n\n         if len(args) == 1:\n             if isinstance(args[0], vtk.vtkTexture):\n                 self._from_texture(args[0])\ndiff --git a/pyvista/plotting/helpers.py b/pyvista/plotting/helpers.py\n     if notebook is None:\n         notebook = scooby.in_ipykernel()\n \n    eye_dome_lighting = kwargs.pop(\"edl\", eye_dome_lighting)\n    show_grid = kwargs.pop('show_grid', False)\n    height = kwargs.get('height', 400)\n    auto_close = kwargs.get('auto_close', rcParams['auto_close'])\n\n     if notebook:\n         off_screen = notebook\n     plotter = Plotter(off_screen=off_screen, notebook=notebook)\n     if text:\n         plotter.add_text(text)\n \n    if show_grid:\n        plotter.show_grid()\n    elif show_bounds:\n        plotter.show_bounds()\n \n     if cpos is None:\n         cpos = plotter.get_default_cam_pos()\n     else:\n         plotter.camera_position = cpos\n \n     if eye_dome_lighting:\n         plotter.enable_eye_dome_lighting()\n \n                           screenshot=screenshot,\n                           return_img=return_img,\n                           use_panel=use_panel,\n                          height=height)\n \n     # close and return camera position and maybe image\n    if auto_close:\n         plotter.close()\n \n     # Result will be handled by plotter.show(): cpos or [cpos, img]\ndiff --git a/pyvista/plotting/plotting.py b/pyvista/plotting/plotting.py\n import warnings\n \n import pyvista\nfrom pyvista.utilities import (assert_empty_kwargs, convert_array,\n                               convert_string_array, get_array,\n                               is_pyvista_dataset, numpy_to_texture,\n                                raise_not_matching, try_callback, wrap)\n \n from .colors import get_cmap_safe\n                  specular_power=100.0, nan_color=None, nan_opacity=1.0,\n                  loc=None, culling=None, rgb=False, categories=False,\n                  use_transparency=False, below_color=None, above_color=None,\n                 annotations=None, pickable=True, preference=\"point\", **kwargs):\n         \"\"\"\n         Adds any PyVista/VTK mesh or dataset that PyVista can wrap to the\n         scene. This method using a mesh representation to view the surfaces\n         if lighting is None:\n             lighting = rcParams['lighting']\n \n        # supported aliases\n        clim = kwargs.pop('rng', clim)\n        cmap = kwargs.pop('colormap', cmap)\n        culling = kwargs.pop(\"backface_culling\", culling)\n \n         if render_points_as_spheres is None:\n             render_points_as_spheres = rcParams['render_points_as_spheres']\n \n         if nan_color is None:\n             nan_color = rcParams['nan_color']\n        nan_color = list(parse_color(nan_color))\n        nan_color.append(nan_opacity)\n         if color is True:\n             color = rcParams['color']\n \n         if texture is False:\n             texture = None\n \n        if culling is True:\n            culling = 'backface'\n\n        rgb = kwargs.pop('rgba', rgb)\n\n        if \"scalar\" in kwargs:\n            raise TypeError(\"`scalar` is an invalid keyword argument for `add_mesh`. Perhaps you mean `scalars` with an s?\")\n        assert_empty_kwargs(**kwargs)\n \n         ##### Handle composite datasets #####\n \n                     raise RuntimeError('Scalar array must be given as a string name for multiblock datasets.')\n \n             the_arguments = locals()\n             the_arguments.pop('self')\n             the_arguments.pop('mesh')\n            the_arguments.pop('kwargs')\n \n             if multi_colors:\n                 # Compute unique colors for each index of the block\n             self.mapper.SetArrayName(scalars)\n             original_scalar_name = scalars\n             scalars = get_array(mesh, scalars,\n                                preference=preference, err=True)\n             if stitle is None:\n                 stitle = original_scalar_name\n \n             try:\n                 # Get array from mesh\n                 opacity = get_array(mesh, opacity,\n                                    preference=preference, err=True)\n                 opacity = normalize(opacity)\n                 _custom_opac = True\n             except:\n             opacity = 255 - opacity\n \n         # Scalar formatting ===================================================\n         if cmap is None: # Set default map if matplotlib is avaialble\n             if has_matplotlib:\n                 cmap = rcParams['cmap']\n                 scalar_bar_args.setdefault('n_labels', 0)\n                 _using_labels = True\n \n             if rgb:\n                 if scalars.ndim != 2 or scalars.shape[1] < 3 or scalars.shape[1] > 4:\n                     raise ValueError('RGB array must be n_points/n_cells by 3/4 in shape.')\n                    loc=None, culling=False, multi_colors=False,\n                    blending='composite', mapper='fixed_point',\n                    stitle=None, scalar_bar_args=None, show_scalar_bar=None,\n                   annotations=None, pickable=True, preference=\"point\", **kwargs):\n         \"\"\"\n         Adds a volume, rendered using a fixed point ray cast mapper by default.\n \n         if name is None:\n             name = '{}({})'.format(type(volume).__name__, str(hex(id(volume))))\n \n        # Supported aliases\n        clim = kwargs.pop('rng', clim)\n        cmap = kwargs.pop('colormap', cmap)\n        culling = kwargs.pop(\"backface_culling\", culling)\n\n        if \"scalar\" in kwargs:\n            raise TypeError(\"`scalar` is an invalid keyword argument for `add_mesh`. Perhaps you mean `scalars` with an s?\")\n        assert_empty_kwargs(**kwargs)\n \n         if scalar_bar_args is None:\n             scalar_bar_args = {}\n         if show_scalar_bar is None:\n             show_scalar_bar = rcParams['show_scalar_bar']\n \n        if culling is True:\n            culling = 'backface'\n \n         # Convert the VTK data object to a pyvista wrapped object if neccessary\n         if not is_pyvista_dataset(volume):\n                                     reset_camera=reset_camera, name=next_name,\n                                     ambient=ambient, categories=categories, loc=loc,\n                                     culling=culling, clim=clim,\n                                    mapper=mapper, pickable=pickable)\n \n                 actors.append(a)\n             return actors\n         if isinstance(scalars, str):\n             title = scalars\n             scalars = get_array(volume, scalars,\n                                preference=preference, err=True)\n             if stitle is None:\n                 stitle = title\n         else:\n             for val, anno in annotations.items():\n                 table.SetAnnotation(float(val), str(anno))\n \n        if cmap is None: # Set default map if matplotlib is avaialble\n            if has_matplotlib:\n                cmap = rcParams['cmap']\n \n         if cmap is not None:\n             if not has_matplotlib:\n                          name=None, shape_color='grey', shape='rounded_rect',\n                          fill_shape=True, margin=3, shape_opacity=1.0,\n                          pickable=False, render_points_as_spheres=False,\n                         tolerance=0.001):\n         \"\"\"\n         Creates a point actor with one label from list labels assigned to\n         each point.\n             font_family = rcParams['font']['family']\n         if font_size is None:\n             font_size = rcParams['font']['size']\n         if point_color is None:\n             point_color = rcParams['color']\n         if text_color is None:\ndiff --git a/pyvista/plotting/qt_plotting.py b/pyvista/plotting/qt_plotting.py\n                  border=None, border_color='k', border_width=2.0,\n                  multi_samples=None, line_smoothing=False,\n                  point_smoothing=False, polygon_smoothing=False,\n                 splitting_position=None):\n         \"\"\" Initialize Qt interactor \"\"\"\n         if not has_pyqt:\n             raise AssertionError('Requires PyQt5')\ndiff --git a/pyvista/plotting/widgets.py b/pyvista/plotting/widgets.py\n \n     def add_box_widget(self, callback, bounds=None, factor=1.25,\n                        rotation_enabled=True, color=None, use_planes=False,\n                       outline_translation=True, pass_widget=False):\n         \"\"\"Add a box widget to the scene. This is useless without a callback\n         function. You can pass a callable function that takes a single\n         argument, the PolyData box output from this widget, and performs a\n                          assign_to_axis=None, tubing=False,\n                          outline_translation=False,\n                          origin_translation=True, implicit=True,\n                         pass_widget=False, test_callback=True):\n         \"\"\"Add a plane widget to the scene. This is useless without a callback\n         function. You can pass a callable function that takes two\n         arguments, the normal and origin of the plane in that order output\n \n     def add_line_widget(self, callback, bounds=None, factor=1.25,\n                         resolution=100, color=None, use_vertices=False,\n                        pass_widget=False):\n         \"\"\"Add a line widget to the scene. This is useless without a callback\n         function. You can pass a callable function that takes a single\n         argument, the PolyData line output from this widget, and performs a\n     def add_spline_widget(self, callback, bounds=None, factor=1.25,\n                           n_hanldes=5, resolution=25, color=\"yellow\",\n                           show_ribbon=False, ribbon_color=\"pink\",\n                          ribbon_opacity=0.5, pass_widget=False):\n         \"\"\"Create and add a spline widget to the scene. Use the bounds\n         argument to place this widget. Several \"handles\" are used to control a\n         parametric function for building this spline. Click directly on the\ndiff --git a/pyvista/utilities/__init__.py b/pyvista/utilities/__init__.py\nfrom .errors import (Observer, Report, assert_empty_kwargs,\n                     send_errors_to_logging, set_error_output_file)\n from .features import *\n from .fileio import *\n from .geometric_objects import *\n from .parametric_objects import *\n from .sphinx_gallery import Scraper, _get_sg_image_scraper\nfrom .helpers import *\ndiff --git a/pyvista/utilities/errors.py b/pyvista/utilities/errors.py\n import os\n import re\n import scooby\nimport sys\n \n import vtk\n \n         scooby.Report.__init__(self, additional=additional, core=core,\n                                optional=optional, ncol=ncol,\n                                text_width=text_width, sort=sort)\n\n\ndef assert_empty_kwargs(**kwargs):\n    \"\"\"An internal helper to assert that all keyword arguments have been used.\n    If any keyword arguments are passed, a ``TypeError`` is raised.\n    \"\"\"\n    n = len(kwargs)\n    if n == 0:\n        return True\n    caller = sys._getframe(1).f_code.co_name\n    keys = list(kwargs.keys())\n    bad_arguments = \"[\" + (\"{}, \" * (n - 1) + \"{}\").format(*keys) + \"]\"\n    if n == 1:\n        grammar = \"is an invalid keyword argument\"\n    else:\n        grammar = \"are invalid keyword arguments\"\n    message = \"{} {} for `{}`\".format(bad_arguments, grammar, caller)\n    raise TypeError(message)\ndiff --git a/pyvista/utilities/geometric_objects.py b/pyvista/utilities/geometric_objects.py\n import vtk\n \n import pyvista\nfrom pyvista.utilities import assert_empty_kwargs\n \n \n NORMALS = {\n \n \n def Cylinder(center=(0.,0.,0.), direction=(1.,0.,0.), radius=0.5, height=1.0,\n             resolution=100, capping=True, **kwargs):\n \n     \"\"\"\n     Create the surface of a cylinder.\n     >>> cylinder.plot() # doctest:+SKIP\n \n     \"\"\"\n    capping = kwargs.pop('cap_ends', capping)\n    assert_empty_kwargs(**kwargs)\n     cylinderSource = vtk.vtkCylinderSource()\n     cylinderSource.SetRadius(radius)\n     cylinderSource.SetHeight(height)\n     return pyvista.wrap(tri_filter.GetOutput())\n \n \ndef SuperToroid(*args, **kwargs):\n     \"\"\"DEPRECATED: use :func:`pyvista.ParametricSuperToroid`\"\"\"\n     raise RuntimeError('use `pyvista.ParametricSuperToroid` instead')\n \n \ndef Ellipsoid(*args, **kwargs):\n     \"\"\"DEPRECATED: use :func:`pyvista.ParametricEllipsoid`\"\"\"\n     raise RuntimeError('use `pyvista.ParametricEllipsoid` instead')\n \ndiff --git a/pyvista/utilities/utilities.py b/pyvista/utilities/helpers.py\nsimilarity index 100%\nrename from pyvista/utilities/utilities.py\nrename to pyvista/utilities/helpers.py\ndiff --git a/pyvista/utilities/parametric_objects.py b/pyvista/utilities/parametric_objects.py\n import vtk\n \n import pyvista\nfrom pyvista.utilities import assert_empty_kwargs\n \n from .geometric_objects import translate\n \n     >>> mesh.plot(color='w', smooth_shading=True)  # doctest:+SKIP\n     \"\"\"\n     parametric_function = vtk.vtkParametricEllipsoid()\n    parametric_keywords(parametric_function, min_u=kwargs.pop(\"min_u\", 0),\n                        max_u=kwargs.pop(\"max_u\", 2*pi),\n                        min_v=kwargs.pop(\"min_v\", 0.0),\n                        max_v=kwargs.pop(\"max_v\", 2*pi),\n                        join_u=kwargs.pop(\"join_u\", False),\n                        join_v=kwargs.pop(\"join_v\", False),\n                        twist_u=kwargs.pop(\"twist_u\", False),\n                        twist_v=kwargs.pop(\"twist_v\", False),\n                        clockwise=kwargs.pop(\"clockwise\", True),)\n \n     if xradius is not None:\n         parametric_function.SetXRadius(xradius)\n \n def parametric_keywords(parametric_function, min_u=0, max_u=2*pi,\n                         min_v=0.0, max_v=2*pi, join_u=False, join_v=False,\n                        twist_u=False, twist_v=False, clockwise=True):\n     \"\"\"Applys keyword arguments to a parametric function.\n \n     Parameters\n \n \n def surface_from_para(parametric_function, u_res=100, v_res=100,\n                      w_res=100):\n     \"\"\"Construct a mesh from a parametric function.\n \n     Parameters"
  },
  {
    "instruction": "circle creates creates one zero length edge\n### Describe the bug, what's wrong, and what you expected.\n\nI expected that:\r\n> circle = pv.Circle(radius, resolution=n)\r\n\r\n1. would create a circle with n points and edge. \r\nIt does yay :-) \r\n\r\n3. That each edge of the would have similar length.\r\nIt does _not_ :-(\r\n\r\nThe problems seems circle  closed is doubly:\r\n- once by the coordinates (circle.points[:, 0] == approx(circle.points[:, -1])\r\n- and a second time by the face (circle.faces[0] == [n, 0, 1, 2, ... n-1])\r\n\r\n\r\n\r\n\n\n### Steps to reproduce the bug.\n\n```python\r\nimport pyvista as pv\r\n\r\ncircle = pv.Circle(radius=1, resolution=4) # lets make a low res circle\r\nprint(circle.faces)  # out: array([4, 0, 1, 2, 3])\r\nprint(circle.n_points)  # out: 4\r\n\r\n# the print outputs gives the expectation that circle.plot() will look like a square \r\ncircle.plot()\r\n```\r\n![pv.Circle(radius=1, resolution=4).plot()](https://user-images.githubusercontent.com/107837123/207049939-9b24ac31-a8e8-4ca7-97d3-3f4105a524dc.png)\r\n\r\n\r\n\n\n### System Information\n\n```shell\n--------------------------------------------------------------------------------\r\n  Date: Mon Dec 12 13:55:16 2022 CET\r\n\r\n                OS : Linux\r\n            CPU(s) : 8\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n       GPU Details : error\r\n\r\n  Python 3.10.4 (main, Mar 23 2022, 20:25:24) [GCC 11.3.0]\r\n\r\n           pyvista : 0.36.1\r\n               vtk : 9.1.0\r\n             numpy : 1.23.5\r\n           imageio : 2.22.4\r\n           appdirs : 1.4.4\r\n            scooby : 0.7.0\r\n        matplotlib : 3.6.2\r\n         pyvistaqt : 0.9.0\r\n             PyQt5 : Version unknown\r\n           IPython : 8.2.0\r\n             scipy : 1.9.3\r\n--------------------------------------------------------------------------------\n```\n\n\n### Screenshots\n\n_No response_\n",
    "input": "     pyvista.PolyData\n         Circle mesh.\n \n     Examples\n     --------\n     >>> import pyvista\n     >>> radius = 0.5\n     >>> circle = pyvista.Circle(radius)\n     >>> circle.plot(show_edges=True, line_width=5)\n     \"\"\"\n     points = np.zeros((resolution, 3))\n    theta = np.linspace(0.0, 2.0 * np.pi, resolution)\n     points[:, 0] = radius * np.cos(theta)\n     points[:, 1] = radius * np.sin(theta)\n     cells = np.array([np.append(np.array([resolution]), np.arange(resolution))])",
    "output": "     pyvista.PolyData\n         Circle mesh.\n \n    Notes\n    -----\n    .. versionchanged:: 0.38.0\n       Prior to version 0.38, this method had incorrect results, producing\n       inconsistent edge lengths and a duplicated point which is now fixed.\n\n     Examples\n     --------\n     >>> import pyvista\n     >>> radius = 0.5\n     >>> circle = pyvista.Circle(radius)\n     >>> circle.plot(show_edges=True, line_width=5)\n\n     \"\"\"\n     points = np.zeros((resolution, 3))\n    theta = np.linspace(0.0, 2.0 * np.pi, resolution, endpoint=False)\n     points[:, 0] = radius * np.cos(theta)\n     points[:, 1] = radius * np.sin(theta)\n     cells = np.array([np.append(np.array([resolution]), np.arange(resolution))])"
  },
  {
    "instruction": "vtkVolume needs wrapping like vtkActor\nWe wrap vtkActor nicely and should do the same for vtkVolume to make lookup table modification during volume rendering nicer.\r\n\r\n```py\r\nimport pyvista as pv\r\nfrom pyvista import examples\r\n\r\nvol = examples.download_knee_full()\r\n\r\np = pv.Plotter(notebook=0)\r\nactor = p.add_volume(vol, cmap=\"bone\", opacity=\"sigmoid\")\r\nactor.mapper.lookup_table.cmap = 'viridis'\r\np.show()\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nInput In [3], in <cell line: 8>()\r\n      6 p = pv.Plotter(notebook=0)\r\n      7 actor = p.add_volume(vol, cmap=\"bone\", opacity=\"sigmoid\")\r\n----> 8 actor.mapper.lookup_table.cmap = 'viridis'\r\n      9 p.show()\r\n\r\nAttributeError: 'vtkmodules.vtkRenderingCore.vtkVolume' object has no attribute 'mapper'\r\n```\n",
    "input": "         vtkPointPicker,\n         vtkPolyDataMapper,\n         vtkPolyDataMapper2D,\n         vtkPropAssembly,\n         vtkProperty,\n         vtkPropPicker,\ndiff --git a/pyvista/plotting/actor.py b/pyvista/plotting/actor.py\n import numpy as np\n \n import pyvista as pv\n from pyvista.utilities.misc import no_new_attr\n \nfrom .._typing import BoundsLike\n from ._property import Property\n from .mapper import _BaseMapper\n \n \n @no_new_attr\nclass Actor(pv._vtk.vtkActor):\n     \"\"\"Wrap vtkActor.\n \n     This class represents the geometry & properties in a rendered\n     def visibility(self, value: bool):\n         return self.SetVisibility(value)\n \n    @property\n    def scale(self) -> tuple:\n        \"\"\"Return or set actor scale.\n\n        Examples\n        --------\n        Create an actor using the :class:`pyvista.Plotter` and then change the\n        scale of the actor.\n\n        >>> import pyvista as pv\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_mesh(pv.Sphere())\n        >>> actor.scale = (2.0, 2.0, 2.0)\n        >>> actor.scale\n        (2.0, 2.0, 2.0)\n\n        \"\"\"\n        return self.GetScale()\n\n    @scale.setter\n    def scale(self, value: tuple):\n        return self.SetScale(value)\n\n     def plot(self, **kwargs):\n         \"\"\"Plot just the actor.\n \n         pl.add_actor(self)\n         pl.show(**kwargs)\n \n    @property\n    def position(self):\n        \"\"\"Return or set the actor position.\n\n        Examples\n        --------\n        Change the position of an actor. Note how this does not change the\n        position of the underlying dataset, just the relative location of the\n        actor in the :class:`pyvista.Plotter`.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Sphere()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(mesh, color='r')\n        >>> actor.position = (0, 0, 1)  # shifts the red sphere up\n        >>> pl.show()\n\n        \"\"\"\n        return self.GetPosition()\n\n    @position.setter\n    def position(self, value: tuple):\n        self.SetPosition(value)\n\n    def rotate_x(self, angle: float):\n        \"\"\"Rotate the actor about the x axis.\n\n        Parameters\n        ----------\n        angle : float\n            Angle to rotate the actor about the x axis in degrees.\n\n        Examples\n        --------\n        Rotate the actor about the x axis 45 degrees. Note how this does not\n        change the location of the underlying dataset.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.rotate_x(45)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        self.RotateX(angle)\n\n    def rotate_y(self, angle: float):\n        \"\"\"Rotate the actor about the y axis.\n\n        Parameters\n        ----------\n        angle : float\n            Angle to rotate the actor about the y axis in degrees.\n\n        Examples\n        --------\n        Rotate the actor about the y axis 45 degrees. Note how this does not\n        change the location of the underlying dataset.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.rotate_y(45)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        self.RotateY(angle)\n\n    def rotate_z(self, angle: float):\n        \"\"\"Rotate the actor about the z axis.\n\n        Parameters\n        ----------\n        angle : float\n            Angle to rotate the actor about the z axis in degrees.\n\n        Examples\n        --------\n        Rotate the actor about the Z axis 45 degrees. Note how this does not\n        change the location of the underlying dataset.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.rotate_z(45)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        self.RotateZ(angle)\n\n    @property\n    def orientation(self) -> tuple:\n        \"\"\"Return or set the actor orientation.\n\n        Orientation is defined as the rotation from the global axes in degrees\n        about the actor's x, y, and z axes.\n\n        Examples\n        --------\n        Show that the orientation changes with rotation.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_mesh(mesh)\n        >>> actor.rotate_x(90)\n        >>> actor.orientation  # doctest:+SKIP\n        (90, 0, 0)\n\n        Set the orientation directly.\n\n        >>> actor.orientation = (0, 45, 45)\n        >>> actor.orientation  # doctest:+SKIP\n        (0, 45, 45)\n\n        Reorient just the actor and plot it. Note how the actor is rotated\n        about its own axes as defined by its position.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.position = (0, 0, 1)\n        >>> actor.orientation = (45, 0, 0)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        return self.GetOrientation()\n\n    @orientation.setter\n    def orientation(self, value: tuple):\n        self.SetOrientation(value)\n\n     def copy(self, deep=True) -> 'Actor':\n         \"\"\"Create a copy of this actor.\n \n             value = pv.vtkmatrix_from_array(value)\n         self.SetUserMatrix(value)\n \n    @property\n    def bounds(self) -> BoundsLike:\n        \"\"\"Return the bounds of the actor.\n\n        Bounds are ``(-X, +X, -Y, +Y, -Z, +Z)``\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> pl = pv.Plotter()\n        >>> mesh = pv.Cube(x_length=0.1, y_length=0.2, z_length=0.3)\n        >>> actor = pl.add_mesh(mesh)\n        >>> actor.bounds\n        (-0.05, 0.05, -0.1, 0.1, -0.15, 0.15)\n\n        \"\"\"\n        return self.GetBounds()\n\n    @property\n    def center(self) -> tuple:\n        \"\"\"Return the center of the actor.\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_mesh(pv.Sphere(center=(0.5, 0.5, 1)))\n        >>> actor.center  # doctest:+SKIP\n        (0.5, 0.5, 1)\n        \"\"\"\n        return self.GetCenter()\n\n     @property\n     def backface_prop(self) -> Optional['pv.Property']:\n         \"\"\"Return or set the backface property.\ndiff --git a/pyvista/plotting/mapper.py b/pyvista/plotting/mapper.py\n \"\"\"An internal module for wrapping the use of mappers.\"\"\"\n import sys\nfrom typing import Optional\n \n import numpy as np\n \n )\n from pyvista.utilities.misc import has_module, no_new_attr\n \n from .colors import Color, get_cmap_safe\n from .lookup_table import LookupTable\n from .tools import normalize\n         self._theme = theme\n         self.lookup_table = LookupTable()\n \n     def copy(self) -> '_BaseMapper':\n         \"\"\"Create a copy of this mapper.\n \n             than zero are mapped to the smallest representable\n             positive float.\n \n        nan_color : ColorLike, optional\n             The color to use for all ``NaN`` values in the plotted\n             scalar array.\n \n        above_color : ColorLike, optional\n             Solid color for values below the scalars range\n             (``clim``). This will automatically set the scalar bar\n             ``above_label`` to ``'Above'``.\n \n        below_color : ColorLike, optional\n             Solid color for values below the scalars range\n             (``clim``). This will automatically set the scalar bar\n             ``below_label`` to ``'Below'``.\n             Opacity array to color the dataset. Array length must match either\n             the number of points or cells.\n \n        color : ColorLike\n             The color to use with the opacity array.\n \n         n_colors : int\n     @property\n     def dataset(self):\n         \"\"\"Return or set the dataset assigned to this mapper.\"\"\"\n        return self.GetInputAsDataSet()\n \n     @dataset.setter\n     def dataset(self, new_dataset: 'pv.core.dataset.DataSet'):\n             self.lookup_table.SetRange(*clim)\n         self._scalar_range = clim\n \n     def __del__(self):\n         self._lut = None\n \ndiff --git a/pyvista/plotting/plotting.py b/pyvista/plotting/plotting.py\n \n import pyvista\n from pyvista import _vtk\n from pyvista.utilities import (\n     FieldAssociation,\n     abstract_class,\n from .renderers import Renderers\n from .scalar_bars import ScalarBars\n from .tools import FONTS, normalize, opacity_transfer_function, parse_font_family  # noqa\n from .widgets import WidgetHelper\n \n SUPPORTED_FORMATS = [\".png\", \".jpeg\", \".jpg\", \".bmp\", \".tif\", \".tiff\"]\n                 self.mapper.lookup_table.annotations = annotations\n \n         self.mapper.dataset = volume\n\n        blending = blending.lower()\n        if blending in ['additive', 'add', 'sum']:\n            self.mapper.SetBlendModeToAdditive()\n        elif blending in ['average', 'avg', 'average_intensity']:\n            self.mapper.SetBlendModeToAverageIntensity()\n        elif blending in ['composite', 'comp']:\n            self.mapper.SetBlendModeToComposite()\n        elif blending in ['maximum', 'max', 'maximum_intensity']:\n            self.mapper.SetBlendModeToMaximumIntensity()\n        elif blending in ['minimum', 'min', 'minimum_intensity']:\n            self.mapper.SetBlendModeToMinimumIntensity()\n        else:\n            raise ValueError(\n                f'Blending mode {blending!r} invalid. '\n                'Please choose either \"additive\", '\n                '\"composite\", \"minimum\" or \"maximum\".'\n            )\n         self.mapper.update()\n \n        self.volume = _vtk.vtkVolume()\n        self.volume.SetMapper(self.mapper)\n\n        prop = _vtk.vtkVolumeProperty()\n        prop.SetColor(self.mapper.lookup_table.to_color_tf())\n        prop.SetScalarOpacity(self.mapper.lookup_table.to_opacity_tf())\n        prop.SetAmbient(ambient)\n        prop.SetScalarOpacityUnitDistance(opacity_unit_distance)\n        prop.SetShade(shade)\n        prop.SetDiffuse(diffuse)\n        prop.SetSpecular(specular)\n        prop.SetSpecularPower(specular_power)\n        self.volume.SetProperty(prop)\n \n         actor, prop = self.add_actor(\n             self.volume,\ndiff --git a/pyvista/plotting/prop3d.py b/pyvista/plotting/prop3d.py\nnew file mode 100644\ndiff --git a/pyvista/plotting/volume.py b/pyvista/plotting/volume.py\nnew file mode 100644\ndiff --git a/pyvista/plotting/volume_property.py b/pyvista/plotting/volume_property.py\nnew file mode 100644",
    "output": "         vtkPointPicker,\n         vtkPolyDataMapper,\n         vtkPolyDataMapper2D,\n        vtkProp3D,\n         vtkPropAssembly,\n         vtkProperty,\n         vtkPropPicker,\ndiff --git a/pyvista/plotting/actor.py b/pyvista/plotting/actor.py\n import numpy as np\n \n import pyvista as pv\nfrom pyvista import _vtk\n from pyvista.utilities.misc import no_new_attr\n \n from ._property import Property\n from .mapper import _BaseMapper\nfrom .prop3d import Prop3D\n \n \n @no_new_attr\nclass Actor(Prop3D, _vtk.vtkActor):\n     \"\"\"Wrap vtkActor.\n \n     This class represents the geometry & properties in a rendered\n     def visibility(self, value: bool):\n         return self.SetVisibility(value)\n \n     def plot(self, **kwargs):\n         \"\"\"Plot just the actor.\n \n         pl.add_actor(self)\n         pl.show(**kwargs)\n \n     def copy(self, deep=True) -> 'Actor':\n         \"\"\"Create a copy of this actor.\n \n             value = pv.vtkmatrix_from_array(value)\n         self.SetUserMatrix(value)\n \n     @property\n     def backface_prop(self) -> Optional['pv.Property']:\n         \"\"\"Return or set the backface property.\ndiff --git a/pyvista/plotting/mapper.py b/pyvista/plotting/mapper.py\n \"\"\"An internal module for wrapping the use of mappers.\"\"\"\n import sys\nfrom typing import Optional, Union\n \n import numpy as np\n \n )\n from pyvista.utilities.misc import has_module, no_new_attr\n \nfrom .._typing import BoundsLike\n from .colors import Color, get_cmap_safe\n from .lookup_table import LookupTable\n from .tools import normalize\n         self._theme = theme\n         self.lookup_table = LookupTable()\n \n    @property\n    def bounds(self) -> BoundsLike:\n        \"\"\"Return the bounds of this mapper.\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> mapper = pv.DataSetMapper(dataset=pv.Cube())\n        >>> mapper.bounds\n        (-0.5, 0.5, -0.5, 0.5, -0.5, 0.5)\n\n        \"\"\"\n        return self.GetBounds()\n\n     def copy(self) -> '_BaseMapper':\n         \"\"\"Create a copy of this mapper.\n \n             than zero are mapped to the smallest representable\n             positive float.\n \n        nan_color : color_like, optional\n             The color to use for all ``NaN`` values in the plotted\n             scalar array.\n \n        above_color : color_like, optional\n             Solid color for values below the scalars range\n             (``clim``). This will automatically set the scalar bar\n             ``above_label`` to ``'Above'``.\n \n        below_color : color_like, optional\n             Solid color for values below the scalars range\n             (``clim``). This will automatically set the scalar bar\n             ``below_label`` to ``'Below'``.\n             Opacity array to color the dataset. Array length must match either\n             the number of points or cells.\n \n        color : color_like\n             The color to use with the opacity array.\n \n         n_colors : int\n     @property\n     def dataset(self):\n         \"\"\"Return or set the dataset assigned to this mapper.\"\"\"\n        # GetInputAsDataSet unavailable on volume mappers\n        return self.GetDataSetInput()\n \n     @dataset.setter\n     def dataset(self, new_dataset: 'pv.core.dataset.DataSet'):\n             self.lookup_table.SetRange(*clim)\n         self._scalar_range = clim\n \n    @property\n    def blend_mode(self) -> str:\n        \"\"\"Return or set the blend mode.\n\n        One of the following:\n\n        * ``\"composite\"``\n        * ``\"maximum\"``\n        * ``\"minimum\"``\n        * ``\"average\"``\n        * ``\"additive\"``\n\n        Also accepts integer values corresponding to\n        ``vtk.vtkVolumeMapper.BlendModes``. For example\n        ``vtk.vtkVolumeMapper.COMPOSITE_BLEND``.\n\n        \"\"\"\n        value = self.GetBlendMode()\n        if value == 0:\n            return 'composite'\n        elif value == 1:\n            return 'maximum'\n        elif value == 2:\n            return 'minimum'\n        elif value == 3:\n            return 'average'\n        elif value == 4:\n            return 'additive'\n\n        raise NotImplementedError(\n            f'Unsupported blend mode return value {value}'\n        )  # pragma: no cover\n\n    @blend_mode.setter\n    def blend_mode(self, value: Union[str, int]):\n        if isinstance(value, int):\n            self.SetBlendMode(value)\n        elif isinstance(value, str):\n            value = value.lower()\n            if value in ['additive', 'add', 'sum']:\n                self.SetBlendModeToAdditive()\n            elif value in ['average', 'avg', 'average_intensity']:\n                self.SetBlendModeToAverageIntensity()\n            elif value in ['composite', 'comp']:\n                self.SetBlendModeToComposite()\n            elif value in ['maximum', 'max', 'maximum_intensity']:\n                self.SetBlendModeToMaximumIntensity()\n            elif value in ['minimum', 'min', 'minimum_intensity']:\n                self.SetBlendModeToMinimumIntensity()\n            else:\n                raise ValueError(\n                    f'Blending mode {value!r} invalid. '\n                    'Please choose either \"additive\", '\n                    '\"composite\", \"minimum\" or \"maximum\".'\n                )\n        else:\n            raise TypeError(f'`blend_mode` should be either an int or str, not `{type(value)}`')\n\n     def __del__(self):\n         self._lut = None\n \ndiff --git a/pyvista/plotting/plotting.py b/pyvista/plotting/plotting.py\n \n import pyvista\n from pyvista import _vtk\nfrom pyvista.plotting.volume import Volume\n from pyvista.utilities import (\n     FieldAssociation,\n     abstract_class,\n from .renderers import Renderers\n from .scalar_bars import ScalarBars\n from .tools import FONTS, normalize, opacity_transfer_function, parse_font_family  # noqa\nfrom .volume_property import VolumeProperty\n from .widgets import WidgetHelper\n \n SUPPORTED_FORMATS = [\".png\", \".jpeg\", \".jpg\", \".bmp\", \".tif\", \".tiff\"]\n                 self.mapper.lookup_table.annotations = annotations\n \n         self.mapper.dataset = volume\n        self.mapper.blend_mode = blending\n         self.mapper.update()\n \n        self.volume = Volume()\n        self.volume.mapper = self.mapper\n\n        self.volume.prop = VolumeProperty(\n            lookup_table=self.mapper.lookup_table,\n            ambient=ambient,\n            shade=shade,\n            specular=specular,\n            specular_power=specular_power,\n            diffuse=diffuse,\n            opacity_unit_distance=opacity_unit_distance,\n        )\n \n         actor, prop = self.add_actor(\n             self.volume,\ndiff --git a/pyvista/plotting/prop3d.py b/pyvista/plotting/prop3d.py\nnew file mode 100644\n\"\"\"Prop3D module.\"\"\"\nfrom typing import Tuple\n\nfrom pyvista import _vtk\n\nfrom .._typing import BoundsLike, Vector\n\n\nclass Prop3D(_vtk.vtkProp3D):\n    \"\"\"Prop3D wrapper for vtkProp3D.\n\n    Used to represent an entity in a rendering scene. It handles functions\n    related to the position, orientation and scaling. Used as parent class\n    in Actor and Volume class.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize Prop3D.\"\"\"\n        super().__init__()\n\n    @property\n    def scale(self) -> Tuple[float, float, float]:\n        \"\"\"Return or set entity scale.\n\n        Examples\n        --------\n        Create an actor using the :class:`pyvista.Plotter` and then change the\n        scale of the actor.\n\n        >>> import pyvista as pv\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_mesh(pv.Sphere())\n        >>> actor.scale = (2.0, 2.0, 2.0)\n        >>> actor.scale\n        (2.0, 2.0, 2.0)\n\n        \"\"\"\n        return self.GetScale()\n\n    @scale.setter\n    def scale(self, value: Vector):\n        return self.SetScale(value)\n\n    @property\n    def position(self) -> Tuple[float, float, float]:\n        \"\"\"Return or set the entity position.\n\n        Examples\n        --------\n        Change the position of an actor. Note how this does not change the\n        position of the underlying dataset, just the relative location of the\n        actor in the :class:`pyvista.Plotter`.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Sphere()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(mesh, color='r')\n        >>> actor.position = (0, 0, 1)  # shifts the red sphere up\n        >>> pl.show()\n\n        \"\"\"\n        return self.GetPosition()\n\n    @position.setter\n    def position(self, value: Vector):\n        self.SetPosition(value)\n\n    def rotate_x(self, angle: float):\n        \"\"\"Rotate the entity about the x axis.\n\n        Parameters\n        ----------\n        angle : float\n            Angle to rotate the entity about the x axis in degrees.\n\n        Examples\n        --------\n        Rotate the actor about the x axis 45 degrees. Note how this does not\n        change the location of the underlying dataset.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.rotate_x(45)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        self.RotateX(angle)\n\n    def rotate_y(self, angle: float):\n        \"\"\"Rotate the entity about the y axis.\n\n        Parameters\n        ----------\n        angle : float\n            Angle to rotate the entity about the y axis in degrees.\n\n        Examples\n        --------\n        Rotate the actor about the y axis 45 degrees. Note how this does not\n        change the location of the underlying dataset.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.rotate_y(45)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        self.RotateY(angle)\n\n    def rotate_z(self, angle: float):\n        \"\"\"Rotate the entity about the z axis.\n\n        Parameters\n        ----------\n        angle : float\n            Angle to rotate the entity about the z axis in degrees.\n\n        Examples\n        --------\n        Rotate the actor about the Z axis 45 degrees. Note how this does not\n        change the location of the underlying dataset.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.rotate_z(45)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        self.RotateZ(angle)\n\n    @property\n    def orientation(self) -> tuple:\n        \"\"\"Return or set the entity orientation.\n\n        Orientation is defined as the rotation from the global axes in degrees\n        about the actor's x, y, and z axes.\n\n        Examples\n        --------\n        Show that the orientation changes with rotation.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_mesh(mesh)\n        >>> actor.rotate_x(90)\n        >>> actor.orientation  # doctest:+SKIP\n        (90, 0, 0)\n\n        Set the orientation directly.\n\n        >>> actor.orientation = (0, 45, 45)\n        >>> actor.orientation  # doctest:+SKIP\n        (0, 45, 45)\n\n        Reorient just the actor and plot it. Note how the actor is rotated\n        about its own axes as defined by its position.\n\n        >>> import pyvista as pv\n        >>> mesh = pv.Cube()\n        >>> pl = pv.Plotter()\n        >>> _ = pl.add_mesh(mesh, color='b')\n        >>> actor = pl.add_mesh(\n        ...     mesh, color='r', style='wireframe', line_width=5, lighting=False,\n        ... )\n        >>> actor.position = (0, 0, 1)\n        >>> actor.orientation = (45, 0, 0)\n        >>> pl.show_axes()\n        >>> pl.show()\n\n        \"\"\"\n        return self.GetOrientation()\n\n    @orientation.setter\n    def orientation(self, value: tuple):\n        self.SetOrientation(value)\n\n    @property\n    def bounds(self) -> BoundsLike:\n        \"\"\"Return the bounds of the entity.\n\n        Bounds are ``(-X, +X, -Y, +Y, -Z, +Z)``\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> pl = pv.Plotter()\n        >>> mesh = pv.Cube(x_length=0.1, y_length=0.2, z_length=0.3)\n        >>> actor = pl.add_mesh(mesh)\n        >>> actor.bounds\n        (-0.05, 0.05, -0.1, 0.1, -0.15, 0.15)\n\n        \"\"\"\n        return self.GetBounds()\n\n    @property\n    def center(self) -> tuple:\n        \"\"\"Return the center of the entity.\n\n        Examples\n        --------\n        >>> import pyvista as pv\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_mesh(pv.Sphere(center=(0.5, 0.5, 1)))\n        >>> actor.center  # doctest:+SKIP\n        (0.5, 0.5, 1)\n        \"\"\"\n        return self.GetCenter()\ndiff --git a/pyvista/plotting/volume.py b/pyvista/plotting/volume.py\nnew file mode 100644\n\"\"\"PyVista volume module.\"\"\"\nfrom pyvista import _vtk\n\nfrom ._property import Property\nfrom .mapper import _BaseMapper\nfrom .prop3d import Prop3D\n\n\nclass Volume(Prop3D, _vtk.vtkVolume):\n    \"\"\"Wrapper class for VTK volume.\n\n    This class represents a volume in a rendered scene. It inherits\n    functions related to the volume's position, orientation and origin\n    from Prop3D.\n\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize volume.\"\"\"\n        super().__init__()\n\n    @property\n    def mapper(self) -> _BaseMapper:\n        \"\"\"Return or set the mapper of the volume.\n\n        Examples\n        --------\n        Add a volume to a :class:`pyvista.Plotter` and get its mapper.\n\n        >>> import pyvista as pv\n        >>> vol = pv.UniformGrid(dimensions=(10, 10, 10))\n        >>> vol['scalars'] = 255 - vol.z*25\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_volume(vol)\n        >>> actor.mapper.bounds\n        (0.0, 9.0, 0.0, 9.0, 0.0, 9.0)\n        \"\"\"\n        return self.GetMapper()\n\n    @mapper.setter\n    def mapper(self, obj):\n        return self.SetMapper(obj)\n\n    @property\n    def prop(self):\n        \"\"\"Return or set the property of this actor.\n\n        Examples\n        --------\n        Create an volume and get its properties.\n\n        >>> import pyvista as pv\n        >>> vol = pv.UniformGrid(dimensions=(10, 10, 10))\n        >>> vol['scalars'] = 255 - vol.z*25\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_volume(vol)\n        >>> actor.prop.GetShade()\n        0\n\n        \"\"\"\n        return self.GetProperty()\n\n    @prop.setter\n    def prop(self, obj: Property):\n        self.SetProperty(obj)\ndiff --git a/pyvista/plotting/volume_property.py b/pyvista/plotting/volume_property.py\nnew file mode 100644\n\"\"\"This module contains the VolumeProperty class.\"\"\"\nimport pyvista as pv\nfrom pyvista import _vtk\nfrom pyvista.utilities.misc import no_new_attr\n\n\n@no_new_attr\nclass VolumeProperty(_vtk.vtkVolumeProperty):\n    \"\"\"Wrap the VTK class vtkVolumeProperty.\n\n    This class is used to represent common properties associated with volume\n    rendering. This includes properties for determining the type of\n    interpolation to use when sampling a volume, the color of a volume, the\n    scalar opacity of a volume, the gradient opacity of a volume, and the\n    shading parameters of a volume.\n\n    Parameters\n    ----------\n    lookup_table : pyvista.LookupTable, optional\n        Lookup table to set the color and opacity transfer functions.\n\n    interpolation_type : str, optional\n        Value must be either ``'linear'`` or ``'nearest'``.\n\n    ambient : float, optional\n        When lighting is enabled, this is the amount of light in\n        the range of 0 to 1 (default 0.0) that reaches the actor\n        when not directed at the light source emitted from the\n        viewer.\n\n    diffuse : float, optional\n        The diffuse lighting coefficient. Default 1.0.\n\n    specular : float, optional\n        The specular lighting coefficient. Default 0.0.\n\n    specular_power : float, optional\n        The specular power. Between 0.0 and 128.0.\n\n    shade : bool, optional\n        Enable or disable volume shading.  If shading is turned off, then the\n        mapper for the volume will not perform shading calculations. If shading\n        is turned on, the mapper may perform shading calculations - in some\n        cases shading does not apply (for example, in a maximum intensity\n        projection) and therefore shading will not be performed even if this\n        flag is on. For a compositing type of mapper, turning shading off is\n        generally the same as setting ``ambient=1``, ``diffuse=0``,\n        ``specular=0``. Shading can be independently turned on/off per\n        component.\n\n    opacity_unit_distance : float, optional\n        This is the unit distance on which the scalar opacity transfer function\n        is defined. By default this is 1.0, meaning that over a distance of 1.0\n        units, a given opacity (from the transfer function) is\n        accumulated. This is adjusted for the actual sampling distance during\n        rendering.\n\n    Examples\n    --------\n    Create a sample dataset from perlin noise and apply a lookup table to the\n    :class:`VolumeProperty`.\n\n    >>> import pyvista as pv\n    >>> noise = pv.perlin_noise(1, (1, 3, 5), (0, 0, 0))\n    >>> grid = pv.sample_function(noise, [0, 3.0, -0, 1.0, 0, 1.0], dim=(40, 40, 40))\n    >>> grid['scalars'] -= grid['scalars'].min()\n    >>> grid['scalars']*= 255/grid['scalars'].max()\n    >>> pl = pv.Plotter()\n    >>> actor = pl.add_volume(grid, show_scalar_bar=False)\n    >>> lut = pv.LookupTable(cmap='bwr')\n    >>> lut.apply_opacity([1.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.3])\n    >>> actor.prop.apply_lookup_table(lut)\n    >>> pl.show()\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        lookup_table=None,\n        interpolation_type=None,\n        ambient=None,\n        diffuse=None,\n        specular=None,\n        specular_power=None,\n        shade=None,\n        opacity_unit_distance=None,\n    ):\n        \"\"\"Initialize the vtkVolumeProperty class.\"\"\"\n        super().__init__()\n        if lookup_table is not None:\n            self.apply_lookup_table(lookup_table)\n        if interpolation_type is not None:\n            self.interpolation_type = interpolation_type\n        if ambient is not None:\n            self.ambient = ambient\n        if diffuse is not None:\n            self.diffuse = diffuse\n        if specular is not None:\n            self.specular = specular\n        if specular_power is not None:\n            self.specular_power = specular_power\n        if shade is not None:\n            self.shade = shade\n        if opacity_unit_distance is not None:\n            self.opacity_unit_distance = opacity_unit_distance\n\n    def apply_lookup_table(self, lookup_table: 'pv.LookupTable'):\n        \"\"\"Apply a lookup table to the volume property.\n\n        Applies both the color and opacity of the lookup table as transfer\n        functions.\n\n        Parameters\n        ----------\n        lookup_table : pyvista.LookupTable, optional\n            Lookup table to set the color and opacity transfer functions.\n\n        Examples\n        --------\n        Plot perlin noise volumetrically using a custom lookup table.\n\n        >>> import pyvista as pv\n        >>> noise = pv.perlin_noise(1, (1, 3, 5), (0, 0, 0))\n        >>> grid = pv.sample_function(noise, [0, 3.0, -0, 1.0, 0, 1.0], dim=(40, 40, 40))\n        >>> grid['scalars'] -= grid['scalars'].min()\n        >>> grid['scalars']*= 255/grid['scalars'].max()\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_volume(grid, show_scalar_bar=False)\n        >>> lut = pv.LookupTable(cmap='bwr')\n        >>> lut.apply_opacity([1.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.3])\n        >>> actor.prop.apply_lookup_table(lut)\n        >>> pl.show()\n\n        \"\"\"\n        if not isinstance(lookup_table, pv.LookupTable):\n            raise TypeError('`lookup_table` must be a `pyvista.LookupTable`')\n\n        self.SetColor(lookup_table.to_color_tf())\n        self.SetScalarOpacity(lookup_table.to_opacity_tf())\n\n    @property\n    def interpolation_type(self) -> str:\n        \"\"\"Return or set the interpolation type.\n\n        Value must be either ``'linear'`` or ``'nearest'``.\n\n        Examples\n        --------\n        Create a sample :class:`pyvista.UniformGrid` dataset.\n\n        >>> import numpy as np\n        >>> import pyvista as pv\n        >>> n = 21\n        >>> c = -(n-1)/2\n        >>> vol = pv.UniformGrid(dimensions=(n, n, n), origin=(c, c, c))\n        >>> scalars = np.linalg.norm(vol.points, axis=1)\n        >>> scalars *= 255/scalars.max()\n        >>> vol['scalars'] = scalars\n\n        Demonstrate nearest (default) interpolation.\n\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_volume(\n        ...     vol,\n        ...     show_scalar_bar=False,\n        ...     opacity=[0.3, 0.0, 0.05, 0.0, 0.0, 0.0, 1.0, 0.0],\n        ...     cmap='plasma'\n        ... )\n        >>> actor.prop.interpolation_type = 'nearest'\n        >>> pl.show()\n\n        Demonstrate linear interpolation.\n\n        >>> pl = pv.Plotter()\n        >>> actor = pl.add_volume(\n        ...     vol,\n        ...     show_scalar_bar=False,\n        ...     opacity=[0.3, 0.0, 0.05, 0.0, 0.0, 0.0, 1.0, 0.0],\n        ...     cmap='plasma'\n        ... )\n        >>> actor.prop.interpolation_type = 'linear'\n        >>> pl.show()\n\n        \"\"\"\n        return self.GetInterpolationTypeAsString().split()[0].lower()\n\n    @interpolation_type.setter\n    def interpolation_type(self, value: str):\n        if value == 'linear':\n            self.SetInterpolationTypeToLinear()\n        elif value == 'nearest':\n            self.SetInterpolationTypeToNearest()\n        else:\n            raise ValueError('`interpolation_type` must be either \"linear\" or \"nearest\"')\n\n    @property\n    def opacity_unit_distance(self) -> float:\n        \"\"\"Return or set the opacity unit distance.\n\n        This is the unit distance on which the scalar opacity transfer function\n        is defined.\n\n        By default this is 1.0, meaning that over a distance of 1.0 units, a\n        given opacity (from the transfer function) is accumulated. This is\n        adjusted for the actual sampling distance during rendering.\n        \"\"\"\n        return self.GetScalarOpacityUnitDistance()\n\n    @opacity_unit_distance.setter\n    def opacity_unit_distance(self, value: float):\n        self.SetScalarOpacityUnitDistance(value)\n\n    @property\n    def shade(self) -> bool:\n        \"\"\"Return or set shading of a volume.\n\n        If shading is turned off, then the mapper for the volume will not\n        perform shading calculations. If shading is turned on, the mapper may\n        perform shading calculations - in some cases shading does not apply\n        (for example, in a maximum intensity projection) and therefore shading\n        will not be performed even if this flag is on. For a compositing type\n        of mapper, turning shading off is generally the same as setting\n        ``ambient=1``, ``diffuse=0``, ``specular=0``. Shading can be\n        independently turned on/off per component.\n\n        \"\"\"\n        return bool(self.GetShade())\n\n    @shade.setter\n    def shade(self, value: bool):\n        self.SetShade(value)\n\n    @property\n    def ambient(self) -> float:\n        \"\"\"Return or set ambient lighting coefficient.\n\n        This is the amount of light in the range of 0 to 1 (default 0.0) that\n        reaches the actor when not directed at the light source emitted from\n        the viewer.\n\n        Changing attribute has no effect unless :attr:`VolumeProperty.shade` is\n        set to ``True``.\n\n        \"\"\"\n        return self.GetAmbient()\n\n    @ambient.setter\n    def ambient(self, value: float):\n        self.SetAmbient(value)\n\n    @property\n    def diffuse(self) -> float:\n        \"\"\"Return or set the diffuse lighting coefficient.\n\n        Default 1.0.\n\n        This is the scattering of light by reflection or transmission. Diffuse\n        reflection results when light strikes an irregular surface such as a\n        frosted window or the surface of a frosted or coated light bulb.\n\n        Changing attribute has no effect unless :attr:`VolumeProperty.shade` is\n        set to ``True``.\n\n        \"\"\"\n        return self.GetDiffuse()\n\n    @diffuse.setter\n    def diffuse(self, value: float):\n        self.SetDiffuse(value)\n\n    @property\n    def specular(self) -> float:\n        \"\"\"Return or set specular.\n\n        Default 0.0\n\n        Specular lighting simulates the bright spot of a light that appears on\n        shiny objects.\n\n        Changing attribute has no effect unless :attr:`VolumeProperty.shade` is\n        set to ``True``.\n\n        \"\"\"\n        return self.GetSpecular()\n\n    @specular.setter\n    def specular(self, value: float):\n        self.SetSpecular(value)\n\n    @property\n    def specular_power(self) -> float:\n        \"\"\"Return or set specular power.\n\n        The specular power. Between 0.0 and 128.0. Default 10.0\n\n        \"\"\"\n        return self.GetSpecularPower()\n\n    @specular_power.setter\n    def specular_power(self, value: float):\n        self.SetSpecularPower(value)\n\n    def copy(self) -> 'VolumeProperty':\n        \"\"\"Create a deep copy of this property.\n\n        Returns\n        -------\n        pyvista.VolumeProperty\n            Deep copy of this property.\n\n        \"\"\"\n        new_prop = VolumeProperty()\n        new_prop.DeepCopy(self)\n        return new_prop\n\n    def __repr__(self):\n        \"\"\"Representation of this property.\"\"\"\n        props = [\n            f'{type(self).__name__} ({hex(id(self))})',\n        ]\n\n        for attr in dir(self):\n            if not attr.startswith('_') and attr[0].islower():\n                name = ' '.join(attr.split('_')).capitalize() + ':'\n                value = getattr(self, attr)\n                if callable(value):\n                    continue\n                if isinstance(value, str):\n                    value = f'\"{value}\"'\n                props.append(f'  {name:28s} {value}')\n\n        return '\\n'.join(props)"
  },
  {
    "instruction": "Rectilinear grid does not allow Sequences as inputs\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nRectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\r\n\r\n### Steps to reproduce the bug.\r\n\r\nThis doesn't work\r\n```python\r\nimport pyvista as pv\r\npv.RectilinearGrid([0, 1], [0, 1], [0, 1])\r\n```\r\n\r\nThis works\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\r\n```\r\n### System Information\r\n\r\n```shell\r\n--------------------------------------------------------------------------------\r\n  Date: Wed Apr 19 20:15:10 2023 UTC\r\n\r\n                OS : Linux\r\n            CPU(s) : 2\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n        GPU Vendor : Mesa/X.org\r\n      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\r\n       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\r\n\r\n  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.38.5\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n           imageio : 2.27.0\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\n_No response_\n",
    "input": "                     self.shallow_copy(args[0])\n             elif isinstance(args[0], (str, pathlib.Path)):\n                 self._from_file(args[0], **kwargs)\n            elif isinstance(args[0], np.ndarray):\n                self._from_arrays(args[0], None, None, check_duplicates)\n             else:\n                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n \n         elif len(args) == 3 or len(args) == 2:\n            arg0_is_arr = isinstance(args[0], np.ndarray)\n            arg1_is_arr = isinstance(args[1], np.ndarray)\n             if len(args) == 3:\n                arg2_is_arr = isinstance(args[2], np.ndarray)\n             else:\n                 arg2_is_arr = False\n \n             if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n                self._from_arrays(args[0], args[1], args[2], check_duplicates)\n             elif all([arg0_is_arr, arg1_is_arr]):\n                self._from_arrays(args[0], args[1], None, check_duplicates)\n             else:\n                 raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n ",
    "output": "                     self.shallow_copy(args[0])\n             elif isinstance(args[0], (str, pathlib.Path)):\n                 self._from_file(args[0], **kwargs)\n            elif isinstance(args[0], (np.ndarray, Sequence)):\n                self._from_arrays(np.asanyarray(args[0]), None, None, check_duplicates)\n             else:\n                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')\n \n         elif len(args) == 3 or len(args) == 2:\n            arg0_is_arr = isinstance(args[0], (np.ndarray, Sequence))\n            arg1_is_arr = isinstance(args[1], (np.ndarray, Sequence))\n             if len(args) == 3:\n                arg2_is_arr = isinstance(args[2], (np.ndarray, Sequence))\n             else:\n                 arg2_is_arr = False\n \n             if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):\n                self._from_arrays(\n                    np.asanyarray(args[0]),\n                    np.asanyarray(args[1]),\n                    np.asanyarray(args[2]),\n                    check_duplicates,\n                )\n             elif all([arg0_is_arr, arg1_is_arr]):\n                self._from_arrays(\n                    np.asanyarray(args[0]), np.asanyarray(args[1]), None, check_duplicates\n                )\n             else:\n                 raise TypeError(\"Arguments not understood by `RectilinearGrid`.\")\n "
  },
  {
    "instruction": "Memory leaks when accessing sequence tags with Dataset.__getattr__.\n**Describe the bug**\r\nAccessing sequences via `Dataset.__getattr__` seems to leak memory. The bug occurred for me when I was processing many DICOMs and manipulating some tags contained in sequences and each leaked a bit of memory, ultimately crashing the process.\r\n\r\n**Expected behavior**\r\nMemory should not leak. It works correctly when you replace the `__getattr__` call with `__getitem__` (by manually constructing the necessary tag beforehand).\r\n\r\nWithout being an expert in the codebase, one difference I think that could explain it is that `__getattr__` sets `value.parent = self` for sequences while `__getitem__` doesn't seem to do that. Maybe this loop of references somehow confuses Python's garbage collection?\r\n\r\n**Steps To Reproduce**\r\nThis increases the memory consumption of the Python process by about 700 MB on my machine. The DICOM file I've tested it with is 27MB and has one item in `SourceImageSequence`. Note that the memory leak plateaus after a while in this example, maybe because it's the same file. In my actual workflow when iterating over many different files, the process filled all memory and crashed.\r\n\r\n```python\r\nimport pydicom\r\nfor i in range(100):\r\n  dcm = pydicom.dcmread(\"my_dicom.dcm\")\r\n  test = dcm.SourceImageSequence\r\n```\r\n\r\nFor comparison, this keeps the memory constant. `(0x0008, 0x2112)` is `SourceImageSequence`: \r\n\r\n```python\r\nimport pydicom\r\nimport pydicom.tag\r\nfor i in range(100):\r\n  dcm = pydicom.dcmread(\"my_dicom.dcm\")\r\n  test = dcm[pydicom.tag.TupleTag((0x0008, 0x2112))]\r\n```\r\n\r\n**Your environment**\r\n\r\n```bash\r\nLinux-4.15.0-72-generic-x86_64-with-Ubuntu-18.04-bionic\r\nPython  3.6.8 (default, Jan 14 2019, 11:02:34)\r\npydicom  1.3.0\r\n```\r\n\n",
    "input": "         if tag is not None:  # `name` isn't a DICOM element keyword\n             tag = Tag(tag)\n             if tag in self._dict:  # DICOM DataElement not in the Dataset\n                data_elem = self[tag]\n                value = data_elem.value\n                if data_elem.VR == 'SQ':\n                    # let a sequence know its parent dataset, as sequence items\n                    # may need parent dataset tags to resolve ambiguous tags\n                    value.parent = self\n                return value\n \n         # no tag or tag not contained in the dataset\n         if name == '_dict':\n         data_elem = self._dict[tag]\n \n         if isinstance(data_elem, DataElement):\n             return data_elem\n         elif isinstance(data_elem, tuple):\n             # If a deferred read, then go get the value now\n         Raises\n         ------\n         ValueError\n            If `name` is not a valid handler name.\n         NotImplementedError\n             If the given handler or any handler, if none given, is able to\n             decompress pixel data with the current transfer syntax\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n         #   For references, see the list at\n         #   https://github.com/darcymason/pydicom/pull/298\n         # PixelRepresentation is usually set in the root dataset\n        while 'PixelRepresentation' not in ds and ds.parent:\n            ds = ds.parent\n         # if no pixel data is present, none if these tags is used,\n         # so we can just ignore a missing PixelRepresentation in this case\n         if ('PixelRepresentation' not in ds and 'PixelData' not in ds or\ndiff --git a/pydicom/sequence.py b/pydicom/sequence.py\n \n Sequence is a list of pydicom Dataset objects.\n \"\"\"\n \n from pydicom.dataset import Dataset\n from pydicom.multival import MultiValue\n \n     @property\n     def parent(self):\n        \"\"\"Return the parent :class:`~pydicom.dataset.Dataset`.\"\"\"\n         return self._parent\n \n     @parent.setter\n         :class:`Sequence` items.\n         \"\"\"\n         if value != self._parent:\n            self._parent = value\n             for item in self._list:\n                 item.parent = self._parent\n ",
    "output": "         if tag is not None:  # `name` isn't a DICOM element keyword\n             tag = Tag(tag)\n             if tag in self._dict:  # DICOM DataElement not in the Dataset\n                return self[tag].value\n \n         # no tag or tag not contained in the dataset\n         if name == '_dict':\n         data_elem = self._dict[tag]\n \n         if isinstance(data_elem, DataElement):\n            if data_elem.VR == 'SQ' and data_elem.value:\n                # let a sequence know its parent dataset, as sequence items\n                # may need parent dataset tags to resolve ambiguous tags\n                data_elem.value.parent = self\n             return data_elem\n         elif isinstance(data_elem, tuple):\n             # If a deferred read, then go get the value now\n         Raises\n         ------\n         ValueError\n            If `handler_name` is not a valid handler name.\n         NotImplementedError\n             If the given handler or any handler, if none given, is able to\n             decompress pixel data with the current transfer syntax\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n         #   For references, see the list at\n         #   https://github.com/darcymason/pydicom/pull/298\n         # PixelRepresentation is usually set in the root dataset\n        while 'PixelRepresentation' not in ds and ds.parent and ds.parent():\n            ds = ds.parent()\n         # if no pixel data is present, none if these tags is used,\n         # so we can just ignore a missing PixelRepresentation in this case\n         if ('PixelRepresentation' not in ds and 'PixelData' not in ds or\ndiff --git a/pydicom/sequence.py b/pydicom/sequence.py\n \n Sequence is a list of pydicom Dataset objects.\n \"\"\"\nimport weakref\n \n from pydicom.dataset import Dataset\n from pydicom.multival import MultiValue\n \n     @property\n     def parent(self):\n        \"\"\"Return a weak reference to the parent\n        :class:`~pydicom.dataset.Dataset`.\"\"\"\n         return self._parent\n \n     @parent.setter\n         :class:`Sequence` items.\n         \"\"\"\n         if value != self._parent:\n            self._parent = weakref.ref(value)\n             for item in self._list:\n                 item.parent = self._parent\n "
  },
  {
    "instruction": "Add support for Extended Offset Table to encaps module\n[CP1818](http://webcache.googleusercontent.com/search?q=cache:xeWXtrAs9G4J:ftp://medical.nema.org/medical/dicom/final/cp1818_ft_whenoffsettabletoosmall.pdf) added the use of an Extended Offset Table for encapsulated pixel data when the Basic Offset Table isn't suitable.\n",
    "input": "     tag = Tag(fp.read_tag())\n \n     if tag != 0xfffee000:\n        raise ValueError(\"Unexpected tag '{}' when parsing the Basic Table \"\n                         \"Offset item.\".format(tag))\n \n     length = fp.read_UL()\n     if length % 4:\n        raise ValueError(\"The length of the Basic Offset Table item is not \"\n                         \"a multiple of 4.\")\n \n     offsets = []\n     # Always return at least a 0 offset\n             # Item\n             length = fp.read_UL()\n             if length == 0xFFFFFFFF:\n                raise ValueError(\"Undefined item length at offset {} when \"\n                                 \"parsing the encapsulated pixel data \"\n                                 \"fragments.\".format(fp.tell() - 4))\n             fp.seek(length, 1)\n             nr_fragments += 1\n         elif tag == 0xFFFEE0DD:\n             # Sequence Delimiter\n             break\n         else:\n            raise ValueError(\"Unexpected tag '{}' at offset {} when parsing \"\n                             \"the encapsulated pixel data fragment items.\"\n                             .format(tag, fp.tell() - 4))\n \n     fp.seek(start)\n     return nr_fragments\n             # Item\n             length = fp.read_UL()\n             if length == 0xFFFFFFFF:\n                raise ValueError(\"Undefined item length at offset {} when \"\n                                 \"parsing the encapsulated pixel data \"\n                                 \"fragments.\".format(fp.tell() - 4))\n             yield fp.read(length)\n         elif tag == 0xFFFEE0DD:\n             # Sequence Delimiter\n             fp.seek(-4, 1)\n             break\n         else:\n            raise ValueError(\"Unexpected tag '{0}' at offset {1} when parsing \"\n                             \"the encapsulated pixel data fragment items.\"\n                             .format(tag, fp.tell() - 4))\n \n \n def generate_pixel_data_frame(\n     frame_length = len(frame)\n     # Add 1 to fix odd length frames not being caught\n     if nr_fragments > (frame_length + 1) / 2.0:\n        raise ValueError('Too many fragments requested (the minimum fragment '\n                         'size is 2 bytes)')\n \n     length = int(frame_length / nr_fragments)\n \n       a 4 byte length.\n     \"\"\"\n     # item tag (fffe,e000)\n    item = bytes(b'\\xFE\\xFF\\x00\\xE0')\n     # fragment length '<I' little endian, 4 byte unsigned int\n     item += pack('<I', len(fragment))\n     # fragment data\n     :dcm:`Annex A.4 <part05/sect_A.4.html>`\n     \"\"\"\n     for fragment in fragment_frame(frame, nr_fragments):\n        yield itemise_fragment(fragment)\n \n \n itemise_frame = itemize_frame\n     For multi-frame data each frame must be encoded separately and then all\n     encoded frames encapsulated together.\n \n     Data will be encapsulated with a Basic Offset Table Item at the beginning,\n     then one or more fragment items. Each item will be of even length and the\n     final fragment of each frame may be padded with ``0x00`` if required.\n     ----------\n     DICOM Standard, Part 5, :dcm:`Section 7.5 <part05/sect_7.5.html>` and\n     :dcm:`Annex A.4 <part05/sect_A.4.html>`\n     \"\"\"\n    no_frames = len(frames)\n     output = bytearray()\n \n     # Add the Basic Offset Table Item\n     # Add the tag\n     output.extend(b'\\xFE\\xFF\\x00\\xE0')\n     if has_bot:\n         # Add the length\n        output.extend(pack('<I', 4 * no_frames))\n         # Reserve 4 x len(frames) bytes for the offsets\n        output.extend(b'\\xFF\\xFF\\xFF\\xFF' * no_frames)\n     else:\n         # Add the length\n         output.extend(pack('<I', 0))\n     for ii, frame in enumerate(frames):\n         # `itemised_length` is the total length of each itemised frame\n         itemised_length = 0\n        for item in itemise_frame(frame, fragments_per_frame):\n             itemised_length += len(item)\n             output.extend(item)\n \n \n     if has_bot:\n         # Go back and write the frame offsets - don't need the last offset\n        output[8:8 + 4 * no_frames] = pack('<{}I'.format(no_frames),\n                                           *bot_offsets[:-1])\n \n     return bytes(output)",
    "output": "     tag = Tag(fp.read_tag())\n \n     if tag != 0xfffee000:\n        raise ValueError(\n            f\"Unexpected tag '{tag}' when parsing the Basic Table Offset item\"\n        )\n \n     length = fp.read_UL()\n     if length % 4:\n        raise ValueError(\n            \"The length of the Basic Offset Table item is not a multiple of 4\"\n        )\n \n     offsets = []\n     # Always return at least a 0 offset\n             # Item\n             length = fp.read_UL()\n             if length == 0xFFFFFFFF:\n                raise ValueError(\n                    f\"Undefined item length at offset {fp.tell() - 4} when \"\n                    \"parsing the encapsulated pixel data fragments\"\n                )\n             fp.seek(length, 1)\n             nr_fragments += 1\n         elif tag == 0xFFFEE0DD:\n             # Sequence Delimiter\n             break\n         else:\n            raise ValueError(\n                f\"Unexpected tag '{tag}' at offset {fp.tell() - 4} when \"\n                \"parsing the encapsulated pixel data fragment items\"\n            )\n \n     fp.seek(start)\n     return nr_fragments\n             # Item\n             length = fp.read_UL()\n             if length == 0xFFFFFFFF:\n                raise ValueError(\n                    f\"Undefined item length at offset {fp.tell() - 4} when \"\n                    \"parsing the encapsulated pixel data fragments\"\n                )\n             yield fp.read(length)\n         elif tag == 0xFFFEE0DD:\n             # Sequence Delimiter\n             fp.seek(-4, 1)\n             break\n         else:\n            raise ValueError(\n                f\"Unexpected tag '{tag}' at offset {fp.tell() - 4} when \"\n                \"parsing the encapsulated pixel data fragment items\"\n            )\n \n \n def generate_pixel_data_frame(\n     frame_length = len(frame)\n     # Add 1 to fix odd length frames not being caught\n     if nr_fragments > (frame_length + 1) / 2.0:\n        raise ValueError(\n            \"Too many fragments requested (the minimum fragment size is \"\n            \"2 bytes)\"\n        )\n \n     length = int(frame_length / nr_fragments)\n \n       a 4 byte length.\n     \"\"\"\n     # item tag (fffe,e000)\n    item = b'\\xFE\\xFF\\x00\\xE0'\n     # fragment length '<I' little endian, 4 byte unsigned int\n     item += pack('<I', len(fragment))\n     # fragment data\n     :dcm:`Annex A.4 <part05/sect_A.4.html>`\n     \"\"\"\n     for fragment in fragment_frame(frame, nr_fragments):\n        yield itemize_fragment(fragment)\n \n \n itemise_frame = itemize_frame\n     For multi-frame data each frame must be encoded separately and then all\n     encoded frames encapsulated together.\n \n    When many large frames are to be encapsulated, the total length of\n    encapsulated data may exceed the maximum length available with the\n    :dcm:`Basic Offset Table<part05/sect_A.4.html>` (2**31 - 1 bytes). Under\n    these circumstances you can:\n\n    * Pass ``has_bot=False`` to :func:`~pydicom.encaps.encapsulate`\n    * Use :func:`~pydicom.encaps.encapsulate_extended` and add the\n      :dcm:`Extended Offset Table<part03/sect_C.7.6.3.html>` elements to your\n      dataset (recommended)\n\n     Data will be encapsulated with a Basic Offset Table Item at the beginning,\n     then one or more fragment items. Each item will be of even length and the\n     final fragment of each frame may be padded with ``0x00`` if required.\n     ----------\n     DICOM Standard, Part 5, :dcm:`Section 7.5 <part05/sect_7.5.html>` and\n     :dcm:`Annex A.4 <part05/sect_A.4.html>`\n\n    See Also\n    --------\n    :func:`~pydicom.encaps.encapsulate_extended`\n     \"\"\"\n    nr_frames = len(frames)\n     output = bytearray()\n \n     # Add the Basic Offset Table Item\n     # Add the tag\n     output.extend(b'\\xFE\\xFF\\x00\\xE0')\n     if has_bot:\n        # Check that the 2**32 - 1 limit in BOT item lengths won't be exceeded\n        total = (nr_frames - 1) * 8 + sum([len(f) for f in frames[:-1]])\n        if total > 2**32 - 1:\n            raise ValueError(\n                f\"The total length of the encapsulated frame data ({total} \"\n                \"bytes) will be greater than the maximum allowed by the Basic \"\n                f\"Offset Table ({2**32 - 1} bytes), it's recommended that you \"\n                \"use the Extended Offset Table instead (see the \"\n                \"'encapsulate_extended' function for more information)\"\n            )\n\n         # Add the length\n        output.extend(pack('<I', 4 * nr_frames))\n         # Reserve 4 x len(frames) bytes for the offsets\n        output.extend(b'\\xFF\\xFF\\xFF\\xFF' * nr_frames)\n     else:\n         # Add the length\n         output.extend(pack('<I', 0))\n     for ii, frame in enumerate(frames):\n         # `itemised_length` is the total length of each itemised frame\n         itemised_length = 0\n        for item in itemize_frame(frame, fragments_per_frame):\n             itemised_length += len(item)\n             output.extend(item)\n \n \n     if has_bot:\n         # Go back and write the frame offsets - don't need the last offset\n        output[8:8 + 4 * nr_frames] = pack(f\"<{nr_frames}I\", *bot_offsets[:-1])\n \n     return bytes(output)\n\n\ndef encapsulate_extended(frames: List[bytes]) -> Tuple[bytes, bytes, bytes]:\n    \"\"\"Return encapsulated image data and values for the Extended Offset Table\n    elements.\n\n    When using a compressed transfer syntax (such as RLE Lossless or one of\n    JPEG formats) then any *Pixel Data* must be :dcm:`encapsulated\n    <part05/sect_A.4.html>`. When many large frames are to be encapsulated, the\n    total length of encapsulated data may exceed the maximum length available\n    with the :dcm:`Basic Offset Table<part05/sect_A.4.html>` (2**32 - 1 bytes).\n    Under these circumstances you can:\n\n    * Pass ``has_bot=False`` to :func:`~pydicom.encaps.encapsulate`\n    * Use :func:`~pydicom.encaps.encapsulate_extended` and add the\n      :dcm:`Extended Offset Table<part03/sect_C.7.6.3.html>` elements to your\n      dataset (recommended)\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        from pydicom.encaps import encapsulate_extended\n\n        # 'frames' is a list of image frames that have been each been encoded\n        # separately using the compression method corresponding to the Transfer\n        # Syntax UID\n        frames: List[bytes] = [...]\n        out: Tuple[bytes, bytes, bytes] = encapsulate_extended(frames)\n\n        ds.PixelData = out[0]\n        ds.ExtendedOffsetTable = out[1]\n        ds.ExtendedOffsetTableLengths = out[2]\n\n    Parameters\n    ----------\n    frames : list of bytes\n        The compressed frame data to encapsulate, one frame per item.\n\n    Returns\n    -------\n    bytes, bytes, bytes\n        The (encapsulated frames, extended offset table, extended offset\n        table lengths).\n\n    See Also\n    --------\n    :func:`~pydicom.encaps.encapsulate`\n    \"\"\"\n    nr_frames = len(frames)\n    frame_lengths = [len(frame) for frame in frames]\n    frame_offsets = [0]\n    for ii, length in enumerate(frame_lengths[:-1]):\n        # Extra 8 bytes for the Item tag and length\n        frame_offsets.append(frame_offsets[ii] + length + 8)\n\n    offsets = pack(f\"<{nr_frames}Q\", *frame_offsets)\n    lengths = pack(f\"<{nr_frames}Q\", *frame_lengths)\n\n    return encapsulate(frames, has_bot=False), offsets, lengths"
  },
  {
    "instruction": "Handle odd-sized dicoms with warning\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n\r\nWe have some uncompressed dicoms with an odd number of pixel bytes (saved by older versions of pydicom actually). \r\n\r\nWhen we re-open with pydicom 1.2.2, we're now unable to extract the image, due to the change made by https://github.com/pydicom/pydicom/pull/601\r\n\r\nWould it be possible to emit a warning instead of rejecting the dicom for such cases?\r\n\r\n#### Version\r\n1.2.2\n",
    "input": "     # Check that the actual length of the pixel data is as expected\n     actual_length = len(ds.PixelData)\n     # Correct for the trailing NULL byte padding for odd length data\n     padded_expected_len = expected_len + expected_len % 2\n     if actual_length < padded_expected_len:\n        raise ValueError(\n            \"The length of the pixel data in the dataset doesn't match the \"\n            \"expected amount ({0} vs. {1} bytes). The dataset may be \"\n            \"corrupted or there may be an issue with the pixel data handler.\"\n            .format(actual_length, padded_expected_len)\n        )\n     elif actual_length > padded_expected_len:\n         # PS 3.5, Section 8.1.1\n         msg = (",
    "output": "     # Check that the actual length of the pixel data is as expected\n     actual_length = len(ds.PixelData)\n     # Correct for the trailing NULL byte padding for odd length data\n\n     padded_expected_len = expected_len + expected_len % 2\n     if actual_length < padded_expected_len:\n        if actual_length == expected_len:\n            warnings.warn(\n                \"The pixel data length is odd and misses a padding byte.\")\n        else:\n            raise ValueError(\n                \"The length of the pixel data in the dataset ({} bytes) \"\n                \"doesn't match the expected length ({} bytes). \"\n                \"The dataset may be corrupted or there may be an issue \"\n                \"with the pixel data handler.\"\n                .format(actual_length, padded_expected_len)\n            )\n     elif actual_length > padded_expected_len:\n         # PS 3.5, Section 8.1.1\n         msg = ("
  },
  {
    "instruction": "LUT Descriptor tag with no value yields TypeError\n**Describe the bug**\r\nI have a DICOM image with the following tag (copied from ImageJ)\r\n\r\n```\r\n0028,1101  Red Palette Color Lookup Table Descriptor: \r\n```\r\n\r\nwhich corresponds to the raw data element, produced by [`DataElement_from_raw`](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L699):\r\n```\r\nRawDataElement(tag=(0028, 1101), VR='US', length=0, value=None, value_tell=1850, is_implicit_VR=False, is_little_endian=True)\r\n```\r\n\r\nBecause this tag is matched by the [LUT Descriptor tags](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L696) and the value is empty (`None`), the [following line](https://github.com/pydicom/pydicom/blob/v1.4.1/pydicom/dataelem.py#L761):\r\n```\r\nif raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\r\n```\r\nresults in \r\n```\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\n**Expected behavior**\r\n\r\nGiven that I discovered this by parsing what seems to be a set of faulty DICOMs (mangled pixel data), I'm not sure if an error should be raised if the colour attribute value is not provided.\r\n\r\nHowever, given that `value` can be `None` for other tags, the simple fix is\r\n\r\n```python\r\ntry:\r\n    if raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\r\n        # We only fix the first value as the third value is 8 or 16\r\n        value[0] += 65536\r\nexcept TypeError:\r\n    pass\r\n```\r\n\r\n(or test if `value` is iterable).\r\n\r\n**Your environment**\r\n```\r\nDarwin-19.3.0-x86_64-i386-64bit\r\nPython  3.7.6 | packaged by conda-forge | (default, Jan  7 2020, 22:05:27)\r\n[Clang 9.0.1 ]\r\npydicom  1.4.1\r\n```\r\n\r\nMany thanks!\n",
    "input": "     except NotImplementedError as e:\n         raise NotImplementedError(\"{0:s} in tag {1!r}\".format(str(e), raw.tag))\n \n    if raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\n         # We only fix the first value as the third value is 8 or 16\n        value[0] += 65536\n \n     return DataElement(raw.tag, VR, value, raw.value_tell,\n                        raw.length == 0xFFFFFFFF, already_converted=True)",
    "output": "     except NotImplementedError as e:\n         raise NotImplementedError(\"{0:s} in tag {1!r}\".format(str(e), raw.tag))\n \n    if raw.tag in _LUT_DESCRIPTOR_TAGS and value:\n         # We only fix the first value as the third value is 8 or 16\n        try:\n            if value[0] < 0:\n                value[0] += 65536\n        except TypeError:\n            pass\n \n     return DataElement(raw.tag, VR, value, raw.value_tell,\n                        raw.length == 0xFFFFFFFF, already_converted=True)"
  },
  {
    "instruction": "Embedded Null character\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in convert_encodings(encodings)\r\n    624         try:\r\n--> 625             py_encodings.append(python_encoding[encoding])\r\n    626         except KeyError:\r\n\r\nKeyError: 'ISO_IR 100\\x00'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-12-605c3c3edcf4> in <module>\r\n      4 print(filename)\r\n      5 dcm = pydicom.dcmread(filename,force=True)\r\n----> 6 dcm = pydicom.dcmread('/home/zhuzhemin/XrayKeyPoints/data/10-31-13_11H18M20_3674972_FACE_0_SC.dcm',force=True)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in dcmread(fp, defer_size, stop_before_pixels, force, specific_tags)\r\n    848     try:\r\n    849         dataset = read_partial(fp, stop_when, defer_size=defer_size,\r\n--> 850                                force=force, specific_tags=specific_tags)\r\n    851     finally:\r\n    852         if not caller_owns_file:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_partial(fileobj, stop_when, defer_size, force, specific_tags)\r\n    726         dataset = read_dataset(fileobj, is_implicit_VR, is_little_endian,\r\n    727                                stop_when=stop_when, defer_size=defer_size,\r\n--> 728                                specific_tags=specific_tags)\r\n    729     except EOFError:\r\n    730         pass  # error already logged in read_dataset\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_dataset(fp, is_implicit_VR, is_little_endian, bytelength, stop_when, defer_size, parent_encoding, specific_tags)\r\n    361     try:\r\n    362         while (bytelength is None) or (fp.tell() - fp_start < bytelength):\r\n--> 363             raw_data_element = next(de_gen)\r\n    364             # Read data elements. Stop on some errors, but return what was read\r\n    365             tag = raw_data_element.tag\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in data_element_generator(fp, is_implicit_VR, is_little_endian, stop_when, defer_size, encoding, specific_tags)\r\n    203                 # Store the encoding value in the generator\r\n    204                 # for use with future elements (SQs)\r\n--> 205                 encoding = convert_encodings(encoding)\r\n    206 \r\n    207             yield RawDataElement(tag, VR, length, value, value_tell,\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in convert_encodings(encodings)\r\n    626         except KeyError:\r\n    627             py_encodings.append(\r\n--> 628                 _python_encoding_for_corrected_encoding(encoding))\r\n    629 \r\n    630     if len(encodings) > 1:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/charset.py in _python_encoding_for_corrected_encoding(encoding)\r\n    664     # fallback: assume that it is already a python encoding\r\n    665     try:\r\n--> 666         codecs.lookup(encoding)\r\n    667         return encoding\r\n    668     except LookupError:\r\n\r\nValueError: embedded null character\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```py\r\nfrom io import BytesIO\r\nfrom pydicom import dcmread\r\n\r\nbytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\r\n             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\r\n             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\r\n             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\r\n             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\r\n\r\nfp = BytesIO(bytestream)\r\nds = dcmread(fp, force=True)\r\n\r\nprint(ds.PatientID)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n\r\nWhen possible use pydicom testing examples to reproduce the errors. Otherwise, provide\r\nan anonymous version of the data in order to replicate the errors.\r\n-->\r\nimport pydicom\r\ndcm = pydicom.dcmread('/home/zhuzhemin/XrayKeyPoints/data/10-31-13_11H18M20_3674972_FACE_0_SC.dcm')\r\n\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\nNo error\r\nI used dcmread function in matlab to read the same file and it was ok. So it should not be the problem of the file.\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\nError: Embedded Null character\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n1.3.0\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "input": "     # 2005.05.25: also check for trailing 0, error made\n     # in PET files we are converting\n \n    if val and (val.endswith(' ') or val.endswith('\\x00')):\n         val = val[:-1]\n     splitup = val.split(\"\\\\\")\n \ndiff --git a/pydicom/values.py b/pydicom/values.py\n     \"\"\"\n     encodings = encodings or [default_encoding]\n     value = decode_string(byte_string, encodings, TEXT_VR_DELIMS)\n    if value and value.endswith(' '):\n         value = value[:-1]\n     return value\n ",
    "output": "     # 2005.05.25: also check for trailing 0, error made\n     # in PET files we are converting\n \n    while val and (val.endswith(' ') or val.endswith('\\x00')):\n         val = val[:-1]\n     splitup = val.split(\"\\\\\")\n \ndiff --git a/pydicom/values.py b/pydicom/values.py\n     \"\"\"\n     encodings = encodings or [default_encoding]\n     value = decode_string(byte_string, encodings, TEXT_VR_DELIMS)\n    while value and (value.endswith(' ') or value.endswith('\\0')):\n         value = value[:-1]\n     return value\n "
  },
  {
    "instruction": "Mypy errors\n**Describe the bug**\r\nSeveral of the type hints are problematic and result in mypy errors.\r\n\r\nOne example:\r\n\r\n```none\r\ncat << EOF > /tmp/test.py\r\nfrom pydicom import Dataset, dcmread\r\n\r\ndataset = Dataset()\r\ndataset.Rows = 10\r\ndataset.Columns = 20\r\ndataset.NumberOfFrames = \"5\"\r\n\r\nassert int(dataset.NumberOfFrames) == 5\r\n\r\nfilename = '/tmp/test.dcm'\r\ndataset.save_as(str(filename))\r\n\r\ndataset = dcmread(filename)\r\n\r\nassert int(dataset.NumberOfFrames) == 5\r\nEOF\r\n```\r\n\r\n```none\r\nmypy /tmp/test.py\r\n/tmp/test.py:15: error: No overload variant of \"int\" matches argument type \"object\"\r\n/tmp/test.py:15: note: Possible overload variant:\r\n/tmp/test.py:15: note:     def int(self, x: Union[str, bytes, SupportsInt, _SupportsIndex] = ...) -> int\r\n/tmp/test.py:15: note:     <1 more non-matching overload not shown>\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\n**Expected behavior**\r\nMypy should not report any errors.\r\n\r\n**Steps To Reproduce**\r\nSee above\r\n\r\n**Your environment**\r\n```none\r\npython -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.6-x86_64-i386-64bit\r\nPython       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.3\r\nPIL          | 8.0.1\r\n```\nImportError: cannot import name 'NoReturn'\n**Describe the bug**\r\nthrow following excetion when import pydicom package:\r\n```\r\nxxx/python3.6/site-packages/pydicom/filebase.py in <module>\r\n5 from struct import unpack, pack\r\n      6 from types import TracebackType\r\n----> 7 from typing import (\r\n      8     Tuple, Optional, NoReturn, BinaryIO, Callable, Type, Union, cast, TextIO,\r\n      9     TYPE_CHECKING, Any\r\n\r\nImportError: cannot import name 'NoReturn'\r\n```\r\n\r\n**Expected behavior**\r\nimort pydicom sucessfully\r\n\r\n**Steps To Reproduce**\r\nHow to reproduce the issue. Please include a minimum working code sample, the\r\ntraceback (if any) and the anonymized DICOM dataset (if relevant).\r\n\r\n**Your environment**\r\npython:3.6.0\r\npydicom:2.1\r\n\n",
    "input": " from typing import Tuple\n \n \n__version__: str = '2.1.0'\n __version_info__: Tuple[str, str, str] = tuple(\n     re.match(r'(\\d+\\.\\d+\\.\\d+).*', __version__).group(1).split('.')\n )\ndiff --git a/pydicom/filebase.py b/pydicom/filebase.py\n from struct import unpack, pack\n from types import TracebackType\n from typing import (\n    Tuple, Optional, NoReturn, BinaryIO, Callable, Type, Union, cast, TextIO,\n     TYPE_CHECKING, Any\n )\n \n         self.close = file_like_obj.close\n         self.name: str = getattr(file_like_obj, 'name', '<no filename>')\n \n    def no_write(self, bytes_read: bytes) -> NoReturn:\n         \"\"\"Used for file-like objects where no write is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no write() method\")\n \n    def no_read(self, bytes_read: Optional[int] = None) -> NoReturn:\n         \"\"\"Used for file-like objects where no read is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no read() method\")\n \n    def no_seek(self, offset: int, from_what: int = 0) -> NoReturn:\n         \"\"\"Used for file-like objects where no seek is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no seek() method\")\n \ndiff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n                     value_key = unique_value_keys[0]\n                     elem = DataElement.from_json(\n                         self.dataset_class, key, vr,\n                        val[value_key], value_key\n                     )\n                 ds.add(elem)\n         return ds\ndiff --git a/setup.py b/setup.py\n     \"Programming Language :: Python :: 3.6\",\n     \"Programming Language :: Python :: 3.7\",\n     \"Programming Language :: Python :: 3.8\",\n     \"Operating System :: OS Independent\",\n     \"Topic :: Scientific/Engineering :: Medical Science Apps.\",\n     \"Topic :: Scientific/Engineering :: Physics\",\n \n \n PACKAGE_DATA = {\n    'pydicom': data_files_inventory() + ['py.typed']\n }\n \n opts = dict(",
    "output": " from typing import Tuple\n \n \n__version__: str = '2.1.1'\n __version_info__: Tuple[str, str, str] = tuple(\n     re.match(r'(\\d+\\.\\d+\\.\\d+).*', __version__).group(1).split('.')\n )\ndiff --git a/pydicom/filebase.py b/pydicom/filebase.py\n from struct import unpack, pack\n from types import TracebackType\n from typing import (\n    Tuple, Optional, BinaryIO, Callable, Type, Union, cast, TextIO,\n     TYPE_CHECKING, Any\n )\n \n         self.close = file_like_obj.close\n         self.name: str = getattr(file_like_obj, 'name', '<no filename>')\n \n    def no_write(self, bytes_read: bytes) -> None:\n         \"\"\"Used for file-like objects where no write is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no write() method\")\n \n    def no_read(self, bytes_read: Optional[int] = None) -> None:\n         \"\"\"Used for file-like objects where no read is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no read() method\")\n \n    def no_seek(self, offset: int, from_what: int = 0) -> None:\n         \"\"\"Used for file-like objects where no seek is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no seek() method\")\n \ndiff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n                     value_key = unique_value_keys[0]\n                     elem = DataElement.from_json(\n                         self.dataset_class, key, vr,\n                        val[value_key], value_key,\n                        self.bulk_data_element_handler\n                     )\n                 ds.add(elem)\n         return ds\ndiff --git a/setup.py b/setup.py\n     \"Programming Language :: Python :: 3.6\",\n     \"Programming Language :: Python :: 3.7\",\n     \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n     \"Operating System :: OS Independent\",\n     \"Topic :: Scientific/Engineering :: Medical Science Apps.\",\n     \"Topic :: Scientific/Engineering :: Physics\",\n \n \n PACKAGE_DATA = {\n    'pydicom': data_files_inventory()\n }\n \n opts = dict("
  },
  {
    "instruction": "Add support for missing VRs\nMissing: OV, SV, UV\r\n\r\n\n",
    "input": " # for write_numbers, the Writer maps to a tuple (function, struct_format)\n #   (struct_format is python's struct module format)\n writers = {\n    'UL': (write_numbers, 'L'),\n    'SL': (write_numbers, 'l'),\n    'US': (write_numbers, 'H'),\n    'SS': (write_numbers, 'h'),\n    'FL': (write_numbers, 'f'),\n     'FD': (write_numbers, 'd'),\n    'OF': (write_numbers, 'f'),\n     'OB': (write_OBvalue, None),\n     'OD': (write_OWvalue, None),\n     'OL': (write_OWvalue, None),\n    'UI': (write_UI, None),\n    'SH': (write_text, None),\n    'DA': (write_DA, None),\n    'TM': (write_TM, None),\n    'CS': (write_string, None),\n     'PN': (write_PN, None),\n    'LO': (write_text, None),\n    'IS': (write_number_string, None),\n    'DS': (write_number_string, None),\n    'AE': (write_string, None),\n    'AS': (write_string, None),\n    'LT': (write_text, None),\n     'SQ': (write_sequence, None),\n     'UC': (write_text, None),\n     'UN': (write_UN, None),\n     'UR': (write_string, None),\n    'AT': (write_ATvalue, None),\n    'ST': (write_text, None),\n    'OW': (write_OWvalue, None),\n     'US or SS': (write_OWvalue, None),\n     'US or OW': (write_OWvalue, None),\n     'US or SS or OW': (write_OWvalue, None),\n     'OB/OW': (write_OBvalue, None),\n     'OB or OW': (write_OBvalue, None),\n     'OW or OB': (write_OBvalue, None),\n    'DT': (write_DT, None),\n    'UT': (write_text, None),\n }  # note OW/OB depends on other items, which we don't know at write time\ndiff --git a/pydicom/values.py b/pydicom/values.py\n     return convert_OBvalue(byte_string, is_little_endian)\n \n \n def convert_PN(byte_string, encodings=None):\n     \"\"\"Return a decoded 'PN' value.\n \n # (function, struct_format)\n # (struct_format in python struct module style)\n converters = {\n    'UL': (convert_numbers, 'L'),\n    'SL': (convert_numbers, 'l'),\n    'US': (convert_numbers, 'H'),\n    'SS': (convert_numbers, 'h'),\n    'FL': (convert_numbers, 'f'),\n     'FD': (convert_numbers, 'd'),\n    'OF': (convert_numbers, 'f'),\n     'OB': convert_OBvalue,\n     'OD': convert_OBvalue,\n     'OL': convert_OBvalue,\n    'UI': convert_UI,\n    'SH': convert_text,\n    'DA': convert_DA_string,\n    'TM': convert_TM_string,\n    'CS': convert_string,\n     'PN': convert_PN,\n    'LO': convert_text,\n    'IS': convert_IS_string,\n    'DS': convert_DS_string,\n    'AE': convert_AE_string,\n    'AS': convert_string,\n    'LT': convert_single_string,\n     'SQ': convert_SQ,\n     'UC': convert_text,\n     'UN': convert_UN,\n     'UR': convert_UR_string,\n    'AT': convert_ATvalue,\n    'ST': convert_single_string,\n    'OW': convert_OWvalue,\n     'OW/OB': convert_OBvalue,  # note OW/OB depends on other items,\n     'OB/OW': convert_OBvalue,  # which we don't know at read time\n     'OW or OB': convert_OBvalue,\n     'US or OW': convert_OWvalue,\n     'US or SS or OW': convert_OWvalue,\n     'US\\\\US or SS\\\\US': convert_OWvalue,\n    'DT': convert_DT_string,\n    'UT': convert_single_string,\n }",
    "output": " # for write_numbers, the Writer maps to a tuple (function, struct_format)\n #   (struct_format is python's struct module format)\n writers = {\n    'AE': (write_string, None),\n    'AS': (write_string, None),\n    'AT': (write_ATvalue, None),\n    'CS': (write_string, None),\n    'DA': (write_DA, None),\n    'DS': (write_number_string, None),\n    'DT': (write_DT, None),\n     'FD': (write_numbers, 'd'),\n    'FL': (write_numbers, 'f'),\n    'IS': (write_number_string, None),\n    'LO': (write_text, None),\n    'LT': (write_text, None),\n     'OB': (write_OBvalue, None),\n     'OD': (write_OWvalue, None),\n    'OF': (write_numbers, 'f'),\n     'OL': (write_OWvalue, None),\n    'OW': (write_OWvalue, None),\n    'OV': (write_OWvalue, None),\n     'PN': (write_PN, None),\n    'SH': (write_text, None),\n    'SL': (write_numbers, 'l'),\n     'SQ': (write_sequence, None),\n    'SS': (write_numbers, 'h'),\n    'ST': (write_text, None),\n    'SV': (write_numbers, 'q'),\n    'TM': (write_TM, None),\n     'UC': (write_text, None),\n    'UI': (write_UI, None),\n    'UL': (write_numbers, 'L'),\n     'UN': (write_UN, None),\n     'UR': (write_string, None),\n    'US': (write_numbers, 'H'),\n    'UT': (write_text, None),\n    'UV': (write_numbers, 'Q'),\n     'US or SS': (write_OWvalue, None),\n     'US or OW': (write_OWvalue, None),\n     'US or SS or OW': (write_OWvalue, None),\n     'OB/OW': (write_OBvalue, None),\n     'OB or OW': (write_OBvalue, None),\n     'OW or OB': (write_OBvalue, None),\n }  # note OW/OB depends on other items, which we don't know at write time\ndiff --git a/pydicom/values.py b/pydicom/values.py\n     return convert_OBvalue(byte_string, is_little_endian)\n \n \ndef convert_OVvalue(byte_string, is_little_endian, struct_format=None):\n    \"\"\"Return the encoded 'OV' value as :class:`bytes` or :class:`str`.\n\n    No byte swapping will be performed.\n    \"\"\"\n    # for now, Maybe later will have own routine\n    return convert_OBvalue(byte_string, is_little_endian)\n\n\n def convert_PN(byte_string, encodings=None):\n     \"\"\"Return a decoded 'PN' value.\n \n # (function, struct_format)\n # (struct_format in python struct module style)\n converters = {\n    'AE': convert_AE_string,\n    'AS': convert_string,\n    'AT': convert_ATvalue,\n    'CS': convert_string,\n    'DA': convert_DA_string,\n    'DS': convert_DS_string,\n    'DT': convert_DT_string,\n     'FD': (convert_numbers, 'd'),\n    'FL': (convert_numbers, 'f'),\n    'IS': convert_IS_string,\n    'LO': convert_text,\n    'LT': convert_single_string,\n     'OB': convert_OBvalue,\n     'OD': convert_OBvalue,\n    'OF': (convert_numbers, 'f'),\n     'OL': convert_OBvalue,\n    'OW': convert_OWvalue,\n    'OV': convert_OVvalue,\n     'PN': convert_PN,\n    'SH': convert_text,\n    'SL': (convert_numbers, 'l'),\n     'SQ': convert_SQ,\n    'SS': (convert_numbers, 'h'),\n    'ST': convert_single_string,\n    'SV': (convert_numbers, 'q'),\n    'TM': convert_TM_string,\n     'UC': convert_text,\n    'UI': convert_UI,\n    'UL': (convert_numbers, 'L'),\n     'UN': convert_UN,\n     'UR': convert_UR_string,\n    'US': (convert_numbers, 'H'),\n    'UT': convert_single_string,\n    'UV': (convert_numbers, 'Q'),\n     'OW/OB': convert_OBvalue,  # note OW/OB depends on other items,\n     'OB/OW': convert_OBvalue,  # which we don't know at read time\n     'OW or OB': convert_OBvalue,\n     'US or OW': convert_OWvalue,\n     'US or SS or OW': convert_OWvalue,\n     'US\\\\US or SS\\\\US': convert_OWvalue,\n }"
  },
  {
    "instruction": "dcmread cannot handle pathlib.Path objects\n**Describe the bug**\r\nThe `dcmread()` currently fails when passed an instance of `pathlib.Path`. The problem is the following line:\r\nhttps://github.com/pydicom/pydicom/blob/8b0bbaf92d7a8218ceb94dedbee3a0463c5123e3/pydicom/filereader.py#L832\r\n\r\n**Expected behavior**\r\n`dcmread()` should open and read the file to which the `pathlib.Path` object points.\r\n\r\nThe line above should probably be:\r\n```python\r\nif isinstance(fp, (str, Path)):\r\n````\r\n\r\n**Steps To Reproduce**\r\n```python\r\nfrom pathlib import Path\r\nfrom pydicom.filereader import dcmread\r\n\r\ndcm_filepath = Path('path/to/file')\r\ndcmread(dcm_filepath)\r\n```\n",
    "input": " import os\n from os.path import abspath, dirname, join\n \n DATA_ROOT = abspath(dirname(__file__))\n \n \n \n     Parameters\n     ----------\n    base : str\n         Base directory to recursively search.\n \n     pattern : str\n         The list of filenames matched.\n     \"\"\"\n \n     # if the user forgot to add them\n     pattern = \"*\" + pattern + \"*\"\n \ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n from pydicom.datadict import (tag_for_keyword, keyword_for_tag,\n                               repeater_has_keyword)\n from pydicom.dataelem import DataElement, DataElement_from_raw, RawDataElement\n from pydicom.pixel_data_handlers.util import (\n     convert_color_space, reshape_pixel_array, get_image_pixel_ids\n )\n \n         Parameters\n         ----------\n        filename : str or file-like\n             Name of file or the file-like to write the new DICOM file to.\n         write_like_original : bool, optional\n             If ``True`` (default), preserves the following information from\n \n         Parameters\n         ----------\n        filename_or_obj : str or BytesIO or None\n             Full path and filename to the file, memory buffer object, or\n             ``None`` if is a :class:`io.BytesIO`.\n         dataset : Dataset or dict\n         self.is_implicit_VR = is_implicit_VR\n         self.is_little_endian = is_little_endian\n         filename = None\n         if isinstance(filename_or_obj, str):\n             filename = filename_or_obj\n             self.fileobj_type = open\ndiff --git a/pydicom/dicomdir.py b/pydicom/dicomdir.py\n \n         Parameters\n         ----------\n        filename_or_obj : str or None\n             Full path and filename to the file of ``None`` if\n             :class:`io.BytesIO`.\n         dataset : dataset.Dataset\ndiff --git a/pydicom/filereader.py b/pydicom/filereader.py\n from pydicom.dicomdir import DicomDir\n from pydicom.errors import InvalidDicomError\n from pydicom.filebase import DicomFile\nfrom pydicom.fileutil import read_undefined_length_value\n from pydicom.misc import size_in_bytes\n from pydicom.sequence import Sequence\n from pydicom.tag import (ItemTag, SequenceDelimiterTag, TupleTag, Tag, BaseTag)\n \n     Parameters\n     ----------\n    fp : str or file-like\n         Either a file-like object, or a string containing the file name. If a\n         file-like object, the caller is responsible for closing it.\n     defer_size : int or str or None, optional\n     \"\"\"\n     # Open file if not already a file object\n     caller_owns_file = True\n     if isinstance(fp, str):\n         # caller provided a file name; we own the file handle\n         caller_owns_file = False\ndiff --git a/pydicom/fileutil.py b/pydicom/fileutil.py\n # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n \"\"\"Functions for reading to certain bytes, e.g. delimiters.\"\"\"\n\n from struct import pack, unpack\n \n from pydicom.misc import size_in_bytes\n                                 is_little_endian,\n                                 delimiter_tag,\n                                 defer_size=None,\n                                read_size=1024*8):\n     \"\"\"Read until `delimiter_tag` and return the value up to that point.\n \n     On completion, the file will be set to the first byte after the delimiter\n     if length != 0:\n         logger.warn(\"Expected delimiter item to have length 0, \"\n                     \"got %d at file position 0x%x\", length, fp.tell() - 4)\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n from pydicom.dataelem import DataElement_from_raw\n from pydicom.dataset import Dataset, validate_file_meta\n from pydicom.filebase import DicomFile, DicomFileLike, DicomBytesIO\n from pydicom.multival import MultiValue\n from pydicom.tag import (Tag, ItemTag, ItemDelimiterTag, SequenceDelimiterTag,\n                          tag_in_exception)\n \n     Parameters\n     ----------\n    filename : str or file-like\n         Name of file or the file-like to write the new DICOM file to.\n     dataset : pydicom.dataset.FileDataset\n         Dataset holding the DICOM information; e.g. an object read with\n \n     caller_owns_file = True\n     # Open file if not already a file object\n     if isinstance(filename, str):\n         fp = DicomFile(filename, 'wb')\n         # caller provided a file name; we own the file handle",
    "output": " import os\n from os.path import abspath, dirname, join\n \nfrom pydicom.fileutil import path_from_pathlike\n\n DATA_ROOT = abspath(dirname(__file__))\n \n \n \n     Parameters\n     ----------\n    base : str or PathLike\n         Base directory to recursively search.\n \n     pattern : str\n         The list of filenames matched.\n     \"\"\"\n \n    base = path_from_pathlike(base)\n     # if the user forgot to add them\n     pattern = \"*\" + pattern + \"*\"\n \ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n from pydicom.datadict import (tag_for_keyword, keyword_for_tag,\n                               repeater_has_keyword)\n from pydicom.dataelem import DataElement, DataElement_from_raw, RawDataElement\nfrom pydicom.fileutil import path_from_pathlike\n from pydicom.pixel_data_handlers.util import (\n     convert_color_space, reshape_pixel_array, get_image_pixel_ids\n )\n \n         Parameters\n         ----------\n        filename : str or PathLike or file-like\n             Name of file or the file-like to write the new DICOM file to.\n         write_like_original : bool, optional\n             If ``True`` (default), preserves the following information from\n \n         Parameters\n         ----------\n        filename_or_obj : str or PathLike or BytesIO or None\n             Full path and filename to the file, memory buffer object, or\n             ``None`` if is a :class:`io.BytesIO`.\n         dataset : Dataset or dict\n         self.is_implicit_VR = is_implicit_VR\n         self.is_little_endian = is_little_endian\n         filename = None\n        filename_or_obj = path_from_pathlike(filename_or_obj)\n         if isinstance(filename_or_obj, str):\n             filename = filename_or_obj\n             self.fileobj_type = open\ndiff --git a/pydicom/dicomdir.py b/pydicom/dicomdir.py\n \n         Parameters\n         ----------\n        filename_or_obj : str or PathLike or file-like or None\n             Full path and filename to the file of ``None`` if\n             :class:`io.BytesIO`.\n         dataset : dataset.Dataset\ndiff --git a/pydicom/filereader.py b/pydicom/filereader.py\n from pydicom.dicomdir import DicomDir\n from pydicom.errors import InvalidDicomError\n from pydicom.filebase import DicomFile\nfrom pydicom.fileutil import read_undefined_length_value, path_from_pathlike\n from pydicom.misc import size_in_bytes\n from pydicom.sequence import Sequence\n from pydicom.tag import (ItemTag, SequenceDelimiterTag, TupleTag, Tag, BaseTag)\n \n     Parameters\n     ----------\n    fp : str or PathLike or file-like\n         Either a file-like object, or a string containing the file name. If a\n         file-like object, the caller is responsible for closing it.\n     defer_size : int or str or None, optional\n     \"\"\"\n     # Open file if not already a file object\n     caller_owns_file = True\n    fp = path_from_pathlike(fp)\n     if isinstance(fp, str):\n         # caller provided a file name; we own the file handle\n         caller_owns_file = False\ndiff --git a/pydicom/fileutil.py b/pydicom/fileutil.py\n # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n \"\"\"Functions for reading to certain bytes, e.g. delimiters.\"\"\"\nimport os\nimport pathlib\nimport sys\n from struct import pack, unpack\n \n from pydicom.misc import size_in_bytes\n                                 is_little_endian,\n                                 delimiter_tag,\n                                 defer_size=None,\n                                read_size=1024 * 8):\n     \"\"\"Read until `delimiter_tag` and return the value up to that point.\n \n     On completion, the file will be set to the first byte after the delimiter\n     if length != 0:\n         logger.warn(\"Expected delimiter item to have length 0, \"\n                     \"got %d at file position 0x%x\", length, fp.tell() - 4)\n\n\ndef path_from_pathlike(file_object):\n    \"\"\"Returns the path if `file_object` is a path-like object, otherwise the\n    original `file_object`.\n\n    Parameters\n    ----------\n    file_object: str or PathLike or file-like\n\n    Returns\n    -------\n    str or file-like\n        the string representation of the given path object, or the object\n        itself in case of an object not representing a path.\n\n    ..note:\n\n        ``PathLike`` objects have been introduced in Python 3.6. In Python 3.5,\n        only objects of type :class:`pathlib.Path` are considered.\n    \"\"\"\n    if sys.version_info < (3, 6):\n        if isinstance(file_object, pathlib.Path):\n            return str(file_object)\n        return file_object\n    try:\n        return os.fspath(file_object)\n    except TypeError:\n        return file_object\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n from pydicom.dataelem import DataElement_from_raw\n from pydicom.dataset import Dataset, validate_file_meta\n from pydicom.filebase import DicomFile, DicomFileLike, DicomBytesIO\nfrom pydicom.fileutil import path_from_pathlike\n from pydicom.multival import MultiValue\n from pydicom.tag import (Tag, ItemTag, ItemDelimiterTag, SequenceDelimiterTag,\n                          tag_in_exception)\n \n     Parameters\n     ----------\n    filename : str or PathLike or file-like\n         Name of file or the file-like to write the new DICOM file to.\n     dataset : pydicom.dataset.FileDataset\n         Dataset holding the DICOM information; e.g. an object read with\n \n     caller_owns_file = True\n     # Open file if not already a file object\n    filename = path_from_pathlike(filename)\n     if isinstance(filename, str):\n         fp = DicomFile(filename, 'wb')\n         # caller provided a file name; we own the file handle"
  },
  {
    "instruction": "Unable to assign single element list to PN field\nI am getting `AttributeError` while trying to assign a list of single element to a `PN` field.\r\nIt's converting `val` to a 2D array [here](https://github.com/pydicom/pydicom/blob/master/pydicom/filewriter.py#L328) when `VM` is 1. \r\n\r\n**Code**\r\n```\r\n>>> from pydicom import dcmread, dcmwrite\r\n>>> ds = dcmread(\"SOP1.dcm\")\r\n>>> a = [\"name1\"]\r\n>>> b = [\"name1\", \"name2\"]\r\n>>> ds.PatientName = a\r\n>>> dcmwrite(\"out.dcm\", ds)     # throws the error below\r\n>>> ds.PatientName = b\r\n>>> dcmwrite(\"out.dcm\", ds)     # works fine\r\n```\r\n\r\n**Error**\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 28, in tag_in_exception\r\n    yield\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 562, in write_data_element\r\n    fn(buffer, elem, encodings=encodings)  # type: ignore[operator]\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in write_PN\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in <listcomp>\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\nAttributeError: 'MultiValue' object has no attribute 'encode'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 1153, in dcmwrite\r\n    _write_dataset(fp, dataset, write_like_original)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 889, in _write_dataset\r\n    write_dataset(fp, get_item(dataset, slice(0x00010000, None)))\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/contextlib.py\", line 131, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 32, in tag_in_exception\r\n    raise type(exc)(msg) from exc\r\nAttributeError: With tag (0010, 0010) got exception: 'MultiValue' object has no attribute 'encode'\r\nTraceback (most recent call last):\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/tag.py\", line 28, in tag_in_exception\r\n    yield\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 662, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 562, in write_data_element\r\n    fn(buffer, elem, encodings=encodings)  # type: ignore[operator]\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in write_PN\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\n  File \"/Users/neonbulb/virtualenv/deid/lib/python3.8/site-packages/pydicom/filewriter.py\", line 333, in <listcomp>\r\n    enc = b'\\\\'.join([elem.encode(encodings) for elem in val])\r\nAttributeError: 'MultiValue' object has no attribute 'encode'\r\n```\n",
    "input": "             val.append\n         except AttributeError:  # not a list\n             return self._convert(val)\n        else:\n            return MultiValue(self._convert, val,\n                              validation_mode=self.validation_mode)\n \n     def _convert(self, val: Any) -> Any:\n         \"\"\"Convert `val` to an appropriate type for the element's VR.\"\"\"",
    "output": "             val.append\n         except AttributeError:  # not a list\n             return self._convert(val)\n        if len(val) == 1:\n            return self._convert(val[0])\n        return MultiValue(self._convert, val,\n                          validation_mode=self.validation_mode)\n \n     def _convert(self, val: Any) -> Any:\n         \"\"\"Convert `val` to an appropriate type for the element's VR.\"\"\""
  },
  {
    "instruction": "Ambiguous VR element could be read in <=1.1.0 but is broken in >=1.2.0\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nAttribute Error thrown when printing (0x0028, 0x0120) PixelPaddingValue\r\n\r\n#### Steps/Code to Reproduce\r\nUsing pydicom 1.2.2 and above (including master branch as of issue creation date):\r\n```\r\nfrom pydicom import dcmread\r\n\r\nds = dcmread('rtss.dcm')\r\nds\r\n\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 157, in correct_ambiguous_vr_element\r\n    _correct_ambiguous_vr_element(elem, ds, is_little_endian)\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 75, in _correct_ambiguous_vr_element\r\n    if ds.PixelRepresentation == 0:\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 711, in __getattr__\r\n    return super(Dataset, self).__getattribute__(name)\r\nAttributeError: 'FileDataset' object has no attribute 'PixelRepresentation'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py\", line 865, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/Users/apanchal/Projects/test.py\", line 107, in processing_thread\r\n    dp.ds, name, patientid, patientdob)\r\n  File \"/Users/apanchal/Projects/test.py\", line 144, in UpdateElements\r\n    for item in data:\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 1045, in __iter__\r\n    yield self[tag]\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/dataset.py\", line 805, in __getitem__\r\n    self[tag], self, data_elem[6])\r\n  File \"/Users/apanchal/Projects/venvs/dicom/lib/python3.7/site-packages/pydicom/filewriter.py\", line 161, in correct_ambiguous_vr_element\r\n    raise AttributeError(reason)\r\nAttributeError: Failed to resolve ambiguous VR for tag (0028, 0120): 'FileDataset' object has no attribute 'PixelRepresentation'\r\n```\r\n\r\nAnonymized RTSTRUCT file is attached: [RTSTRUCT.zip](https://github.com/pydicom/pydicom/files/3124625/RTSTRUCT.zip)\r\n\r\n#### Expected Results\r\nThe dataset is printed. This worked in pydicom 1.1.0 and below.\r\n\r\nSince `PixelRepresentation` is not defined in the dataset, this attribute cannot be printed anymore.\r\n\r\nWhat's strange is that according to the standard PixelPaddingValue (0028, 0120) is 1C for RTSTRUCT, but in this file it has no other tags referencing PixelData. So it probably should not have been included by the vendor.\r\n\r\nI am wondering if there should be another path like in #809 that can handle the missing PixelRepresentation attribute.\r\n\r\n#### Actual Results\r\n```AttributeError: Failed to resolve ambiguous VR for tag (0028, 0120): 'FileDataset' object has no attribute 'PixelRepresentation'```\r\n\r\n#### Versions\r\n```\r\nDarwin-17.7.0-x86_64-i386-64bit\r\nPython 3.7.0 (default, Jul 23 2018, 20:22:55) \r\n[Clang 9.1.0 (clang-902.0.39.2)]\r\npydicom 1.2.2\r\n```\n",
    "input": "         # PixelRepresentation is usually set in the root dataset\n         while 'PixelRepresentation' not in ds and ds.parent:\n             ds = ds.parent\n        if ds.PixelRepresentation == 0:\n             elem.VR = 'US'\n             byte_type = 'H'\n         else:",
    "output": "         # PixelRepresentation is usually set in the root dataset\n         while 'PixelRepresentation' not in ds and ds.parent:\n             ds = ds.parent\n        # if no pixel data is present, none if these tags is used,\n        # so we can just ignore a missing PixelRepresentation in this case\n        if ('PixelRepresentation' not in ds and 'PixelData' not in ds or\n                ds.PixelRepresentation == 0):\n             elem.VR = 'US'\n             byte_type = 'H'\n         else:"
  },
  {
    "instruction": "Dataset.pixel_array doesn't change unless PixelData does\n#### Description\r\nCurrently `ds.pixel_array` produces a numpy array that depends on element values for Rows, Columns, Samples Per Pixel, etc, however the code for `ds.pixel_array` only changes the returned array if the value for `ds.PixelData` changes. This may lead to confusion/undesirable behaviour if the values for related elements are changed after `ds.pixel_array` is called but not the underlying pixel data.\r\n\r\nI can't think of any real use cases except maybe in an interactive session when debugging a non-conformant dataset, but I suggest we change the way `Dataset._pixel_id` is calculated so that it takes into account changes in related elements as well.\r\n\n",
    "input": " from pydicom.datadict import (tag_for_keyword, keyword_for_tag,\n                               repeater_has_keyword)\n from pydicom.dataelem import DataElement, DataElement_from_raw, RawDataElement\nfrom pydicom.pixel_data_handlers.util import (convert_color_space,\n                                              reshape_pixel_array)\n from pydicom.tag import Tag, BaseTag, tag_in_exception\n from pydicom.uid import (ExplicitVRLittleEndian, ImplicitVRLittleEndian,\n                          ExplicitVRBigEndian, PYDICOM_IMPLEMENTATION_UID)\n         decompressed and any related data elements are changed accordingly.\n         \"\"\"\n         # Check if already have converted to a NumPy array\n        # Also check if self.PixelData has changed. If so, get new NumPy array\n        keywords = ['PixelData', 'FloatPixelData', 'DoubleFloatPixelData']\n        px_keyword = [kw for kw in keywords if kw in self]\n        if len(px_keyword) != 1:\n            raise AttributeError(\n                \"Unable to convert the pixel data: one of Pixel Data, Float \"\n                \"Pixel Data or Double Float Pixel Data must be present in \"\n                \"the dataset\"\n            )\n\n         already_have = True\n         if not hasattr(self, \"_pixel_array\"):\n             already_have = False\n        elif self._pixel_id != id(getattr(self, px_keyword[0])):\n             already_have = False\n \n         if already_have:\n                 self._pixel_array, 'YBR_FULL', 'RGB'\n             )\n \n        keywords = ['PixelData', 'FloatPixelData', 'DoubleFloatPixelData']\n        px_keyword = [kw for kw in keywords if kw in self]\n        self._pixel_id = id(getattr(self, px_keyword[0]))\n \n     def decompress(self, handler_name=''):\n         \"\"\"Decompresses *Pixel Data* and modifies the :class:`Dataset`\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n     return length\n \n \n def pixel_dtype(ds, as_float=False):\n     \"\"\"Return a :class:`numpy.dtype` for the pixel data in `ds`.\n ",
    "output": " from pydicom.datadict import (tag_for_keyword, keyword_for_tag,\n                               repeater_has_keyword)\n from pydicom.dataelem import DataElement, DataElement_from_raw, RawDataElement\nfrom pydicom.pixel_data_handlers.util import (\n    convert_color_space, reshape_pixel_array, get_image_pixel_ids\n)\n from pydicom.tag import Tag, BaseTag, tag_in_exception\n from pydicom.uid import (ExplicitVRLittleEndian, ImplicitVRLittleEndian,\n                          ExplicitVRBigEndian, PYDICOM_IMPLEMENTATION_UID)\n         decompressed and any related data elements are changed accordingly.\n         \"\"\"\n         # Check if already have converted to a NumPy array\n        # Also check if pixel data has changed. If so, get new NumPy array\n         already_have = True\n         if not hasattr(self, \"_pixel_array\"):\n             already_have = False\n        elif self._pixel_id != get_image_pixel_ids(self):\n             already_have = False\n \n         if already_have:\n                 self._pixel_array, 'YBR_FULL', 'RGB'\n             )\n \n        self._pixel_id = get_image_pixel_ids(self)\n \n     def decompress(self, handler_name=''):\n         \"\"\"Decompresses *Pixel Data* and modifies the :class:`Dataset`\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n     return length\n \n \ndef get_image_pixel_ids(ds):\n    \"\"\"Return a dict of the pixel data affecting element's :func:`id` values.\n\n    +------------------------------------------------+\n    | Element                                        |\n    +-------------+---------------------------+------+\n    | Tag         | Keyword                   | Type |\n    +=============+===========================+======+\n    | (0028,0002) | SamplesPerPixel           | 1    |\n    +-------------+---------------------------+------+\n    | (0028,0004) | PhotometricInterpretation | 1    |\n    +-------------+---------------------------+------+\n    | (0028,0006) | PlanarConfiguration       | 1C   |\n    +-------------+---------------------------+------+\n    | (0028,0008) | NumberOfFrames            | 1C   |\n    +-------------+---------------------------+------+\n    | (0028,0010) | Rows                      | 1    |\n    +-------------+---------------------------+------+\n    | (0028,0011) | Columns                   | 1    |\n    +-------------+---------------------------+------+\n    | (0028,0100) | BitsAllocated             | 1    |\n    +-------------+---------------------------+------+\n    | (0028,0103) | PixelRepresentation       | 1    |\n    +-------------+---------------------------+------+\n    | (7FE0,0008) | FloatPixelData            | 1C   |\n    +-------------+---------------------------+------+\n    | (7FE0,0009) | DoubleFloatPixelData      | 1C   |\n    +-------------+---------------------------+------+\n    | (7FE0,0010) | PixelData                 | 1C   |\n    +-------------+---------------------------+------+\n\n    Parameters\n    ----------\n    ds : Dataset\n        The :class:`~pydicom.dataset.Dataset` containing the pixel data.\n\n    Returns\n    -------\n    dict\n        A dict containing the :func:`id` values for the elements that affect\n        the pixel data.\n\n    \"\"\"\n    keywords = [\n        'SamplesPerPixel', 'PhotometricInterpretation', 'PlanarConfiguration',\n        'NumberOfFrames', 'Rows', 'Columns', 'BitsAllocated',\n        'PixelRepresentation', 'FloatPixelData', 'DoubleFloatPixelData',\n        'PixelData'\n    ]\n\n    return {kw: id(getattr(ds, kw, None)) for kw in keywords}\n\n\n def pixel_dtype(ds, as_float=False):\n     \"\"\"Return a :class:`numpy.dtype` for the pixel data in `ds`.\n "
  },
  {
    "instruction": "Segmented LUTs are incorrectly expanded\n**Describe the bug**\r\n`pydicom.pixel_data_handlers.util._expand_segmented_lut()` expands segmented LUTs to an incorrect length.\r\n\r\n**Expected behavior**\r\nA correct length LUT to be produced.\r\n\r\n**Steps To Reproduce**\r\nInitialize the following variables.\r\n```\r\nimport numpy as np\r\nlength = 48\r\ny0 = 163\r\ny1 = 255\r\n```\r\n\r\nRun the following two lines from [`pydicom.pixel_data_handlers.util._expand_segmented_lut()`](https://github.com/pydicom/pydicom/blob/699c9f0a8e190d463dd828822106250523d38154/pydicom/pixel_data_handlers/util.py#L875\r\n)\r\n```\r\nstep = (y1 - y0) / length\r\nvals = np.around(np.arange(y0 + step, y1 + step, step))\r\n```\r\n\r\nConfirm that variable `vals` if of incorrect length\r\n```\r\nprint(len(vals) == length)\r\n> False\r\n```\r\n\r\nAlternatively, the code below produces similarly false results\r\n\r\n```\r\nfrom pydicom.pixel_data_handlers.util import _expand_segmented_lut \r\nlut = _expand_segmented_lut(([0, 1, 163, 1, 48, 255]), \"B\")\r\nprint(len(lut) == (1+48))\r\n> False\r\n```\r\n\r\n`np.arange` [explicitly states](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) that it's \"results will often not be consistent\" when using \"non-integer step\", which is a very possible scenario in this function. The following alternative code does function correctly:\r\n\r\n```\r\nvals = np.around(np.linspace(y0 + step, y1, length))\r\n```\r\n\r\n**Your environment**\r\n```bash\r\n$ python -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | Darwin-20.5.0-x86_64-i386-64bit\r\nPython       | 3.7.10 (default, Feb 26 2021, 10:16:00)  [Clang 10.0.0 ]\r\npydicom      | 2.1.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.20.3\r\nPIL          | 8.2.0\r\n```\n",
    "input": "                 lut.extend([y1] * length)\n             else:\n                 step = (y1 - y0) / length\n                vals = np.around(np.arange(y0 + step, y1 + step, step))\n                 lut.extend([int(vv) for vv in vals])\n         elif opcode == 2:\n             # C.7.9.2.3: Indirect segment",
    "output": "                 lut.extend([y1] * length)\n             else:\n                 step = (y1 - y0) / length\n                vals = np.around(np.linspace(y0 + step, y1, length))\n                 lut.extend([int(vv) for vv in vals])\n         elif opcode == 2:\n             # C.7.9.2.3: Indirect segment"
  },
  {
    "instruction": "Heuristic for Explicit VR acting in sequence datasets\n**Describe the bug**\r\nThere is a check to confirm implicit VR by looking for two ascii characters and switching to explicit with a warning (#823).  It was thought this was safe because length in first data elements would not be that large.  However, in sequence item datasets this may not be true.\r\n\r\nNoted in google group conversation at https://groups.google.com/forum/#!topic/pydicom/VUmvUYmQxc0 (note that the title of that thread is not correct. that was not the problem).\r\n\r\nTest demonstrating it and fix already done - PR to follow shortly.\r\n\n",
    "input": " \n def read_dataset(fp, is_implicit_VR, is_little_endian, bytelength=None,\n                  stop_when=None, defer_size=None,\n                 parent_encoding=default_encoding, specific_tags=None):\n     \"\"\"Return a :class:`~pydicom.dataset.Dataset` instance containing the next\n     dataset in the file.\n \n         Character Set* isn't specified.\n     specific_tags : list or None\n         See :func:`dcmread` for parameter info.\n \n     Returns\n     -------\n     \"\"\"\n     raw_data_elements = dict()\n     fp_start = fp.tell()\n    is_implicit_VR = _is_implicit_vr(\n        fp, is_implicit_VR, is_little_endian, stop_when)\n     fp.seek(fp_start)\n     de_gen = data_element_generator(fp, is_implicit_VR, is_little_endian,\n                                     stop_when, defer_size, parent_encoding,\n             fp.tell() - 4 + offset, bytes2hex(bytes_read)))\n     if length == 0xFFFFFFFF:\n         ds = read_dataset(fp, is_implicit_VR, is_little_endian,\n                          bytelength=None, parent_encoding=encoding)\n         ds.is_undefined_length_sequence_item = True\n     else:\n         ds = read_dataset(fp, is_implicit_VR, is_little_endian, length,\n                          parent_encoding=encoding)\n         ds.is_undefined_length_sequence_item = False\n         logger.debug(\"%08x: Finished sequence item\" % (fp.tell() + offset,))\n     ds.seq_item_tell = seq_item_tell",
    "output": " \n def read_dataset(fp, is_implicit_VR, is_little_endian, bytelength=None,\n                  stop_when=None, defer_size=None,\n                 parent_encoding=default_encoding, specific_tags=None,\n                 at_top_level=True):\n     \"\"\"Return a :class:`~pydicom.dataset.Dataset` instance containing the next\n     dataset in the file.\n \n         Character Set* isn't specified.\n     specific_tags : list or None\n         See :func:`dcmread` for parameter info.\n    at_top_level: bool\n        If dataset is top level (not within a sequence).\n        Used to turn off explicit VR heuristic within sequences\n \n     Returns\n     -------\n     \"\"\"\n     raw_data_elements = dict()\n     fp_start = fp.tell()\n    if at_top_level:\n        is_implicit_VR = _is_implicit_vr(\n            fp, is_implicit_VR, is_little_endian, stop_when)\n     fp.seek(fp_start)\n     de_gen = data_element_generator(fp, is_implicit_VR, is_little_endian,\n                                     stop_when, defer_size, parent_encoding,\n             fp.tell() - 4 + offset, bytes2hex(bytes_read)))\n     if length == 0xFFFFFFFF:\n         ds = read_dataset(fp, is_implicit_VR, is_little_endian,\n                          bytelength=None, parent_encoding=encoding,\n                          at_top_level=False)\n         ds.is_undefined_length_sequence_item = True\n     else:\n         ds = read_dataset(fp, is_implicit_VR, is_little_endian, length,\n                          parent_encoding=encoding,\n                          at_top_level=False)\n         ds.is_undefined_length_sequence_item = False\n         logger.debug(\"%08x: Finished sequence item\" % (fp.tell() + offset,))\n     ds.seq_item_tell = seq_item_tell"
  },
  {
    "instruction": "apply_color_lut() incorrect exception when missing RedPaletteColorLUTDescriptor\n**Describe the bug**\r\n`AttributeError` when used on a dataset without `RedPaletteColorLookupTableDescriptor `\r\n\r\n**Expected behavior**\r\nShould raise `ValueError` for consistency with later exceptions\r\n\r\n**Steps To Reproduce**\r\n```python\r\nfrom pydicom.pixel_data_handlers.util import apply_color_lut\r\nds = dcmread(\"CT_small.dcm\")\r\narr = ds.apply_color_lut(arr, ds)\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".../pydicom/pixel_data_handlers/util.py\", line 116, in apply_color_lut\r\n    lut_desc = ds.RedPaletteColorLookupTableDescriptor\r\n  File \".../pydicom/dataset.py\", line 768, in __getattr__\r\n    return object.__getattribute__(self, name)\r\nAttributeError: 'FileDataset' object has no attribute 'RedPaletteColorLookupTableDescriptor'\r\n```\n",
    "input": "             \"Table Module is not currently supported\"\n         )\n \n     # All channels are supposed to be identical\n     lut_desc = ds.RedPaletteColorLookupTableDescriptor\n     # A value of 0 = 2^16 entries",
    "output": "             \"Table Module is not currently supported\"\n         )\n \n    if 'RedPaletteColorLookupTableDescriptor' not in ds:\n        raise ValueError(\"No suitable Palette Color Lookup Table Module found\")\n\n     # All channels are supposed to be identical\n     lut_desc = ds.RedPaletteColorLookupTableDescriptor\n     # A value of 0 = 2^16 entries"
  },
  {
    "instruction": "Exception decompressing RLE encoded data with non-conformant padding\nGetting Following error \r\n\"Could not convert:  The amount of decoded RLE segment data doesn't match the expected amount (786433 vs. 786432 bytes)\"\r\nFor following code\r\nplt.imsave(os.path.join(output_folder,file)+'.png', convert_color_space(ds.pixel_array, ds[0x28,0x04].value, 'RGB'))\r\nAlso attaching DICOM file \r\n[US.1.2.156.112536.1.2127.130145051254127131.13912524190.144.txt](https://github.com/pydicom/pydicom/files/6799721/US.1.2.156.112536.1.2127.130145051254127131.13912524190.144.txt)\r\n\r\nPlease remove .txt extension to use DICOM file\r\n\r\n\r\n\n",
    "input": " from struct import unpack\n import sys\n from typing import List, TYPE_CHECKING, cast\n \n try:\n     import numpy as np\n             # ii is 1, 0, 3, 2, 5, 4 for the example above\n             # This is where the segment order correction occurs\n             segment = _rle_decode_segment(data[offsets[ii]:offsets[ii + 1]])\n            # Check that the number of decoded pixels is correct\n            if len(segment) != rows * columns:\n                 raise ValueError(\n                     \"The amount of decoded RLE segment data doesn't match the \"\n                    f\"expected amount ({len(segment)} vs. \"\n                     f\"{rows * columns} bytes)\"\n                 )\n \n             if segment_order == '>':\n                 byte_offset = bytes_per_sample - byte_offset - 1\n             # For 100 pixel/plane, 32-bit, 3 sample data, `start` will be\n             #   0, 1, 2, 3, 400, 401, 402, 403, 800, 801, 802, 803\n             start = byte_offset + (sample_number * stride)\n            decoded[start:start + stride:bytes_per_sample] = segment\n \n     return decoded\n ",
    "output": " from struct import unpack\n import sys\n from typing import List, TYPE_CHECKING, cast\nimport warnings\n \n try:\n     import numpy as np\n             # ii is 1, 0, 3, 2, 5, 4 for the example above\n             # This is where the segment order correction occurs\n             segment = _rle_decode_segment(data[offsets[ii]:offsets[ii + 1]])\n\n            # Check that the number of decoded bytes is correct\n            actual_length = len(segment)\n            if actual_length < rows * columns:\n                 raise ValueError(\n                     \"The amount of decoded RLE segment data doesn't match the \"\n                    f\"expected amount ({actual_length} vs. \"\n                     f\"{rows * columns} bytes)\"\n                 )\n            elif actual_length != rows * columns:\n                warnings.warn(\n                    \"The decoded RLE segment contains non-conformant padding \"\n                    f\"- {actual_length} vs. {rows * columns} bytes expected\"\n                )\n \n             if segment_order == '>':\n                 byte_offset = bytes_per_sample - byte_offset - 1\n             # For 100 pixel/plane, 32-bit, 3 sample data, `start` will be\n             #   0, 1, 2, 3, 400, 401, 402, 403, 800, 801, 802, 803\n             start = byte_offset + (sample_number * stride)\n            decoded[start:start + stride:bytes_per_sample] = (\n                segment[:rows * columns]\n            )\n \n     return decoded\n "
  },
  {
    "instruction": "can open my dicom, error in re.match('^ISO[^_]IR', encoding)\n```\r\n(test) root@DESKTOP-COPUCVT:/mnt/e/test# python3 mydicom.py\r\nTraceback (most recent call last):\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 625, in convert_encodings\r\n    py_encodings.append(python_encoding[encoding])\r\nKeyError: 73\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"mydicom.py\", line 12, in <module>\r\n    pydicom.dcmread(\"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\")\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 850, in dcmread\r\n    force=force, specific_tags=specific_tags)\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 728, in read_partial\r\n    specific_tags=specific_tags)\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/filereader.py\", line 382, in read_dataset\r\n    encoding = convert_encodings(char_set)\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 628, in convert_encodings\r\n    _python_encoding_for_corrected_encoding(encoding))\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/site-packages/pydicom/charset.py\", line 647, in _python_encoding_for_corrected_encoding\r\n    if re.match('^ISO[^_]IR', encoding) is not None:\r\n  File \"/root/.local/share/virtualenvs/test-LINKoilU/lib/python3.6/re.py\", line 172, in match\r\n    return _compile(pattern, flags).match(string)\r\nTypeError: expected string or bytes-like object\r\n```\r\n\r\n#### Description\r\n I dont know why pydicom cant open my pictures, but other python library can read the picture and read some meta data correctly. I suspect \" if re.match('^ISO[^_]IR', encoding) is not None:\"  the encoding here is not string for my dicom picture.   I am new to pydicom, \r\nHas anyone encountered a similar problem? how to solve it?  need help,thanks!\r\n\r\nhere is some dicom tags:\r\n![image](https://user-images.githubusercontent.com/32253100/61868213-8016f500-af0b-11e9-8736-8703230229cf.png)\r\n![image](https://user-images.githubusercontent.com/32253100/61868247-91600180-af0b-11e9-8767-a4045e901b8f.png)\r\n![image](https://user-images.githubusercontent.com/32253100/61868284-a50b6800-af0b-11e9-88fd-10180e0acf56.png)\r\n\r\n\r\n\r\n#### Steps/Code to Reproduce\r\n```py\r\n\r\nimport pydicom\r\nimport os\r\nimport numpy\r\nchild_path = \"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\"\r\npydicom.dcmread(\"DX.X.1.2.276.0.7230010.3.1.4.313262848.25.1563878256.444385.dcm\"\uff09\r\n\r\n```\r\n\r\n#### Expected Results\r\nExample: read the file without error\r\n\r\n#### Actual Results\r\ncant read the file\r\n\r\n#### Versions\r\nv1.3.0\r\n\r\npython v3.6\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "input": "         if not isinstance(tag, BaseTag):\n             tag = Tag(tag)\n         self.tag = tag\n         self.VR = VR  # Note!: you must set VR before setting value\n         if already_converted:\n             self._value = value",
    "output": "         if not isinstance(tag, BaseTag):\n             tag = Tag(tag)\n         self.tag = tag\n\n        # a known tag shall only have the VR 'UN' if it has a length that\n        # exceeds the size that can be encoded in 16 bit - all other cases\n        # can be seen as an encoding error and can be corrected\n        if VR == 'UN' and (is_undefined_length or value is None or\n                           len(value) < 0xffff):\n            try:\n                VR = dictionary_VR(tag)\n            except KeyError:\n                pass\n\n         self.VR = VR  # Note!: you must set VR before setting value\n         if already_converted:\n             self._value = value"
  },
  {
    "instruction": "Inconsistencies in value testing for PersonName3\n```python\r\nfrom pydicom.dataset import Dataset\r\n\r\nds = Dataset()\r\nds.PatientName = None  # or ''\r\nif ds.PatientName:\r\n    print('Has a value')\r\nelse:\r\n    print('Has no value')\r\n\r\nif None:  # or ''\r\n    print('Evaluates as True')\r\nelse:\r\n    print('Evaluates as False')\r\n```\r\nPrints `Has a value` then `Evaluates as False`. Should print `Has no value` instead (encoded dataset will have a zero-length element).\r\n\r\nCurrent master, python 3.6.\n",
    "input": "         elif self.VR == 'TM' and config.datetime_conversion:\n             return pydicom.valuerep.TM(val)\n         elif self.VR == \"UI\":\n            return UID(val)\n         elif not in_py2 and self.VR == \"PN\":\n             return PersonName(val)\n         # Later may need this for PersonName as for UI,\ndiff --git a/pydicom/multival.py b/pydicom/multival.py\n \"\"\"Code for multi-value data elements values,\n or any list of items that must all be the same type.\n \"\"\"\n \n try:\n     from collections.abc import MutableSequence\n         :param type_constructor: a constructor for the required\n                            type for all list items. Could be the\n                            class, or a factory function. For DICOM\n                           mult-value data elements, this will be the\n                            class or type corresponding to the VR.\n         :param iterable: an iterable (e.g. list, tuple) of items\n                         to initialize the MultiValue list\n             self._list.__setitem__(i, self.type_constructor(val))\n \n     def __str__(self):\n        lines = [str(x) for x in self]\n        return \"['\" + \"', '\".join(lines) + \"']\"\n \n     __repr__ = __str__\n \ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n \n class PersonName3(object):\n     def __init__(self, val, encodings=None, original_string=None):\n        # handle None `val` as empty string\n        val = val or ''\n         if isinstance(val, PersonName3):\n             encodings = val.encodings\n             self.original_string = val.original_string\n             self.original_string = val\n             self._components = None\n         else:\n             # this is the decoded string - save the original string if\n             # available for easier writing back\n             self.original_string = original_string\n         self._create_dict()\n         return format_str % self._dict\n \n \n class PersonNameBase(object):\n     \"\"\"Base class for Person Name classes\"\"\"",
    "output": "         elif self.VR == 'TM' and config.datetime_conversion:\n             return pydicom.valuerep.TM(val)\n         elif self.VR == \"UI\":\n            return UID(val if val else '')\n         elif not in_py2 and self.VR == \"PN\":\n             return PersonName(val)\n         # Later may need this for PersonName as for UI,\ndiff --git a/pydicom/multival.py b/pydicom/multival.py\n \"\"\"Code for multi-value data elements values,\n or any list of items that must all be the same type.\n \"\"\"\nfrom pydicom import compat\n \n try:\n     from collections.abc import MutableSequence\n         :param type_constructor: a constructor for the required\n                            type for all list items. Could be the\n                            class, or a factory function. For DICOM\n                           multi-value data elements, this will be the\n                            class or type corresponding to the VR.\n         :param iterable: an iterable (e.g. list, tuple) of items\n                         to initialize the MultiValue list\n             self._list.__setitem__(i, self.type_constructor(val))\n \n     def __str__(self):\n        if not self:\n            return ''\n        lines = [\"'{}'\".format(x) if isinstance(x, compat.char_types)\n                 else str(x) for x in self]\n        return \"[\" + \", \".join(lines) + \"]\"\n \n     __repr__ = __str__\n \ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n \n class PersonName3(object):\n     def __init__(self, val, encodings=None, original_string=None):\n         if isinstance(val, PersonName3):\n             encodings = val.encodings\n             self.original_string = val.original_string\n             self.original_string = val\n             self._components = None\n         else:\n            # handle None `val` as empty string\n            val = val or ''\n\n             # this is the decoded string - save the original string if\n             # available for easier writing back\n             self.original_string = original_string\n         self._create_dict()\n         return format_str % self._dict\n \n    def __bool__(self):\n        if self.original_string is None:\n            return (self._components is not None and\n                    (len(self._components) > 1 or bool(self._components[0])))\n        return bool(self.original_string)\n\n \n class PersonNameBase(object):\n     \"\"\"Base class for Person Name classes\"\"\""
  },
  {
    "instruction": "Handling of DS too long to be encoded in explicit encoding\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nThis is probably not a bug, but I'm not sure about the wanted behavior.\r\nAn RTPlan dataset encoded as Little Endian Implicit contains multiple values in the DS tag DHV Data (3004,0058) with an overall length not fitting into 2 bytes. Trying to write this as explicit Little Endian fails with an exception (`\"ushort format requires 0 &lt;= number &lt;= (0x7fff * 2 + 1)\"`) which is raised by the `pack` call in `write_leUS` while trying to write the length.\r\n\r\nThe standard says for this case in PS3.5, Table 6.2-1 (for VR DS):\r\n```\r\nNote\r\nData Elements with multiple values using this VR may not be properly encoded if Explicit-VR transfer syntax is used and the VL of this attribute exceeds 65534 bytes.\r\n```\r\nSo, as I understand it, this is valid DICOM, that cannot be converted to explicit encoding without data loss.\r\nThe question is how to handle this. What comes to mind:\r\n- truncate the value and log a warning\r\n- raise a meaningful exception\r\n- adapt the behavior depending on some config setting\r\n\r\nAny thoughts?\r\n\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```py\r\nfrom io import BytesIO\r\nfrom pydicom import dcmread\r\n\r\nbytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\r\n             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\r\n             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\r\n             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\r\n             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\r\n\r\nfp = BytesIO(bytestream)\r\nds = dcmread(fp, force=True)\r\n\r\nprint(ds.PatientID)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n\r\nWhen possible use pydicom testing examples to reproduce the errors. Otherwise, provide\r\nan anonymous version of the data in order to replicate the errors.\r\n-->\r\n\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "input": " \n from __future__ import absolute_import\n \nimport struct\n from struct import pack\n \n from pydicom import compat\n     # Write element's tag\n     fp.write_tag(data_element.tag)\n \n    # If explicit VR, write the VR\n    VR = data_element.VR\n    if not fp.is_implicit_VR:\n        if len(VR) != 2:\n            msg = (\"Cannot write ambiguous VR of '{}' for data element with \"\n                   \"tag {}.\\nSet the correct VR before writing, or use an \"\n                   \"implicit VR transfer syntax\".format(\n                       VR, repr(data_element.tag)))\n            raise ValueError(msg)\n        if not in_py2:\n            fp.write(bytes(VR, default_encoding))\n        else:\n            fp.write(VR)\n        if VR in extra_length_VRs:\n            fp.write_US(0)  # reserved 2 bytes\n\n     # write into a buffer to avoid seeking back which can be expansive\n     buffer = DicomBytesIO()\n     buffer.is_little_endian = fp.is_little_endian\n     buffer.is_implicit_VR = fp.is_implicit_VR\n \n     if data_element.is_raw:\n         # raw data element values can be written as they are\n         buffer.write(data_element.value)\n                              'start with an item tag')\n \n     value_length = buffer.tell()\n     if (not fp.is_implicit_VR and VR not in extra_length_VRs and\n             not is_undefined_length):\n        try:\n            fp.write_US(value_length)  # Explicit VR length field is 2 bytes\n        except struct.error:\n            msg = ('The value for the data element {} exceeds the size '\n                   'of 64 kByte and cannot be written in an explicit transfer '\n                   'syntax. You can save it using Implicit Little Endian '\n                   'transfer syntax, or you have to truncate the value to not '\n                   'exceed the maximum size of 64 kByte.'\n                   .format(data_element.tag))\n            raise ValueError(msg)\n     else:\n         # write the proper length of the data_element in the length slot,\n         # unless is SQ with undefined length.",
    "output": " \n from __future__ import absolute_import\n \nimport warnings\n from struct import pack\n \n from pydicom import compat\n     # Write element's tag\n     fp.write_tag(data_element.tag)\n \n     # write into a buffer to avoid seeking back which can be expansive\n     buffer = DicomBytesIO()\n     buffer.is_little_endian = fp.is_little_endian\n     buffer.is_implicit_VR = fp.is_implicit_VR\n \n    VR = data_element.VR\n    if not fp.is_implicit_VR and len(VR) != 2:\n        msg = (\"Cannot write ambiguous VR of '{}' for data element with \"\n               \"tag {}.\\nSet the correct VR before writing, or use an \"\n               \"implicit VR transfer syntax\".format(\n                   VR, repr(data_element.tag)))\n        raise ValueError(msg)\n\n     if data_element.is_raw:\n         # raw data element values can be written as they are\n         buffer.write(data_element.value)\n                              'start with an item tag')\n \n     value_length = buffer.tell()\n    if (not fp.is_implicit_VR and VR not in extra_length_VRs and\n            not is_undefined_length and value_length > 0xffff):\n        # see PS 3.5, section 6.2.2 for handling of this case\n        msg = ('The value for the data element {} exceeds the size '\n               'of 64 kByte and cannot be written in an explicit transfer '\n               'syntax. The data element VR is changed from \"{}\" to \"UN\" '\n               'to allow saving the data.'\n               .format(data_element.tag, VR))\n        warnings.warn(msg)\n        VR = 'UN'\n\n    # write the VR for explicit transfer syntax\n    if not fp.is_implicit_VR:\n        if not in_py2:\n            fp.write(bytes(VR, default_encoding))\n        else:\n            fp.write(VR)\n        if VR in extra_length_VRs:\n            fp.write_US(0)  # reserved 2 bytes\n\n     if (not fp.is_implicit_VR and VR not in extra_length_VRs and\n             not is_undefined_length):\n        fp.write_US(value_length)  # Explicit VR length field is 2 bytes\n     else:\n         # write the proper length of the data_element in the length slot,\n         # unless is SQ with undefined length."
  },
  {
    "instruction": "to_json does not work with binary data in pixel_array\n**Describe the issue**\r\nLoading a dicom file and then performing a to_json() on it does not work with binary data in pixel_array.\r\n\r\n\r\n\r\n**Expected behavior**\r\nI would have expected that a base64 conversion is first performed on the binary data and then encoded to json. \r\n\r\n**Steps To Reproduce**\r\nHow to reproduce the issue. Please include:\r\n1. A minimum working code sample\r\n\r\nimport pydicom\r\nds = pydicom.dcmread('path_to_file')\r\noutput = ds.to_json()\r\n\r\n\r\n2. The traceback (if one occurred)\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/.virtualenvs/my_env/lib/python3.7/site-packages/pydicom/dataset.py\", line 2003, in to_json\r\n    dump_handler=dump_handler\r\n  File \"/.virtualenvs/my_env/lib/python3.7/site-packages/pydicom/dataset.py\", line 1889, in _data_element_to_json\r\n    binary_value = data_element.value.encode('utf-8')\r\nAttributeError: 'bytes' object has no attribute 'encode'\r\n\r\n\r\n3. Which of the following packages are available and their versions:\r\n  * Numpy\r\nnumpy==1.17.2\r\n  * Pillow\r\nPillow==6.1.0\r\n  * JPEG-LS\r\n  * GDCM\r\n4. The anonymized DICOM dataset (if possible).\r\n\r\n**Your environment**\r\nPlease run the following and paste the output.\r\n```bash\r\n$ python -c \"import platform; print(platform.platform())\"\r\nDarwin-19.2.0-x86_64-i386-64bit\r\n$ python -c \"import sys; print('Python ', sys.version)\"\r\nPython  3.7.6 (default, Dec 30 2019, 19:38:26) \r\n[Clang 11.0.0 (clang-1100.0.33.16)]\r\n$ python -c \"import pydicom; print('pydicom ', pydicom.__version__)\"\r\npydicom  1.3.0\r\n```\r\n\n",
    "input": "             via DICOMweb WADO-RS\n         bulk_data_threshold: int\n             Size of base64 encoded data element above which a value will be\n            provided in form of a \"BulkDataURI\" rather than \"InlineBinary\"\n \n         Returns\n         -------\n         dict\n             Mapping representing a JSON encoded data element\n\n        Raises\n        ------\n        TypeError\n            When size of encoded data element exceeds `bulk_data_threshold`\n            but `bulk_data_element_handler` is ``None`` and hence not callable\n\n         \"\"\"\n         json_element = {'vr': self.VR, }\n         if self.VR in jsonrep.BINARY_VR_VALUES:\n             if not self.is_empty:\n                 binary_value = self.value\n                 encoded_value = base64.b64encode(binary_value).decode('utf-8')\n                if len(encoded_value) > bulk_data_threshold:\n                    if bulk_data_element_handler is None:\n                        raise TypeError(\n                            'No bulk data element handler provided to '\n                            'generate URL for value of data element \"{}\".'\n                            .format(self.name)\n                        )\n                     json_element['BulkDataURI'] = bulk_data_element_handler(\n                         self\n                     )\n             )\n         return json_element\n \n    def to_json(self, bulk_data_threshold=1, bulk_data_element_handler=None,\n                 dump_handler=None):\n         \"\"\"Return a JSON representation of the :class:`DataElement`.\n \n             via DICOMweb WADO-RS\n         bulk_data_threshold: int\n             Size of base64 encoded data element above which a value will be\n            provided in form of a \"BulkDataURI\" rather than \"InlineBinary\"\n         dump_handler : callable, optional\n             Callable function that accepts a :class:`dict` and returns the\n             serialized (dumped) JSON string (by default uses\n         dict\n             Mapping representing a JSON encoded data element\n \n        Raises\n        ------\n        TypeError\n            When size of encoded data element exceeds `bulk_data_threshold`\n            but `bulk_data_element_handler` is ``None`` and hence not callable\n\n         See also\n         --------\n         Dataset.to_json\ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n             dataset.add(data_element)\n         return dataset\n \n    def to_json_dict(self, bulk_data_threshold=1,\n                      bulk_data_element_handler=None):\n         \"\"\"Return a dictionary representation of the :class:`Dataset`\n         conforming to the DICOM JSON Model as described in the DICOM\n        Standard, Part 18, :dcm:`Annex F<part18/chaptr_F.html>`.\n \n         .. versionadded:: 1.4\n \n             Threshold for the length of a base64-encoded binary data element\n             above which the element should be considered bulk data and the\n             value provided as a URI rather than included inline (default:\n            ``1``).\n         bulk_data_element_handler : callable, optional\n             Callable function that accepts a bulk data element and returns a\n             JSON representation of the data element (dictionary including the\n             )\n         return json_dataset\n \n    def to_json(self, bulk_data_threshold=1, bulk_data_element_handler=None,\n                 dump_handler=None):\n         \"\"\"Return a JSON representation of the :class:`Dataset`.\n \n             Threshold for the length of a base64-encoded binary data element\n             above which the element should be considered bulk data and the\n             value provided as a URI rather than included inline (default:\n            ``1``).\n         bulk_data_element_handler : callable, optional\n             Callable function that accepts a bulk data element and returns a\n             JSON representation of the data element (dictionary including the",
    "output": "             via DICOMweb WADO-RS\n         bulk_data_threshold: int\n             Size of base64 encoded data element above which a value will be\n            provided in form of a \"BulkDataURI\" rather than \"InlineBinary\".\n            Ignored if no bulk data handler is given.\n \n         Returns\n         -------\n         dict\n             Mapping representing a JSON encoded data element\n         \"\"\"\n         json_element = {'vr': self.VR, }\n         if self.VR in jsonrep.BINARY_VR_VALUES:\n             if not self.is_empty:\n                 binary_value = self.value\n                 encoded_value = base64.b64encode(binary_value).decode('utf-8')\n                if (bulk_data_element_handler is not None and\n                        len(encoded_value) > bulk_data_threshold):\n                     json_element['BulkDataURI'] = bulk_data_element_handler(\n                         self\n                     )\n             )\n         return json_element\n \n    def to_json(self, bulk_data_threshold=1024, bulk_data_element_handler=None,\n                 dump_handler=None):\n         \"\"\"Return a JSON representation of the :class:`DataElement`.\n \n             via DICOMweb WADO-RS\n         bulk_data_threshold: int\n             Size of base64 encoded data element above which a value will be\n            provided in form of a \"BulkDataURI\" rather than \"InlineBinary\".\n            Ignored if no bulk data handler is given.\n         dump_handler : callable, optional\n             Callable function that accepts a :class:`dict` and returns the\n             serialized (dumped) JSON string (by default uses\n         dict\n             Mapping representing a JSON encoded data element\n \n         See also\n         --------\n         Dataset.to_json\ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n             dataset.add(data_element)\n         return dataset\n \n    def to_json_dict(self, bulk_data_threshold=1024,\n                      bulk_data_element_handler=None):\n         \"\"\"Return a dictionary representation of the :class:`Dataset`\n         conforming to the DICOM JSON Model as described in the DICOM\n        Standard, Part 18, :dcm:`Annex F<part18/chapter_F.html>`.\n \n         .. versionadded:: 1.4\n \n             Threshold for the length of a base64-encoded binary data element\n             above which the element should be considered bulk data and the\n             value provided as a URI rather than included inline (default:\n            ``1024``). Ignored if no bulk data handler is given.\n         bulk_data_element_handler : callable, optional\n             Callable function that accepts a bulk data element and returns a\n             JSON representation of the data element (dictionary including the\n             )\n         return json_dataset\n \n    def to_json(self, bulk_data_threshold=1024, bulk_data_element_handler=None,\n                 dump_handler=None):\n         \"\"\"Return a JSON representation of the :class:`Dataset`.\n \n             Threshold for the length of a base64-encoded binary data element\n             above which the element should be considered bulk data and the\n             value provided as a URI rather than included inline (default:\n            ``1024``). Ignored if no bulk data handler is given.\n         bulk_data_element_handler : callable, optional\n             Callable function that accepts a bulk data element and returns a\n             JSON representation of the data element (dictionary including the"
  },
  {
    "instruction": "Error writing values with VR OF\n[Related to this comment](https://github.com/pydicom/pydicom/issues/452#issuecomment-614038937) (I think)\r\n\r\n```python\r\nfrom pydicom.dataset import Dataset\r\nds = Dataset()\r\nds.is_little_endian = True\r\nds.is_implicit_VR = True\r\nds.FloatPixelData = b'\\x00\\x00\\x00\\x00'\r\nds.save_as('out.dcm')\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \".../pydicom/filewriter.py\", line 228, in write_numbers\r\n    value.append  # works only if list, not if string or number\r\nAttributeError: 'bytes' object has no attribute 'append'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".../pydicom/filewriter.py\", line 230, in write_numbers\r\n    fp.write(pack(format_string, value))\r\nstruct.error: required argument is not a float\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \".../pydicom/tag.py\", line 27, in tag_in_exception\r\n    yield\r\n  File \".../pydicom/filewriter.py\", line 543, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \".../pydicom/filewriter.py\", line 472, in write_data_element\r\n    writer_function(buffer, data_element, writer_param)\r\n  File \".../pydicom/filewriter.py\", line 236, in write_numbers\r\n    \"{0}\\nfor data_element:\\n{1}\".format(str(e), str(data_element)))\r\nOSError: required argument is not a float\r\nfor data_element:\r\n(7fe0, 0008) Float Pixel Data                    OF: b'\\x00\\x00\\x00\\x00'\r\n\r\n[skip]\r\n```\r\n[Error in filewriter](https://github.com/pydicom/pydicom/blob/master/pydicom/filewriter.py#L1007) using `write_numbers` instead of `write_OBvalue`/`write_OWvalue`. Looks like it's been wrong [since 2008](https://github.com/pydicom/pydicom/commit/5d3ea61ffe6877ae79267bf233f258c07c726998). I'm a bit surprised it hasn't come up before.\n",
    "input": "     'LT': (write_text, None),\n     'OB': (write_OBvalue, None),\n     'OD': (write_OWvalue, None),\n    'OF': (write_numbers, 'f'),\n     'OL': (write_OWvalue, None),\n     'OW': (write_OWvalue, None),\n     'OV': (write_OWvalue, None),\ndiff --git a/pydicom/values.py b/pydicom/values.py\n     'LT': convert_single_string,\n     'OB': convert_OBvalue,\n     'OD': convert_OBvalue,\n    'OF': (convert_numbers, 'f'),\n     'OL': convert_OBvalue,\n     'OW': convert_OWvalue,\n     'OV': convert_OVvalue,",
    "output": "     'LT': (write_text, None),\n     'OB': (write_OBvalue, None),\n     'OD': (write_OWvalue, None),\n    'OF': (write_OWvalue, None),\n     'OL': (write_OWvalue, None),\n     'OW': (write_OWvalue, None),\n     'OV': (write_OWvalue, None),\ndiff --git a/pydicom/values.py b/pydicom/values.py\n     'LT': convert_single_string,\n     'OB': convert_OBvalue,\n     'OD': convert_OBvalue,\n    'OF': convert_OWvalue,\n     'OL': convert_OBvalue,\n     'OW': convert_OWvalue,\n     'OV': convert_OVvalue,"
  },
  {
    "instruction": "Pixel Representation attribute should be optional for pixel data handler\n**Describe the bug**\r\nThe NumPy pixel data handler currently [requires the Pixel Representation attribute](https://github.com/pydicom/pydicom/blob/8da0b9b215ebfad5756051c891def88e426787e7/pydicom/pixel_data_handlers/numpy_handler.py#L46). This is problematic, because in case of Float Pixel Data or Double Float Pixel Data the attribute shall be absent. Compare [Floating Point Image Pixel Module Attributes](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.24.html) versus [Image Pixel Description Macro Attributes](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.7.6.3.html#table_C.7-11c)\r\n\r\n**Expected behavior**\r\nI would expect the `Dataset.pixel_array` property to be able to decode a Float Pixel Data or Double Float Pixel Data element without presence of the Pixel Representation element in the metadata.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nimport numpy as np\r\nfrom pydicom.dataset import Dataset, FileMetaDataset\r\n\r\n\r\nds = Dataset()\r\nds.file_meta = FileMetaDataset()\r\nds.file_meta.TransferSyntaxUID = '1.2.840.10008.1.2.1'\r\n\r\nds.BitsAllocated = 32\r\nds.SamplesPerPixel = 1\r\nds.Rows = 5\r\nds.Columns = 5\r\nds.PhotometricInterpretation = 'MONOCHROME2'\r\n\r\npixel_array = np.zeros((ds.Rows, ds.Columns), dtype=np.float32)\r\nds.FloatPixelData = pixel_array.flatten().tobytes()\r\n\r\nnp.array_equal(ds.pixel_array, pixel_array)\r\n```\n",
    "input": " | (0028,0100) | BitsAllocated             | 1    | 1, 8, 16, 32, | Required |\n |             |                           |      | 64            |          |\n +-------------+---------------------------+------+---------------+----------+\n| (0028,0103) | PixelRepresentation       | 1    | 0, 1          | Required |\n +-------------+---------------------------+------+---------------+----------+\n \n \"\"\"\n             \"the dataset\"\n         )\n \n     required_elements = [\n        'BitsAllocated', 'Rows', 'Columns', 'PixelRepresentation',\n         'SamplesPerPixel', 'PhotometricInterpretation'\n     ]\n     missing = [elem for elem in required_elements if elem not in ds]\n     if missing:\n         raise AttributeError(\n             \"Unable to convert the pixel data as the following required \"\n             \"elements are missing from the dataset: \" + \", \".join(missing)\n         )\n \n     # May be Pixel Data, Float Pixel Data or Double Float Pixel Data\n     pixel_data = getattr(ds, px_keyword[0])",
    "output": " | (0028,0100) | BitsAllocated             | 1    | 1, 8, 16, 32, | Required |\n |             |                           |      | 64            |          |\n +-------------+---------------------------+------+---------------+----------+\n| (0028,0101) | BitsStored                | 1    | 1, 8, 12, 16  | Optional |\n+-------------+---------------------------+------+---------------+----------+\n| (0028,0103) | PixelRepresentation       | 1C   | 0, 1          | Optional |\n +-------------+---------------------------+------+---------------+----------+\n \n \"\"\"\n             \"the dataset\"\n         )\n \n    # Attributes required by both Floating Point Image Pixel Module Attributes\n    # and Image Pixel Description Macro Attributes\n     required_elements = [\n        'BitsAllocated', 'Rows', 'Columns',\n         'SamplesPerPixel', 'PhotometricInterpretation'\n     ]\n    if px_keyword[0] == 'PixelData':\n        # Attributess required by Image Pixel Description Macro Attributes\n        required_elements.extend(['PixelRepresentation', 'BitsStored'])\n     missing = [elem for elem in required_elements if elem not in ds]\n     if missing:\n         raise AttributeError(\n             \"Unable to convert the pixel data as the following required \"\n             \"elements are missing from the dataset: \" + \", \".join(missing)\n         )\n    if ds.SamplesPerPixel > 1:\n        if not hasattr(ds, 'PlanarConfiguration'):\n            raise AttributeError(\n                \"Unable to convert the pixel data as the following \"\n                \"conditionally required element is missing from the dataset: \"\n                \"PlanarConfiguration\"\n            )\n \n     # May be Pixel Data, Float Pixel Data or Double Float Pixel Data\n     pixel_data = getattr(ds, px_keyword[0])"
  },
  {
    "instruction": "Strict adherence to VR during parsing is detrimental due to commonplace vendor interpretations\n**Describe the bug**\r\nDICOM Files from GE modalities, which when parsed, raise a TypeError caused by \"violating\" the VR imposed by the DICOM standard; however, real world modalities have and continue to generate such files for good cause.\r\n\r\nFor example the following is raised\r\n\r\n`TypeError('Could not convert value to integer without loss')`\r\n\r\nby a real world DICOM file which has a value\r\n\r\n`(0018,1152) IS [14.5]                                   #   4, 1 Exposure`\r\n\r\nwhere IS is a Value Representation defined as\r\n\r\n> IS - Integer String\r\n\r\n> A string of characters representing an Integer in base-10 (decimal), shall contain only the characters 0 - 9, with an optional leading \"+\" or \"-\". It may be padded with leading and/or trailing spaces. Embedded spaces are not allowed.\r\n\r\n> The integer, n, represented shall be in the range: -231<= n <= (231-1).\r\n\r\n[See DICOM Part 5 Section 6.2](https://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html)\r\n\r\nwhich means `14.5` is an invalid value due to the fractional portion .5 which definitely would lead to a loss in precision if converted to a pure integer value (of 14). \r\n\r\nAfter discussion with a senior engineer for the vendor, the following dialogue was obtained which quotes an article by David Clune, a well-respected, long-time member of the DICOM committee and community:\r\n\r\n> The tag pair in question is meant to contain the mAs value used for the exposure, which is not constrained to integer values, but for some reason the DICOM standard defines it as such.\r\n\r\n> An interesting article from someone responsible for maintaining the DICOM documentation explains the conundrum quite well:  \r\n\r\nhttp://dclunie.blogspot.com/2008/11/dicom-exposure-attribute-fiasco.html\r\n\r\n> Of note are two excerpts from that article:\r\n\r\n> \"The original ACR-NEMA standard specified ASCII numeric data elements for Exposure, Exposure Time and X-Ray Tube Current that could be decimal values; for no apparent reason DICOM 3.0 in 1993 constrained these to be integers, which for some modalities and subjects are too small to be sufficiently precise\"\r\n\r\n> and\r\n\r\n> \"The authors of DICOM, in attempting to maintain some semblance of backward compatibility with ACR-NEMA and at the same time apply more precise constraints, re-defined all ACR-NEMA data elements of VR AN as either IS or DS, the former being the AN integer numbers (with new size constraints), and the latter being the AN fixed point and floating point numbers. In the process of categorizing the old data elements into either IS or DS, not only were the obvious integers (like counts of images and other things) made into integers, but it appears that also any \"real world\" attribute that in somebody's expert opinion did not need greater precision than a whole integer, was so constrained as well.\"\r\n\r\n> I have inspected a few random DICOM files generated by various modalities and the value is stored accurately, even though it is a violation of the explicit value representation. Additionally, I have worked with (and support) various PACS platforms, and this is the first time this has been raised as an issue. So technically, you are correct that encoding that value as decimal violates the explicit VR, but it appears to be common practice to do so. \r\n\r\n**Expected behavior**\r\nTo deal with the reality of history with respect to the current standard, my opinion, as a long-standing DICOM PACS implementer at Medstrat, is that there is nothing to gain and everything to lose by raising a `TypeError` here. For cases where an integer VR, such as `IS`, could be read as a floating point number instead, then it should be allowed to be so, for at least a limited whitelist of tags.\r\n\r\nArguments against which come to mind are of the ilk that do not heed \"Although practicality beats purity\" as can be read if you \r\n\r\n[`>>> import this`](https://peps.python.org/pep-0020/)\r\n\r\n> Special cases aren't special enough to break the rules.\r\n> Although practicality beats purity.\r\n\r\n**Steps To Reproduce**\r\n\r\n`(0018,1152) IS [14.5]                                   #   4, 1 Exposure`\r\n\r\nSet any DICOM file to have the above for `Exposure` and then do this:\r\n\r\n```\r\n>>> from pydicom import config\r\n>>> pydicom.__version__\r\n'2.3.0'\r\n>>> config.settings.reading_validation_mode = config.IGNORE\r\n>>> ds = pydicom.dcmread('1.2.840.113619.2.107.20220429121335.1.1.dcm')\r\n>>> ds\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 2306, in __str__\r\n    return self._pretty_str()\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 2020, in _pretty_str\r\n    for elem in self:\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 1240, in __iter__\r\n    yield self[tag]\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataset.py\", line 939, in __getitem__\r\n    self[tag] = DataElement_from_raw(elem, character_set, self)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/dataelem.py\", line 859, in DataElement_from_raw\r\n    value = convert_value(vr, raw, encoding)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/values.py\", line 771, in convert_value\r\n    return converter(byte_string, is_little_endian, num_format)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/values.py\", line 348, in convert_IS_string\r\n    return MultiString(num_string, valtype=pydicom.valuerep.IS)\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/valuerep.py\", line 1213, in MultiString\r\n    return valtype(splitup[0])\r\n  File \"/usr/local/lib/python3.7/site-packages/pydicom/valuerep.py\", line 1131, in __new__\r\n    raise TypeError(\"Could not convert value to integer without loss\")\r\nTypeError: Could not convert value to integer without loss\r\n```\r\n\r\n**Your environment**\r\n\r\n```bash\r\nmodule       | version\r\n------       | -------\r\nplatform     | Darwin-21.5.0-x86_64-i386-64bit\r\nPython       | 3.7.5 (v3.7.5:5c02a39a0b, Oct 14 2019, 18:49:57)  [Clang 6.0 (clang-600.0.57)]\r\npydicom      | 2.2.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | _module not found_\r\nPIL          | _module not found_\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\n",
    "input": "         settings._writing_validation_mode = writing_mode\n \n \n convert_wrong_length_to_UN = False\n \"\"\"Convert a field VR to \"UN\" and return bytes if bytes length is invalid.\n Default ``False``.\ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n \"\"\"\n import copy\n from bisect import bisect_left\n import io\n from importlib.util import find_spec as have_package\n import inspect  # for __dir__\n             :class:`Dataset` representation based on the DICOM JSON Model.\n         \"\"\"\n         json_dataset = {}\n        for key in self.keys():\n            json_key = '{:08X}'.format(key)\n            try:\n                data_element = self[key]\n                json_dataset[json_key] = data_element.to_json_dict(\n                    bulk_data_element_handler=bulk_data_element_handler,\n                    bulk_data_threshold=bulk_data_threshold\n                )\n            except Exception as exc:\n                logger.error(f\"Error while processing tag {json_key}\")\n                if not suppress_invalid_tags:\n                    raise exc\n \n         return json_dataset\n \ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n     return DSfloat(val, auto_format, validation_mode)\n \n \n class IS(int):\n     \"\"\"Store value for an element with VR **IS** as :class:`int`.\n \n     def __new__(  # type: ignore[misc]\n             cls: Type[\"IS\"], val: Union[None, str, int, float, Decimal],\n             validation_mode: int = None\n    ) -> Optional[Union[str, \"IS\"]]:\n         \"\"\"Create instance if new integer string\"\"\"\n         if val is None:\n             return val\n             validate_value(\"IS\", val, validation_mode)\n \n         try:\n            newval = super().__new__(cls, val)\n         except ValueError:\n             # accept float strings when no integer loss, e.g. \"1.0\"\n             newval = super().__new__(cls, float(val))\n \n        # check if a float or Decimal passed in, then could have lost info,\n        # and will raise error. E.g. IS(Decimal('1')) is ok, but not IS(1.23)\n        #   IS('1.23') will raise ValueError\n         if isinstance(val, (float, Decimal, str)) and newval != float(val):\n            raise TypeError(\"Could not convert value to integer without loss\")\n \n         # Checks in case underlying int is >32 bits, DICOM does not allow this\n         if (not -2**31 <= newval < 2**31 and",
    "output": "         settings._writing_validation_mode = writing_mode\n \n \n@contextmanager\ndef strict_reading() -> Generator:\n    \"\"\"Context manager to temporarily enably strict value validation\n    for reading.\"\"\"\n    original_reading_mode = settings._reading_validation_mode\n    try:\n        settings.reading_validation_mode = RAISE\n        yield\n    finally:\n        settings._reading_validation_mode = original_reading_mode\n\n\n convert_wrong_length_to_UN = False\n \"\"\"Convert a field VR to \"UN\" and return bytes if bytes length is invalid.\n Default ``False``.\ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n \"\"\"\n import copy\n from bisect import bisect_left\nfrom contextlib import nullcontext\n import io\n from importlib.util import find_spec as have_package\n import inspect  # for __dir__\n             :class:`Dataset` representation based on the DICOM JSON Model.\n         \"\"\"\n         json_dataset = {}\n        context = (\n            config.strict_reading() if suppress_invalid_tags\n            else nullcontext()\n        )\n        with context:\n            for key in self.keys():\n                json_key = '{:08X}'.format(key)\n                try:\n                    data_element = self[key]\n                    json_dataset[json_key] = data_element.to_json_dict(\n                        bulk_data_element_handler=bulk_data_element_handler,\n                        bulk_data_threshold=bulk_data_threshold\n                    )\n                except Exception as exc:\n                    logger.error(f\"Error while processing tag {json_key}\")\n                    if not suppress_invalid_tags:\n                        raise exc\n \n         return json_dataset\n \ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n     return DSfloat(val, auto_format, validation_mode)\n \n \nclass ISfloat(float):\n    \"\"\"Store value for an element with VR **IS** as :class:`float`.\n\n    Stores original integer string for exact rewriting of the string\n    originally read or stored.\n\n    Note: By the DICOM standard, IS can only be an :class:`int`,\n    however, it is not uncommon to see float IS values.  This class\n    is used if the config settings allow non-strict reading.\n\n    Generally, use :class:`~pydicom.valuerep.IS` to create IS values,\n    this is returned instead if the value cannot be represented as an\n    :class:`int`.  See :class:`~pydicom.valuerep.IS` for details of the\n    parameters and return values.\n    \"\"\"\n    def __new__(  # type: ignore[misc]\n            cls: Type[\"ISfloat\"], val: Union[str, float, Decimal],\n            validation_mode: int = None\n    ) -> float:\n        return super().__new__(cls, val)\n\n    def __init__(self, val: Union[str, float, Decimal],\n                 validation_mode: int = None) -> None:\n        # If a string passed, then store it\n        if isinstance(val, str):\n            self.original_string = val.strip()\n        elif isinstance(val, (IS, ISfloat)) and hasattr(val, 'original_string'):\n            self.original_string = val.original_string\n        if validation_mode:\n            msg = (\n                f'Value \"{str(self)}\" is not valid for elements with a VR '\n                'of IS'\n            )\n            if validation_mode == config.WARN:\n                warnings.warn(msg)\n            elif validation_mode == config.RAISE:\n                msg += (\n                    \"\\nSet reading_validation_mode to WARN or IGNORE to bypass\"\n                )\n                raise TypeError(msg)\n\n\n class IS(int):\n     \"\"\"Store value for an element with VR **IS** as :class:`int`.\n \n     def __new__(  # type: ignore[misc]\n             cls: Type[\"IS\"], val: Union[None, str, int, float, Decimal],\n             validation_mode: int = None\n    ) -> Optional[Union[str, \"IS\", \"ISfloat\"]]:\n         \"\"\"Create instance if new integer string\"\"\"\n         if val is None:\n             return val\n             validate_value(\"IS\", val, validation_mode)\n \n         try:\n            newval: Union[IS, ISfloat] = super().__new__(cls, val)\n         except ValueError:\n             # accept float strings when no integer loss, e.g. \"1.0\"\n             newval = super().__new__(cls, float(val))\n \n        # If a float or Decimal was passed in, check for non-integer,\n        # i.e. could lose info if converted to int\n        # If so, create an ISfloat instead (if allowed by settings)\n         if isinstance(val, (float, Decimal, str)) and newval != float(val):\n            newval = ISfloat(val, validation_mode)\n \n         # Checks in case underlying int is >32 bits, DICOM does not allow this\n         if (not -2**31 <= newval < 2**31 and"
  },
  {
    "instruction": "LookupError: unknown encoding: Not Supplied\n#### Description\r\nOutput from `ds = pydicom.read_file(dcmFile)` (an RTSTRUCT dicom file, SOP UID 1.2.840.10008.5.1.4.1.1.481.3) results in some tags throwing a LookupError: \"LookupError: unknown encoding: Not Supplied\"\r\nSpecific tags which cannot be decoded are as follows:\r\n['DeviceSerialNumber',\r\n 'Manufacturer',\r\n 'ManufacturerModelName',\r\n 'PatientID',\r\n 'PatientName',\r\n 'RTROIObservationsSequence',\r\n 'ReferringPhysicianName',\r\n 'SeriesDescription',\r\n 'SoftwareVersions',\r\n 'StructureSetLabel',\r\n 'StructureSetName',\r\n 'StructureSetROISequence',\r\n 'StudyDescription',\r\n 'StudyID']\r\n\r\nI suspect that it's due to the fact that `ds.SpecificCharacterSet = 'Not Supplied'`, but when I try to set `ds.SpecificCharacterSet` to something reasonable (ie ISO_IR_100 or 'iso8859'), it doesn't seem to make any difference.\r\n\r\nReading the same file, with NO modifications, in gdcm does not result in any errors and all fields are readable.\r\n\r\n#### Steps/Code to Reproduce\r\n```py\r\nimport pydicom \r\nds = pydicom.read_file(dcmFile)\r\nprint(ds.PatientName)\r\n```\r\n\r\n#### Expected Results\r\nNo error is thrown and the name of the patient is printed.\r\n\r\n#### Actual Results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 706, in __str__\r\n    return '='.join(self.components).__str__()\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 641, in components\r\n    self._components = _decode_personname(groups, self.encodings)\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 564, in _decode_personname\r\n    for comp in components]\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\valuerep.py\", line 564, in <listcomp>\r\n    for comp in components]\r\n  File \"C:\\Users\\Amanda\\AppData\\Local\\Continuum\\anaconda3\\envs\\itk\\lib\\site-packages\\pydicom\\charset.py\", line 129, in decode_string\r\n    return value.decode(encodings[0])\r\nLookupError: unknown encoding: Not Supplied\r\n\r\n#### Versions\r\nPlatform: Windows-10-10.0.17763-SP0\r\nPython Version: Python 3.6.4 |Anaconda, Inc.| (default, Mar 12 2018, 20:20:50) [MSC v.1900 64 bit (AMD64)]\r\npydicom Version: pydicom 1.2.2\r\n\n",
    "input": "     \"\"\"\n     # shortcut for the common case - no escape sequences present\n     if ESC not in value:\n         try:\n            return value.decode(encodings[0])\n         except UnicodeError:\n             if config.enforce_valid_values:\n                 raise\n            warnings.warn(u\"Failed to decode byte string with encoding {} - \"\n                           u\"using replacement characters in decoded \"\n                          u\"string\".format(encodings[0]))\n            return value.decode(encodings[0], errors='replace')\n \n     # Each part of the value that starts with an escape sequence is decoded\n     # separately. If it starts with an escape sequence, the",
    "output": "     \"\"\"\n     # shortcut for the common case - no escape sequences present\n     if ESC not in value:\n        first_encoding = encodings[0]\n         try:\n            return value.decode(first_encoding)\n        except LookupError:\n            if config.enforce_valid_values:\n                raise\n            warnings.warn(u\"Unknown encoding '{}' - \"\n                          u\"using default encoding instead\"\n                          .format(first_encoding))\n            first_encoding = default_encoding\n            return value.decode(first_encoding)\n         except UnicodeError:\n             if config.enforce_valid_values:\n                 raise\n            warnings.warn(u\"Failed to decode byte string with encoding '{}' - \"\n                           u\"using replacement characters in decoded \"\n                          u\"string\".format(first_encoding))\n            return value.decode(first_encoding, errors='replace')\n \n     # Each part of the value that starts with an escape sequence is decoded\n     # separately. If it starts with an escape sequence, the"
  },
  {
    "instruction": "To_Json 'str' object has no attribute 'components'\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n\r\nWhen converting a dataset to json the following error occurs.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/anaconda3/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"~/pacs-proxy/pacs/service.py\", line 172, in saveFunction\r\n    jsonObj = ds.to_json()\r\n  File \"~/lib/python3.6/site-packages/pydicom/dataset.py\", line 2046, in to_json\r\n    dump_handler=dump_handler\r\n  File \"~/lib/python3.6/site-packages/pydicom/dataelem.py\", line 447, in to_json\r\n    if len(elem_value.components) > 2:\r\nAttributeError: 'str' object has no attribute 'components'\r\n```\r\n#### Steps/Code to Reproduce\r\n\r\nds = pydicom.dcmread(\"testImg\")\r\njsonObj = ds.to_json()\r\n\r\nI'm working on getting an anonymous version of the image, will update. But any advice, suggestions would be appreciated.\r\n\r\n#### \n",
    "input": "                     # Some DICOMweb services get this wrong, so we\n                     # workaround the issue and warn the user\n                     # rather than raising an error.\n                    logger.error(\n                         'value of data element \"{}\" with VR Person Name (PN) '\n                         'is not formatted correctly'.format(tag)\n                     )\n                     elem_value.append(v)\n                 else:\n                    elem_value.extend(list(v.values()))\n            if vm == '1':\n                 try:\n                    elem_value = elem_value[0]\n                except IndexError:\n                    elem_value = ''\n         else:\n            if vm == '1':\n                if value_key == 'InlineBinary':\n                    elem_value = base64.b64decode(value)\n                elif value_key == 'BulkDataURI':\n                    if bulk_data_uri_handler is None:\n                        logger.warning(\n                            'no bulk data URI handler provided for retrieval '\n                            'of value of data element \"{}\"'.format(tag)\n                        )\n                        elem_value = b''\n                    else:\n                        elem_value = bulk_data_uri_handler(value)\n                 else:\n                    if value:\n                        elem_value = value[0]\n                    else:\n                        elem_value = value\n             else:\n                 elem_value = value\n         if elem_value is None:\n            logger.warning('missing value for data element \"{}\"'.format(tag))\n            elem_value = ''\n \n         elem_value = jsonrep.convert_to_python_number(elem_value, vr)\n \n             but `bulk_data_element_handler` is ``None`` and hence not callable\n \n         \"\"\"\n        # TODO: Determine whether more VRs need to be converted to strings\n        _VRs_TO_QUOTE = ['AT', ]\n         json_element = {'vr': self.VR, }\n         if self.VR in jsonrep.BINARY_VR_VALUES:\n            if self.value is not None:\n                 binary_value = self.value\n                 encoded_value = base64.b64encode(binary_value).decode('utf-8')\n                 if len(encoded_value) > bulk_data_threshold:\n             ]\n             json_element['Value'] = value\n         elif self.VR == 'PN':\n            elem_value = self.value\n            if elem_value is not None:\n                if compat.in_py2:\n                    elem_value = PersonNameUnicode(elem_value, 'UTF8')\n                if len(elem_value.components) > 2:\n                    json_element['Value'] = [\n                        {'Phonetic': elem_value.components[2], },\n                    ]\n                elif len(elem_value.components) > 1:\n                    json_element['Value'] = [\n                        {'Ideographic': elem_value.components[1], },\n                    ]\n                 else:\n                    json_element['Value'] = [\n                        {'Alphabetic': elem_value.components[0], },\n                    ]\n         else:\n            if self.value is not None:\n                is_multivalue = isinstance(self.value, MultiValue)\n                if self.VM > 1 or is_multivalue:\n                     value = self.value\n                 else:\n                     value = [self.value]\n                # ensure it's a list and not another iterable\n                # (e.g. tuple), which would not be JSON serializable\n                if self.VR in _VRs_TO_QUOTE:\n                    json_element['Value'] = [str(v) for v in value]\n                else:\n                    json_element['Value'] = [v for v in value]\n         if hasattr(json_element, 'Value'):\n             json_element['Value'] = jsonrep.convert_to_python_number(\n                 json_element['Value'], self.VR\ndiff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n \"\"\"Methods for converting Datasets and DataElements to/from json\"\"\"\n \n # Order of keys is significant!\n JSON_VALUE_KEYS = ('Value', 'BulkDataURI', 'InlineBinary',)\n \n BINARY_VR_VALUES = ['OW', 'OB', 'OD', 'OF', 'OL', 'UN',\n         return None\n     number_type = None\n     if vr in VRs_TO_BE_INTS:\n        number_type = int\n     if vr in VRs_TO_BE_FLOATS:\n         number_type = float\n     if number_type is not None:\ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n             # this is the decoded string - save the original string if\n             # available for easier writing back\n             self.original_string = original_string\n            self._components = tuple(val.split('='))\n \n        # if the encoding is not given, leave it as undefined (None)\n         self.encodings = _verify_encodings(encodings)\n         self._dict = {}\n \n \n     def __bool__(self):\n         if self.original_string is None:\n            return (self._components is not None and\n                     (len(self._components) > 1 or bool(self._components[0])))\n         return bool(self.original_string)\n ",
    "output": "                     # Some DICOMweb services get this wrong, so we\n                     # workaround the issue and warn the user\n                     # rather than raising an error.\n                    logger.warning(\n                         'value of data element \"{}\" with VR Person Name (PN) '\n                         'is not formatted correctly'.format(tag)\n                     )\n                     elem_value.append(v)\n                 else:\n                    if 'Phonetic' in v:\n                        comps = ['', '', '']\n                    elif 'Ideographic' in v:\n                        comps = ['', '']\n                    else:\n                        comps = ['']\n                    if 'Alphabetic' in v:\n                        comps[0] = v['Alphabetic']\n                    if 'Ideographic' in v:\n                        comps[1] = v['Ideographic']\n                    if 'Phonetic' in v:\n                        comps[2] = v['Phonetic']\n                    elem_value.append('='.join(comps))\n            if len(elem_value) == 1:\n                elem_value = elem_value[0]\n            elif not elem_value:\n                elem_value = empty_value_for_VR(vr)\n        elif vr == 'AT':\n            elem_value = []\n            for v in value:\n                 try:\n                    elem_value.append(int(v, 16))\n                except ValueError:\n                    warnings.warn('Invalid value \"{}\" for AT element - '\n                                  'ignoring it'.format(v))\n                value = value[0]\n            if not elem_value:\n                elem_value = empty_value_for_VR(vr)\n            elif len(elem_value) == 1:\n                elem_value = elem_value[0]\n         else:\n            if isinstance(value, list) and len(value) == 1:\n                value = value[0]\n            if value_key == 'InlineBinary':\n                elem_value = base64.b64decode(value)\n            elif value_key == 'BulkDataURI':\n                if bulk_data_uri_handler is None:\n                    logger.warning(\n                        'no bulk data URI handler provided for retrieval '\n                        'of value of data element \"{}\"'.format(tag)\n                    )\n                    elem_value = empty_value_for_VR(vr, raw=True)\n                 else:\n                    elem_value = bulk_data_uri_handler(value)\n             else:\n                 elem_value = value\n         if elem_value is None:\n            elem_value = empty_value_for_VR(vr)\n \n         elem_value = jsonrep.convert_to_python_number(elem_value, vr)\n \n             but `bulk_data_element_handler` is ``None`` and hence not callable\n \n         \"\"\"\n         json_element = {'vr': self.VR, }\n         if self.VR in jsonrep.BINARY_VR_VALUES:\n            if not self.is_empty:\n                 binary_value = self.value\n                 encoded_value = base64.b64encode(binary_value).decode('utf-8')\n                 if len(encoded_value) > bulk_data_threshold:\n             ]\n             json_element['Value'] = value\n         elif self.VR == 'PN':\n            if not self.is_empty:\n                elem_value = []\n                if self.VM > 1:\n                    value = self.value\n                 else:\n                    value = [self.value]\n                for v in value:\n                    if compat.in_py2:\n                        v = PersonNameUnicode(v, 'UTF8')\n                    comps = {'Alphabetic': v.components[0]}\n                    if len(v.components) > 1:\n                        comps['Ideographic'] = v.components[1]\n                    if len(v.components) > 2:\n                        comps['Phonetic'] = v.components[2]\n                    elem_value.append(comps)\n                json_element['Value'] = elem_value\n        elif self.VR == 'AT':\n            if not self.is_empty:\n                value = self.value\n                if self.VM == 1:\n                    value = [value]\n                json_element['Value'] = [format(v, '08X') for v in value]\n         else:\n            if not self.is_empty:\n                if self.VM > 1:\n                     value = self.value\n                 else:\n                     value = [self.value]\n                json_element['Value'] = [v for v in value]\n         if hasattr(json_element, 'Value'):\n             json_element['Value'] = jsonrep.convert_to_python_number(\n                 json_element['Value'], self.VR\ndiff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n \"\"\"Methods for converting Datasets and DataElements to/from json\"\"\"\n \n # Order of keys is significant!\nfrom pydicom.compat import int_type\n\n JSON_VALUE_KEYS = ('Value', 'BulkDataURI', 'InlineBinary',)\n \n BINARY_VR_VALUES = ['OW', 'OB', 'OD', 'OF', 'OL', 'UN',\n         return None\n     number_type = None\n     if vr in VRs_TO_BE_INTS:\n        number_type = int_type\n     if vr in VRs_TO_BE_FLOATS:\n         number_type = float\n     if number_type is not None:\ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n             # this is the decoded string - save the original string if\n             # available for easier writing back\n             self.original_string = original_string\n            components = val.split('=')\n            # Remove empty elements from the end to avoid trailing '='\n            while len(components) and not components[-1]:\n                components.pop()\n            self._components = tuple(components)\n \n            # if the encoding is not given, leave it as undefined (None)\n         self.encodings = _verify_encodings(encodings)\n         self._dict = {}\n \n \n     def __bool__(self):\n         if self.original_string is None:\n            return (bool(self._components) and\n                     (len(self._components) > 1 or bool(self._components[0])))\n         return bool(self.original_string)\n "
  },
  {
    "instruction": "Codify not generating content sequences correctly\n**Describe the bug**\r\nI am trying to generate a radiation dose structure report. I ran Codify on an existing RDSR to generate a template. The sequence content is reproduced but does not seem to be attached  to the base dataset. When I run the generated python file the dicom file it saves has no sequence content information.\r\n\r\n**Expected behavior**\r\nI expect the dicom file generated by the python code from Codify to be similar to the original file.\r\n\r\n**Steps To Reproduce**\r\n$ python codify X-RayRadiationDoseReport001_ESR.dcm rdsr.py\r\n$ python rsdr.py\r\n\r\nI am not able to attached the above files but can supply them.\r\n\r\n**Your environment**\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-5.18.7-200.fc36.x86_64-x86_64-with-glibc2.35\r\nPython       | 3.10.5 (main, Jun  9 2022, 00:00:00) [GCC 12.1.1 20220507 (Red Hat 12.1.1-1)]\r\npydicom      | 2.3.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.22.4\r\nPIL          | 9.2.0\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n\r\nRegards\r\nAlan\n",
    "input": " import argparse\n import pkg_resources\n import re\n from typing import Tuple, cast, List, Any, Dict, Optional, Callable\n \n from pydicom import dcmread\nfrom pydicom.data.data_manager import get_testdata_file\n from pydicom.dataset import Dataset\n \n \n     except NotImplementedError:  # will get this if absolute path passed\n         pydicom_filename = \"\"\n \n     if prefix == \"pydicom\":\n         filename = pydicom_filename\n \n     \"\"\"\n     global subparsers\n \n     parser = argparse.ArgumentParser(\n        prog=\"pydicom\", description=\"pydicom command line utilities\"\n     )\n     subparsers = parser.add_subparsers(help=\"subcommand help\")\n \ndiff --git a/pydicom/util/codify.py b/pydicom/util/codify.py\n import os.path\n import re\n import sys\nfrom typing import Optional, List, Callable\n \n import pydicom\n from pydicom.datadict import dictionary_keyword\n     dataelem: DataElement,\n     dataset_name: str = \"ds\",\n     exclude_size: Optional[int] = None,\n    include_private: bool = False\n ) -> str:\n     \"\"\"Code lines for a single DICOM data element\n \n         will only have a commented string for a value,\n         causing a syntax error when the code is run,\n         and thus prompting the user to remove or fix that line.\n\n     Returns\n     -------\n     str\n \n     if dataelem.VR == VR.SQ:\n         return code_sequence(\n            dataelem, dataset_name, exclude_size, include_private\n         )\n \n     # If in DICOM dictionary, set using the keyword\n     exclude_size: Optional[int] = None,\n     include_private: bool = False,\n     name_filter: Callable[[str], str] = default_name_filter,\n ) -> str:\n     \"\"\"Code lines for recreating a Sequence data element\n \n     name_filter: Callable[[str], str]\n         A callable taking a sequence name or sequence item name, and returning\n         a shorter name for easier code reading\n \n     Returns\n     -------\n         A string containing code lines to recreate a DICOM sequence\n     \"\"\"\n \n     lines = []\n     seq = dataelem.value\n     seq_name = dataelem.name\n     lines.append(\"# \" + seq_name)\n \n     # Code line to create a new Sequence object\n    if name_filter:\n        seq_var = name_filter(seq_keyword)\n     lines.append(seq_var + \" = Sequence()\")\n \n     # Code line to add the sequence to its parent\n         lines.append(\"# \" + seq_name + \": \" + seq_item_name + \" \" + index_str)\n \n         # Determine the variable name to use for the sequence item (dataset)\n        ds_name = seq_var.replace(\"_sequence\", \"\") + index_str\n \n        # Code the sequence item\n        code_item = code_dataset(ds, ds_name, exclude_size, include_private)\n        lines.append(code_item)\n \n        # Code the line to append the item to its parent sequence\n        lines.append(seq_var + \".append(\" + ds_name + \")\")\n \n     # Join the lines and return a single string\n     return line_term.join(lines)\n     exclude_size: Optional[int] = None,\n     include_private: bool = False,\n     is_file_meta: bool = False,\n ) -> str:\n     \"\"\"Return Python code for creating `ds`.\n \n         data elements will be coded.\n     is_file_meta : bool, optional\n         ``True`` if `ds` contains file meta information elements.\n \n     Returns\n     -------\n         The codified dataset.\n     \"\"\"\n \n     lines = []\n     ds_class = \" = FileMetaDataset()\" if is_file_meta else \" = Dataset()\"\n     lines.append(dataset_name + ds_class)\n     for dataelem in ds:\n         # If a private data element and flag says so, skip it and go to next\n             continue\n         # Otherwise code the line and add it to the lines list\n         code_line = code_dataelem(\n            dataelem, dataset_name, exclude_size, include_private\n         )\n         lines.append(code_line)\n         # Add blank line if just coded a sequence\n     # If sequence was end of this dataset, remove the extra blank line\n     if len(lines) and lines[-1] == \"\":\n         lines.pop()\n     # Join all the code lines and return them\n     return line_term.join(lines)\n \n \n     Parameters\n     ----------\n    filename : str\n        Complete path and filename of a DICOM file to convert\n     exclude_size : Union[int,None]\n         If not None, values longer than this (in bytes)\n         will only have a commented string for a value,\n     filename = ds.get(\"filename\")\n     identifier = f\"DICOM file '{filename}'\" if filename else \"non-file dataset\"\n \n     lines.append(f\"# Coded version of {identifier}\")\n     lines.append(\"# Produced by pydicom codify utility script\")\n \n     parser.add_argument(\n         \"outfile\",\n         nargs=\"?\",\n        type=argparse.FileType(\"w\"),\n         help=(\n             \"Filename to write Python code to, if not specified then code is \"\n             \"written to stdout\"",
    "output": " import argparse\n import pkg_resources\n import re\nimport sys\n from typing import Tuple, cast, List, Any, Dict, Optional, Callable\n \n from pydicom import dcmread\nfrom pydicom.data.data_manager import get_charset_files, get_testdata_file\n from pydicom.dataset import Dataset\n \n \n     except NotImplementedError:  # will get this if absolute path passed\n         pydicom_filename = \"\"\n \n    # Check if filename is in charset files\n    if not pydicom_filename:\n        try:\n            char_filenames = get_charset_files(filename)\n            pydicom_filename = char_filenames[0]\n        except NotImplementedError:  # will get this if absolute path passed\n            pass\n\n     if prefix == \"pydicom\":\n         filename = pydicom_filename\n \n     \"\"\"\n     global subparsers\n \n    py_version = sys.version.split()[0]\n\n     parser = argparse.ArgumentParser(\n        prog=\"pydicom\",\n        description=f\"pydicom command line utilities (Python {py_version})\"\n     )\n     subparsers = parser.add_subparsers(help=\"subcommand help\")\n \ndiff --git a/pydicom/util/codify.py b/pydicom/util/codify.py\n import os.path\n import re\n import sys\nfrom typing import Optional, List, Callable, cast\nfrom collections import deque\n \n import pydicom\n from pydicom.datadict import dictionary_keyword\n     dataelem: DataElement,\n     dataset_name: str = \"ds\",\n     exclude_size: Optional[int] = None,\n    include_private: bool = False,\n    var_names: Optional[deque] = None\n ) -> str:\n     \"\"\"Code lines for a single DICOM data element\n \n         will only have a commented string for a value,\n         causing a syntax error when the code is run,\n         and thus prompting the user to remove or fix that line.\n    var_names: Union[deque, None]\n        Used internally to ensure unique variable names in nested sequences.\n     Returns\n     -------\n     str\n \n     if dataelem.VR == VR.SQ:\n         return code_sequence(\n            dataelem, dataset_name, exclude_size, include_private,\n            var_names=var_names\n         )\n \n     # If in DICOM dictionary, set using the keyword\n     exclude_size: Optional[int] = None,\n     include_private: bool = False,\n     name_filter: Callable[[str], str] = default_name_filter,\n    var_names: Optional[deque] = None,\n ) -> str:\n     \"\"\"Code lines for recreating a Sequence data element\n \n     name_filter: Callable[[str], str]\n         A callable taking a sequence name or sequence item name, and returning\n         a shorter name for easier code reading\n    var_names: Union[deque, None]\n        Used internally to ensure unique variable names in nested sequences.\n \n     Returns\n     -------\n         A string containing code lines to recreate a DICOM sequence\n     \"\"\"\n \n    # Normally var_names is given from code_dataset, but for some tests need\n    #   to initialize it\n    if var_names is None:\n        var_names = deque()\n\n    def unique_name(name: str) -> str:\n        name_count = cast(deque, var_names).count(name) - 1\n        return name if name_count == 0 else name + f\"_{name_count}\"\n\n     lines = []\n     seq = dataelem.value\n     seq_name = dataelem.name\n     lines.append(\"# \" + seq_name)\n \n     # Code line to create a new Sequence object\n    seq_var = name_filter(seq_keyword)\n    var_names.append(seq_var)\n    orig_seq_var = seq_var\n    seq_var = unique_name(seq_var)\n\n     lines.append(seq_var + \" = Sequence()\")\n \n     # Code line to add the sequence to its parent\n         lines.append(\"# \" + seq_name + \": \" + seq_item_name + \" \" + index_str)\n \n         # Determine the variable name to use for the sequence item (dataset)\n        ds_name = orig_seq_var.replace(\"_sequence\", \"\") + index_str\n \n        # Append \"_#\" if name already in use (in parent sequences)\n        var_names.append(ds_name)\n        ds_name = unique_name(ds_name)\n\n        # Code the sequence item dataset\n        code_item = code_dataset(\n            ds, ds_name, exclude_size, include_private, var_names=var_names\n        )\n \n        # Remove variable name from stored list, this dataset complete\n        var_names.pop()\n\n        # Code dataset creation and appending that to sequence, then the rest\n        # This keeps the logic close together, rather than after many items set\n        code_split = code_item.splitlines()\n        lines.append(code_split[0])  # \"<ds_name> = Dataset()\"\n        lines.append(f\"{seq_var}.append({ds_name})\")\n        lines.extend(code_split[1:])\n\n    # Remove sequence variable name we've used\n    var_names.pop()\n \n     # Join the lines and return a single string\n     return line_term.join(lines)\n     exclude_size: Optional[int] = None,\n     include_private: bool = False,\n     is_file_meta: bool = False,\n    var_names: Optional[deque] = None\n ) -> str:\n     \"\"\"Return Python code for creating `ds`.\n \n         data elements will be coded.\n     is_file_meta : bool, optional\n         ``True`` if `ds` contains file meta information elements.\n    var_names: deque, optional\n        Used internally to ensure unique variable names in nested sequences.\n \n     Returns\n     -------\n         The codified dataset.\n     \"\"\"\n \n    if var_names is None:\n        var_names = deque()\n     lines = []\n\n     ds_class = \" = FileMetaDataset()\" if is_file_meta else \" = Dataset()\"\n\n     lines.append(dataset_name + ds_class)\n     for dataelem in ds:\n         # If a private data element and flag says so, skip it and go to next\n             continue\n         # Otherwise code the line and add it to the lines list\n         code_line = code_dataelem(\n            dataelem, dataset_name, exclude_size, include_private,\n            var_names=var_names\n         )\n         lines.append(code_line)\n         # Add blank line if just coded a sequence\n     # If sequence was end of this dataset, remove the extra blank line\n     if len(lines) and lines[-1] == \"\":\n         lines.pop()\n\n     # Join all the code lines and return them\n     return line_term.join(lines)\n \n \n     Parameters\n     ----------\n    ds : Dataset\n        A pydicom Dataset to convert\n     exclude_size : Union[int,None]\n         If not None, values longer than this (in bytes)\n         will only have a commented string for a value,\n     filename = ds.get(\"filename\")\n     identifier = f\"DICOM file '{filename}'\" if filename else \"non-file dataset\"\n \n    lines.append(\"# -*- coding: utf-8 -*-\")\n     lines.append(f\"# Coded version of {identifier}\")\n     lines.append(\"# Produced by pydicom codify utility script\")\n \n     parser.add_argument(\n         \"outfile\",\n         nargs=\"?\",\n        type=argparse.FileType(\"w\", encoding=\"UTF-8\"),\n         help=(\n             \"Filename to write Python code to, if not specified then code is \"\n             \"written to stdout\""
  },
  {
    "instruction": "Add Tag and VR to the bulk data handling in `from_json`\nCurrently, if you convert back to a Dataset format from a JSON format, you MUST re-hydrate all of the bulk data URI's or you will loose the information.\r\n\r\nThis causes a problem if you just wish to use the Dataset's header (maybe to extract some data, or rearrange some data), because now you have to pay the cost of getting all the pixel data and then handling the pixel data again upon conversion back to JSON\r\n\r\n**Describe the solution you'd like**\r\nAdd the tag and the vr to the bulk data handler in `from_json` (this can be done in a backwards compatible way). This will allow the user to store the BulkDataURI's by tag in a map, return dummy data large enough to trigger the bulk handling when to_json is called next, and to use the map to convert back to the original URI's when bulk handling is triggered from to_json.\r\n\r\nI'm going to drop a PR tomorrow that does this in a fully backward compatible, non-breaking fashion.\r\n\n",
    "input": "         vr: str,\n         value: object,\n         value_key: Union[str, None],\n        bulk_data_uri_handler: Optional[Callable[[str], object]] = None\n     ) -> _DataElement:\n         \"\"\"Return a :class:`DataElement` from JSON.\n \n             Key of the data element that contains the value\n             (options: ``{\"Value\", \"InlineBinary\", \"BulkDataURI\"}``)\n         bulk_data_uri_handler: callable or None\n            Callable function that accepts the \"BulkDataURI\" of the JSON\n             representation of a data element and returns the actual value of\n             that data element (retrieved via DICOMweb WADO-RS)\n \n         DataElement\n         \"\"\"\n         # TODO: test wado-rs retrieve wrapper\n        converter = JsonDataElementConverter(dataset_class, tag, vr, value,\n                                             value_key, bulk_data_uri_handler)\n         elem_value = converter.get_element_values()\n         try:\n             return cls(tag=tag, value=elem_value, VR=vr)\ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n     def from_json(\n         cls: Type[_Dataset],\n         json_dataset: Union[Dict[str, bytes], str],\n        bulk_data_uri_handler: Optional[Callable[[bytes], object]] = None\n     ) -> _Dataset:\n         \"\"\"Add elements to the :class:`Dataset` from DICOM JSON format.\n \n             :class:`dict` or :class:`str` representing a DICOM Data Set\n             formatted based on the DICOM JSON Model.\n         bulk_data_uri_handler : callable, optional\n            Callable function that accepts the \"BulkDataURI\" of the JSON\n             representation of a data element and returns the actual value of\n             data element (retrieved via DICOMweb WADO-RS).\n \n     def to_json_dict(\n         self,\n         bulk_data_threshold: int = 1024,\n        bulk_data_element_handler: Optional[Callable[[DataElement], bytes]] = None  # noqa\n     ) -> _Dataset:\n         \"\"\"Return a dictionary representation of the :class:`Dataset`\n         conforming to the DICOM JSON Model as described in the DICOM\n     def to_json(\n         self,\n         bulk_data_threshold: int = 1024,\n        bulk_data_element_handler: Optional[Callable[[DataElement], bytes]] = None,  # noqa\n         dump_handler: Optional[Callable[[\"Dataset\"], str]] = None\n     ) -> str:\n         \"\"\"Return a JSON representation of the :class:`Dataset`.\ndiff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n \"\"\"Methods for converting Datasets and DataElements to/from json\"\"\"\n \n import base64\n import warnings\n \nfrom pydicom.valuerep import PersonName\n \n # Order of keys is significant!\n JSON_VALUE_KEYS = ('Value', 'BulkDataURI', 'InlineBinary',)\n     .. versionadded:: 1.4\n     \"\"\"\n \n    def __init__(self, dataset_class, tag, vr, value, value_key,\n                 bulk_data_uri_handler):\n         \"\"\"Create a new converter instance.\n \n         Parameters\n         dataset_class : dataset.Dataset derived class\n             Class used to create sequence items.\n         tag : BaseTag\n            The data element tag.\n         vr : str\n             The data element value representation.\n         value : list\n             Key of the data element that contains the value\n             (options: ``{\"Value\", \"InlineBinary\", \"BulkDataURI\"}``)\n         bulk_data_uri_handler: callable or None\n            Callable function that accepts the \"BulkDataURI\" of the JSON\n             representation of a data element and returns the actual value of\n             that data element (retrieved via DICOMweb WADO-RS)\n         \"\"\"\n         self.vr = vr\n         self.value = value\n         self.value_key = value_key\n        self.bulk_data_uri_handler = bulk_data_uri_handler\n \n     def get_element_values(self):\n         \"\"\"Return a the data element value or list of values.\n             if not isinstance(value, str):\n                 fmt = '\"{}\" of data element \"{}\" must be a string.'\n                 raise TypeError(fmt.format(self.value_key, self.tag))\n            if self.bulk_data_uri_handler is None:\n                 warnings.warn(\n                     'no bulk data URI handler provided for retrieval '\n                     'of value of data element \"{}\"'.format(self.tag)\n                 )\n                 return empty_value_for_VR(self.vr, raw=True)\n            return self.bulk_data_uri_handler(value)\n         return empty_value_for_VR(self.vr)\n \n     def get_regular_element_value(self, value):",
    "output": "         vr: str,\n         value: object,\n         value_key: Union[str, None],\n        bulk_data_uri_handler: Optional[\n            Union[\n                Callable[[BaseTag, str, str], object],\n                Callable[[str], object]\n            ]\n        ] = None\n     ) -> _DataElement:\n         \"\"\"Return a :class:`DataElement` from JSON.\n \n             Key of the data element that contains the value\n             (options: ``{\"Value\", \"InlineBinary\", \"BulkDataURI\"}``)\n         bulk_data_uri_handler: callable or None\n            Callable function that accepts either the tag, vr and \"BulkDataURI\"\n            or just the \"BulkDataURI\" of the JSON\n             representation of a data element and returns the actual value of\n             that data element (retrieved via DICOMweb WADO-RS)\n \n         DataElement\n         \"\"\"\n         # TODO: test wado-rs retrieve wrapper\n        converter = JsonDataElementConverter(\n            dataset_class,\n            tag,\n            vr,\n            value,\n            value_key,\n            bulk_data_uri_handler\n        )\n         elem_value = converter.get_element_values()\n         try:\n             return cls(tag=tag, value=elem_value, VR=vr)\ndiff --git a/pydicom/dataset.py b/pydicom/dataset.py\n     def from_json(\n         cls: Type[_Dataset],\n         json_dataset: Union[Dict[str, bytes], str],\n        bulk_data_uri_handler: Optional[\n            Union[\n                Callable[[BaseTag, str, str], object],\n                Callable[[str], object]\n            ]\n        ] = None\n     ) -> _Dataset:\n         \"\"\"Add elements to the :class:`Dataset` from DICOM JSON format.\n \n             :class:`dict` or :class:`str` representing a DICOM Data Set\n             formatted based on the DICOM JSON Model.\n         bulk_data_uri_handler : callable, optional\n            Callable function that accepts either the tag, vr and \"BulkDataURI\"\n            or just the \"BulkDataURI\" of the JSON\n             representation of a data element and returns the actual value of\n             data element (retrieved via DICOMweb WADO-RS).\n \n     def to_json_dict(\n         self,\n         bulk_data_threshold: int = 1024,\n        bulk_data_element_handler: Optional[Callable[[DataElement], str]] = None  # noqa\n     ) -> _Dataset:\n         \"\"\"Return a dictionary representation of the :class:`Dataset`\n         conforming to the DICOM JSON Model as described in the DICOM\n     def to_json(\n         self,\n         bulk_data_threshold: int = 1024,\n        bulk_data_element_handler: Optional[Callable[[DataElement], str]] = None,  # noqa\n         dump_handler: Optional[Callable[[\"Dataset\"], str]] = None\n     ) -> str:\n         \"\"\"Return a JSON representation of the :class:`Dataset`.\ndiff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py\n \"\"\"Methods for converting Datasets and DataElements to/from json\"\"\"\n \n import base64\nfrom inspect import signature\nimport inspect\nfrom typing import Callable, Optional, Union\n import warnings\n \nfrom pydicom.tag import BaseTag\n \n # Order of keys is significant!\n JSON_VALUE_KEYS = ('Value', 'BulkDataURI', 'InlineBinary',)\n     .. versionadded:: 1.4\n     \"\"\"\n \n    def __init__(\n        self,\n        dataset_class,\n        tag,\n        vr,\n        value,\n        value_key,\n        bulk_data_uri_handler: Optional[\n            Union[\n                Callable[[BaseTag, str, str], object],\n                Callable[[str], object]\n            ]\n        ] = None\n    ):\n         \"\"\"Create a new converter instance.\n \n         Parameters\n         dataset_class : dataset.Dataset derived class\n             Class used to create sequence items.\n         tag : BaseTag\n            The data element tag or int.\n         vr : str\n             The data element value representation.\n         value : list\n             Key of the data element that contains the value\n             (options: ``{\"Value\", \"InlineBinary\", \"BulkDataURI\"}``)\n         bulk_data_uri_handler: callable or None\n            Callable function that accepts either the tag, vr and \"BulkDataURI\"\n            or just the \"BulkDataURI\" of the JSON\n             representation of a data element and returns the actual value of\n             that data element (retrieved via DICOMweb WADO-RS)\n         \"\"\"\n         self.vr = vr\n         self.value = value\n         self.value_key = value_key\n        if (\n            bulk_data_uri_handler and\n            len(signature(bulk_data_uri_handler).parameters) == 1\n        ):\n            def wrapped_bulk_data_handler(tag, vr, value):\n                return bulk_data_uri_handler(value)\n            self.bulk_data_element_handler = wrapped_bulk_data_handler\n        else:\n            self.bulk_data_element_handler = bulk_data_uri_handler\n \n     def get_element_values(self):\n         \"\"\"Return a the data element value or list of values.\n             if not isinstance(value, str):\n                 fmt = '\"{}\" of data element \"{}\" must be a string.'\n                 raise TypeError(fmt.format(self.value_key, self.tag))\n            if self.bulk_data_element_handler is None:\n                 warnings.warn(\n                     'no bulk data URI handler provided for retrieval '\n                     'of value of data element \"{}\"'.format(self.tag)\n                 )\n                 return empty_value_for_VR(self.vr, raw=True)\n            return self.bulk_data_element_handler(self.tag, self.vr, value)\n         return empty_value_for_VR(self.vr)\n \n     def get_regular_element_value(self, value):"
  },
  {
    "instruction": "DA class is inconsistent\n**Describe the bug**\r\npydicom.valuerep.DA accepts strings or datetime.date objects - but DA objects created with datetime.date inputs are invalid. \r\n\r\n**Expected behavior**\r\nI would expect both of these expressions to generate the same output:\r\n```\r\nprint(f'DA(\"20201117\") => {DA(\"20201117\")}')\r\nprint(f'DA(date(2020, 11, 17)) => {DA(date(2020, 11, 17))}')\r\n```\r\nbut instead I get\r\n```\r\nDA(\"20201117\") => 20201117\r\nDA(date(2020, 11, 17)) => 2020-11-17\r\n```\r\nThe hyphens inserted into the output are not valid DICOM - see the DA description in [Table 6.2-1](http://dicom.nema.org/dicom/2013/output/chtml/part05/sect_6.2.html)\r\n\r\n**Steps To Reproduce**\r\nRun the following commands:\r\n```\r\nfrom pydicom.valuerep import DA\r\nfrom pydicom.dataset import Dataset\r\nfrom datetime import date, datetime\r\n\r\nprint(f'DA(\"20201117\") => {DA(\"20201117\")}')\r\nprint(f'DA(date(2020, 11, 17)) => {DA(date(2020, 11, 17))}')\r\n\r\n# 1. JSON serialization with formatted string works\r\nds = Dataset()\r\nds.ContentDate = '20201117'\r\njson_output = ds.to_json()\r\nprint(f'json_output works = {json_output}')\r\n\r\n# 2. JSON serialization with date object input is invalid.\r\nds = Dataset()\r\nds.ContentDate = str(DA(date(2020, 11, 17)))\r\njson_output = ds.to_json()\r\nprint(f'json_output with str(DA..) - invalid DICOM {json_output}')\r\n\r\n# 3. JSON serialization with date object fails\r\nds = Dataset()\r\nds.ContentDate = DA(date(2020, 11, 17))\r\n\r\n# Exception on this line: TypeError: Object of type DA is not JSON serializable\r\njson_output = ds.to_json()\r\n\r\n```\r\n\r\nI believe that all three approaches should work - but only the first is valid. The method signature on DA's `__new__` method accepts datetime.date objects. \r\n\r\n**Your environment**\r\n```\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.7-x86_64-i386-64bit\r\nPython       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.4\r\nPIL          | 8.0.1\r\n```\r\n\r\n\n",
    "input": "             self.original_string = val\n         elif isinstance(val, DA) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n \n class DT(_DateTimeBase, datetime.datetime):\n             self.original_string = val\n         elif isinstance(val, DT) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n \n class TM(_DateTimeBase, datetime.time):\n             self.original_string = val\n         elif isinstance(val, TM) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n     if platform.python_implementation() == \"PyPy\":\n         # Workaround for CPython/PyPy bug in time.__reduce_ex__()",
    "output": "             self.original_string = val\n         elif isinstance(val, DA) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n        elif isinstance(val, datetime.date):\n            self.original_string = f\"{val.year}{val.month:02}{val.day:02}\"\n \n \n class DT(_DateTimeBase, datetime.datetime):\n             self.original_string = val\n         elif isinstance(val, DT) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n        elif isinstance(val, datetime.datetime):\n            self.original_string = (\n                f\"{val.year:04}{val.month:02}{val.day:02}\"\n                f\"{val.hour:02}{val.minute:02}{val.second:02}\"\n            )\n            # milliseconds are seldom used, add them only if needed\n            if val.microsecond > 0:\n                self.original_string += f\".{val.microsecond:06}\"\n            if val.tzinfo is not None:\n                offset = val.tzinfo.utcoffset(val)\n                offset_min = offset.days * 24 * 60 + offset.seconds // 60\n                sign = \"+\" if offset_min >= 0 else \"-\"\n                offset_min = abs(offset_min)\n                self.original_string += (\n                    f\"{sign}{offset_min // 60:02}{offset_min % 60:02}\"\n                )\n \n \n class TM(_DateTimeBase, datetime.time):\n             self.original_string = val\n         elif isinstance(val, TM) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n        elif isinstance(val, datetime.time):\n            self.original_string = (\n                f\"{val.hour:02}{val.minute:02}{val.second:02}\"\n            )\n            # milliseconds are seldom used, add them only if needed\n            if val.microsecond > 0:\n                self.original_string += f\".{val.microsecond:06}\"\n \n     if platform.python_implementation() == \"PyPy\":\n         # Workaround for CPython/PyPy bug in time.__reduce_ex__()"
  },
  {
    "instruction": "Empty data elements with value representation SQ are set to None\n**Describe the bug**\r\nIn the current `master`, empty data elements are not read correctly from files. The attribute value is set to `None` instead of `[]`.\r\n\r\n**Expected behavior**\r\nCreate empty list `[]` for empty sequence, i.e., a sequence with zero items.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nimport pydicom\r\nds = pydicom.Dataset()\r\nds.AcquisitionContextSequence = []\r\nprint(ds)\r\nds.is_little_endian = True\r\nds.is_implicit_VR = True\r\nds.save_as('/tmp/test.dcm')\r\n\r\nreloaded_ds = pydicom.dcmread('/tmp/test.dcm', force=True)\r\nprint(reloaded_ds)\r\n```\r\nThis prints:\r\n```\r\n(0040, 0555)  Acquisition Context Sequence   0 item(s) ----\r\n...\r\nTypeError: With tag (0040, 0555) got exception: object of type 'NoneType' has no len()\r\nTraceback (most recent call last):\r\n  File \"/private/tmp/pydicom/pydicom/tag.py\", line 30, in tag_in_exception\r\n    yield\r\n  File \"/private/tmp/pydicom/pydicom/dataset.py\", line 1599, in _pretty_str\r\n    len(data_element.value)))\r\nTypeError: object of type 'NoneType' has no len()\r\n```\r\n\r\n**Your environment**\r\n```\r\nDarwin-18.6.0-x86_64-i386-64bit\r\nPython  3.7.3 (default, Mar 27 2019, 09:23:15)\r\n[Clang 10.0.1 (clang-1001.0.46.3)]\r\npydicom  1.4.0.dev0\r\n```\n",
    "input": " \"\"\"\n \n use_none_as_empty_text_VR_value = False\n\"\"\" If ``True``, the value of decoded empty data element is always ``None``.\nIf ``False`` (the default), the value of an empty data element with\na text VR is an empty string, for all other VRs it is also ``None``.\n Note that the default of this value will change to ``True`` in version 2.0.\n \"\"\"\n \ndiff --git a/pydicom/dataelem.py b/pydicom/dataelem.py\n \n     The behavior of this property depends on the setting of\n     :attr:`config.use_none_as_empty_value`. If that is set to ``True``,\n    an empty value is always represented by ``None``, otherwise it depends\n    on `VR`. For text VRs (this includes 'AE', 'AS', 'CS', 'DA', 'DT', 'LO',\n    'LT', 'PN', 'SH', 'ST', 'TM', 'UC', 'UI', 'UR' and 'UT') an empty string\n    is used as empty value representation, for all other VRs, ``None``.\n     Note that this is used only if decoding the element - it is always\n     possible to set the value to another empty value representation,\n     which will be preserved during the element object lifetime.\n \n     Returns\n     -------\n    str or bytes or None\n         The value a data element with `VR` is assigned on decoding\n         if it is empty.\n     \"\"\"\n     if config.use_none_as_empty_text_VR_value:\n         return None\n     if VR in ('AE', 'AS', 'CS', 'DA', 'DT', 'LO', 'LT',",
    "output": " \"\"\"\n \n use_none_as_empty_text_VR_value = False\n\"\"\" If ``True``, the value of a decoded empty data element with\na text VR is ``None``, otherwise (the default), it is is an empty string.\nFor all other VRs the behavior does not change - the value is en empty\nlist for VR 'SQ' and ``None`` for all other VRs.\n Note that the default of this value will change to ``True`` in version 2.0.\n \"\"\"\n \ndiff --git a/pydicom/dataelem.py b/pydicom/dataelem.py\n \n     The behavior of this property depends on the setting of\n     :attr:`config.use_none_as_empty_value`. If that is set to ``True``,\n    an empty value is represented by ``None`` (except for VR 'SQ'), otherwise\n    it depends on `VR`. For text VRs (this includes 'AE', 'AS', 'CS', 'DA',\n    'DT', 'LO', 'LT', 'PN', 'SH', 'ST', 'TM', 'UC', 'UI', 'UR' and 'UT') an\n    empty string is used as empty value representation, for all other VRs\n    except 'SQ', ``None``. For empty sequence values (VR 'SQ') an empty list\n    is used in all cases.\n     Note that this is used only if decoding the element - it is always\n     possible to set the value to another empty value representation,\n     which will be preserved during the element object lifetime.\n \n     Returns\n     -------\n    str or bytes or None or list\n         The value a data element with `VR` is assigned on decoding\n         if it is empty.\n     \"\"\"\n    if VR == 'SQ':\n        return []\n     if config.use_none_as_empty_text_VR_value:\n         return None\n     if VR in ('AE', 'AS', 'CS', 'DA', 'DT', 'LO', 'LT',"
  },
  {
    "instruction": "Pickling/unpickling timezone in DT does not work\n**Describe the bug**\r\n\r\nThe following tests fail because the timezone is not set in the unpickled `DT`:\r\n```py\r\n    def test_pickling_with_timezone():\r\n        dt = pydicom.valuerep.DT(\"19111213212123-0630\")\r\n        loaded_dt = pickle.loads(pickle.dumps(dt))\r\n        assert dt == loaded_dt\r\n\r\n    def test_pickling_dt_from_datetime_with_timezone():\r\n        tz_info = timezone(timedelta(seconds=-23400), '-0630')\r\n        dt_object = datetime(2022, 12, 31, 23, 59, 59, 42, tzinfo=tz_info)\r\n        dt = pydicom.valuerep.DT(dt_object)\r\n        loaded_dt = pickle.loads(pickle.dumps(dt))\r\n        assert dt == loaded_dt\r\n```\r\n\r\nThis is a spin-off of PR #1365, see [this comment](https://github.com/pydicom/pydicom/pull/1365#issuecomment-829544827).\n",
    "input": " \"\"\"Special classes for DICOM value representations (VR)\"\"\"\n \n import datetime\nfrom decimal import Decimal\nfrom math import floor, isfinite, log10\nimport platform\n import re\n import sys\n from typing import (\n    TypeVar, Type, Tuple, Optional, List, Dict, Union, Any, Generator, AnyStr,\n    Callable, Iterator, overload\n )\nfrom typing import Sequence as SequenceType\nimport warnings\n \n # don't import datetime_conversion directly\n from pydicom import config\n from pydicom.multival import MultiValue\nfrom pydicom.uid import UID\n\n \n # Types\n _T = TypeVar('_T')\n \n     Note that the :class:`datetime.date` base class is immutable.\n     \"\"\"\n    def __new__(\n        cls: Type[_DA], val: Union[None, str, _DA, datetime.date]\n    ) -> Optional[_DA]:\n         \"\"\"Create an instance of DA object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n        Parameters\n        ----------\n        val : str\n            A string conformant to the DA definition in the DICOM Standard,\n            Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n        if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty date\n             return super().__new__(cls, val.year, val.month, val.day)\n \n         try:\n            return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DA' object\"\n             ) from exc\n \n    def __init__(self, val: Union[str, _DA, datetime.date]) -> None:\n         \"\"\"Create a new **DA** element value.\"\"\"\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DA) and hasattr(val, 'original_string'):\n             name=value\n         )\n \n    def __new__(\n        cls: Type[_DT], val: Union[None, str, _DT, datetime.datetime]\n    ) -> Optional[_DT]:\n         \"\"\"Create an instance of DT object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n        Parameters\n        ----------\n        val : str\n            A string conformant to the DT definition in the DICOM Standard,\n            Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n        if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None\n             )\n \n         try:\n            return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DT' object\"\n             ) from exc\n \n    def __init__(self, val: Union[str, _DT, datetime.datetime]) -> None:\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DT) and hasattr(val, 'original_string'):\n         r\"(?(7)(\\.(?P<ms>([0-9]{1,6})?))?))$\"\n     )\n \n    def __new__(\n        cls: Type[_TM], val: Union[None, str, _TM, datetime.time]\n    ) -> Optional[_TM]:\n         \"\"\"Create an instance of TM object from a string.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n        Parameters\n        ----------\n        val : str\n            A string conformant to the TM definition in the DICOM Standard,\n            Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n        if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty time\n             )\n \n         try:\n            return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'TM' object\"\n             ) from exc\n \n    def __init__(self, val: Union[str, _TM, datetime.time]) -> None:\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, TM) and hasattr(val, 'original_string'):\n             if val.microsecond > 0:\n                 self.original_string += f\".{val.microsecond:06}\"\n \n    if platform.python_implementation() == \"PyPy\":\n        # Workaround for CPython/PyPy bug in time.__reduce_ex__()\n        #   caused by returning (time, ...) rather than (self.__class__, ...)\n        def __reduce_ex__(self, protocol: int) -> Union[str, Tuple[Any, ...]]:\n            return (\n                self.__class__,\n                super()._getstate(protocol),\n                self.__getstate__()\n            )\n\n \n # Regex to match strings that represent valid DICOM decimal strings (DS)\n _DS_REGEX = re.compile(r'\\s*[\\+\\-]?\\d+(\\.\\d+)?([eE][\\+\\-]?\\d+)?\\s*$')",
    "output": " \"\"\"Special classes for DICOM value representations (VR)\"\"\"\n \n import datetime\n import re\n import sys\nimport warnings\nfrom decimal import Decimal\nfrom math import floor, isfinite, log10\nfrom typing import Sequence as SequenceType\n from typing import (\n    TypeVar, Type, Tuple, Optional, List, Dict, Union, Any, Generator, Callable\n )\n \n # don't import datetime_conversion directly\n from pydicom import config\n from pydicom.multival import MultiValue\n \n # Types\n _T = TypeVar('_T')\n \n     Note that the :class:`datetime.date` base class is immutable.\n     \"\"\"\n    def __new__(cls: Type[_DA], *args, **kwargs) -> Optional[_DA]:\n         \"\"\"Create an instance of DA object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n        The arguments (``*args`` and ``**kwargs``) are either the ones\n        inherited from :class:`datetime.date`, or the first argument is\n        a string conformant to the DA definition in the DICOM Standard,\n        Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`,\n        or it is a :class:`datetime.date` object, or an object of type\n        :class:`~pydicom.valuerep.DA`.\n         \"\"\"\n        if not args or args[0] is None:\n             return None\n \n        val = args[0]\n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty date\n             return super().__new__(cls, val.year, val.month, val.day)\n \n         try:\n            return super().__new__(cls, *args, **kwargs)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DA' object\"\n             ) from exc\n \n    def __init__(self, *args, **kwargs) -> None:\n         \"\"\"Create a new **DA** element value.\"\"\"\n        val = args[0]\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DA) and hasattr(val, 'original_string'):\n             name=value\n         )\n \n    def __new__(cls: Type[_DT], *args, **kwargs) -> Optional[_DT]:\n         \"\"\"Create an instance of DT object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n        The arguments (``*args`` and ``**kwargs``) are either the ones\n        inherited from :class:`datetime.datetime`, or the first argument is\n        a string conformant to the DT definition in the DICOM Standard,\n        Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`,\n        or it is a :class:`datetime.datetime` object, or an object of type\n        :class:`~pydicom.valuerep.DT`.\n         \"\"\"\n        if not args or args[0] is None:\n             return None\n \n        val = args[0]\n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None\n             )\n \n         try:\n            return super().__new__(cls, *args, **kwargs)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DT' object\"\n             ) from exc\n \n    def __init__(self, *args, **kwargs) -> None:\n        \"\"\"Create a new **DT** element value.\"\"\"\n        val = args[0]\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DT) and hasattr(val, 'original_string'):\n         r\"(?(7)(\\.(?P<ms>([0-9]{1,6})?))?))$\"\n     )\n \n    def __new__(cls: Type[_TM], *args, **kwargs) -> Optional[_TM]:\n         \"\"\"Create an instance of TM object from a string.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n        The arguments (``*args`` and ``**kwargs``) are either the ones\n        inherited from :class:`datetime.time`, or the first argument is\n        a string conformant to the TM definition in the DICOM Standard,\n        Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`,\n        or it is a :class:`datetime.time` object, or an object of type\n        :class:`~pydicom.valuerep.TM`.\n         \"\"\"\n        if not args or args[0] is None:\n             return None\n \n        val = args[0]\n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty time\n             )\n \n         try:\n            return super().__new__(cls, *args, **kwargs)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'TM' object\"\n             ) from exc\n \n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__()\n        val = args[0]\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, TM) and hasattr(val, 'original_string'):\n             if val.microsecond > 0:\n                 self.original_string += f\".{val.microsecond:06}\"\n \n \n # Regex to match strings that represent valid DICOM decimal strings (DS)\n _DS_REGEX = re.compile(r'\\s*[\\+\\-]?\\d+(\\.\\d+)?([eE][\\+\\-]?\\d+)?\\s*$')"
  },
  {
    "instruction": "Strings with Value Representation DS are too long\n**Describe the bug**\r\nStrings of Value Representation DS are restricted to a maximum length of 16 bytes according to [Part 5 Section 6.2](http://dicom.nema.org/medical/dicom/current/output/chtml/part05/sect_6.2.html#para_15754884-9ca2-4b12-9368-d66f32bc8ce1), but `pydicom.valuerep.DS` may represent numbers with more than 16 bytes.\r\n\r\n**Expected behavior**\r\n`pydicom.valuerep.DS` should create a string of maximum length 16, when passed a fixed point number with many decimals.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nlen(str(pydicom.valuerep.DS(3.14159265358979323846264338327950288419716939937510582097)).encode('utf-8'))\r\nlen(str(pydicom.valuerep.DS(\"3.14159265358979323846264338327950288419716939937510582097\")).encode('utf-8'))\r\n```\r\nreturns `17` and `58`, respectively, instead of `16`.\r\n\r\n**Your environment**\r\n```\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.6-x86_64-i386-64bit\r\nPython       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\r\npydicom      | 2.0.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.4\r\nPIL          | 8.0.1\r\n```\n",
    "input": " \n import datetime\n from decimal import Decimal\n import platform\n import re\n import sys\n             )\n \n \n class DSfloat(float):\n     \"\"\"Store value for an element with VR **DS** as :class:`float`.\n \n     If constructed from an empty string, return the empty string,\n     not an instance of this class.\n \n     \"\"\"\n     def __init__(\n        self, val: Union[str, int, float, Decimal]\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later.\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_attribute:\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n        if hasattr(self, 'original_string'):\n             return self.original_string\n \n         # Issue #937 (Python 3.8 compatibility)\n         return repr(self)[1:-1]\n \n     def __repr__(self) -> str:\n         return f'\"{super().__repr__()}\"'\n \n \n class DSdecimal(Decimal):\n     \"\"\"Store value for an element with VR **DS** as :class:`decimal.Decimal`.\n \n     Notes\n     -----\n     If constructed from an empty string, returns the empty string, not an\n     instance of this class.\n     \"\"\"\n     def __new__(\n         cls: Type[_DSdecimal],\n        val: Union[str, int, float, Decimal]\n     ) -> Optional[_DSdecimal]:\n         \"\"\"Create an instance of DS object, or return a blank string if one is\n         passed in, e.g. from a type 2 DICOM blank value.\n                 return None\n \n         val = super().__new__(cls, val)\n        if len(str(val)) > 16 and config.enforce_valid_values:\n            raise OverflowError(\n                \"Values for elements with a VR of 'DS' values must be <= 16 \"\n                \"characters long. Use a smaller string, set \"\n                \"'config.enforce_valid_values' to False to override the \"\n                \"length check, or use 'Decimal.quantize()' and initialize \"\n                \"with a 'Decimal' instance.\"\n            )\n \n         return val\n \n     def __init__(\n        self, val: Union[str, int, float, Decimal]\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later. E.g. if set ``'1.23e2'``, :class:`~decimal.Decimal` would\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_str:\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n         has_str = hasattr(self, 'original_string')\n         if has_str and len(self.original_string) <= 16:\n         return super().__str__()\n \n     def __repr__(self) -> str:\n         return f'\"{str(self)}\"'\n \n \n \n \n def DS(\n    val: Union[None, str, int, float, Decimal]\n ) -> Union[None, str, DSfloat, DSdecimal]:\n     \"\"\"Factory function for creating DS class instances.\n \n     if val == '' or val is None:\n         return val\n \n    return DSclass(val)\n \n \n class IS(int):",
    "output": " \n import datetime\n from decimal import Decimal\nfrom math import floor, isfinite, log10\n import platform\n import re\n import sys\n             )\n \n \n# Regex to match strings that represent valid DICOM decimal strings (DS)\n_DS_REGEX = re.compile(r'\\s*[\\+\\-]?\\d+(\\.\\d+)?([eE][\\+\\-]?\\d+)?\\s*$')\n\n\ndef is_valid_ds(s: str) -> bool:\n    \"\"\"Check whether this string is a valid decimal string.\n\n    Valid decimal strings must be 16 characters or fewer, and contain only\n    characters from a limited set.\n\n    Parameters\n    ----------\n    s: str\n        String to test.\n\n    Returns\n    -------\n    bool\n        True if the string is a valid decimal string. Otherwise False.\n    \"\"\"\n    # Check that the length is within the limits\n    if len(s) > 16:\n        return False\n\n    return _DS_REGEX.match(s) is not None\n\n\ndef format_number_as_ds(val: Union[float, Decimal]) -> str:\n    \"\"\"Truncate a float's representation to give a valid Decimal String (DS).\n\n    DICOM's decimal string (DS) representation is limited to strings with 16\n    characters and a limited set of characters. This function represents a\n    float that satisfies these constraints while retaining as much\n    precision as possible. Some floats are represented using scientific\n    notation to make more efficient use of the limited number of characters.\n\n    Note that this will incur a loss of precision if the number cannot be\n    represented with 16 characters. Furthermore, non-finite floats (infs and\n    nans) cannot be represented as decimal strings and will cause an error to\n    be raised.\n\n    Parameters\n    ----------\n    val: Union[float, Decimal]\n        The floating point value whose representation is required.\n\n    Returns\n    -------\n    str\n        String representation of the float satisfying the constraints of the\n        decimal string representation.\n\n    Raises\n    ------\n    ValueError\n        If val does not represent a finite value\n\n    \"\"\"\n    if not isinstance(val, (float, Decimal)):\n        raise TypeError(\"'val' must be of type float or decimal.Decimal\")\n    if not isfinite(val):\n        raise ValueError(\n            \"Cannot encode non-finite floats as DICOM decimal strings. \"\n            f\"Got '{val}'\"\n        )\n\n    valstr = str(val)\n\n    # In the simple case, the default python string representation\n    # will do\n    if len(valstr) <= 16:\n        return valstr\n\n    # Decide whether to use scientific notation\n    logval = log10(abs(val))\n\n    # Characters needed for '-' at start\n    sign_chars = 1 if val < 0.0 else 0\n\n    # Numbers larger than 1e14 cannot be correctly represented by truncating\n    # their string representations to 16 chars, e.g pi * 10^13 would become\n    # '314159265358979.', which may not be universally understood. This limit\n    # is 1e13 for negative numbers because of the minus sign.\n    # For negative exponents, the point of equal precision between scientific\n    # and standard notation is 1e-4 e.g. '0.00031415926535' and\n    # '3.1415926535e-04' are both 16 chars\n    use_scientific = logval < -4 or logval >= (14 - sign_chars)\n\n    if use_scientific:\n        # In principle, we could have a number where the exponent\n        # needs three digits to be represented (bigger than this cannot be\n        # represented by floats). Due to floating point limitations\n        # this is best checked for by doing the string conversion\n        remaining_chars = 10 - sign_chars\n        trunc_str = f'%.{remaining_chars}e' % val\n        if len(trunc_str) > 16:\n            trunc_str = f'%.{remaining_chars - 1}e' % val\n        return trunc_str\n    else:\n        if logval >= 1.0:\n            # chars remaining for digits after sign, digits left of '.' and '.'\n            remaining_chars = 14 - sign_chars - int(floor(logval))\n        else:\n            remaining_chars = 14 - sign_chars\n        return f'%.{remaining_chars}f' % val\n\n\n class DSfloat(float):\n     \"\"\"Store value for an element with VR **DS** as :class:`float`.\n \n     If constructed from an empty string, return the empty string,\n     not an instance of this class.\n \n    Parameters\n    ----------\n    val: Union[str, int, float, Decimal]\n        Value to store as a DS.\n    auto_format: bool\n        If True, automatically format the string representation of this\n        number to ensure it satisfies the constraints in the DICOM standard.\n        Note that this will lead to loss of precision for some numbers.\n\n     \"\"\"\n    def __new__(\n            cls,\n            val: Union[str, int, float, Decimal],\n            auto_format: bool = False\n    ) -> [_DSfloat]:\n        return super().__new__(cls, val)\n\n     def __init__(\n        self, val: Union[str, int, float, Decimal],\n        auto_format: bool = False\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later.\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_attribute:\n             self.original_string = val.original_string\n \n        self.auto_format = auto_format\n        if self.auto_format:\n            # If auto_format is True, keep the float value the same, but change\n            # the string representation stored in original_string if necessary\n            if hasattr(self, 'original_string'):\n                if not is_valid_ds(self.original_string):\n                    self.original_string = format_number_as_ds(\n                        float(self.original_string)\n                    )\n            else:\n                self.original_string = format_number_as_ds(self)\n\n        if config.enforce_valid_values and not self.auto_format:\n            if len(repr(self).strip('\"')) > 16:\n                raise OverflowError(\n                    \"Values for elements with a VR of 'DS' must be <= 16 \"\n                    \"characters long, but the float provided requires > 16 \"\n                    \"characters to be accurately represented. Use a smaller \"\n                    \"string, set 'config.enforce_valid_values' to False to \"\n                    \"override the length check, or explicitly construct a DS \"\n                    \"object with 'auto_format' set to True\"\n                )\n            if not is_valid_ds(repr(self).strip('\"')):\n                # This will catch nan and inf\n                raise ValueError(\n                    f'Value \"{str(self)}\" is not valid for elements with a VR '\n                    'of DS'\n                )\n\n     def __str__(self) -> str:\n        if hasattr(self, 'original_string') and not self.auto_format:\n             return self.original_string\n \n         # Issue #937 (Python 3.8 compatibility)\n         return repr(self)[1:-1]\n \n     def __repr__(self) -> str:\n        if self.auto_format and hasattr(self, 'original_string'):\n            return f'\"{self.original_string}\"'\n         return f'\"{super().__repr__()}\"'\n \n \n class DSdecimal(Decimal):\n     \"\"\"Store value for an element with VR **DS** as :class:`decimal.Decimal`.\n \n    Parameters\n    ----------\n    val: Union[str, int, float, Decimal]\n        Value to store as a DS.\n    auto_format: bool\n        If True, automatically format the string representation of this\n        number to ensure it satisfies the constraints in the DICOM standard.\n        Note that this will lead to loss of precision for some numbers.\n\n     Notes\n     -----\n     If constructed from an empty string, returns the empty string, not an\n     instance of this class.\n\n     \"\"\"\n     def __new__(\n         cls: Type[_DSdecimal],\n        val: Union[str, int, float, Decimal],\n        auto_format: bool = False\n     ) -> Optional[_DSdecimal]:\n         \"\"\"Create an instance of DS object, or return a blank string if one is\n         passed in, e.g. from a type 2 DICOM blank value.\n                 return None\n \n         val = super().__new__(cls, val)\n \n         return val\n \n     def __init__(\n        self,\n        val: Union[str, int, float, Decimal],\n        auto_format: bool = False\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later. E.g. if set ``'1.23e2'``, :class:`~decimal.Decimal` would\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_str:\n             self.original_string = val.original_string\n \n        self.auto_format = auto_format\n        if self.auto_format:\n            # If auto_format is True, keep the float value the same, but change\n            # the string representation stored in original_string if necessary\n            if hasattr(self, 'original_string'):\n                if not is_valid_ds(self.original_string):\n                    self.original_string = format_number_as_ds(\n                        float(self.original_string)\n                    )\n            else:\n                self.original_string = format_number_as_ds(self)\n\n        if config.enforce_valid_values:\n            if len(repr(self).strip('\"')) > 16:\n                raise OverflowError(\n                    \"Values for elements with a VR of 'DS' values must be \"\n                    \"<= 16 characters long. Use a smaller string, set \"\n                    \"'config.enforce_valid_values' to False to override the \"\n                    \"length check, use 'Decimal.quantize()' and initialize \"\n                    \"with a 'Decimal' instance, or explicitly construct a DS \"\n                    \"instance with 'auto_format' set to True\"\n                )\n            if not is_valid_ds(repr(self).strip('\"')):\n                # This will catch nan and inf\n                raise ValueError(\n                    f'Value \"{str(self)}\" is not valid for elements with a VR '\n                    'of DS'\n                )\n\n     def __str__(self) -> str:\n         has_str = hasattr(self, 'original_string')\n         if has_str and len(self.original_string) <= 16:\n         return super().__str__()\n \n     def __repr__(self) -> str:\n        if self.auto_format and hasattr(self, 'original_string'):\n            return f'\"{self.original_string}\"'\n         return f'\"{str(self)}\"'\n \n \n \n \n def DS(\n    val: Union[None, str, int, float, Decimal],\n    auto_format: bool = False\n ) -> Union[None, str, DSfloat, DSdecimal]:\n     \"\"\"Factory function for creating DS class instances.\n \n     if val == '' or val is None:\n         return val\n \n    return DSclass(val, auto_format=auto_format)\n \n \n class IS(int):"
  },
  {
    "instruction": "Encoding to ISO 2022 IR 159 doesn't work\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nEncoding to ISO 2022 IR 159 doesn't work even if 'ISO 2022 IR 159' is passed to pydicom.charset.convert_encodings.\r\n\r\n#### Steps/Code to Reproduce\r\nISO 2022 IR 159 is designed as supplement characters to ISO 2022 IR 87. So these characters are not frequent use. But person name sometimes contains them. In the following example, the letter of \"\u9dd7\" is only in ISO 2022 IR 159. But we cannot encode them correctly. \r\n\r\n```\r\nimport pydicom\r\n\r\njapanese_pn = u\"Mori^Ogai=\u68ee^\u9dd7\u5916=\u3082\u308a^\u304a\u3046\u304c\u3044\"\r\nspecific_character_sets = [\"ISO 2022 IR 6\", \"ISO 2022 IR 87\", \"ISO 2022 IR 159\"]\r\nexpect_encoded = (\r\n    b\"\\x4d\\x6f\\x72\\x69\\x5e\\x4f\\x67\\x61\\x69\\x3d\\x1b\\x24\\x42\\x3f\"\r\n    b\"\\x39\\x1b\\x28\\x42\\x5e\\x1b\\x24\\x28\\x44\\x6c\\x3f\\x1b\\x24\\x42\"\r\n    b\"\\x33\\x30\\x1b\\x28\\x42\\x3d\\x1b\\x24\\x42\\x24\\x62\\x24\\x6a\\x1b\"\r\n    b\"\\x28\\x42\\x5e\\x1b\\x24\\x42\\x24\\x2a\\x24\\x26\\x24\\x2c\\x24\\x24\"\r\n    b\"\\x1b\\x28\\x42\"\r\n)\r\n\r\npython_encodings = pydicom.charset.convert_encodings(specific_character_sets)\r\nactual_encoded = pydicom.charset.encode_string(japanese_pn, python_encodings)\r\n\r\nprint(\"actual:{}\".format(actual_encoded))\r\nprint(\"expect:{}\".format(expect_encoded))\r\n```\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\n```\r\nb'Mori^Ogai=\\x1b$B?9\\x1b(B^\\x1b$(Dl?\\x1b$B30\\x1b(B=\\x1b$B$b$j\\x1b(B^\\x1b$B$*$&$,$$\\x1b(B'\r\n```\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\n```\r\nb'Mori^Ogai=?^??=??^????'\r\n```\r\n\r\nAnd the followin exception occurs.\r\n\r\n```\r\n/PATH/TO/MY/PYTHON/PACKAGES/pydicom/charset.py:488: UserWarning: Failed to encode value with encodings: iso8859, iso2022_jp, iso-2022-jp - using replacement characters in encoded string\r\n  .format(', '.join(encodings)))\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n```\r\nLinux-4.15.0-55-generic-x86_64-with-debian-buster-sid\r\nPython 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)\r\n[GCC 7.3.0]\r\npydicom 1.3.0\r\n```\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "input": "     'ISO 2022 IR 144': 'iso_ir_144',\n     'ISO 2022 IR 148': 'iso_ir_148',\n     'ISO 2022 IR 149': 'euc_kr',\n    'ISO 2022 IR 159': 'iso-2022-jp',\n     'ISO 2022 IR 166': 'iso_ir_166',\n     'ISO 2022 IR 58': 'iso_ir_58',\n     'ISO_IR 192': 'UTF8',  # from Chinese example, 2008 PS3.5 Annex J p1-4\n     ESC + b'-M': 'iso_ir_148',\n     ESC + b'-T': 'iso_ir_166',\n     ESC + b'$)C': 'euc_kr',\n    ESC + b'$(D': 'iso-2022-jp',\n     ESC + b'$)A': 'iso_ir_58',\n }\n \n # To decode them, the escape sequence shall be preserved in the input byte\n # string, and will be removed during decoding by Python.\n handled_encodings = ('iso2022_jp',\n                     'iso-2022-jp',\n                      'iso_ir_58')\n \n \n \n     return encoded\n \n\n def _encode_to_jis_x_0208(value, errors='strict'):\n    \"\"\"Convert a unicode string into JIS X 0208 byte string using iso2022_jp\n    encodings.\n     The escape sequence which is located at the end of the encoded value has\n     to vary depending on the value 1 of SpecificCharacterSet. So we have to\n     trim it and append the correct escape sequence manually.\n     ----------\n     value : text type\n         The unicode string as presented to the user.\n     errors : str\n         The behavior of a character which could not be encoded. This value\n         is passed to errors argument of str.encode().\n     -------\n     byte string\n         The encoded string. If some characters in value could not be encoded to\n        JIS X 0208, it depends on the behavior of iso2022_jp encoder.\n \n     Raises\n     ------\n     UnicodeEncodeError\n         If errors is set to 'strict' and `value` could not be encoded with\n        JIS X 0208.\n     \"\"\"\n \n     # If errors is not strict, this function is used as fallback.\n     # So keep the tail escape sequence of encoded for backward compatibility.\n     if errors != 'strict':\n        return value.encode('iso2022_jp', errors=errors)\n \n    Encoder = codecs.getincrementalencoder('iso2022-jp')\n     encoder = Encoder()\n \n     encoded = encoder.encode(value[0])\n    if encoded[:3] != ENCODINGS_TO_CODES['iso2022_jp']:\n         raise UnicodeEncodeError(\n            'iso2022_jp', value, 0, len(value),\n            'Given character is out of ISO IR 87')\n \n     for i, c in enumerate(value[1:], 1):\n         try:\n             e.start = i\n             e.end = len(value)\n             raise e\n        if b[:3] == ENCODINGS_TO_CODES['iso8859']:\n             raise UnicodeEncodeError(\n                'iso2022_jp', value, i, len(value),\n                'Given character is out of ISO IR 87')\n         encoded += b\n     return encoded\n \n \n \n # These encodings need escape sequence to handle alphanumeric characters.\nneed_tail_escape_sequence_encodings = ('iso2022_jp', 'iso-2022-jp')\n \n \n custom_encoders = {\n     'shift_jis': _encode_to_jis_x_0201,\n     'iso2022_jp': _encode_to_jis_x_0208,\n    'iso-2022-jp': _encode_to_jis_x_0208\n }\n \n \n     # unencoded_part is empty - we are done, return the encoded string\n     if best_encoding in need_tail_escape_sequence_encodings:\n         encoded += _get_escape_sequence_for_encoding(encodings[0])\n    return encoded\n \n \n def _encode_string_impl(value, encoding, errors='strict'):",
    "output": "     'ISO 2022 IR 144': 'iso_ir_144',\n     'ISO 2022 IR 148': 'iso_ir_148',\n     'ISO 2022 IR 149': 'euc_kr',\n    'ISO 2022 IR 159': 'iso2022_jp_2',\n     'ISO 2022 IR 166': 'iso_ir_166',\n     'ISO 2022 IR 58': 'iso_ir_58',\n     'ISO_IR 192': 'UTF8',  # from Chinese example, 2008 PS3.5 Annex J p1-4\n     ESC + b'-M': 'iso_ir_148',\n     ESC + b'-T': 'iso_ir_166',\n     ESC + b'$)C': 'euc_kr',\n    ESC + b'$(D': 'iso2022_jp_2',\n     ESC + b'$)A': 'iso_ir_58',\n }\n \n # To decode them, the escape sequence shall be preserved in the input byte\n # string, and will be removed during decoding by Python.\n handled_encodings = ('iso2022_jp',\n                     'iso2022_jp_2',\n                      'iso_ir_58')\n \n \n \n     return encoded\n \n def _encode_to_jis_x_0208(value, errors='strict'):\n    \"\"\"Convert a unicode string into JIS X 0208 byte string.\"\"\"\n    return _encode_to_given_charset(value, 'ISO 2022 IR 87', errors=errors)\n\n\ndef _encode_to_jis_x_0212(value, errors='strict'):\n    \"\"\"Convert a unicode string into JIS X 0212 byte string.\"\"\"\n    return _encode_to_given_charset(value, 'ISO 2022 IR 159', errors=errors)\n\n\ndef _encode_to_given_charset(value, character_set, errors='strict'):\n    \"\"\"Convert a unicode string into given character set.\n     The escape sequence which is located at the end of the encoded value has\n     to vary depending on the value 1 of SpecificCharacterSet. So we have to\n     trim it and append the correct escape sequence manually.\n     ----------\n     value : text type\n         The unicode string as presented to the user.\n    character_set: str:\n        Character set for result.\n     errors : str\n         The behavior of a character which could not be encoded. This value\n         is passed to errors argument of str.encode().\n     -------\n     byte string\n         The encoded string. If some characters in value could not be encoded to\n        given character_set, it depends on the behavior of corresponding python\n        encoder.\n \n     Raises\n     ------\n     UnicodeEncodeError\n         If errors is set to 'strict' and `value` could not be encoded with\n        given character_set.\n     \"\"\"\n \n    encoding = python_encoding[character_set]\n     # If errors is not strict, this function is used as fallback.\n     # So keep the tail escape sequence of encoded for backward compatibility.\n     if errors != 'strict':\n        return value.encode(encoding, errors=errors)\n \n    Encoder = codecs.getincrementalencoder(encoding)\n     encoder = Encoder()\n \n     encoded = encoder.encode(value[0])\n    if not encoded.startswith(ENCODINGS_TO_CODES[encoding]):\n         raise UnicodeEncodeError(\n            encoding, value, 0, len(value),\n            'Given character is out of {}'.format(character_set))\n \n     for i, c in enumerate(value[1:], 1):\n         try:\n             e.start = i\n             e.end = len(value)\n             raise e\n        if b[:1] == ESC:\n             raise UnicodeEncodeError(\n                encoding, value, i, len(value),\n                'Given character is out of {}'.format(character_set))\n         encoded += b\n     return encoded\n \n \n \n # These encodings need escape sequence to handle alphanumeric characters.\nneed_tail_escape_sequence_encodings = ('iso2022_jp', 'iso2022_jp_2')\n \n \n custom_encoders = {\n     'shift_jis': _encode_to_jis_x_0201,\n     'iso2022_jp': _encode_to_jis_x_0208,\n    'iso2022_jp_2': _encode_to_jis_x_0212\n }\n \n \n     # unencoded_part is empty - we are done, return the encoded string\n     if best_encoding in need_tail_escape_sequence_encodings:\n         encoded += _get_escape_sequence_for_encoding(encodings[0])\n    return bytes(encoded)\n \n \n def _encode_string_impl(value, encoding, errors='strict'):"
  },
  {
    "instruction": "Wrong encoding occurs if the value 1 of SpecificCharacterSets is ISO 2022 IR 13.\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nAll Japanese characters are encoded into shift_jis if the value 1 of SpecificCharacterSets (0x0008, 0x0005) is  ISO 2022 IR 13.\r\n\r\n#### Steps/Code to Reproduce\r\nThe japanese_pn and expect_encoded in the following code came from \r\n[H.3.2 Value 1 of Attribute Specific Character Set (0008,0005) is ISO 2022 IR 13.](http://dicom.nema.org/medical/dicom/2015b/output/chtml/part05/sect_H.3.2.html)\r\n\r\n```py\r\nimport pydicom\r\n\r\njapanese_pn = u\"\uff94\uff8f\uff80\uff9e^\uff80\uff9b\uff73=\u5c71\u7530^\u592a\u90ce=\u3084\u307e\u3060^\u305f\u308d\u3046\"\r\nspecific_character_sets = [\"ISO 2022 IR 13\", \"ISO 2022 IR 87\"]\r\nexpect_encoded = (\r\n    b\"\\xd4\\xcf\\xc0\\xde\\x5e\\xc0\\xdb\\xb3\\x3d\\x1b\\x24\\x42\\x3b\\x33\"\r\n    b\"\\x45\\x44\\x1b\\x28\\x4a\\x5e\\x1b\\x24\\x42\\x42\\x40\\x4f\\x3a\\x1b\"\r\n    b\"\\x28\\x4a\\x3d\\x1b\\x24\\x42\\x24\\x64\\x24\\x5e\\x24\\x40\\x1b\\x28\"\r\n    b\"\\x4a\\x5e\\x1b\\x24\\x42\\x24\\x3f\\x24\\x6d\\x24\\x26\\x1b\\x28\\x4a\"\r\n)\r\n\r\npython_encodings = pydicom.charset.convert_encodings(specific_character_sets)\r\nactual_encoded = pydicom.charset.encode_string(japanese_pn, python_encodings)\r\n\r\nprint(\"actual:{}\".format(actual_encoded))\r\nprint(\"expect:{}\".format(expect_encoded))\r\n```\r\n<!--\r\nExample:\r\n```py\r\nfrom io import BytesIO\r\nfrom pydicom import dcmread\r\n\r\nbytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\r\n             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\r\n             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\r\n             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\r\n             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\r\n\r\nfp = BytesIO(bytestream)\r\nds = dcmread(fp, force=True)\r\n\r\nprint(ds.PatientID)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n\r\nWhen possible use pydicom testing examples to reproduce the errors. Otherwise, provide\r\nan anonymous version of the data in order to replicate the errors.\r\n-->\r\n\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\n```\r\nb'\\xd4\\xcf\\xc0\\xde^\\xc0\\xdb\\xb3=\\x1b$B;3ED\\x1b(J^\\x1b$BB@O:\\x1b(J=\\x1b$B$d$^$@\\x1b(J^\\x1b$B$?$m$&\\x1b(J'\r\n```\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\n```\r\nb'\\xd4\\xcf\\xc0\\xde^\\xc0\\xdb\\xb3=\\x8eR\\x93c^\\x91\\xbe\\x98Y=\\x82\\xe2\\x82\\xdc\\x82\\xbe^\\x82\\xbd\\x82\\xeb\\x82\\xa4'\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n```\r\nLinux-4.15.0-50-generic-x86_64-with-debian-buster-sid\r\nPython 3.6.8 |Anaconda, Inc.| (default, Dec 30 2018, 01:22:34)\r\n[GCC 7.3.0]\r\npydicom 1.2.2\r\n```\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "input": " }\n \n ENCODINGS_TO_CODES = {v: k for k, v in CODES_TO_ENCODINGS.items()}\n \n # Multi-byte character sets except Korean are handled by Python.\n # To decode them, the escape sequence shall be preserved in the input byte\n                      'iso_ir_58')\n \n \n def decode_string(value, encodings, delimiters):\n     \"\"\"Convert a raw byte string into a unicode string using the given\n     list of encodings.\n     \"\"\"\n     for i, encoding in enumerate(encodings):\n         try:\n            encoded = value.encode(encoding)\n             if i > 0 and encoding not in handled_encodings:\n                return ENCODINGS_TO_CODES.get(encoding, b'') + encoded\n             return encoded\n         except UnicodeError:\n             continue\n         warnings.warn(\"Failed to encode value with encodings: {} - using \"\n                       \"replacement characters in encoded string\"\n                       .format(', '.join(encodings)))\n        return value.encode(encodings[0], errors='replace')\n \n \n def _encode_string_parts(value, encodings):\n     \"\"\"\n     encoded = bytearray()\n     unencoded_part = value\n     while unencoded_part:\n         # find the encoding that can encode the longest part of the rest\n         # of the string still to be encoded\n         max_index = 0\n        best_encoding = None\n         for encoding in encodings:\n             try:\n                unencoded_part.encode(encoding)\n                 # if we get here, the whole rest of the value can be encoded\n                 best_encoding = encoding\n                 max_index = len(unencoded_part)\n                     max_index = e.start\n                     best_encoding = encoding\n         # none of the given encodings can encode the first character - give up\n        if best_encoding is None:\n            raise ValueError()\n \n         # encode the part that can be encoded with the found encoding\n        encoded_part = unencoded_part[:max_index].encode(best_encoding)\n         if best_encoding not in handled_encodings:\n            encoded += ENCODINGS_TO_CODES.get(best_encoding, b'')\n         encoded += encoded_part\n         # set remaining unencoded part of the string and handle that\n         unencoded_part = unencoded_part[max_index:]\n     # unencoded_part is empty - we are done, return the encoded string\n     return encoded\n \n \n # DICOM PS3.5-2008 6.1.1 (p 18) says:\n #   default is ISO-IR 6 G0, equiv to common chr set of ISO 8859 (PS3.5 6.1.2.1)\n #    (0008,0005)  value 1 can *replace* the default encoding...",
    "output": " }\n \n ENCODINGS_TO_CODES = {v: k for k, v in CODES_TO_ENCODINGS.items()}\nENCODINGS_TO_CODES['shift_jis'] = ESC + b')I'\n \n # Multi-byte character sets except Korean are handled by Python.\n # To decode them, the escape sequence shall be preserved in the input byte\n                      'iso_ir_58')\n \n \ndef _encode_to_jis_x_0201(value, errors='strict'):\n    \"\"\"Convert a unicode string into JIS X 0201 byte string using shift_jis\n    encodings.\n    shift_jis is a superset of jis_x_0201. So we can regard the encoded value\n    as jis_x_0201 if it is single byte character.\n\n    Parameters\n    ----------\n    value : text type\n        The unicode string as presented to the user.\n    errors : str\n        The behavior of a character which could not be encoded. If 'strict' is\n        passed, raise an UnicodeEncodeError. If any other value is passed,\n        non ISO IR 14 characters are replaced by the ASCII '?'.\n\n    Returns\n    -------\n    byte string\n        The encoded string. If some characters in value could not be encoded to\n        JIS X 0201, and `errors` is not set to 'strict', they are replaced to\n        '?'.\n\n    Raises\n    ------\n    UnicodeEncodeError\n        If errors is set to 'strict' and `value` could not be encoded with\n        JIS X 0201.\n    \"\"\"\n\n    Encoder = codecs.getincrementalencoder('shift_jis')\n    encoder = Encoder()\n\n    # If errors is not strict, this function is used as fallback.\n    # In this case, we use only ISO IR 14 to encode given value\n    # without escape sequence.\n    if errors != 'strict' or value == '':\n        encoded = b''\n        for c in value:\n            try:\n                b = encoder.encode(c)\n            except UnicodeEncodeError as e:\n                b = b'?'\n\n            if len(b) != 1 or 0x80 <= ord(b):\n                b = b'?'\n            encoded += b\n        return encoded\n\n    encoded = encoder.encode(value[0])\n    if len(encoded) != 1:\n        raise UnicodeEncodeError(\n            'shift_jis', value, 0, len(value), 'illegal multibyte sequence')\n\n    msb = ord(encoded) & 0x80  # msb is 1 for ISO IR 13, 0 for ISO IR 14\n    for i, c in enumerate(value[1:], 1):\n        try:\n            b = encoder.encode(c)\n        except UnicodeEncodeError as e:\n            e.start = i\n            e.end = len(value)\n            raise e\n        if len(b) != 1 or ((ord(b) & 0x80) ^ msb) != 0:\n            character_set = 'ISO IR 14' if msb == 0 else 'ISO IR 13'\n            msg = 'Given character is out of {}'.format(character_set)\n            raise UnicodeEncodeError('shift_jis', value, i, len(value), msg)\n        encoded += b\n\n    return encoded\n\n\ndef _encode_to_jis_x_0208(value, errors='strict'):\n    \"\"\"Convert a unicode string into JIS X 0208 byte string using iso2022_jp\n    encodings.\n    The escape sequence which is located at the end of the encoded value has\n    to vary depending on the value 1 of SpecificCharacterSet. So we have to\n    trim it and append the correct escape sequence manually.\n\n    Parameters\n    ----------\n    value : text type\n        The unicode string as presented to the user.\n    errors : str\n        The behavior of a character which could not be encoded. This value\n        is passed to errors argument of str.encode().\n\n    Returns\n    -------\n    byte string\n        The encoded string. If some characters in value could not be encoded to\n        JIS X 0208, it depends on the behavior of iso2022_jp encoder.\n\n    Raises\n    ------\n    UnicodeEncodeError\n        If errors is set to 'strict' and `value` could not be encoded with\n        JIS X 0208.\n    \"\"\"\n\n    # If errors is not strict, this function is used as fallback.\n    # So keep the tail escape sequence of encoded for backward compatibility.\n    if errors != 'strict':\n        return value.encode('iso2022_jp', errors=errors)\n\n    Encoder = codecs.getincrementalencoder('iso2022-jp')\n    encoder = Encoder()\n\n    encoded = encoder.encode(value[0])\n    if encoded[:3] != ENCODINGS_TO_CODES['iso2022_jp']:\n        raise UnicodeEncodeError(\n            'iso2022_jp', value, 0, len(value),\n            'Given character is out of ISO IR 87')\n\n    for i, c in enumerate(value[1:], 1):\n        try:\n            b = encoder.encode(c)\n        except UnicodeEncodeError as e:\n            e.start = i\n            e.end = len(value)\n            raise e\n        if b[:3] == ENCODINGS_TO_CODES['iso8859']:\n            raise UnicodeEncodeError(\n                'iso2022_jp', value, i, len(value),\n                'Given character is out of ISO IR 87')\n        encoded += b\n    return encoded\n\n\ndef _get_escape_sequence_for_encoding(encoding, encoded=None):\n    \"\"\" Return an escape sequence corresponding to the given encoding. If\n    encoding is 'shift_jis', return 'ESC)I' or 'ESC(J' depending on the first\n    byte of encoded.\n\n    Parameters\n    ----------\n    encoding : str\n        An encoding is used to specify  an escape sequence.\n\n    encoded : bytes or str\n        The encoded value is used to chose an escape sequence if encoding is\n        'shift_jis'\n\n    Returns\n    -------\n    string\n        Escape sequence for encoded value.\n    \"\"\"\n\n    ESC_ISO_IR_14 = ESC + b'(J'\n    ESC_ISO_IR_13 = ESC + b')I'\n\n    if encoding == 'shift_jis':\n        if encoded is None:\n            return ESC_ISO_IR_14\n\n        if not in_py2:\n            first_byte = encoded[0]\n        else:\n            first_byte = ord(encoded[0])\n        if 0x80 <= first_byte:\n            return ESC_ISO_IR_13\n\n        return ESC_ISO_IR_14\n    return ENCODINGS_TO_CODES.get(encoding, b'')\n\n\n# These encodings need escape sequence to handle alphanumeric characters.\nneed_tail_escape_sequence_encodings = ('iso2022_jp', 'iso-2022-jp')\n\n\ncustom_encoders = {\n    'shift_jis': _encode_to_jis_x_0201,\n    'iso2022_jp': _encode_to_jis_x_0208,\n    'iso-2022-jp': _encode_to_jis_x_0208\n}\n\n\n def decode_string(value, encodings, delimiters):\n     \"\"\"Convert a raw byte string into a unicode string using the given\n     list of encodings.\n     \"\"\"\n     for i, encoding in enumerate(encodings):\n         try:\n            encoded = _encode_string_impl(value, encoding)\n\n             if i > 0 and encoding not in handled_encodings:\n                escape_sequence = _get_escape_sequence_for_encoding(\n                        encoding, encoded=encoded)\n                encoded = escape_sequence + encoded\n            if encoding in need_tail_escape_sequence_encodings:\n                encoded += _get_escape_sequence_for_encoding(encodings[0])\n             return encoded\n         except UnicodeError:\n             continue\n         warnings.warn(\"Failed to encode value with encodings: {} - using \"\n                       \"replacement characters in encoded string\"\n                       .format(', '.join(encodings)))\n        return _encode_string_impl(value, encodings[0], errors='replace')\n \n \n def _encode_string_parts(value, encodings):\n     \"\"\"\n     encoded = bytearray()\n     unencoded_part = value\n    best_encoding = None\n     while unencoded_part:\n         # find the encoding that can encode the longest part of the rest\n         # of the string still to be encoded\n         max_index = 0\n         for encoding in encodings:\n             try:\n                _encode_string_impl(unencoded_part, encoding)\n                 # if we get here, the whole rest of the value can be encoded\n                 best_encoding = encoding\n                 max_index = len(unencoded_part)\n                     max_index = e.start\n                     best_encoding = encoding\n         # none of the given encodings can encode the first character - give up\n        if max_index == 0:\n            raise ValueError(\"None of the given encodings can encode the \"\n                             \"first character\")\n \n         # encode the part that can be encoded with the found encoding\n        encoded_part = _encode_string_impl(unencoded_part[:max_index],\n                                           best_encoding)\n         if best_encoding not in handled_encodings:\n            encoded += _get_escape_sequence_for_encoding(\n                    best_encoding, encoded=encoded_part)\n         encoded += encoded_part\n         # set remaining unencoded part of the string and handle that\n         unencoded_part = unencoded_part[max_index:]\n     # unencoded_part is empty - we are done, return the encoded string\n    if best_encoding in need_tail_escape_sequence_encodings:\n        encoded += _get_escape_sequence_for_encoding(encodings[0])\n     return encoded\n \n \ndef _encode_string_impl(value, encoding, errors='strict'):\n    \"\"\"Convert a unicode string into a byte string. If given encoding is in\n    custom_encoders, use a corresponding custom_encoder. If given encoding\n    is not in custom_encoders, use a corresponding python handled encoder.\n    \"\"\"\n    if encoding in custom_encoders:\n        return custom_encoders[encoding](value, errors=errors)\n    else:\n        return value.encode(encoding, errors=errors)\n\n\n # DICOM PS3.5-2008 6.1.1 (p 18) says:\n #   default is ISO-IR 6 G0, equiv to common chr set of ISO 8859 (PS3.5 6.1.2.1)\n #    (0008,0005)  value 1 can *replace* the default encoding..."
  },
  {
    "instruction": "Dataset.to_json_dict can still generate exceptions when suppress_invalid_tags=True\n**Describe the bug**\r\nI'm using `Dataset.to_json_dict(suppress_invalid_tags=True)` and can live with losing invalid tags.  Unfortunately, I can still trigger an exception with something like  `2.0` in an `IS` field.\r\n\r\n**Expected behavior**\r\nto_json_dict shouldn't throw an error about an invalid tag when `suppress_invalid_tags` is enabled.\r\n\r\nMy thought was simply to move the `data_element = self[key]` into the try/catch block that's right after it.\r\n\r\n**Steps To Reproduce**\r\n\r\nTraceback:\r\n```\r\n  File \"dicom.py\", line 143, in create_dict\r\n    json_ds = ds.to_json_dict(suppress_invalid_tags=True)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataset.py\", line 2495, in to_json_dict\r\n    data_element = self[key]\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataset.py\", line 939, in __getitem__\r\n    self[tag] = DataElement_from_raw(elem, character_set, self)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataelem.py\", line 859, in DataElement_from_raw\r\n    value = convert_value(vr, raw, encoding)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/values.py\", line 771, in convert_value\r\n    return converter(byte_string, is_little_endian, num_format)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/values.py\", line 348, in convert_IS_string\r\n    return MultiString(num_string, valtype=pydicom.valuerep.IS)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/valuerep.py\", line 1213, in MultiString\r\n    return valtype(splitup[0])\r\n  File \"/usr/lib/python3/dist-packages/pydicom/valuerep.py\", line 1131, in __new__\r\n    raise TypeError(\"Could not convert value to integer without loss\")\r\nTypeError: Could not convert value to integer without loss\r\n```\r\n\r\n**Your environment**\r\npython 3.7, pydicom 2.3\r\n\r\n\n",
    "input": "         json_dataset = {}\n         for key in self.keys():\n             json_key = '{:08X}'.format(key)\n            data_element = self[key]\n             try:\n                 json_dataset[json_key] = data_element.to_json_dict(\n                     bulk_data_element_handler=bulk_data_element_handler,\n                     bulk_data_threshold=bulk_data_threshold",
    "output": "         json_dataset = {}\n         for key in self.keys():\n             json_key = '{:08X}'.format(key)\n             try:\n                data_element = self[key]\n                 json_dataset[json_key] = data_element.to_json_dict(\n                     bulk_data_element_handler=bulk_data_element_handler,\n                     bulk_data_threshold=bulk_data_threshold"
  },
  {
    "instruction": "Revise the type annotation for pydicom.datadict.dictionary_has_tag()\n**Describe the bug**\r\n\r\nThe documentation of [`pydicom.datadict.dictionary_has_tag()`](https://pydicom.github.io/pydicom/dev/reference/generated/pydicom.datadict.dictionary_has_tag.html#pydicom.datadict.dictionary_has_tag) suggests that a query using keywords (instead of a tag integer) would work:\r\n\r\n```python\r\npydicom.datadict.dictionary_has_tag(tag: Union[int, str, Tuple[int, int], pydicom.tag.BaseTag]) -> bool\r\n```\r\n\r\nHowever, the function only accepts integer arguments.\r\n\r\n```python\r\nfrom pydicom.datadict import dictionary_has_tag, keyword_dict\r\ndictionary_has_tag(\"PixelData\")\r\n# Returns False\r\n\r\ndictionary_has_tag(keyword_dict[\"PixelData\"])\r\n# Returns True\r\n```\r\n\r\n(The problem may apply to other functions as well...)\r\n\r\n**Expected behavior**\r\nFollowing the docs, `dictionary_has_tag(\"PixelData\")` should return True. \r\n\r\nIt would be nice, if the flexible conversion of tags from names or hex-tuples (as the type annotation suggests) would also be possible for this function.\r\n\r\n**Your environment**\r\n```text\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.14.6-x86_64-i386-64bit\r\nPython       | 3.9.0 (v3.9.0:9cf6752276, Oct  5 2020, 11:29:23)  [Clang 6.0 (clang-600.0.57)]\r\npydicom      | 2.2.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.20.1\r\nPIL          | 8.0.1\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\r\n\n",
    "input": " \n     Parameters\n     ----------\n    tag : int\n        The tag for the element whose entry is to be retrieved. Only entries\n        in the official DICOM dictionary will be checked, not entries in the\n         private dictionary.\n \n     Returns\n \n     Parameters\n     ----------\n    tag : int\n        The tag for the element whose retirement status is being checked.\n \n     Returns\n     -------\n     KeyError\n         If the tag is not present in the DICOM data dictionary.\n     \"\"\"\n    if 'retired' in get_entry(tag)[3].lower():\n        return True\n    return False\n \n \n def dictionary_VR(tag: TagType) -> str:\n \n     Parameters\n     ----------\n    tag : int\n         The tag for the element whose value representation (VR) is being\n        retrieved.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int\n         The tag for the element whose value multiplicity (VM) is being\n        retrieved.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int\n        The tag for the element whose description is being retrieved.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int\n        The tag for the element whose keyword is being retrieved.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int\n        The tag to check.\n \n     Returns\n     -------\n         ``True`` if the tag corresponds to an element present in the official\n         DICOM data dictionary, ``False`` otherwise.\n     \"\"\"\n    return (tag in DicomDictionary)\n \n \n def keyword_for_tag(tag: TagType) -> str:\n \n     Parameters\n     ----------\n    tag : int\n        The tag for the element whose keyword is being retrieved.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int\n        The tag for the element whose entry is to be retrieved. Only entries\n        in the private dictionary will be checked.\n     private_creator : str\n         The name of the private creator.\n \n \n     Parameters\n     ----------\n    tag : int\n         The tag for the element whose value representation (VR) is being\n        retrieved.\n     private_creator : str\n         The name of the private creator.\n \n \n     Parameters\n     ----------\n    tag : int\n         The tag for the element whose value multiplicity (VM) is being\n        retrieved.\n     private_creator : str\n         The name of the private creator.\n \n \n     Parameters\n     ----------\n    tag : int\n        The tag for the element whose description is being retrieved.\n     private_creator : str\n         The name of the private createor.\n ",
    "output": " \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag for the element whose entry is to be retrieved, in any of the\n        forms accepted by :func:`~pydicom.tag.Tag`. Only entries in the\n        official DICOM dictionary will be checked, not entries in the\n         private dictionary.\n \n     Returns\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag for the element whose retirement status is being checked, in\n        any of the forms accepted by :func:`~pydicom.tag.Tag`.\n \n     Returns\n     -------\n     KeyError\n         If the tag is not present in the DICOM data dictionary.\n     \"\"\"\n    return 'retired' in get_entry(tag)[3].lower()\n \n \n def dictionary_VR(tag: TagType) -> str:\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n         The tag for the element whose value representation (VR) is being\n        retrieved, in any of the forms accepted by :func:`~pydicom.tag.Tag`.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n         The tag for the element whose value multiplicity (VM) is being\n        retrieved, in any of the forms accepted by :func:`~pydicom.tag.Tag`.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag for the element whose description is being retrieved, in any\n        of the forms accepted by :func:`~pydicom.tag.Tag`.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag for the element whose keyword is being retrieved, in any of\n        the forms accepted by :func:`~pydicom.tag.Tag`.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag to check, in any of the forms accepted by\n        :func:`~pydicom.tag.Tag`.\n \n     Returns\n     -------\n         ``True`` if the tag corresponds to an element present in the official\n         DICOM data dictionary, ``False`` otherwise.\n     \"\"\"\n    try:\n        return Tag(tag) in DicomDictionary\n    except Exception:\n        return False\n \n \n def keyword_for_tag(tag: TagType) -> str:\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag for the element whose keyword is being retrieved, in any of\n        the forms accepted by :func:`~pydicom.tag.Tag`.\n \n     Returns\n     -------\n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag for the element whose entry is to be retrieved, in any of the\n        forms accepted by :func:`~pydicom.tag.Tag`. Only entries in the\n        private dictionary will be checked.\n     private_creator : str\n         The name of the private creator.\n \n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n         The tag for the element whose value representation (VR) is being\n        retrieved, in any of the forms accepted by :func:`~pydicom.tag.Tag`.\n     private_creator : str\n         The name of the private creator.\n \n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n         The tag for the element whose value multiplicity (VM) is being\n        retrieved, in any of the forms accepted by :func:`~pydicom.tag.Tag`.\n     private_creator : str\n         The name of the private creator.\n \n \n     Parameters\n     ----------\n    tag : int or str or Tuple[int, int]\n        The tag for the element whose description is being retrieved, in any\n        of the forms accepted by :func:`~pydicom.tag.Tag`.\n     private_creator : str\n         The name of the private createor.\n "
  },
  {
    "instruction": "Error : a bytes-like object is required, not 'MultiValue'\nHello,\r\n\r\nI am getting following error while updating the tag LongTrianglePointIndexList (0066,0040),\r\n**TypeError: a bytes-like object is required, not 'MultiValue'**\r\n\r\nI noticed that the error  gets produced only when the VR is given as \"OL\" , works fine with \"OB\", \"OF\" etc.\r\n\r\nsample code (assume 'lineSeq' is the dicom dataset sequence):\r\n```python\r\nimport pydicom\r\nimport array\r\ndata=list(range(1,10))\r\ndata=array.array('H', indexData).tostring()  # to convert to unsigned short\r\nlineSeq.add_new(0x00660040, 'OL', data)   \r\nds.save_as(\"mydicom\")\r\n```\r\noutcome: **TypeError: a bytes-like object is required, not 'MultiValue'**\r\n\r\nusing version - 2.0.0.0\r\n\r\nAny help is appreciated.\r\n\r\nThank you\n",
    "input": "     @value.setter\n     def value(self, val: Any) -> None:\n         \"\"\"Convert (if necessary) and set the value of the element.\"\"\"\n         # Check if is a string with multiple values separated by '\\'\n         # If so, turn them into a list of separate strings\n         #  Last condition covers 'US or SS' etc\n        if isinstance(val, (str, bytes)) and self.VR not in \\\n                ['UT', 'ST', 'LT', 'FL', 'FD', 'AT', 'OB', 'OW', 'OF', 'SL',\n                 'SQ', 'SS', 'UL', 'OB/OW', 'OW/OB', 'OB or OW',\n                 'OW or OB', 'UN'] and 'US' not in self.VR:\n             try:\n                 if _backslash_str in val:\n                     val = cast(str, val).split(_backslash_str)",
    "output": "     @value.setter\n     def value(self, val: Any) -> None:\n         \"\"\"Convert (if necessary) and set the value of the element.\"\"\"\n        # Ignore backslash characters in these VRs, based on:\n        # * Which str VRs can have backslashes in Part 5, Section 6.2\n        # * All byte VRs\n        exclusions = [\n            'LT', 'OB', 'OD', 'OF', 'OL', 'OV', 'OW', 'ST', 'UN', 'UT',\n            'OB/OW', 'OW/OB', 'OB or OW', 'OW or OB',\n            # Probably not needed\n            'AT', 'FD', 'FL', 'SQ', 'SS', 'SL', 'UL',\n        ]\n\n         # Check if is a string with multiple values separated by '\\'\n         # If so, turn them into a list of separate strings\n         #  Last condition covers 'US or SS' etc\n        if (\n            isinstance(val, (str, bytes))\n            and self.VR not in exclusions\n            and 'US' not in self.VR\n        ):\n             try:\n                 if _backslash_str in val:\n                     val = cast(str, val).split(_backslash_str)"
  },
  {
    "instruction": "Write deflated content when called Transfer Syntax is Deflated Explicit VR Little Endian\n**Describe the bug**\r\nAfter using `dcmread` to read a deflated .dcm file created from pydicom's [CT_small.dcm sample](https://github.com/pydicom/pydicom/blob/v1.4.2/pydicom/data/test_files/CT_small.dcm), with the following file meta information\r\n```\r\n(0002, 0000) File Meta Information Group Length  UL: 178\r\n(0002, 0001) File Meta Information Version       OB: b'\\x00\\x01'\r\n(0002, 0002) Media Storage SOP Class UID         UI: CT Image Storage\r\n(0002, 0003) Media Storage SOP Instance UID      UI: 1.3.6.1.4.1.5962.1.1.1.1.1.20040119072730.12322\r\n(0002, 0010) Transfer Syntax UID                 UI: Deflated Explicit VR Little Endian\r\n(0002, 0012) Implementation Class UID            UI: 1.2.40.0.13.1.1\r\n(0002, 0013) Implementation Version Name         SH: 'dcm4che-2.0'\r\n```\r\n\r\nI use `save_as` to save the file. The output file has an unaltered file meta information section, but the group 8 elements and beyond are not written in deflated format, instead appearing to be LEE. In particular, the specific character set element is easily readable from a hex representation of the file, rather than appearing as gobbledygook like one would expect from a deflated stream.\r\n\r\n**Expected behavior**\r\nThe bulk of the DCM to be written as Deflated Explicit VR Little Endian or the Transfer Syntax UID to be saved with a value that reflects the actual format of the DCM\r\n\r\n**Steps To Reproduce**\r\n```python\r\n\u276f py\r\n>>> # CT_small_deflated.dcm is CT_small.dcm, deflated using dcm2dcm\r\n>>> ds = pydicom.dcmread(\"CT_small_deflated.dcm\")\r\n\r\n>>> ds.save_as(\"ds_like_orig.dcm\", write_like_original=True)\r\n>>> pydicom.dcmread(\"ds_like_orig.dcm\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 869, in dcmread\r\n    dataset = read_partial(fp, stop_when, defer_size=defer_size,\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 729, in read_partial\r\n    unzipped = zlib.decompress(zipped, -zlib.MAX_WBITS)\r\nzlib.error: Error -3 while decompressing data: invalid stored block lengths\r\n\r\n>>> ds.save_as(\"ds_not_like_orig.dcm\", write_like_original=False)\r\n>>> pydicom.dcmread(\"ds_not_like_orig.dcm\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 869, in dcmread\r\n    dataset = read_partial(fp, stop_when, defer_size=defer_size,\r\n  File \"C:\\Users\\blairyat\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\pydicom\\filereader.py\", line 729, in read_partial\r\n    unzipped = zlib.decompress(zipped, -zlib.MAX_WBITS)\r\nzlib.error: Error -3 while decompressing data: invalid stored block lengths\r\n```\r\n\r\n**Your environment**\r\nPlease run the following and paste the output.\r\n```powershell\r\n\u276f py -c \"import platform; print(platform.platform())\"\r\nWindows-10-10.0.18362-SP0\r\n\r\n\u276f py -c \"import sys; print('Python ', sys.version)\"\r\nPython  3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 22:39:24) [MSC v.1916 32 bit (Intel)]\r\n\r\n\u276f py -c \"import pydicom; print('pydicom ', pydicom.__version__)\"\r\npydicom  1.4.2\r\n```\r\n\n",
    "input": "         is_implicit_VR = False\n         is_little_endian = False\n     elif transfer_syntax == pydicom.uid.DeflatedExplicitVRLittleEndian:\n        # See PS3.6-2008 A.5 (p 71)\n         # when written, the entire dataset following\n         #     the file metadata was prepared the normal way,\n         #     then \"deflate\" compression applied.\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n \n \n import warnings\n from struct import pack\n \n from pydicom.charset import (\n from pydicom.multival import MultiValue\n from pydicom.tag import (Tag, ItemTag, ItemDelimiterTag, SequenceDelimiterTag,\n                          tag_in_exception)\nfrom pydicom.uid import UncompressedPixelTransferSyntaxes\n from pydicom.valuerep import extra_length_VRs\n from pydicom.values import convert_numbers\n \n     fp.write(buffer.getvalue())\n \n \n def dcmwrite(filename, dataset, write_like_original=True):\n     \"\"\"Write `dataset` to the `filename` specified.\n \n     else:\n         fp = DicomFileLike(filename)\n \n    # if we want to write with the same endianess and VR handling as\n    # the read dataset we want to preserve raw data elements for\n    # performance reasons (which is done by get_item);\n    # otherwise we use the default converting item getter\n    if dataset.is_original_encoding:\n        get_item = Dataset.get_item\n    else:\n        get_item = Dataset.__getitem__\n\n     try:\n         # WRITE FILE META INFORMATION\n         if preamble:\n             fp.write(preamble)\n             fp.write(b'DICM')\n \n         if dataset.file_meta:  # May be an empty Dataset\n             # If we want to `write_like_original`, don't enforce_standard\n             write_file_meta_info(fp, dataset.file_meta,\n                                  enforce_standard=not write_like_original)\n \n        # WRITE DATASET\n        # The transfer syntax used to encode the dataset can't be changed\n        #   within the dataset.\n        # Write any Command Set elements now as elements must be in tag order\n        #   Mixing Command Set with other elements is non-conformant so we\n        #   require `write_like_original` to be True\n        command_set = get_item(dataset, slice(0x00000000, 0x00010000))\n        if command_set and write_like_original:\n            fp.is_implicit_VR = True\n            fp.is_little_endian = True\n            write_dataset(fp, command_set)\n\n        # Set file VR and endianness. MUST BE AFTER writing META INFO (which\n        #   requires Explicit VR Little Endian) and COMMAND SET (which requires\n        #   Implicit VR Little Endian)\n        fp.is_implicit_VR = dataset.is_implicit_VR\n        fp.is_little_endian = dataset.is_little_endian\n\n        # Write non-Command Set elements now\n        write_dataset(fp, get_item(dataset, slice(0x00010000, None)))\n     finally:\n         if not caller_owns_file:\n             fp.close()",
    "output": "         is_implicit_VR = False\n         is_little_endian = False\n     elif transfer_syntax == pydicom.uid.DeflatedExplicitVRLittleEndian:\n        # See PS3.5 section A.5\n         # when written, the entire dataset following\n         #     the file metadata was prepared the normal way,\n         #     then \"deflate\" compression applied.\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n \n \n import warnings\nimport zlib\n from struct import pack\n \n from pydicom.charset import (\n from pydicom.multival import MultiValue\n from pydicom.tag import (Tag, ItemTag, ItemDelimiterTag, SequenceDelimiterTag,\n                          tag_in_exception)\nfrom pydicom.uid import (UncompressedPixelTransferSyntaxes,\n                         DeflatedExplicitVRLittleEndian)\n from pydicom.valuerep import extra_length_VRs\n from pydicom.values import convert_numbers\n \n     fp.write(buffer.getvalue())\n \n \ndef _write_dataset(fp, dataset, write_like_original):\n    \"\"\"Write the Data Set to a file-like. Assumes the file meta information,\n    if any, has been written.\n    \"\"\"\n\n    # if we want to write with the same endianess and VR handling as\n    # the read dataset we want to preserve raw data elements for\n    # performance reasons (which is done by get_item);\n    # otherwise we use the default converting item getter\n    if dataset.is_original_encoding:\n        get_item = Dataset.get_item\n    else:\n        get_item = Dataset.__getitem__\n\n    # WRITE DATASET\n    # The transfer syntax used to encode the dataset can't be changed\n    #   within the dataset.\n    # Write any Command Set elements now as elements must be in tag order\n    #   Mixing Command Set with other elements is non-conformant so we\n    #   require `write_like_original` to be True\n    command_set = get_item(dataset, slice(0x00000000, 0x00010000))\n    if command_set and write_like_original:\n        fp.is_implicit_VR = True\n        fp.is_little_endian = True\n        write_dataset(fp, command_set)\n\n    # Set file VR and endianness. MUST BE AFTER writing META INFO (which\n    #   requires Explicit VR Little Endian) and COMMAND SET (which requires\n    #   Implicit VR Little Endian)\n    fp.is_implicit_VR = dataset.is_implicit_VR\n    fp.is_little_endian = dataset.is_little_endian\n\n    # Write non-Command Set elements now\n    write_dataset(fp, get_item(dataset, slice(0x00010000, None)))\n\n\n def dcmwrite(filename, dataset, write_like_original=True):\n     \"\"\"Write `dataset` to the `filename` specified.\n \n     else:\n         fp = DicomFileLike(filename)\n \n     try:\n         # WRITE FILE META INFORMATION\n         if preamble:\n             fp.write(preamble)\n             fp.write(b'DICM')\n \n        tsyntax = None\n         if dataset.file_meta:  # May be an empty Dataset\n             # If we want to `write_like_original`, don't enforce_standard\n             write_file_meta_info(fp, dataset.file_meta,\n                                  enforce_standard=not write_like_original)\n            tsyntax = getattr(dataset.file_meta, \"TransferSyntaxUID\", None)\n\n        if (tsyntax == DeflatedExplicitVRLittleEndian):\n            # See PS3.5 section A.5\n            # when writing, the entire dataset following\n            #     the file metadata is prepared the normal way,\n            #     then \"deflate\" compression applied.\n            buffer = DicomBytesIO()\n            _write_dataset(buffer, dataset, write_like_original)\n\n            # Compress the encoded data and write to file\n            compressor = zlib.compressobj(wbits=-zlib.MAX_WBITS)\n            deflated = compressor.compress(buffer.parent.getvalue())\n            deflated += compressor.flush()\n            if len(deflated) % 2:\n                deflated += b'\\x00'\n\n            fp.write(deflated)\n        else:\n            _write_dataset(fp, dataset, write_like_original)\n \n     finally:\n         if not caller_owns_file:\n             fp.close()"
  },
  {
    "instruction": "The function generate_uid() generates non-conforming \u201c2.25 .\u201d DICOM UIDs\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nIt seems there was already a discussion about this function in the past (#125), but the current implementation generates non-conforming DICOM UIDs when called with prefix \u2018none\u2019 to trigger that the function generate_uid() should generate a UUID derived UID.\r\n\r\nThe DICOM Standard requires (see DICOM PS 3.5, B.2 that when a UUID derived UID is constructed it should be in the format \u201c2.25.\u201d + uuid(in its decimal representation string representation)\r\nFor example a UUID of f81d4fae-7dec-11d0-a765-00a0c91e6bf6 should become 2.25.329800735698586629295641978511506172918\r\n\r\nThe current implementation extends the uuid part to the remaining 59 characters. By not following the DICOM formatting rule, receiving systems that are processing DICOM instances created with this library are not capable of converting the generated \u201c2.25\u201d UID back to a UUID. Due to the extra sha512 operation on the UUID, the variant and version info of the UUID are also lost.\r\n\r\n#### Steps/Code to Reproduce\r\n- call generate_uid() to generate a \"2.25.\" DICOM UID\r\n\r\n#### Expected Results\r\nA conforming unique DICOM UID is returned.\r\n\r\n#### Actual Results\r\nNon conforming UID is returned.\n",
    "input": "     ----------\n     prefix : str or None\n         The UID prefix to use when creating the UID. Default is the pydicom\n        root UID '1.2.826.0.1.3680043.8.498.'. If None then a value of '2.25.'\n        will be used (as described on `David Clunie's website\n        <http://www.dclunie.com/medical-image-faq/html/part2.html#UID>`_).\n     entropy_srcs : list of str or None\n        If a list of str, the prefix will be appended with a SHA512 hash of the\n        list which means the result is deterministic and should make the\n        original data unrecoverable. If None random data will be used\n        (default).\n \n     Returns\n     -------\n     pydicom.uid.UID\n        A 64 character DICOM UID.\n \n     Raises\n     ------\n     >>> generate_uid()\n     1.2.826.0.1.3680043.8.498.22463838056059845879389038257786771680\n     >>> generate_uid(prefix=None)\n    2.25.12586835699909622925962004639368649121731805922235633382942\n     >>> generate_uid(entropy_srcs=['lorem', 'ipsum'])\n     1.2.826.0.1.3680043.8.498.87507166259346337659265156363895084463\n     >>> generate_uid(entropy_srcs=['lorem', 'ipsum'])\n     1.2.826.0.1.3680043.8.498.87507166259346337659265156363895084463\n     \"\"\"\n    max_uid_len = 64\n\n     if prefix is None:\n        prefix = '2.25.'\n \n     if len(prefix) > max_uid_len - 1:\n         raise ValueError(\"The prefix must be less than 63 chars\")\n     if not re.match(RE_VALID_UID_PREFIX, prefix):",
    "output": "     ----------\n     prefix : str or None\n         The UID prefix to use when creating the UID. Default is the pydicom\n        root UID '1.2.826.0.1.3680043.8.498.'. If None then a prefix of '2.25.'\n        will be used with the integer form of a UUID generated using the\n        UUID4 algorithm.\n     entropy_srcs : list of str or None\n        If `prefix` is not None, then the prefix will be appended with a\n        SHA512 hash of the list which means the result is deterministic and\n        should make the original data unrecoverable. If None random data will\n        be used (default).\n \n     Returns\n     -------\n     pydicom.uid.UID\n        A DICOM UID of up to 64 characters.\n \n     Raises\n     ------\n     >>> generate_uid()\n     1.2.826.0.1.3680043.8.498.22463838056059845879389038257786771680\n     >>> generate_uid(prefix=None)\n    2.25.167161297070865690102504091919570542144\n     >>> generate_uid(entropy_srcs=['lorem', 'ipsum'])\n     1.2.826.0.1.3680043.8.498.87507166259346337659265156363895084463\n     >>> generate_uid(entropy_srcs=['lorem', 'ipsum'])\n     1.2.826.0.1.3680043.8.498.87507166259346337659265156363895084463\n     \"\"\"\n     if prefix is None:\n        # UUID -> as 128-bit int -> max 39 characters long\n        return UID('2.25.{}'.format(uuid.uuid4().int))\n \n    max_uid_len = 64\n     if len(prefix) > max_uid_len - 1:\n         raise ValueError(\"The prefix must be less than 63 chars\")\n     if not re.match(RE_VALID_UID_PREFIX, prefix):"
  },
  {
    "instruction": "[python 3.8] failing tests: various issues but \"max recursion depth reached\" seems to be one\n#### Description\r\nFedora is beginning to test python packages against python 3.8. Pydicom builds but tests fail with errors.\r\n\r\n#### Steps/Code to Reproduce\r\n\r\n```\r\npython setup.py build\r\npython setup.py install\r\npytest\r\n```\r\n\r\nThe complete build log is attached. It includes the complete build process. The root log is also attached. These are the versions of other python libraries that are in use:\r\n\r\n```\r\npython3-dateutil-1:2.8.0-5.fc32.noarch\r\npython3-devel-3.8.0~b3-4.fc32.x86_64\r\npython3-numpy-1:1.17.0-3.fc32.x86_64\r\npython3-numpydoc-0.9.1-3.fc32.noarch\r\npython3-pytest-4.6.5-3.fc32.noarch\r\npython3-setuptools-41.0.1-8.fc32.noarch\r\npython3-six-1.12.0-5.fc32.noarch\r\n```\r\n\r\n[build-log.txt](https://github.com/pydicom/pydicom/files/3527558/build-log.txt)\r\n[root-log.txt](https://github.com/pydicom/pydicom/files/3527559/root-log.txt)\r\n\n",
    "input": "     def __str__(self):\n         if hasattr(self, 'original_string'):\n             return self.original_string\n        else:\n            return super(DSfloat, self).__str__()\n \n     def __repr__(self):\n        return \"\\\"\" + str(self) + \"\\\"\"\n \n \n class DSdecimal(Decimal):\n         elif isinstance(val, IS) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n    def __repr__(self):\n         if hasattr(self, 'original_string'):\n            return \"\\\"\" + self.original_string + \"\\\"\"\n        else:\n            return \"\\\"\" + int.__str__(self) + \"\\\"\"\n \n \n def MultiString(val, valtype=str):",
    "output": "     def __str__(self):\n         if hasattr(self, 'original_string'):\n             return self.original_string\n\n        # Issue #937 (Python 3.8 compatibility)\n        return repr(self)[1:-1]\n \n     def __repr__(self):\n        return '\"{}\"'.format(super(DSfloat, self).__repr__())\n \n \n class DSdecimal(Decimal):\n         elif isinstance(val, IS) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n    def __str__(self):\n         if hasattr(self, 'original_string'):\n            return self.original_string\n\n        # Issue #937 (Python 3.8 compatibility)\n        return repr(self)[1:-1]\n\n    def __repr__(self):\n        return '\"{}\"'.format(super(IS, self).__repr__())\n \n \n def MultiString(val, valtype=str):"
  },
  {
    "instruction": "Error decoding dataset with ambiguous VR element when the value is None\nHi all,\r\n    I used the storescu in pynetdicom 1.5.3 to send the dicom ct files(both on mac and ubuntu): \r\n**python storescu.py 192.168.1.120 9002 ~/Downloads/test/**\r\n(I also tried https://pydicom.github.io/pynetdicom/stable/examples/storage.html#storage-scu)\r\nbut it throwed errors: \r\n\r\n_E: Failed to encode the supplied Dataset\r\nE: Store failed: /Users/me/Downloads/test/CT_S1_118.dcm\r\nE: Failed to encode the supplied Dataset\r\nTraceback (most recent call last):\r\n  File \"storescu.py\", line 283, in main\r\n    status = assoc.send_c_store(ds, ii)\r\n  File \"/Users/me/.pyenv/versions/3.8.2/lib/python3.8/site-packages/pynetdicom/association.py\", line 1736, in send_c_store\r\n    raise ValueError('Failed to encode the supplied Dataset')\r\nValueError: Failed to encode the supplied Dataset_\r\n\r\nBut I used to send same files with storescu in dcm4che successfully.\r\nFile attached.\r\n\r\n[test.zip](https://github.com/pydicom/pynetdicom/files/5258867/test.zip)\r\n\n",
    "input": "             elem.VR = 'SS'\n             byte_type = 'h'\n \n         # Need to handle type check for elements with VM > 1\n         elem_value = elem.value if elem.VM == 1 else elem.value[0]\n         if not isinstance(elem_value, int):\n         # As per PS3.3 C.11.1.1.1\n         if ds.LUTDescriptor[0] == 1:\n             elem.VR = 'US'\n             elem_value = elem.value if elem.VM == 1 else elem.value[0]\n             if not isinstance(elem_value, int):\n                 elem.value = convert_numbers(elem.value, is_little_endian, 'H')",
    "output": "             elem.VR = 'SS'\n             byte_type = 'h'\n \n        if elem.VM == 0:\n            return elem\n\n         # Need to handle type check for elements with VM > 1\n         elem_value = elem.value if elem.VM == 1 else elem.value[0]\n         if not isinstance(elem_value, int):\n         # As per PS3.3 C.11.1.1.1\n         if ds.LUTDescriptor[0] == 1:\n             elem.VR = 'US'\n            if elem.VM == 0:\n                return elem\n\n             elem_value = elem.value if elem.VM == 1 else elem.value[0]\n             if not isinstance(elem_value, int):\n                 elem.value = convert_numbers(elem.value, is_little_endian, 'H')"
  },
  {
    "instruction": "Generators in encaps don't handle single fragment per frame correctly with no BOT value\n#### Description\r\nGenerators in `encaps.py` handling of encapsulated pixel data incorrect when the Basic Offset Table has no value and each frame is a single fragment.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom pydicom import dcmread\r\nfrom pydicom.encaps import generate_pixel_data_frame\r\n\r\nfpath = 'pydicom/data/test_files/emri_small_jpeg_2k_lossless.dcm'\r\nds = dcmread(fpath)\r\nds.NumberOfFrames  # 10\r\nframe_generator = generate_pixel_data_frame(ds.PixelData)\r\nnext(frame_generator)\r\nnext(frame_generator) # StopIteration raised\r\n```\r\n\r\n#### Expected Results\r\nAll 10 frames of the pixel data should be accessible.\r\n\r\n#### Actual Results\r\nOnly the first frame is accessible.\n[MRG] Some pixel handlers will not decode multiple fragments per frame\n\r\n\r\nAdded test cases to demonstrate failures for jpeg ls with multiple fragments per frame.  The test files were created with dcmtk 3.6.1 using dcmcjpls +fs 1. One file has an offset table, the other does not.\r\n\r\n#### Reference Issue\r\nSee #685 \r\n\r\n\r\n#### What does this implement/fix? Explain your changes.\r\nThese test cases show that the pixel decoders (jpeg and jpeg_ls most likely) will not handle multiple fragments per frame.\r\n\r\nNo fix yet...\r\n\r\nAny suggestions?\n",
    "input": " \"\"\"Functions for working with encapsulated (compressed) pixel data.\"\"\"\n \n from struct import pack\n \n import pydicom.config\n from pydicom.filebase import DicomBytesIO\n \n     Returns\n     -------\n    list of int\n        The byte offsets to the first fragment of each frame, as measured from\n        the start of the first item following the Basic Offset Table item.\n \n     Raises\n     ------\n     for ii in range(length // 4):\n         offsets.append(fp.read_UL())\n \n    return offsets\n \n \n def generate_pixel_data_fragment(fp):\n                              .format(tag, fp.tell() - 4))\n \n \ndef generate_pixel_data_frame(bytestream):\n     \"\"\"Yield an encapsulated pixel data frame.\n \n     Parameters\n         The value of the (7fe0, 0010) *Pixel Data* element from an encapsulated\n         dataset. The Basic Offset Table item should be present and the\n         Sequence Delimiter item may or may not be present.\n \n     Yields\n     ------\n     ----------\n     DICOM Standard Part 5, :dcm:`Annex A <part05/chapter_A.html>`\n     \"\"\"\n    for fragmented_frame in generate_pixel_data(bytestream):\n         yield b''.join(fragmented_frame)\n \n \ndef generate_pixel_data(bytestream):\n     \"\"\"Yield an encapsulated pixel data frame.\n \n     For the following transfer syntaxes, a fragment may not contain encoded\n         The value of the (7fe0, 0010) *Pixel Data* element from an encapsulated\n         dataset. The Basic Offset Table item should be present and the\n         Sequence Delimiter item may or may not be present.\n \n     Yields\n     -------\n         An encapsulated pixel data frame, with the contents of the\n         :class:`tuple` the frame's fragmented data.\n \n     References\n     ----------\n     DICOM Standard Part 5, :dcm:`Annex A <part05/chapter_A.html>`\n     fp.is_little_endian = True\n \n     # `offsets` is a list of the offsets to the first fragment in each frame\n    offsets = get_frame_offsets(fp)\n     # Doesn't actually matter what the last offset value is, as long as its\n     # greater than the total number of bytes in the fragments\n     offsets.append(len(bytestream))\n \n    frame = []\n    frame_length = 0\n    frame_number = 0\n    for fragment in generate_pixel_data_fragment(fp):\n        if frame_length < offsets[frame_number + 1]:\n            frame.append(fragment)\n         else:\n            yield tuple(frame)\n            frame = [fragment]\n            frame_number += 1\n\n        frame_length += len(fragment) + 8\n\n    # Yield the final frame - required here because the frame_length will\n    # never be greater than offsets[-1] and thus never trigger the final yield\n    # within the for block\n    yield tuple(frame)\n \n \n def decode_data_sequence(data):",
    "output": " \"\"\"Functions for working with encapsulated (compressed) pixel data.\"\"\"\n \n from struct import pack\nimport warnings\n \n import pydicom.config\n from pydicom.filebase import DicomBytesIO\n \n     Returns\n     -------\n    bool, list of int\n        Whether or not the BOT is empty, and a list of the byte offsets\n        to the first fragment of each frame, as measured from the start of the\n        first item following the Basic Offset Table item.\n \n     Raises\n     ------\n     for ii in range(length // 4):\n         offsets.append(fp.read_UL())\n \n    return bool(length), offsets\n\n\ndef get_nr_fragments(fp):\n    \"\"\"Return the number of fragments in `fp`.\"\"\"\n    if not fp.is_little_endian:\n        raise ValueError(\"'fp.is_little_endian' must be True\")\n\n    nr_fragments = 0\n    start = fp.tell()\n    while True:\n        try:\n            tag = Tag(fp.read_tag())\n        except EOFError:\n            break\n\n        if tag == 0xFFFEE000:\n            # Item\n            length = fp.read_UL()\n            if length == 0xFFFFFFFF:\n                raise ValueError(\"Undefined item length at offset {} when \"\n                                 \"parsing the encapsulated pixel data \"\n                                 \"fragments.\".format(fp.tell() - 4))\n            fp.seek(length, 1)\n            nr_fragments += 1\n        elif tag == 0xFFFEE0DD:\n            # Sequence Delimiter\n            break\n        else:\n            raise ValueError(\"Unexpected tag '{}' at offset {} when parsing \"\n                             \"the encapsulated pixel data fragment items.\"\n                             .format(tag, fp.tell() - 4))\n\n    fp.seek(start)\n    return nr_fragments\n \n \n def generate_pixel_data_fragment(fp):\n                              .format(tag, fp.tell() - 4))\n \n \ndef generate_pixel_data_frame(bytestream, nr_frames=None):\n     \"\"\"Yield an encapsulated pixel data frame.\n \n     Parameters\n         The value of the (7fe0, 0010) *Pixel Data* element from an encapsulated\n         dataset. The Basic Offset Table item should be present and the\n         Sequence Delimiter item may or may not be present.\n    nr_frames : int, optional\n        Required for multi-frame data when the Basic Offset Table is empty\n        and there are multiple frames. This should be the value of (0028,0008)\n        *Number of Frames*.\n \n     Yields\n     ------\n     ----------\n     DICOM Standard Part 5, :dcm:`Annex A <part05/chapter_A.html>`\n     \"\"\"\n    for fragmented_frame in generate_pixel_data(bytestream, nr_frames):\n         yield b''.join(fragmented_frame)\n \n \ndef generate_pixel_data(bytestream, nr_frames=None):\n     \"\"\"Yield an encapsulated pixel data frame.\n \n     For the following transfer syntaxes, a fragment may not contain encoded\n         The value of the (7fe0, 0010) *Pixel Data* element from an encapsulated\n         dataset. The Basic Offset Table item should be present and the\n         Sequence Delimiter item may or may not be present.\n    nr_frames : int, optional\n        Required for multi-frame data when the Basic Offset Table is empty\n        and there are multiple frames. This should be the value of (0028,0008)\n        *Number of Frames*.\n \n     Yields\n     -------\n         An encapsulated pixel data frame, with the contents of the\n         :class:`tuple` the frame's fragmented data.\n \n    Notes\n    -----\n    If the Basic Offset Table is empty and there are multiple fragments per\n    frame then an attempt will be made to locate the frame boundaries by\n    searching for the JPEG/JPEG-LS/JPEG2000 EOI/EOC marker (``0xFFD9``). If the\n    marker is not present or the pixel data hasn't been compressed using one of\n    the JPEG standards then the generated pixel data may be incorrect.\n\n     References\n     ----------\n     DICOM Standard Part 5, :dcm:`Annex A <part05/chapter_A.html>`\n     fp.is_little_endian = True\n \n     # `offsets` is a list of the offsets to the first fragment in each frame\n    has_bot, offsets = get_frame_offsets(fp)\n     # Doesn't actually matter what the last offset value is, as long as its\n     # greater than the total number of bytes in the fragments\n     offsets.append(len(bytestream))\n \n    if has_bot:\n        # Use the BOT to determine the frame boundaries\n        frame = []\n        frame_length = 0\n        frame_number = 0\n        for fragment in generate_pixel_data_fragment(fp):\n            if frame_length < offsets[frame_number + 1]:\n                frame.append(fragment)\n            else:\n                yield tuple(frame)\n                frame = [fragment]\n                frame_number += 1\n\n            frame_length += len(fragment) + 8\n\n        # Yield the final frame - required here because the frame_length will\n        # never be greater than offsets[-1] and thus never trigger the final\n        # yield within the for block\n        yield tuple(frame)\n    else:\n        nr_fragments = get_nr_fragments(fp)\n        if nr_fragments == 1:\n            # Single fragment: 1 frame\n            for fragment in generate_pixel_data_fragment(fp):\n                yield tuple([fragment])\n        elif nr_frames:\n            # Multiple fragments: 1 or more frames\n            if nr_fragments == nr_frames:\n                # 1 fragment per frame\n                # Covers RLE and others if 1:1 ratio\n                for fragment in generate_pixel_data_fragment(fp):\n                    yield tuple([fragment])\n            elif nr_frames == 1:\n                # Multiple fragments: 1 frame\n                frame = []\n                for fragment in generate_pixel_data_fragment(fp):\n                    frame.append(fragment)\n                yield tuple(frame)\n            elif nr_fragments > nr_frames:\n                # More fragments then frames\n                # Search for JPEG/JPEG-LS/JPEG2K EOI/EOC marker\n                # Should be the last two bytes of a frame\n                # May fail if no EOI/EOC marker or not JPEG\n                eoi_marker = b'\\xff\\xd9'\n                frame = []\n                frame_nr = 0\n                for fragment in generate_pixel_data_fragment(fp):\n                    frame.append(fragment)\n                    if eoi_marker in fragment[-10:]:\n                        yield tuple(frame)\n                        frame_nr += 1\n                        frame = []\n\n                if frame or frame_nr != nr_frames:\n                    # If data in `frame` or fewer frames yielded then we\n                    #   must've missed a frame boundary\n                    warnings.warn(\n                        \"The end of the encapsulated pixel data has been \"\n                        \"reached but one or more frame boundaries may have \"\n                        \"been missed; please confirm that the generated frame \"\n                        \"data is correct\"\n                    )\n                    if frame:\n                        yield tuple(frame)\n\n            else:\n                # Fewer fragments than frames\n                raise ValueError(\n                    \"Unable to parse encapsulated pixel data as the Basic \"\n                    \"Offset Table is empty and there are fewer fragments then \"\n                    \"frames; the dataset may be corrupt\"\n                )\n         else:\n            # Multiple fragments but unknown number of frames\n            raise ValueError(\n                \"Unable to determine the frame boundaries for the \"\n                \"encapsulated pixel data as the Basic Offset Table is empty \"\n                \"and `nr_frames` parameter is None\"\n            )\n \n \n def decode_data_sequence(data):"
  },
  {
    "instruction": "0 byte file causes traceback on dcmreader\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nTrying to open a 0 byte file with dcmreader causes a traceback originating in the read_partial method. The problem is line 692 in filereader.py (GitHub):\r\n`    fileobj.seek(-1, 1)`\r\nChanging this to:\r\n`    if peek != b'':\r\n        fileobj.seek(-1, 1)`\r\nAppears to solve the problem, but I don't have the experience to test thoroughly.\r\n\r\n#### Steps/Code to Reproduce\r\nCreate a 0 byte file\r\n$ touch mysillyfile.dcm\r\nStart python, import pydicom and read the the file\r\n$ python3\r\nPython 3.6.8 (default, Jan  3 2019, 16:11:14) \r\n[GCC 8.2.1 20181215 (Red Hat 8.2.1-6)] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import pydicom\r\n>>> image = pydicom.dcmread('mysillyfile.dcm',force=True)\r\n\r\n#### Expected Results\r\nShould either warn that the file is not DICOM or exit gracefully\r\n\r\n#### Actual Results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/site-packages/pydicom/filereader.py\", line 880, in dcmread\r\n    force=force, specific_tags=specific_tags)\r\n  File \"/usr/local/lib/python3.6/site-packages/pydicom/filereader.py\", line 693, in read_partial\r\n    fileobj.seek(-1, 1)\r\nOSError: [Errno 22] Invalid argument\r\n\r\n#### Versions\r\n>>> import platform; print(platform.platform())\r\nLinux-4.19.16-200.fc28.x86_64-x86_64-with-fedora-28-Twenty_Eight\r\n>>> import sys; print(\"Python\", sys.version)\r\nPython 3.6.8 (default, Jan  3 2019, 16:11:14) \r\n[GCC 8.2.1 20181215 (Red Hat 8.2.1-6)]\r\n>>> import numpy; print(\"numpy\", numpy.__version__)\r\nnumpy 1.16.1\r\n>>> import pydicom; print(\"pydicom\", pydicom.__version__)\r\npydicom 1.2.2\r\n>>> \r\n\r\nRegards\r\nAlan\r\n\r\n<!-- Thanks for contributing! -->\r\n\n",
    "input": " \n     # Check to see if there's anything left to read\n     peek = fileobj.read(1)\n    fileobj.seek(-1, 1)\n \n     # `filobj` should be positioned at the start of the dataset by this point.\n     # Ensure we have appropriate values for `is_implicit_VR` and",
    "output": " \n     # Check to see if there's anything left to read\n     peek = fileobj.read(1)\n    if peek != b'':\n        fileobj.seek(-1, 1)\n \n     # `filobj` should be positioned at the start of the dataset by this point.\n     # Ensure we have appropriate values for `is_implicit_VR` and"
  },
  {
    "instruction": "pydicom produces invalid DICOM files if ds.EncapsulatedDocument contains byte array of odd length\n**Bug Description**\r\nWhen inserting a byte array of odd length into the ds.EncapsulatedDocument field, and saving this as a DICOM file, the DICOM file produced is not valid. This happens because the resulting file produced also have an odd number of bytes in the (0042,0011) OB Encapsulated Document DICOM tag which is not allowed according to the DICOM sepcification for Value Fields, http://dicom.nema.org/dicom/2013/output/chtml/part05/chapter_7.html\r\n\r\n**Expected behavior**\r\nEither pydicom could through and error specifying that the ds.EncapsulatedDocument field should contain an array of even length, or it could fix the problem by add and extra zero byte to the end of the ds.EncapsulatedDocument byte array when the length is odd.\r\n\r\n**Steps To Reproduce**\r\nI have written the following pdf2dcm.py command line utility to mimic the behaviour of pdf2dcm in the dcmtk suite:\r\n\r\n```python\r\n# inspired by: https://github.com/rohithkumar31/pdf2dicom\r\n\r\nimport argparse\r\nimport pydicom\r\n\r\nEncapsulatedPDFStorage = '1.2.840.10008.5.1.4.1.1.104.1'\r\n\r\n\r\ndef generate_dicom_from_pdf(input_file, output_file, zero_pad=True):\r\n    file_meta = pydicom.dataset.Dataset()\r\n\r\n    # FileMetaInformationGroupLength only gets rewritten when saved if present\r\n    file_meta.FileMetaInformationGroupLength = 206\r\n\r\n    file_meta.MediaStorageSOPClassUID = EncapsulatedPDFStorage\r\n\r\n    file_meta.MediaStorageSOPInstanceUID = pydicom.uid.generate_uid(pydicom.uid.PYDICOM_ROOT_UID)\r\n\r\n    # from: https://pydicom.github.io/pydicom/dev/reference/uid.html\r\n    file_meta.TransferSyntaxUID = pydicom.uid.ExplicitVRLittleEndian\r\n\r\n    pydicom.dataset.validate_file_meta(file_meta, enforce_standard=True)\r\n\r\n    # see: http://dicom.nema.org/dicom/2013/output/chtml/part10/chapter_7.html\r\n    preamble = b\"\\0\" * 128\r\n\r\n    ds = pydicom.dataset.FileDataset(output_file, {}, file_meta=file_meta, preamble=preamble)\r\n    # ds.fix_meta_info()\r\n\r\n    ds.is_little_endian = True\r\n    ds.is_implicit_VR = False\r\n\r\n    ds.SpecificCharacterSet = 'ISO_IR 100'\r\n\r\n    import datetime\r\n    dt = datetime.datetime.now()\r\n    ds.InstanceCreationDate = dt.strftime('%Y%m%d')\r\n    ds.InstanceCreationTime = dt.strftime('%H%M%S')  # ('%H%M%S.%f')\r\n\r\n    ds.SOPClassUID = EncapsulatedPDFStorage\r\n    ds.SOPInstanceUID = file_meta.MediaStorageSOPInstanceUID\r\n    ds.StudyDate = None\r\n    ds.AcquisitionDateTime = None\r\n    ds.StudyTime = None\r\n    ds.ContentTime = None\r\n    ds.ContentDate = None\r\n    ds.AccessionNumber = None\r\n    ds.Modality = 'DOC'  # document\r\n    ds.ConversionType = 'WSD'  # workstation\r\n    ds.Manufacturer = None\r\n    ds.ReferringPhysicianName = None\r\n    ds.PatientName = None\r\n    ds.PatientID = None\r\n    ds.PatientBirthDate = None\r\n    ds.PatientSex = None\r\n    ds.StudyInstanceUID = pydicom.uid.generate_uid()\r\n    ds.SeriesInstanceUID = pydicom.uid.generate_uid()\r\n    ds.StudyID = None\r\n    ds.SeriesNumber = 1\r\n    ds.InstanceNumber = 1\r\n    ds.BurnedInAnnotation = 'YES'\r\n    ds.ConceptNameCodeSequence = None\r\n    # ConceptNameCodeSequence also sets: ds.SequenceDelimitationItem\r\n    ds.DocumentTitle = None\r\n\r\n    with open(input_file, 'rb') as f:\r\n        pdf_file_as_bytes = f.read()\r\n\r\n    # DICOM Value Fields must according to the\r\n    # specification be an even number of bytes, see:\r\n    # http://dicom.nema.org/dicom/2013/output/chtml/part05/chapter_7.html\r\n    if zero_pad and len(pdf_file_as_bytes) % 2 != 0:\r\n        pdf_file_as_bytes += b\"\\0\"\r\n\r\n    ds.EncapsulatedDocument = pdf_file_as_bytes\r\n    ds.MIMETypeOfEncapsulatedDocument = 'application/pdf'\r\n\r\n    ds.save_as(output_file)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--disable-zero-padding', action='store_false')\r\n    parser.add_argument('-i', '--input-file')\r\n    parser.add_argument('-o', '--output-file')\r\n    args = parser.parse_args()\r\n\r\n    generate_dicom_from_pdf(args.input_file, args.output_file, args.disable_zero_padding)\r\n```\r\n\r\nTo reproduce the problem the --disable-zero-padding parameter can be added, and a pdf file of odd number of bytes in length can be used as input to the program, this will then product an output DICOM file containing an odd number of bytes in the (0042,0011) OB Encapsulated Document DICOM tag, which can be checked using the dciodvfy validation tool from the dicom3tools package:\r\n\r\n```bash\r\nwget http://dicom.nema.org/medical/dicom/current/output/pdf/part05.pdf\r\nls -l part05.pdf # should be odd number of bytes, currently 4676213 for this file\r\npython pdf2dcm.py --disable-zero-padding -i part05.pdf -o part05.dcm\r\n\r\n(0x0042,0x0011) OB Encapsulated Document  - Error - Bad Value Length - not a multiple of 2 - VL is 0x475a75 should be 0x475a76\r\nError - Dicom dataset read failed\r\n```\r\n\r\n**Environment**\r\n\r\n```bash\r\n$ python -m pydicom.env_info\r\n\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-5.13.0-7614-generic-x86_64-with-glibc2.31\r\nPython       | 3.9.5 (default, Jun  4 2021, 12:28:51)  [GCC 7.5.0]\r\npydicom      | 2.2.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.21.2\r\nPIL          | _module not found_\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\n",
    "input": " \n def write_OBvalue(fp: DicomIO, elem: DataElement) -> None:\n     \"\"\"Write a data_element with VR of 'other byte' (OB).\"\"\"\n    fp.write(cast(bytes, elem.value))\n \n \n def write_OWvalue(fp: DicomIO, elem: DataElement) -> None:",
    "output": " \n def write_OBvalue(fp: DicomIO, elem: DataElement) -> None:\n     \"\"\"Write a data_element with VR of 'other byte' (OB).\"\"\"\n    if len(elem.value) % 2:\n        # Pad odd length values\n        fp.write(cast(bytes, elem.value))\n        fp.write(b'\\x00')\n    else:\n        fp.write(cast(bytes, elem.value))\n \n \n def write_OWvalue(fp: DicomIO, elem: DataElement) -> None:"
  },
  {
    "instruction": "pydicom should not define handler, formatter and log level.\nThe `config` module (imported when pydicom is imported) defines a handler and set the log level for the pydicom logger. This should not be the case IMO. It should be the responsibility of the client code of pydicom to configure the logging module to its convenience. Otherwise one end up having multiple logs record as soon as pydicom is imported:\r\n\r\nExample:\r\n```\r\nCould not import pillow\r\n2018-03-25 15:27:29,744 :: DEBUG :: pydicom \r\n  Could not import pillow\r\nCould not import jpeg_ls\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import jpeg_ls\r\nCould not import gdcm\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import gdcm\r\n``` \r\nOr am I missing something?\n",
    "input": " \n # Logging system and debug function to change logging level\n logger = logging.getLogger('pydicom')\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\"%(message)s\")\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n \n \n import pydicom.pixel_data_handlers.numpy_handler as np_handler  # noqa\n \"\"\"\n \n \ndef debug(debug_on=True):\n    \"\"\"Turn debugging of DICOM file reading and writing on or off.\n     When debugging is on, file location and details about the\n     elements read at that location are logged to the 'pydicom'\n     logger using python's logging module.\n \n    :param debug_on: True (default) to turn on debugging,\n    False to turn off.\n     \"\"\"\n     global logger, debugging\n     if debug_on:\n         logger.setLevel(logging.DEBUG)\n         debugging = True\n \n \n # force level=WARNING, in case logging default is set differently (issue 103)\ndebug(False)",
    "output": " \n # Logging system and debug function to change logging level\n logger = logging.getLogger('pydicom')\nlogger.addHandler(logging.NullHandler())\n \n \n import pydicom.pixel_data_handlers.numpy_handler as np_handler  # noqa\n \"\"\"\n \n \ndef debug(debug_on=True, default_handler=True):\n    \"\"\"Turn on/off debugging of DICOM file reading and writing.\n\n     When debugging is on, file location and details about the\n     elements read at that location are logged to the 'pydicom'\n     logger using python's logging module.\n \n    Parameters\n    ----------\n    debug_on : bool, optional\n        If True (default) then turn on debugging, False to turn off.\n    default_handler : bool, optional\n        If True (default) then use ``logging.StreamHandler()`` as the handler\n        for log messages.\n     \"\"\"\n     global logger, debugging\n\n    if default_handler:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\"%(message)s\")\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n\n     if debug_on:\n         logger.setLevel(logging.DEBUG)\n         debugging = True\n \n \n # force level=WARNING, in case logging default is set differently (issue 103)\ndebug(False, False)"
  },
  {
    "instruction": "KeyError when saving a FileSet\n**Describe the bug**\r\nSaving a fileset that was loaded using DICOMDIR returns a Key Error.\r\n\r\n**Expected behavior**\r\nFileset is saved without error\r\n\r\n**Steps To Reproduce**\r\nCode:\r\n```python\r\nfrom pydicom.fileset import FileSet\r\n\r\nfpath=\"DICOMDIR\"\r\ndata=FileSet(fpath)\r\n\r\nprint(data)\r\n\r\ndata.write(use_existing=True)\r\n```\r\n\r\n```\r\nTraceback:\r\nKeyError                                  \r\n\r\nTraceback (most recent call last) \r\n\\<ipython-input-183-effc2d1f6bc9\\> in \\<module\\>\r\n      6 print(data)\r\n      7 \r\n----> 8 data.write(use_existing=True)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/fileset.py in write(self, path, use_existing, force_implicit)\r\n   2146                 self._write_dicomdir(f, force_implicit=force_implicit)\r\n   2147 \r\n-> 2148             self.load(p, raise_orphans=True)\r\n   2149 \r\n   2150             return\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/fileset.py in load(self, ds_or_path, include_orphans, raise_orphans)\r\n   1641             ds = ds_or_path\r\n   1642         else:\r\n-> 1643             ds = dcmread(ds_or_path)\r\n   1644 \r\n   1645         sop_class = ds.file_meta.get(\"MediaStorageSOPClassUID\", None)\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in dcmread(fp, defer_size, stop_before_pixels, force, specific_tags)\r\n   1032             defer_size=size_in_bytes(defer_size),\r\n   1033             force=force,\r\n-> 1034             specific_tags=specific_tags,\r\n   1035         )\r\n   1036     finally:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/filereader.py in read_partial(fileobj, stop_when, defer_size, force, specific_tags)\r\n    885             file_meta_dataset,\r\n    886             is_implicit_VR,\r\n--> 887             is_little_endian,\r\n    888         )\r\n    889     else:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/dicomdir.py in __init__(self, filename_or_obj, dataset, preamble, file_meta, is_implicit_VR, is_little_endian)\r\n     94 \r\n     95         self.patient_records: List[Dataset] = []\r\n---> 96         self.parse_records()\r\n     97 \r\n     98     def parse_records(self) -> None:\r\n\r\n~/anaconda3/lib/python3.7/site-packages/pydicom/dicomdir.py in parse_records(self)\r\n    143                 )\r\n    144                 if child_offset:\r\n--> 145                     child = map_offset_to_record[child_offset]\r\n    146                     record.children = get_siblings(child, map_offset_to_record)\r\n    147 \r\n\r\nKeyError: 572\r\n```\r\n\r\n**Your environment**\r\n\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-4.15.0-142-generic-x86_64-with-debian-stretch-sid\r\nPython       | 3.7.10 (default, Feb 26 2021, 18:47:35)  [GCC 7.3.0]\r\npydicom      | 2.2.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.2\r\nPIL          | 8.2.0\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n\n",
    "input": "         self.is_little_endian: Optional[bool] = None\n         self.is_implicit_VR: Optional[bool] = None\n \n         # the parent data set, if this dataset is a sequence item\n         self.parent: \"Optional[weakref.ReferenceType[Dataset]]\" = None\n \ndiff --git a/pydicom/fileset.py b/pydicom/fileset.py\n                     f\"{cast(Path, self.path) / file_id}\"\n                 )\n                 continue\n             # If the instance's existing directory structure doesn't match\n             #   the pydicom semantics then stage for movement\n             if instance.for_moving:\n             offset += 8  # a sequence item's (tag + length)\n             # Copy safe - only modifies RecordNode._offset\n             offset += node._encode_record(force_implicit)\n \n         # Step 2: Update the records and add to *Directory Record Sequence*\n         ds.DirectoryRecordSequence = []",
    "output": "         self.is_little_endian: Optional[bool] = None\n         self.is_implicit_VR: Optional[bool] = None\n \n        # True if the dataset is a sequence item with undefined length\n        self.is_undefined_length_sequence_item = False\n\n         # the parent data set, if this dataset is a sequence item\n         self.parent: \"Optional[weakref.ReferenceType[Dataset]]\" = None\n \ndiff --git a/pydicom/fileset.py b/pydicom/fileset.py\n                     f\"{cast(Path, self.path) / file_id}\"\n                 )\n                 continue\n\n             # If the instance's existing directory structure doesn't match\n             #   the pydicom semantics then stage for movement\n             if instance.for_moving:\n             offset += 8  # a sequence item's (tag + length)\n             # Copy safe - only modifies RecordNode._offset\n             offset += node._encode_record(force_implicit)\n            # If the sequence item has undefined length then it uses a\n            #   sequence item delimiter item\n            if node._record.is_undefined_length_sequence_item:\n                offset += 8\n \n         # Step 2: Update the records and add to *Directory Record Sequence*\n         ds.DirectoryRecordSequence = []"
  },
  {
    "instruction": "Make PersonName3 iterable\n```python\r\nfrom pydicom import Dataset\r\n\r\nds = Dataset()\r\nds.PatientName = 'SomeName'\r\n\r\n'S' in ds.PatientName\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: argument of type 'PersonName3' is not iterable\r\n```\r\n\r\nI'm not really sure if this is intentional or if PN elements should support `str` methods. And yes I know I can `str(ds.PatientName)` but it's a bit silly, especially when I keep having to write exceptions to my element iterators just for PN elements.\n",
    "input": " # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n \"\"\"Special classes for DICOM value representations (VR)\"\"\"\nfrom copy import deepcopy\n from decimal import Decimal\n import re\n \n     def __str__(self):\n         return '='.join(self.components).__str__()\n \n     def __repr__(self):\n         return '='.join(self.components).__repr__()\n ",
    "output": " # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n \"\"\"Special classes for DICOM value representations (VR)\"\"\"\n from decimal import Decimal\n import re\n \n     def __str__(self):\n         return '='.join(self.components).__str__()\n \n    def __next__(self):\n        # Get next character or stop iteration\n        if self._i < self._rep_len:\n            c = self._str_rep[self._i]\n            self._i += 1\n            return c\n        else:\n            raise StopIteration\n\n    def __iter__(self):\n        # Get string rep. and length, initialize index counter\n        self._str_rep = self.__str__()\n        self._rep_len = len(self._str_rep)\n        self._i = 0\n        return self\n\n    def __contains__(self, x):\n        return x in self.__str__()\n\n     def __repr__(self):\n         return '='.join(self.components).__repr__()\n "
  },
  {
    "instruction": "Print byte values for unknown VR during read\n#### Description\r\nIf the dataset read fails due to an unknown VR then the exception message prints the VR bytes in a format that isn't useful for debugging.\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom io import BytesIO\r\nfrom pydicom.filereader import read_dataset\r\nds = read_dataset(BytesIO(b'\\x08\\x00\\x01\\x00\\x04\\x00\\x00\\x00\\x00\\x08\\x00\\x49'), False, True)\r\nprint(ds)\r\n```\r\n\r\n#### Expected Results\r\n```\r\nNotImplementedError: Unknown Value Representation: '32 31' in tag (0000, 0002)\r\n```\r\n#### Actual Results\r\n```\r\nFile \"<stdin>\", line 1, in <module>\r\n  File \".../pydicom/pydicom/dataset.py\", line 1284, in __str__\r\n    return self._pretty_str()\r\n  File \".../pydicom/pydicom/dataset.py\", line 1022, in _pretty_str\r\n    for data_element in self:\r\n  File \".../pydicom/pydicom/dataset.py\", line 751, in __iter__\r\n    yield self[tag]\r\n  File \".../pydicom/pydicom/dataset.py\", line 637, in __getitem__\r\n    self[tag] = DataElement_from_raw(data_elem, character_set)\r\n  File \".../pydicom/pydicom/dataelem.py\", line 447, in DataElement_from_raw\r\n    raise NotImplementedError(\"{0:s} in tag {1!r}\".format(str(e), raw.tag))\r\nNotImplementedError: Unknown Value Representation '\u0004' in tag (0008, 0001)\r\n```\r\n[Or see here for another example](https://user-images.githubusercontent.com/28559755/51027486-4abf4100-1591-11e9-8f44-a739b00ca300.PNG)\r\n\r\n\n",
    "input": " def convert_value(VR, raw_data_element, encodings=None):\n     \"\"\"Return the converted value (from raw bytes) for the given VR\"\"\"\n     if VR not in converters:\n        message = \"Unknown Value Representation '{0}'\".format(VR)\n         raise NotImplementedError(message)\n \n     # Look up the function to convert that VR",
    "output": " def convert_value(VR, raw_data_element, encodings=None):\n     \"\"\"Return the converted value (from raw bytes) for the given VR\"\"\"\n     if VR not in converters:\n        # `VR` characters are in the ascii alphabet ranges 65 - 90, 97 - 122\n        char_range = list(range(65, 91)) + list(range(97, 123))\n        # If the VR characters are outside that range then print hex values\n        if ord(VR[0]) not in char_range or ord(VR[1]) not in char_range:\n            VR = ' '.join(['0x{:02x}'.format(ord(ch)) for ch in VR])\n        message = \"Unknown Value Representation '{}'\".format(VR)\n         raise NotImplementedError(message)\n \n     # Look up the function to convert that VR"
  },
  {
    "instruction": "\"TypeError: 'NoneType' object is not subscriptable\" when reading dcm file with empty string as Chartset and \"use_none_as_empty_text_VR_value=True\"\n**Describe the bug**\r\nOnce thing I noticed is that `convert_encodings` in `charset.py` expects a list of encodings (according to the docstrings) from tag `0008,0005` but it can be just a value. \r\n\r\nThe problem is when reading Dicom files in production environments I noticed that some devices that are capturing the DICOMs are not very DICOM Compliant and is sending empty string , which it should be allowed as `0008,0005` is a 1C type, which means that if present it should have a valid value. \r\n\r\nI enabled `use_none_as_empty_text_VR_value` to make sure other tags whose value should be float or int have None instead of empty string, but if `0008,0005` value is empty string is switched to None and `convert_encodings` fails with `TypeError: 'NoneType' object is not subscriptable`\r\n\r\n**Expected behavior**\r\nThe expected behavior should be that if empty string or not present it should default to:\r\n```\r\n# default encoding if no encoding defined - corresponds to ISO IR 6 / ASCII\r\ndefault_encoding = \"iso8859\"\r\n```\r\n\r\n**Steps To Reproduce**\r\n\r\nout.dcm file if provided for testing with mock data but `Specific Character Set` set to empty string\r\n\r\nIf setting the `(0008, 0005) Specific Character Set` to empty string and setting `pydicom.config.use_none_as_empty_text_VR_value = True`\r\n\r\n```\r\n>>> import pydicom\r\n>>> pydicom.config.datetime_conversion = True\r\n>>> pydicom.config.allow_DS_float = True\r\n>>> pydicom.config.use_none_as_empty_text_VR_value = True\r\n>>> dataset = pydicom.dcmread(\"test.dcm\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/filereader.py\", line 871, in dcmread\r\n    force=force, specific_tags=specific_tags)\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/filereader.py\", line 744, in read_partial\r\n    specific_tags=specific_tags)\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/filereader.py\", line 383, in read_dataset\r\n    encoding = convert_encodings(char_set)\r\n  File \"/Users/bernardo/.virtualenvs/backend-api/lib/python3.7/site-packages/pydicom/charset.py\", line 638, in convert_encodings\r\n    encodings = encodings[:]\r\nTypeError: 'NoneType' object is not subscriptable\r\n>>> pydicom.config.use_none_as_empty_text_VR_value = False\r\n>>> dataset = pydicom.dcmread(\"test.dcm\")\r\n```\r\n`(0008, 0005) Specific Character Set              CS: ''`\r\n\r\n**Your environment**\r\n\r\n```bash\r\npython -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | Darwin-19.6.0-x86_64-i386-64bit\r\nPython       | 3.7.6 (default, Dec 30 2019, 19:38:26)  [Clang 11.0.0 (clang-1100.0.33.16)]\r\npydicom      | 2.0.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | _module not found_\r\nPIL          | 7.0.0\r\n```\r\n\r\n\r\n[out.dcm.zip](https://github.com/pydicom/pydicom/files/5248618/out.dcm.zip)\r\n\n",
    "input": "         ``True``.\n     \"\"\"\n \n    # If a list if passed, we don't want to modify the list in place so copy it\n    encodings = encodings[:]\n \n     if isinstance(encodings, str):\n         encodings = [encodings]\n    elif not encodings[0]:\n        encodings[0] = 'ISO_IR 6'\n \n     py_encodings = []\n     for encoding in encodings:",
    "output": "         ``True``.\n     \"\"\"\n \n    encodings = encodings or ['']\n \n     if isinstance(encodings, str):\n         encodings = [encodings]\n    else:\n        # If a list if passed, we don't want to modify the list\n        # in place so copy it\n        encodings = encodings[:]\n        if not encodings[0]:\n            encodings[0] = 'ISO_IR 6'\n \n     py_encodings = []\n     for encoding in encodings:"
  },
  {
    "instruction": "apply_voi_lut - unclear what it does if both WL/VOILUTFunction _and_ VOILUTSequence are present\nhttps://pydicom.github.io/pydicom/dev/reference/generated/pydicom.pixel_data_handlers.util.html#pydicom.pixel_data_handlers.util.apply_voi_lut\r\n\r\nHi all,\r\n\r\nI'm working with some mammo image (digital) that have both \r\n- window/level (0028,1050 0028,1051) plus VOILUTFunction (0028,1056) (set to SIGMOID) (set of 3 WL values)\r\n- VOILUT sequences (0028, 3010)\r\n\r\nspecified.\r\n\r\nProblem\r\n---\r\n\r\nIt's unclear from the documentation when both a VOILUT (0028,3010) _and_ WL (0028,1051...) are present which is applied - the lut or the wl.\r\n\r\nIt just says if a LUT's present, it will apply that, and if a WL set is present it will apply that.\r\n\r\nQuestions\r\n---\r\n\r\n- If both LUT and WL are supplied, by the dicom standard, which should be applied?\r\n- Separately to the above question about which is applied, if _both_ LUT and WL sequences are supplied, is there a way in `apply_voi_lut` to specify applying one or the other?  (ie force application of the WL instead of LUT etc)\r\n\r\n- Also, if an image has a sequence of WL values rather than being single valued (so 0028,1050 & 0028,1051 are sequences), does the `index` parameter to `apply_voi_lut` apply to specify which in the sequence you want to use?\r\n\r\nThanks!\r\n\napply_voi_lut can't handle missing DICOM meta info\nI have encountered two real life examples where `apply_voi_lut` does not handle corruption in DICOM meta fields\r\n\r\ncase 1:\r\n```\r\n(0028, 1050) Window Center                       DS: \"128.0\"\r\n(0028, 1051) Window Width                        DS: \"256.0\"\r\n(0028, 1052) Rescale Intercept                   DS: None\r\n(0028, 1053) Rescale Slope                       DS: None\r\n```\r\nthrows an exception\r\n\r\n```\r\n  File \"python3.7/site-packages/pydicom/pixel_data_handlers/util.py\", line 380, in apply_voi_lut\r\n    y_min = y_min * ds.RescaleSlope + ds.RescaleIntercept\r\nTypeError: unsupported operand type(s) for *: 'int' and 'NoneType' \r\n```\r\n\r\n\r\ncase 2:\r\n\r\n```\r\n(0028, 1050) Window Center                       DS: \"2607.0\"\r\n(0028, 1051) Window Width                        DS: \"2785.0\"\r\n(0028, 1052) Rescale Intercept                   DS: \"0.0\"\r\n(0028, 1053) Rescale Slope                       DS: \"1.0\"\r\n(0028, 1054) Rescale Type                        LO: 'US'\r\n(0028, 2110) Lossy Image Compression             CS: '00'\r\n(0028, 3010)  VOI LUT Sequence   1 item(s) ---- \r\n   (0028, 3002) LUT Descriptor                      SS: None\r\n   (0028, 3003) LUT Explanation                     LO: 'Noramal'\r\n   (0028, 3006) LUT Data                            OW: None\r\n```\r\n\r\nthrows an exception\r\n\r\n```\r\n  File \"python3.7/site-packages/pydicom/pixel_data_handlers/util.py\", line 312, in apply_voi_lut\r\n    nr_entries = item.LUTDescriptor[0] or 2**16\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\r\n\r\n\r\nSo far I have handled this with:\r\n\r\n```\r\n    def _lut_convert(self):\r\n        return apply_voi_lut(self.input_dicom.pixel_array, self.input_dicom)\r\n\r\n    def _get_raw_data(self):\r\n\r\n        # convert to presentation LUT\r\n        try:\r\n            data = self._lut_convert()\r\n        # many things can be corrupted in the VOILUTSequence attribute,\r\n        # fall back to default WC/WW conversion\r\n        except Exception as e:\r\n            try:\r\n                if \"VOILUTSequence\" in self.input_dicom:\r\n                    del self.input_dicom[\"VOILUTSequence\"]\r\n                    data = self._lut_convert()\r\n            except Exception as e:\r\n                raise InvalidImage(f\"Could not convert to presentation LUT due to: {e}\")\r\n```\r\n\r\nWhile the case 1 could be seen as an expected behavior (?), I imagine case 2 should be handled by WC/WW transformations if followed DICOM standard?\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n",
    "input": " \n from pydicom.pixel_data_handlers.util import (\n     apply_color_lut, apply_modality_lut, apply_voi_lut, convert_color_space,\n )\n \n apply_rescale = apply_modality_lut\napply_windowing = apply_voi_lut\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n \n from struct import unpack\n from sys import byteorder\nfrom typing import Dict\n import warnings\n \n try:\n from pydicom.data import get_palette_files\n from pydicom.uid import UID\n \n \ndef apply_color_lut(arr, ds=None, palette=None):\n     \"\"\"Apply a color palette lookup table to `arr`.\n \n     .. versionadded:: 1.4\n     return out\n \n \ndef apply_modality_lut(arr, ds):\n     \"\"\"Apply a modality lookup table or rescale operation to `arr`.\n \n     .. versionadded:: 1.4\n     return arr\n \n \ndef apply_voi_lut(arr, ds, index=0):\n     \"\"\"Apply a VOI lookup table or windowing operation to `arr`.\n \n     .. versionadded:: 1.4\n \n     Parameters\n     ----------\n     arr : numpy.ndarray\n         ``np.float64``. If neither are present then `arr` will be returned\n         unchanged.\n     index : int, optional\n        Where the VOI LUT Module contains multiple possible views, this is\n         the index of the view to return (default ``0``).\n \n     Returns\n     -------\n     See Also\n     --------\n     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n \n     References\n     ----------\n     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n     if 'VOILUTSequence' in ds:\n        if not np.issubdtype(arr.dtype, np.integer):\n            warnings.warn(\n                \"Applying a VOI LUT on a float input array may give \"\n                \"incorrect results\"\n            )\n \n        # VOI LUT Sequence contains one or more items\n        item = ds.VOILUTSequence[index]\n        nr_entries = item.LUTDescriptor[0] or 2**16\n        first_map = item.LUTDescriptor[1]\n \n        # PS3.3 C.8.11.3.1.5: may be 8, 10-16\n        nominal_depth = item.LUTDescriptor[2]\n        if nominal_depth in list(range(10, 17)):\n            dtype = 'uint16'\n        elif nominal_depth == 8:\n            dtype = 'uint8'\n        else:\n            raise NotImplementedError(\n                \"'{}' bits per LUT entry is not supported\"\n                .format(nominal_depth)\n            )\n \n        # Ambiguous VR, US or OW\n        if item['LUTData'].VR == 'OW':\n            endianness = '<' if ds.is_little_endian else '>'\n            unpack_fmt = '{}{}H'.format(endianness, nr_entries)\n            lut_data = unpack(unpack_fmt, item.LUTData)\n        else:\n            lut_data = item.LUTData\n        lut_data = np.asarray(lut_data, dtype=dtype)\n \n        # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n        clipped_iv = np.zeros(arr.shape, dtype=dtype)\n        # IVs >= `first_map` are mapped by the VOI LUT\n        # `first_map` may be negative, positive or 0\n        mapped_pixels = arr >= first_map\n        clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n        # IVs > number of entries get set to last entry\n        np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n \n        return lut_data[clipped_iv]\n    elif 'WindowCenter' in ds and 'WindowWidth' in ds:\n        if ds.PhotometricInterpretation not in ['MONOCHROME1', 'MONOCHROME2']:\n            raise ValueError(\n                \"When performing a windowing operation only 'MONOCHROME1' and \"\n                \"'MONOCHROME2' are allowed for (0028,0004) Photometric \"\n                \"Interpretation\"\n            )\n \n        # May be LINEAR (default), LINEAR_EXACT, SIGMOID or not present, VM 1\n        voi_func = getattr(ds, 'VOILUTFunction', 'LINEAR').upper()\n        # VR DS, VM 1-n\n        elem = ds['WindowCenter']\n        center = elem.value[index] if elem.VM > 1 else elem.value\n        elem = ds['WindowWidth']\n        width = elem.value[index] if elem.VM > 1 else elem.value\n\n        # The output range depends on whether or not a modality LUT or rescale\n        #   operation has been applied\n        if 'ModalityLUTSequence' in ds:\n            # Unsigned - see PS3.3 C.11.1.1.1\n            y_min = 0\n            bit_depth = ds.ModalityLUTSequence[0].LUTDescriptor[2]\n            y_max = 2**bit_depth - 1\n        elif ds.PixelRepresentation == 0:\n            # Unsigned\n            y_min = 0\n            y_max = 2**ds.BitsStored - 1\n        else:\n            # Signed\n            y_min = -2**(ds.BitsStored - 1)\n            y_max = 2**(ds.BitsStored - 1) - 1\n\n        if 'RescaleSlope' in ds and 'RescaleIntercept' in ds:\n            # Otherwise its the actual data range\n            y_min = y_min * ds.RescaleSlope + ds.RescaleIntercept\n            y_max = y_max * ds.RescaleSlope + ds.RescaleIntercept\n\n        y_range = y_max - y_min\n        arr = arr.astype('float64')\n\n        if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n            # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n            if voi_func == 'LINEAR':\n                if width < 1:\n                    raise ValueError(\n                        \"The (0028,1051) Window Width must be greater than or \"\n                        \"equal to 1 for a 'LINEAR' windowing operation\"\n                    )\n                center -= 0.5\n                width -= 1\n            elif width <= 0:\n                raise ValueError(\n                    \"The (0028,1051) Window Width must be greater than 0 \"\n                    \"for a 'LINEAR_EXACT' windowing operation\"\n                )\n \n            below = arr <= (center - width / 2)\n            above = arr > (center + width / 2)\n            between = np.logical_and(~below, ~above)\n \n            arr[below] = y_min\n            arr[above] = y_max\n            if between.any():\n                arr[between] = (\n                    ((arr[between] - center) / width + 0.5) * y_range + y_min\n                )\n        elif voi_func == 'SIGMOID':\n            # PS3.3 C.11.2.1.3.1\n            if width <= 0:\n                 raise ValueError(\n                    \"The (0028,1051) Window Width must be greater than 0 \"\n                    \"for a 'SIGMOID' windowing operation\"\n                 )\n \n            arr = y_range / (1 + np.exp(-4 * (arr - center) / width)) + y_min\n        else:\n             raise ValueError(\n                \"Unsupported (0028,1056) VOI LUT Function value '{}'\"\n                .format(voi_func)\n             )\n \n     return arr\n \n \ndef convert_color_space(arr, current, desired):\n     \"\"\"Convert the image(s) in `arr` from one color space to another.\n \n     .. versionchanged:: 1.4\n     return converter(arr)\n \n \ndef _convert_RGB_to_YBR_FULL(arr):\n     \"\"\"Return an ndarray converted from RGB to YBR_FULL color space.\n \n     Parameters\n     return arr.astype(orig_dtype)\n \n \ndef _convert_YBR_FULL_to_RGB(arr):\n     \"\"\"Return an ndarray converted from YBR_FULL to RGB color space.\n \n     Parameters\n     return arr.astype(orig_dtype)\n \n \ndef dtype_corrected_for_endianness(is_little_endian, numpy_dtype):\n     \"\"\"Return a :class:`numpy.dtype` corrected for system and :class:`Dataset`\n     endianness.\n \n     return numpy_dtype\n \n \ndef _expand_segmented_lut(data, fmt, nr_segments=None, last_value=None):\n     \"\"\"Return a list containing the expanded lookup table data.\n \n     Parameters\n     # Little endian: e.g. 0x0302 0x0100, big endian, e.g. 0x0203 0x0001\n     indirect_ii = [3, 2, 1, 0] if '<' in fmt else [2, 3, 0, 1]\n \n    lut = []\n     offset = 0\n     segments_read = 0\n     # Use `offset + 1` to account for possible trailing null\n     return lut\n \n \ndef get_expected_length(ds, unit='bytes'):\n     \"\"\"Return the expected length (in terms of bytes or pixels) of the *Pixel\n     Data*.\n \n         The expected length of the *Pixel Data* in either whole bytes or\n         pixels, excluding the NULL trailing padding byte for odd length data.\n     \"\"\"\n    length = ds.Rows * ds.Columns * ds.SamplesPerPixel\n     length *= get_nr_frames(ds)\n \n     if unit == 'pixels':\n         return length\n \n     # Correct for the number of bytes per pixel\n    bits_allocated = ds.BitsAllocated\n     if bits_allocated == 1:\n         # Determine the nearest whole number of bytes needed to contain\n         #   1-bit pixel data. e.g. 10 x 10 1-bit pixels is 100 bits, which\n     return length\n \n \ndef get_image_pixel_ids(ds) -> Dict[str, int]:\n     \"\"\"Return a dict of the pixel data affecting element's :func:`id` values.\n \n     .. versionadded:: 1.4\n     return {kw: id(getattr(ds, kw, None)) for kw in keywords}\n \n \ndef get_j2k_parameters(codestream):\n     \"\"\"Return a dict containing JPEG 2000 component parameters.\n \n     .. versionadded:: 2.1\n     return {}\n \n \ndef get_nr_frames(ds):\n     \"\"\"Return NumberOfFrames or 1 if NumberOfFrames is None.\n \n     Parameters\n     int\n         An integer for the NumberOfFrames or 1 if NumberOfFrames is None\n     \"\"\"\n    nr_frames = getattr(ds, 'NumberOfFrames', 1)\n     # 'NumberOfFrames' may exist in the DICOM file but have value equal to None\n     if nr_frames is None:\n         warnings.warn(\"A value of None for (0028,0008) 'Number of Frames' is \"\n     return nr_frames\n \n \ndef pixel_dtype(ds, as_float=False):\n     \"\"\"Return a :class:`numpy.dtype` for the pixel data in `ds`.\n \n     Suitable for use with IODs containing the Image Pixel module (with\n     return dtype\n \n \ndef reshape_pixel_array(ds, arr):\n     \"\"\"Return a reshaped :class:`numpy.ndarray` `arr`.\n \n     +------------------------------------------+-----------+----------+",
    "output": " \n from pydicom.pixel_data_handlers.util import (\n     apply_color_lut, apply_modality_lut, apply_voi_lut, convert_color_space,\n    apply_voi, apply_windowing\n )\n \n apply_rescale = apply_modality_lut\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n \n from struct import unpack\n from sys import byteorder\nfrom typing import Dict, Optional, Union, List, Tuple, TYPE_CHECKING, cast\n import warnings\n \n try:\n from pydicom.data import get_palette_files\n from pydicom.uid import UID\n \nif TYPE_CHECKING:\n    from pydicom.dataset import Dataset\n \n\ndef apply_color_lut(\n    arr: \"np.ndarray\",\n    ds: Optional[\"Dataset\"] = None,\n    palette: Optional[Union[str, UID]] = None\n) -> \"np.ndarray\":\n     \"\"\"Apply a color palette lookup table to `arr`.\n \n     .. versionadded:: 1.4\n     return out\n \n \ndef apply_modality_lut(arr: \"np.ndarray\", ds: \"Dataset\") -> \"np.ndarray\":\n     \"\"\"Apply a modality lookup table or rescale operation to `arr`.\n \n     .. versionadded:: 1.4\n     return arr\n \n \ndef apply_voi_lut(\n    arr: \"np.ndarray\",\n    ds: \"Dataset\",\n    index: int = 0,\n    prefer_lut: bool = True\n) -> \"np.ndarray\":\n     \"\"\"Apply a VOI lookup table or windowing operation to `arr`.\n \n     .. versionadded:: 1.4\n \n    .. versionchanged:: 2.1\n\n        Added the `prefer_lut` keyword parameter\n\n     Parameters\n     ----------\n     arr : numpy.ndarray\n         ``np.float64``. If neither are present then `arr` will be returned\n         unchanged.\n     index : int, optional\n        When the VOI LUT Module contains multiple alternative views, this is\n         the index of the view to return (default ``0``).\n    prefer_lut : bool\n        When the VOI LUT Module contains both *Window Width*/*Window Center*\n        and *VOI LUT Sequence*, if ``True`` (default) then apply the VOI LUT,\n        otherwise apply the windowing operation.\n \n     Returns\n     -------\n     See Also\n     --------\n     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n    :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n    :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n \n     References\n     ----------\n     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n    valid_voi = False\n     if 'VOILUTSequence' in ds:\n        valid_voi = None not in [\n            ds.VOILUTSequence[0].get('LUTDescriptor', None),\n            ds.VOILUTSequence[0].get('LUTData', None)\n        ]\n    valid_windowing = None not in [\n        ds.get('WindowCenter', None),\n        ds.get('WindowWidth', None)\n    ]\n \n    if valid_voi and valid_windowing:\n        if prefer_lut:\n            return apply_voi(arr, ds, index)\n \n        return apply_windowing(arr, ds, index)\n \n    if valid_voi:\n        return apply_voi(arr, ds, index)\n \n    if valid_windowing:\n        return apply_windowing(arr, ds, index)\n \n    return arr\n \n \ndef apply_voi(\n    arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n) -> \"np.ndarray\":\n    \"\"\"Apply a VOI lookup table to `arr`.\n \n    .. versionadded:: 2.1\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        The :class:`~numpy.ndarray` to apply the VOI LUT to.\n    ds : dataset.Dataset\n        A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n        If (0028,3010) *VOI LUT Sequence* is present then returns an array\n        of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n        (0028,3002) *LUT Descriptor*, otherwise `arr` will be returned\n        unchanged.\n    index : int, optional\n        When the VOI LUT Module contains multiple alternative views, this is\n        the index of the view to return (default ``0``).\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with applied VOI LUT.\n\n    See Also\n    --------\n    :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n    :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n\n    References\n    ----------\n    * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n      <part03/sect_C.11.html#sect_C.11.2>`\n    * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n      <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n    * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n      <part04/sect_N.2.html#sect_N.2.1.1>`\n    \"\"\"\n    if \"VOILUTSequence\" not in ds:\n        return arr\n\n    if not np.issubdtype(arr.dtype, np.integer):\n        warnings.warn(\n            \"Applying a VOI LUT on a float input array may give \"\n            \"incorrect results\"\n        )\n\n    # VOI LUT Sequence contains one or more items\n    item = ds.VOILUTSequence[index]\n    nr_entries = item.LUTDescriptor[0] or 2**16\n    first_map = item.LUTDescriptor[1]\n\n    # PS3.3 C.8.11.3.1.5: may be 8, 10-16\n    nominal_depth = item.LUTDescriptor[2]\n    if nominal_depth in list(range(10, 17)):\n        dtype = 'uint16'\n    elif nominal_depth == 8:\n        dtype = 'uint8'\n    else:\n        raise NotImplementedError(\n            f\"'{nominal_depth}' bits per LUT entry is not supported\"\n        )\n\n    # Ambiguous VR, US or OW\n    if item['LUTData'].VR == 'OW':\n        endianness = '<' if ds.is_little_endian else '>'\n        unpack_fmt = f'{endianness}{nr_entries}H'\n        lut_data = unpack(unpack_fmt, item.LUTData)\n    else:\n        lut_data = item.LUTData\n    lut_data = np.asarray(lut_data, dtype=dtype)\n\n    # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n    clipped_iv = np.zeros(arr.shape, dtype=dtype)\n    # IVs >= `first_map` are mapped by the VOI LUT\n    # `first_map` may be negative, positive or 0\n    mapped_pixels = arr >= first_map\n    clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n    # IVs > number of entries get set to last entry\n    np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n\n    return lut_data[clipped_iv]\n\n\ndef apply_windowing(\n    arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n) -> \"np.ndarray\":\n    \"\"\"Apply a windowing operation to `arr`.\n\n    .. versionadded:: 2.1\n\n    Parameters\n    ----------\n    arr : numpy.ndarray\n        The :class:`~numpy.ndarray` to apply the windowing operation to.\n    ds : dataset.Dataset\n        A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n        If (0028,1050) *Window Center* and (0028,1051) *Window Width* are\n        present then returns an array of ``np.float64``, otherwise `arr` will\n        be returned unchanged.\n    index : int, optional\n        When the VOI LUT Module contains multiple alternative views, this is\n        the index of the view to return (default ``0``).\n\n    Returns\n    -------\n    numpy.ndarray\n        An array with applied windowing operation.\n\n    Notes\n    -----\n    When the dataset requires a modality LUT or rescale operation as part of\n    the Modality LUT module then that must be applied before any windowing\n    operation.\n\n    See Also\n    --------\n    :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n    :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n\n    References\n    ----------\n    * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n      <part03/sect_C.11.html#sect_C.11.2>`\n    * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n      <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n    * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n      <part04/sect_N.2.html#sect_N.2.1.1>`\n    \"\"\"\n    if \"WindowWidth\" not in ds and \"WindowCenter\" not in ds:\n        return arr\n\n    if ds.PhotometricInterpretation not in ['MONOCHROME1', 'MONOCHROME2']:\n        raise ValueError(\n            \"When performing a windowing operation only 'MONOCHROME1' and \"\n            \"'MONOCHROME2' are allowed for (0028,0004) Photometric \"\n            \"Interpretation\"\n        )\n\n    # May be LINEAR (default), LINEAR_EXACT, SIGMOID or not present, VM 1\n    voi_func = cast(str, getattr(ds, 'VOILUTFunction', 'LINEAR')).upper()\n    # VR DS, VM 1-n\n    elem = ds['WindowCenter']\n    center = elem.value[index] if elem.VM > 1 else elem.value\n    elem = ds['WindowWidth']\n    width = elem.value[index] if elem.VM > 1 else elem.value\n\n    # The output range depends on whether or not a modality LUT or rescale\n    #   operation has been applied\n    if 'ModalityLUTSequence' in ds:\n        # Unsigned - see PS3.3 C.11.1.1.1\n        y_min = 0\n        bit_depth = ds.ModalityLUTSequence[0].LUTDescriptor[2]\n        y_max = 2**bit_depth - 1\n    elif ds.PixelRepresentation == 0:\n        # Unsigned\n        y_min = 0\n        y_max = 2**ds.BitsStored - 1\n    else:\n        # Signed\n        y_min = -2**(ds.BitsStored - 1)\n        y_max = 2**(ds.BitsStored - 1) - 1\n\n    slope = ds.get('RescaleSlope', None)\n    intercept = ds.get('RescaleIntercept', None)\n    if slope is not None and intercept is not None:\n        # Otherwise its the actual data range\n        y_min = y_min * ds.RescaleSlope + ds.RescaleIntercept\n        y_max = y_max * ds.RescaleSlope + ds.RescaleIntercept\n\n    y_range = y_max - y_min\n    arr = arr.astype('float64')\n\n    if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n        # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n        if voi_func == 'LINEAR':\n            if width < 1:\n                 raise ValueError(\n                    \"The (0028,1051) Window Width must be greater than or \"\n                    \"equal to 1 for a 'LINEAR' windowing operation\"\n                 )\n            center -= 0.5\n            width -= 1\n        elif width <= 0:\n            raise ValueError(\n                \"The (0028,1051) Window Width must be greater than 0 \"\n                \"for a 'LINEAR_EXACT' windowing operation\"\n            )\n \n        below = arr <= (center - width / 2)\n        above = arr > (center + width / 2)\n        between = np.logical_and(~below, ~above)\n\n        arr[below] = y_min\n        arr[above] = y_max\n        if between.any():\n            arr[between] = (\n                ((arr[between] - center) / width + 0.5) * y_range + y_min\n            )\n    elif voi_func == 'SIGMOID':\n        # PS3.3 C.11.2.1.3.1\n        if width <= 0:\n             raise ValueError(\n                \"The (0028,1051) Window Width must be greater than 0 \"\n                \"for a 'SIGMOID' windowing operation\"\n             )\n \n        arr = y_range / (1 + np.exp(-4 * (arr - center) / width)) + y_min\n    else:\n        raise ValueError(\n            f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\"\n        )\n\n     return arr\n \n \ndef convert_color_space(\n    arr: \"np.ndarray\", current: str, desired: str\n) -> \"np.ndarray\":\n     \"\"\"Convert the image(s) in `arr` from one color space to another.\n \n     .. versionchanged:: 1.4\n     return converter(arr)\n \n \ndef _convert_RGB_to_YBR_FULL(arr: \"np.ndarray\") -> \"np.ndarray\":\n     \"\"\"Return an ndarray converted from RGB to YBR_FULL color space.\n \n     Parameters\n     return arr.astype(orig_dtype)\n \n \ndef _convert_YBR_FULL_to_RGB(arr: \"np.ndarray\") -> \"np.ndarray\":\n     \"\"\"Return an ndarray converted from YBR_FULL to RGB color space.\n \n     Parameters\n     return arr.astype(orig_dtype)\n \n \ndef dtype_corrected_for_endianness(\n    is_little_endian: bool, numpy_dtype: \"np.dtype\"\n) -> \"np.dtype\":\n     \"\"\"Return a :class:`numpy.dtype` corrected for system and :class:`Dataset`\n     endianness.\n \n     return numpy_dtype\n \n \ndef _expand_segmented_lut(\n    data: Tuple[int, ...],\n    fmt: str,\n    nr_segments: Optional[int] = None,\n    last_value: Optional[int] = None\n) -> List[int]:\n     \"\"\"Return a list containing the expanded lookup table data.\n \n     Parameters\n     # Little endian: e.g. 0x0302 0x0100, big endian, e.g. 0x0203 0x0001\n     indirect_ii = [3, 2, 1, 0] if '<' in fmt else [2, 3, 0, 1]\n \n    lut: List[int] = []\n     offset = 0\n     segments_read = 0\n     # Use `offset + 1` to account for possible trailing null\n     return lut\n \n \ndef get_expected_length(ds: \"Dataset\", unit: str = 'bytes') -> int:\n     \"\"\"Return the expected length (in terms of bytes or pixels) of the *Pixel\n     Data*.\n \n         The expected length of the *Pixel Data* in either whole bytes or\n         pixels, excluding the NULL trailing padding byte for odd length data.\n     \"\"\"\n    length: int = ds.Rows * ds.Columns * ds.SamplesPerPixel\n     length *= get_nr_frames(ds)\n \n     if unit == 'pixels':\n         return length\n \n     # Correct for the number of bytes per pixel\n    bits_allocated = cast(int, ds.BitsAllocated)\n     if bits_allocated == 1:\n         # Determine the nearest whole number of bytes needed to contain\n         #   1-bit pixel data. e.g. 10 x 10 1-bit pixels is 100 bits, which\n     return length\n \n \ndef get_image_pixel_ids(ds: \"Dataset\") -> Dict[str, int]:\n     \"\"\"Return a dict of the pixel data affecting element's :func:`id` values.\n \n     .. versionadded:: 1.4\n     return {kw: id(getattr(ds, kw, None)) for kw in keywords}\n \n \ndef get_j2k_parameters(codestream: bytes) -> Dict[str, object]:\n     \"\"\"Return a dict containing JPEG 2000 component parameters.\n \n     .. versionadded:: 2.1\n     return {}\n \n \ndef get_nr_frames(ds: \"Dataset\") -> int:\n     \"\"\"Return NumberOfFrames or 1 if NumberOfFrames is None.\n \n     Parameters\n     int\n         An integer for the NumberOfFrames or 1 if NumberOfFrames is None\n     \"\"\"\n    nr_frames: Optional[int] = getattr(ds, 'NumberOfFrames', 1)\n     # 'NumberOfFrames' may exist in the DICOM file but have value equal to None\n     if nr_frames is None:\n         warnings.warn(\"A value of None for (0028,0008) 'Number of Frames' is \"\n     return nr_frames\n \n \ndef pixel_dtype(ds: \"Dataset\", as_float: bool = False) -> \"np.dtype\":\n     \"\"\"Return a :class:`numpy.dtype` for the pixel data in `ds`.\n \n     Suitable for use with IODs containing the Image Pixel module (with\n     return dtype\n \n \ndef reshape_pixel_array(ds: \"Dataset\", arr: \"np.ndarray\") -> \"np.ndarray\":\n     \"\"\"Return a reshaped :class:`numpy.ndarray` `arr`.\n \n     +------------------------------------------+-----------+----------+"
  },
  {
    "instruction": "LUT Descriptor values don't follow standard\n**Describe the bug**\r\n(0028,3002) [LUT Descriptor](http://dicom.nema.org/medical/dicom/current/output/chtml/part03/sect_C.11.html#sect_C.11.1.1) has VM = 3, with value as `[number of entries in LUT, first stored pixel value mapped, LUT entry bit depth]`. The VR for the element is ambiguous and may be US or SS depending on the value of (0028,0103) Pixel Representation, however this only affects the second value, not the first or last which are always US.\r\n\r\nThe problem is that a Pixel Representation value of 1 (i.e. 2s complement) gives a LUT Descriptor value 1 as signed when it should always be unsigned.\r\n\r\n> Since LUT Descriptor (0028,3002) is multi-valued, in an Explicit VR Transfer Syntax, only one value representation (US or SS) may be specified, even though the first and third values are always by definition interpreted as unsigned. The explicit VR actually used is dictated by the VR needed to represent the second value, which will be consistent with Pixel Representation (0028,0103).\r\n\r\nAlso affects Red/Green/Blue Palette Color Lookup Table Descriptor.\r\n\r\n**Steps To Reproduce**\r\n```python\r\nfrom pydicom import dcmread\r\nfrom pydicom.filebase import DicomBytesIO\r\n\r\n# Explicit VR: SS\r\nlut = b'\\x28\\x00\\x02\\x30\\x53\\x53\\x06\\x00\\x00\\xf5\\x00\\xf8\\x10\\x00'\r\n\r\nbs = DicomBytesIO(lut)\r\nbs.is_little_endian = True\r\nbs.is_implicit_VR = False\r\n\r\nds = dcmread(bs, force=True)\r\nassert ds.LUTDescriptor == [62720, -2048, 16]\r\n```\r\n\r\n**Your environment**\r\nAffects current `master`\r\n\n",
    "input": " RawDataElement.is_raw = True\n \n \n def DataElement_from_raw(raw_data_element, encoding=None):\n     \"\"\"Return a :class:`DataElement` created from `raw_data_element`.\n \n         value = convert_value(VR, raw, encoding)\n     except NotImplementedError as e:\n         raise NotImplementedError(\"{0:s} in tag {1!r}\".format(str(e), raw.tag))\n     return DataElement(raw.tag, VR, value, raw.value_tell,\n                        raw.length == 0xFFFFFFFF, already_converted=True)\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n     lut_desc = ds.RedPaletteColorLookupTableDescriptor\n     # A value of 0 = 2^16 entries\n     nr_entries = lut_desc[0] or 2**16\n    # Workaround for #942: first value is always unsigned\n    if nr_entries < 0:\n        nr_entries += 2**16\n \n     # May be negative if Pixel Representation is 1\n     first_map = lut_desc[1]\n     if hasattr(ds, 'ModalityLUTSequence'):\n         item = ds.ModalityLUTSequence[0]\n         nr_entries = item.LUTDescriptor[0] or 2**16\n        # Workaround for #942: first value is always unsigned\n        if nr_entries < 0:\n            nr_entries += 2**16\n         first_map = item.LUTDescriptor[1]\n         nominal_depth = item.LUTDescriptor[2]\n ",
    "output": " RawDataElement.is_raw = True\n \n \n# The first and third values of the following elements are always US\n#   even if the VR is SS (PS3.3 C.7.6.3.1.5, C.11.1, C.11.2).\n# (0028,1101-1103) RGB Palette Color LUT Descriptor\n# (0028,3002) LUT Descriptor\n_LUT_DESCRIPTOR_TAGS = (0x00281101, 0x00281102, 0x00281103, 0x00283002)\n\n\n def DataElement_from_raw(raw_data_element, encoding=None):\n     \"\"\"Return a :class:`DataElement` created from `raw_data_element`.\n \n         value = convert_value(VR, raw, encoding)\n     except NotImplementedError as e:\n         raise NotImplementedError(\"{0:s} in tag {1!r}\".format(str(e), raw.tag))\n\n    if raw.tag in _LUT_DESCRIPTOR_TAGS and value[0] < 0:\n        # We only fix the first value as the third value is 8 or 16\n        value[0] += 65536\n\n     return DataElement(raw.tag, VR, value, raw.value_tell,\n                        raw.length == 0xFFFFFFFF, already_converted=True)\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n     lut_desc = ds.RedPaletteColorLookupTableDescriptor\n     # A value of 0 = 2^16 entries\n     nr_entries = lut_desc[0] or 2**16\n \n     # May be negative if Pixel Representation is 1\n     first_map = lut_desc[1]\n     if hasattr(ds, 'ModalityLUTSequence'):\n         item = ds.ModalityLUTSequence[0]\n         nr_entries = item.LUTDescriptor[0] or 2**16\n         first_map = item.LUTDescriptor[1]\n         nominal_depth = item.LUTDescriptor[2]\n "
  },
  {
    "instruction": "Crash writing DICOM with 1.4.0\npydicom 1.4.0\r\nWindows-10-10.0.18362-SP0\r\nPython  3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\r\nGDCM 3.0.2\r\nPillow 7.0.0\r\n\r\nType error raises when writing file with pydicom 1.4.0, works in 1.3.0.\r\n\r\n```\r\nds = pydicom.read_file('fail2404.anon.dcm')\r\n#print(ds.get((0x0040, 0x0275)))\r\nds.save_as('bort.dcm')\r\n```\r\n\r\nInterestingly, the crash goes away if the offending tag is accessed (uncomment the print and then the `save_as` works fine).\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\tag.py\", line 30, in tag_in_exception\r\n    yield\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 555, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 463, in write_data_element\r\n    buffer.write(data_element.value)\r\nTypeError: a bytes-like object is required, not 'list'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./pydcmbug.py\", line 7, in <module>\r\n    ds.save_as('bort.dcm')\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\dataset.py\", line 1810, in save_as\r\n    pydicom.dcmwrite(filename, self, write_like_original)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 946, in dcmwrite\r\n    write_dataset(fp, get_item(dataset, slice(0x00010000, None)))\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 555, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"C:\\Program Files\\Python37\\lib\\contextlib.py\", line 130, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\tag.py\", line 37, in tag_in_exception\r\n    raise type(ex)(msg)\r\nTypeError: With tag (0040, 0275) got exception: a bytes-like object is required, not 'list'\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\tag.py\", line 30, in tag_in_exception\r\n    yield\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 555, in write_dataset\r\n    write_data_element(fp, dataset.get_item(tag), dataset_encoding)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\pydicom\\filewriter.py\", line 463, in write_data_element\r\n    buffer.write(data_element.value)\r\nTypeError: a bytes-like object is required, not 'list'\r\n```\r\n\r\n[fail.zip](https://github.com/pydicom/pydicom/files/4072693/fail.zip)\r\n\n",
    "input": "         if it is empty.\n     \"\"\"\n     if VR == 'SQ':\n        return []\n     if config.use_none_as_empty_text_VR_value:\n         return None\n     if VR in ('AE', 'AS', 'CS', 'DA', 'DT', 'LO', 'LT',",
    "output": "         if it is empty.\n     \"\"\"\n     if VR == 'SQ':\n        return b'' if raw else []\n     if config.use_none_as_empty_text_VR_value:\n         return None\n     if VR in ('AE', 'AS', 'CS', 'DA', 'DT', 'LO', 'LT',"
  },
  {
    "instruction": "Converting Dicom image to Png\n**Describe the issue**\r\nhi, i am trying to convert Dicom image to png but in case of some particular file i am getting this \"list out of range error\".\r\n\r\n**Expected behavior**\r\ndicom image converted to png pne\r\n\r\n**Steps To Reproduce**\r\nHow to reproduce the issue. Please include:\r\n1. A minimum working code sample\r\n```\r\nfrom pydicom import dcmread\r\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\r\n    dicom = dcmread(path, force=True)\r\n    \r\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\r\n    if voi_lut:\r\n        data = apply_voi_lut(dicom.pixel_array, dicom)\r\n    else:\r\n        data = dicom.pixel_array\r\n               \r\n    # depending on this value, X-ray may look inverted - fix that:\r\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\r\n        data = np.amax(data) - data\r\n        \r\n    data = data - np.min(data)\r\n    data = data / np.max(data)\r\n    data = (data * 255).astype(np.uint8)\r\n        \r\n    return data\r\n\r\nimg = read_xray('/content/a.5545da1153f57ff8425be6f4bc712c090e7e22efff194da525210c84aba2a947.dcm')\r\nplt.figure(figsize = (12,12))\r\nplt.imshow(img)\r\n```\r\n2. The traceback (if one occurred)\r\n```\r\nIndexError                                Traceback (most recent call last)\r\n<ipython-input-13-6e53d7d16b90> in <module>()\r\n     19     return data\r\n     20 \r\n---> 21 img = read_xray('/content/a.5545da1153f57ff8425be6f4bc712c090e7e22efff194da525210c84aba2a947.dcm')\r\n     22 plt.figure(figsize = (12,12))\r\n     23 plt.imshow(img)\r\n\r\n2 frames\r\n/usr/local/lib/python3.7/dist-packages/pydicom/multival.py in __getitem__(self, index)\r\n     93         self, index: Union[slice, int]\r\n     94     ) -> Union[MutableSequence[_ItemType], _ItemType]:\r\n---> 95         return self._list[index]\r\n     96 \r\n     97     def insert(self, position: int, val: _T) -> None:\r\n\r\nIndexError: list index out of range\r\n```\r\n\r\n3. Which of the following packages are available and their versions:\r\n  * Numpy : latest as of 29th dec\r\n  * Pillow : latest as of 29th dec\r\n  * JPEG-LS : latest as of 29th dec\r\n  * GDCM : latest as of 29th dec\r\n4. The anonymized DICOM dataset (if possible).\r\nimage link : https://drive.google.com/file/d/1j13XTTPCLX-8e7FE--1n5Staxz7GGNWm/view?usp=sharing\r\n\r\n**Your environment**\r\nIf you're using **pydicom 2 or later**, please use the `pydicom.env_info`\r\nmodule to gather information about your environment and paste it in the issue:\r\n\r\n```bash\r\n$ python -m pydicom.env_info\r\n```\r\n\r\nFor **pydicom 1.x**, please run the following code snippet and paste the\r\noutput.\r\n\r\n```python\r\nimport platform, sys, pydicom\r\nprint(platform.platform(),\r\n      \"\\nPython\", sys.version,\r\n      \"\\npydicom\", pydicom.__version__)\r\n```\r\n\n",
    "input": "         possible_handlers = [\n             hh for hh in pydicom.config.pixel_data_handlers\n             if hh is not None\n            and hh.supports_transfer_syntax(ts)  # type: ignore[attr-defined]\n         ]\n \n         # No handlers support the transfer syntax\n         #   dependencies met\n         available_handlers = [\n             hh for hh in possible_handlers\n            if hh.is_available()  # type: ignore[attr-defined]\n         ]\n \n         # There are handlers that support the transfer syntax but none of them\n             )\n             pkg_msg = []\n             for hh in possible_handlers:\n                hh_deps = hh.DEPENDENCIES  # type: ignore[attr-defined]\n                 # Missing packages\n                 missing = [dd for dd in hh_deps if have_package(dd) is None]\n                 # Package names\n                 names = [hh_deps[name][1] for name in missing]\n                 pkg_msg.append(\n                    f\"{hh.HANDLER_NAME} \"  # type: ignore[attr-defined]\n                     f\"(req. {', '.join(names)})\"\n                 )\n \n \n         available_handlers = [\n             hh for hh in overlay_data_handlers\n            if hh.is_available()  # type: ignore[attr-defined]\n         ]\n         if not available_handlers:\n             # For each of the handlers we want to find which\n             )\n             pkg_msg = []\n             for hh in overlay_data_handlers:\n                hh_deps = hh.DEPENDENCIES  # type: ignore[attr-defined]\n                 # Missing packages\n                 missing = [dd for dd in hh_deps if have_package(dd) is None]\n                 # Package names\n                 names = [hh_deps[name][1] for name in missing]\n                 pkg_msg.append(\n                    f\"{hh.HANDLER_NAME} \"  # type: ignore[attr-defined]\n                     f\"(req. {', '.join(names)})\"\n                 )\n \n         for handler in available_handlers:\n             try:\n                 # Use the handler to get an ndarray of the pixel data\n                func = handler.get_overlay_array  # type: ignore[attr-defined]\n                 return cast(\"numpy.ndarray\", func(self, group))\n             except Exception as exc:\n                 logger.debug(\ndiff --git a/pydicom/encoders/base.py b/pydicom/encoders/base.py\n         module = import_module(import_path[0])\n \n         # `is_available(UID)` is required for plugins\n        if module.is_available(self.UID):  # type: ignore[attr-defined]\n             self._available[label] = getattr(module, import_path[1])\n         else:\n             # `ENCODER_DEPENDENCIES[UID]` is required for plugins\n            deps = module.ENCODER_DEPENDENCIES  # type: ignore[attr-defined]\n             self._unavailable[label] = deps[self.UID]\n \n     @staticmethod\ndiff --git a/pydicom/filebase.py b/pydicom/filebase.py\n         self.close = file_like_obj.close\n         self.name: str = getattr(file_like_obj, 'name', '<no filename>')\n \n    def no_write(self, bytes_read: bytes) -> None:\n         \"\"\"Used for file-like objects where no write is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no write() method\")\n \n    def no_read(self, bytes_read: Optional[int] = None) -> None:\n         \"\"\"Used for file-like objects where no read is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no read() method\")\n \n    def no_seek(self, offset: int, from_what: int = 0) -> None:\n         \"\"\"Used for file-like objects where no seek is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no seek() method\")\n \ndiff --git a/pydicom/fileset.py b/pydicom/fileset.py\n \n def _single_level_record_type(ds: Dataset) -> str:\n     \"\"\"Return a single-level *Directory Record Type* for `ds`.\"\"\"\n    sop_class = getattr(ds, \"SOPClassUID\", None)\n \n     try:\n        return _SINGLE_LEVEL_SOP_CLASSES[sop_class]\n     except KeyError:\n         return \"PATIENT\"\n \n     if \"RTPlanLabel\" in ds:\n         return \"RT PLAN\"\n \n    sop_class = getattr(ds, \"SOPClassUID\", None)\n \n     try:\n        return _FOUR_LEVEL_SOP_CLASSES[sop_class]\n     except KeyError:\n         return \"IMAGE\"\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n         Write a DICOM file from a dataset that was read in with ``dcmread()``.\n         ``save_as()`` wraps ``dcmwrite()``.\n     \"\"\"\n \n     # Ensure is_little_endian and is_implicit_VR are set\n     if None in (dataset.is_little_endian, dataset.is_implicit_VR):\n             fp.write(preamble)\n             fp.write(b'DICM')\n \n        tsyntax: Optional[UID] = None  # type: ignore[no-redef]\n         if dataset.file_meta:  # May be an empty Dataset\n             # If we want to `write_like_original`, don't enforce_standard\n             write_file_meta_info(\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n    if 'ModalityLUTSequence' in ds:\n         item = cast(List[\"Dataset\"], ds.ModalityLUTSequence)[0]\n         nr_entries = cast(List[int], item.LUTDescriptor)[0] or 2**16\n         first_map = cast(List[int], item.LUTDescriptor)[1]\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n     valid_voi = False\n    if 'VOILUTSequence' in ds:\n         ds.VOILUTSequence = cast(List[\"Dataset\"], ds.VOILUTSequence)\n         valid_voi = None not in [\n             ds.VOILUTSequence[0].get('LUTDescriptor', None),\n     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n    if \"VOILUTSequence\" not in ds:\n         return arr\n \n     if not np.issubdtype(arr.dtype, np.integer):\n     ds.BitsStored = cast(int, ds.BitsStored)\n     y_min: float\n     y_max: float\n    if 'ModalityLUTSequence' in ds:\n         # Unsigned - see PS3.3 C.11.1.1.1\n         y_min = 0\n         item = cast(List[\"Dataset\"], ds.ModalityLUTSequence)[0]\ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n     def __setstate__(self, state: Dict[str, Any]) -> None:\n         self.__dict__.update(state)\n \n    def __reduce_ex__(self, protocol: int) -> Tuple[Any, ...]:\n         # datetime.time, and datetime.datetime return Tuple[Any, ...]\n         # datetime.date doesn't define __reduce_ex__\n         reduce_ex = cast(Tuple[Any, ...], super().__reduce_ex__(protocol))\n             if match.group('ms'):\n                 microsecond = int(match.group('ms').rstrip().ljust(6, '0'))\n \n            return super().__new__(  # type: ignore[call-arg, no-any-return]\n                 cls, hour, minute, second, microsecond\n             )\n \n         if isinstance(val, datetime.time):\n            return super().__new__(  # type: ignore[call-arg, no-any-return]\n                 cls, val.hour, val.minute, val.second, val.microsecond\n             )\n \n         try:\n            return super().__new__(  # type: ignore[call-arg, no-any-return]\n                 cls, *args, **kwargs\n             )\n         except Exception as exc:\n         if len(args) and args[0] is None:\n             return None\n \n        return cast(\"PersonName\", super().__new__(cls))\n \n     def __init__(\n         self,",
    "output": "         possible_handlers = [\n             hh for hh in pydicom.config.pixel_data_handlers\n             if hh is not None\n            and hh.supports_transfer_syntax(ts)\n         ]\n \n         # No handlers support the transfer syntax\n         #   dependencies met\n         available_handlers = [\n             hh for hh in possible_handlers\n            if hh.is_available()\n         ]\n \n         # There are handlers that support the transfer syntax but none of them\n             )\n             pkg_msg = []\n             for hh in possible_handlers:\n                hh_deps = hh.DEPENDENCIES\n                 # Missing packages\n                 missing = [dd for dd in hh_deps if have_package(dd) is None]\n                 # Package names\n                 names = [hh_deps[name][1] for name in missing]\n                 pkg_msg.append(\n                    f\"{hh.HANDLER_NAME} \"\n                     f\"(req. {', '.join(names)})\"\n                 )\n \n \n         available_handlers = [\n             hh for hh in overlay_data_handlers\n            if hh.is_available()\n         ]\n         if not available_handlers:\n             # For each of the handlers we want to find which\n             )\n             pkg_msg = []\n             for hh in overlay_data_handlers:\n                hh_deps = hh.DEPENDENCIES\n                 # Missing packages\n                 missing = [dd for dd in hh_deps if have_package(dd) is None]\n                 # Package names\n                 names = [hh_deps[name][1] for name in missing]\n                 pkg_msg.append(\n                    f\"{hh.HANDLER_NAME} \"\n                     f\"(req. {', '.join(names)})\"\n                 )\n \n         for handler in available_handlers:\n             try:\n                 # Use the handler to get an ndarray of the pixel data\n                func = handler.get_overlay_array\n                 return cast(\"numpy.ndarray\", func(self, group))\n             except Exception as exc:\n                 logger.debug(\ndiff --git a/pydicom/encoders/base.py b/pydicom/encoders/base.py\n         module = import_module(import_path[0])\n \n         # `is_available(UID)` is required for plugins\n        if module.is_available(self.UID):\n             self._available[label] = getattr(module, import_path[1])\n         else:\n             # `ENCODER_DEPENDENCIES[UID]` is required for plugins\n            deps = module.ENCODER_DEPENDENCIES\n             self._unavailable[label] = deps[self.UID]\n \n     @staticmethod\ndiff --git a/pydicom/filebase.py b/pydicom/filebase.py\n         self.close = file_like_obj.close\n         self.name: str = getattr(file_like_obj, 'name', '<no filename>')\n \n    def no_write(self, bytes_read: bytes) -> int:\n         \"\"\"Used for file-like objects where no write is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no write() method\")\n \n    def no_read(self, size: int = -1) -> bytes:\n         \"\"\"Used for file-like objects where no read is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no read() method\")\n \n    def no_seek(self, offset: int, whence: int = 0) -> int:\n         \"\"\"Used for file-like objects where no seek is available\"\"\"\n         raise IOError(\"This DicomFileLike object has no seek() method\")\n \ndiff --git a/pydicom/fileset.py b/pydicom/fileset.py\n \n def _single_level_record_type(ds: Dataset) -> str:\n     \"\"\"Return a single-level *Directory Record Type* for `ds`.\"\"\"\n    sop_class = cast(Optional[UID], getattr(ds, \"SOPClassUID\", None))\n \n     try:\n        return _SINGLE_LEVEL_SOP_CLASSES[sop_class]  # type: ignore[index]\n     except KeyError:\n         return \"PATIENT\"\n \n     if \"RTPlanLabel\" in ds:\n         return \"RT PLAN\"\n \n    sop_class = cast(Optional[UID], getattr(ds, \"SOPClassUID\", None))\n \n     try:\n        return _FOUR_LEVEL_SOP_CLASSES[sop_class]  # type: ignore[index]\n     except KeyError:\n         return \"IMAGE\"\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n         Write a DICOM file from a dataset that was read in with ``dcmread()``.\n         ``save_as()`` wraps ``dcmwrite()``.\n     \"\"\"\n    tsyntax: Optional[UID]\n \n     # Ensure is_little_endian and is_implicit_VR are set\n     if None in (dataset.is_little_endian, dataset.is_implicit_VR):\n             fp.write(preamble)\n             fp.write(b'DICM')\n \n        tsyntax = None\n         if dataset.file_meta:  # May be an empty Dataset\n             # If we want to `write_like_original`, don't enforce_standard\n             write_file_meta_info(\ndiff --git a/pydicom/pixel_data_handlers/util.py b/pydicom/pixel_data_handlers/util.py\n     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n    if ds.get(\"ModalityLUTSequence\"):\n         item = cast(List[\"Dataset\"], ds.ModalityLUTSequence)[0]\n         nr_entries = cast(List[int], item.LUTDescriptor)[0] or 2**16\n         first_map = cast(List[int], item.LUTDescriptor)[1]\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n     valid_voi = False\n    if ds.get('VOILUTSequence'):\n         ds.VOILUTSequence = cast(List[\"Dataset\"], ds.VOILUTSequence)\n         valid_voi = None not in [\n             ds.VOILUTSequence[0].get('LUTDescriptor', None),\n     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n       <part04/sect_N.2.html#sect_N.2.1.1>`\n     \"\"\"\n    if not ds.get('VOILUTSequence'):\n         return arr\n \n     if not np.issubdtype(arr.dtype, np.integer):\n     ds.BitsStored = cast(int, ds.BitsStored)\n     y_min: float\n     y_max: float\n    if ds.get('ModalityLUTSequence'):\n         # Unsigned - see PS3.3 C.11.1.1.1\n         y_min = 0\n         item = cast(List[\"Dataset\"], ds.ModalityLUTSequence)[0]\ndiff --git a/pydicom/valuerep.py b/pydicom/valuerep.py\n     def __setstate__(self, state: Dict[str, Any]) -> None:\n         self.__dict__.update(state)\n \n    def __reduce_ex__(  # type: ignore[override]\n        self, protocol: int\n    ) -> Tuple[Any, ...]:\n        # Python 3.8 - protocol: SupportsIndex (added in 3.8)\n         # datetime.time, and datetime.datetime return Tuple[Any, ...]\n         # datetime.date doesn't define __reduce_ex__\n         reduce_ex = cast(Tuple[Any, ...], super().__reduce_ex__(protocol))\n             if match.group('ms'):\n                 microsecond = int(match.group('ms').rstrip().ljust(6, '0'))\n \n            return super().__new__(\n                 cls, hour, minute, second, microsecond\n             )\n \n         if isinstance(val, datetime.time):\n            return super().__new__(\n                 cls, val.hour, val.minute, val.second, val.microsecond\n             )\n \n         try:\n            return super().__new__(\n                 cls, *args, **kwargs\n             )\n         except Exception as exc:\n         if len(args) and args[0] is None:\n             return None\n \n        return super().__new__(cls)\n \n     def __init__(\n         self,"
  },
  {
    "instruction": "\"Printing\" of certain dicom files fails once, but works the second time\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n\"Printing\" of certain dicom files (see [example](https://github.com/pydicom/pydicom/files/2865551/dicom_exception.zip)) fails once, but not the second time\r\n\r\n#### Steps/Code to Reproduce\r\n```python\r\nfrom pydicom import read_file\r\n\r\na = read_file('...')\r\nprint(a)\r\n# triggers exception: AttributeError: With tag (0028, 3000) got exception: Failed to resolve ambiguous VR for tag (0028, 3002): 'Dataset' object has no attribute 'PixelRepresentation'\r\n\r\n# try same thing again...\r\nprint(a)\r\n# just works...\r\n```\r\n\r\n#### Versions\r\nBehaviour as described above at least on:\r\n```\r\nLinux-4.18.0-15-generic-x86_64-with-Ubuntu-18.10-cosmic\r\n('Python', '2.7.15+ (default, Oct  2 2018, 22:12:08) \\n[GCC 8.2.0]')\r\n('numpy', '1.14.5')\r\n('pydicom', '1.3.0.dev0')\r\n```\r\nand\r\n\r\n\r\n```\r\n('pydicom', '1.2.2')\r\n```\r\n\r\nWorks as expected on:\r\n```\r\nLinux-4.18.0-15-generic-x86_64-with-Ubuntu-18.10-cosmic\r\n('Python', '2.7.15+ (default, Oct  2 2018, 22:12:08) \\n[GCC 8.2.0]')\r\n('pydicom', '1.0.1')\r\n```\n",
    "input": "         self.is_little_endian = None\n         self.is_implicit_VR = None\n \n     def __enter__(self):\n         \"\"\"Method invoked on entry to a with statement.\"\"\"\n         return self\n             # Try the base class attribute getter (fix for issue 332)\n             return super(Dataset, self).__getattribute__(name)\n         else:\n            return self[tag].value\n \n     @property\n     def _character_set(self):\n                 # don't have this tag yet->create the data_element instance\n                 VR = dictionary_VR(tag)\n                 data_element = DataElement(tag, VR, value)\n             else:\n                 # already have this data_element, just changing its value\n                 data_element = self[tag]\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n         # US if PixelRepresentation value is 0x0000, else SS\n         #   For references, see the list at\n         #   https://github.com/darcymason/pydicom/pull/298\n         if ds.PixelRepresentation == 0:\n             elem.VR = 'US'\n             byte_type = 'H'\ndiff --git a/pydicom/sequence.py b/pydicom/sequence.py\n         if isinstance(iterable, Dataset):\n             raise TypeError('The Sequence constructor requires an iterable')\n \n         # If no inputs are provided, we create an empty Sequence\n         if not iterable:\n             iterable = list()\n         # validate_dataset is used as a pseudo type_constructor\n         super(Sequence, self).__init__(validate_dataset, iterable)\n \n     def __str__(self):\n         \"\"\"String description of the Sequence.\"\"\"\n         lines = [str(x) for x in self]",
    "output": "         self.is_little_endian = None\n         self.is_implicit_VR = None\n \n        # the parent data set, if this dataset is a sequence item\n        self.parent = None\n\n     def __enter__(self):\n         \"\"\"Method invoked on entry to a with statement.\"\"\"\n         return self\n             # Try the base class attribute getter (fix for issue 332)\n             return super(Dataset, self).__getattribute__(name)\n         else:\n            data_elem = self[tag]\n            value = data_elem.value\n            if data_elem.VR == 'SQ':\n                # let a sequence know its parent dataset, as sequence items\n                # may need parent dataset tags to resolve ambiguous tags\n                value.parent = self\n            return value\n \n     @property\n     def _character_set(self):\n                 # don't have this tag yet->create the data_element instance\n                 VR = dictionary_VR(tag)\n                 data_element = DataElement(tag, VR, value)\n                if VR == 'SQ':\n                    # let a sequence know its parent dataset to pass it\n                    # to its items, who may need parent dataset tags\n                    # to resolve ambiguous tags\n                    data_element.parent = self\n             else:\n                 # already have this data_element, just changing its value\n                 data_element = self[tag]\ndiff --git a/pydicom/filewriter.py b/pydicom/filewriter.py\n         # US if PixelRepresentation value is 0x0000, else SS\n         #   For references, see the list at\n         #   https://github.com/darcymason/pydicom/pull/298\n        # PixelRepresentation is usually set in the root dataset\n        while 'PixelRepresentation' not in ds and ds.parent:\n            ds = ds.parent\n         if ds.PixelRepresentation == 0:\n             elem.VR = 'US'\n             byte_type = 'H'\ndiff --git a/pydicom/sequence.py b/pydicom/sequence.py\n         if isinstance(iterable, Dataset):\n             raise TypeError('The Sequence constructor requires an iterable')\n \n        # the parent dataset\n        self._parent = None\n\n         # If no inputs are provided, we create an empty Sequence\n         if not iterable:\n             iterable = list()\n         # validate_dataset is used as a pseudo type_constructor\n         super(Sequence, self).__init__(validate_dataset, iterable)\n \n    @property\n    def parent(self):\n        \"\"\"Return the parent dataset.\"\"\"\n        return self._parent\n\n    @parent.setter\n    def parent(self, value):\n        \"\"\"Set the parent dataset and pass it to all items.\"\"\"\n        if value != self._parent:\n            self._parent = value\n            for item in self._list:\n                item.parent = self._parent\n\n    def __setitem__(self, i, val):\n        \"\"\"Set the parent dataset to the new sequence item\"\"\"\n        super(Sequence, self).__setitem__(i, val)\n        val.parent = self._parent\n\n     def __str__(self):\n         \"\"\"String description of the Sequence.\"\"\"\n         lines = [str(x) for x in self]"
  },
  {
    "instruction": "Deferred Read Fails For File-Like Objects\n#### Description\r\nDeferred reads are failing when dcmread is passed a file-like object (instead of a filepath).  There are two old issues from 2014 which describe the same issue which were apparently fixed, but I'm still seeing it on v1.3:\r\nhttps://github.com/pydicom/pydicom/issues/104\r\nhttps://github.com/pydicom/pydicom/issues/74\r\n\r\n#### Steps/Code to Reproduce\r\n```\r\nimport io\r\nimport pydicom\r\n\r\nwith open(\"./0.dcm\", \"rb\") as fp:\r\n   data = fp.read()\r\nfilelike = io.BytesIO(data)\r\n\r\ndataset = pydicom.dcmread(filelike, defer_size=1024)\r\nprint(len(dataset.PixelData))\r\n```\r\n\r\n#### Expected Results\r\nPydicom should hold onto the supplied file-like and use that for the deferred read, rather than trying to grab the file-like's .name/.filename attr and use that to re-open.  It could also hold onto it's own open'd file-like (if supplied a file_path) and use that for deferred reads to simplify things.\r\n\r\n#### Actual Results\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/pydicom/dataset.py\", line 747, in __getattr__\r\n    data_elem = self[tag]\r\n  File \"/usr/local/lib/python3.6/dist-packages/pydicom/dataset.py\", line 826, in __getitem__\r\n    data_elem)\r\n  File \"/usr/local/lib/python3.6/dist-packages/pydicom/filereader.py\", line 911, in read_deferred_data_element\r\n    raise IOError(\"Deferred read -- original filename not stored. \"\r\nOSError: Deferred read -- original filename not stored. Cannot re-open\r\n\r\n#### Versions\r\nLinux-4.18.0-25-generic-x86_64-with-Ubuntu-18.10-cosmic\r\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \r\npydicom 1.3.0\n",
    "input": "         self.file_meta = file_meta\n         self.is_implicit_VR = is_implicit_VR\n         self.is_little_endian = is_little_endian\n         if isinstance(filename_or_obj, compat.string_types):\n            self.filename = filename_or_obj\n             self.fileobj_type = open\n         elif isinstance(filename_or_obj, io.BufferedReader):\n            self.filename = filename_or_obj.name\n             # This is the appropriate constructor for io.BufferedReader\n             self.fileobj_type = open\n         else:\n             # http://docs.python.org/reference/datamodel.html\n             self.fileobj_type = filename_or_obj.__class__\n             if getattr(filename_or_obj, \"name\", False):\n                self.filename = filename_or_obj.name\n             elif getattr(filename_or_obj, \"filename\",\n                          False):  # gzip python <2.7?\n                self.filename = filename_or_obj.filename\n             else:\n                 # e.g. came from BytesIO or something file-like\n                self.filename = None\n         self.timestamp = None\n        if self.filename and os.path.exists(self.filename):\n            statinfo = os.stat(self.filename)\n            self.timestamp = statinfo.st_mtime\n \n     def __eq__(self, other):\n         \"\"\"Compare `self` and `other` for equality.\ndiff --git a/pydicom/filereader.py b/pydicom/filereader.py\n     Parameters\n     ----------\n     fp : file-like\n        The file like to read from.\n     is_implicit_VR : bool\n         ``True`` if the data is encoded as implicit VR, ``False`` otherwise.\n     is_little_endian : bool\n     return offset\n \n \ndef read_deferred_data_element(fileobj_type, filename, timestamp,\n                                raw_data_elem):\n     \"\"\"Read the previously deferred value from the file into memory\n    and return a raw data element\"\"\"\n     logger.debug(\"Reading deferred element %r\" % str(raw_data_elem.tag))\n     # If it wasn't read from a file, then return an error\n    if filename is None:\n         raise IOError(\"Deferred read -- original filename not stored. \"\n                       \"Cannot re-open\")\n     # Check that the file is the same as when originally read\n    if not os.path.exists(filename):\n         raise IOError(u\"Deferred read -- original file \"\n                      \"{0:s} is missing\".format(filename))\n     if timestamp is not None:\n        statinfo = os.stat(filename)\n         if statinfo.st_mtime != timestamp:\n             warnings.warn(\"Deferred read warning -- file modification time \"\n                           \"has changed.\")\n \n     # Open the file, position to the right place\n    # fp = self.typefileobj(self.filename, \"rb\")\n    fp = fileobj_type(filename, 'rb')\n     is_implicit_VR = raw_data_elem.is_implicit_VR\n     is_little_endian = raw_data_elem.is_little_endian\n     offset = data_element_offset_to_value(is_implicit_VR, raw_data_elem.VR)",
    "output": "         self.file_meta = file_meta\n         self.is_implicit_VR = is_implicit_VR\n         self.is_little_endian = is_little_endian\n        filename = None\n         if isinstance(filename_or_obj, compat.string_types):\n            filename = filename_or_obj\n             self.fileobj_type = open\n         elif isinstance(filename_or_obj, io.BufferedReader):\n            filename = filename_or_obj.name\n             # This is the appropriate constructor for io.BufferedReader\n             self.fileobj_type = open\n         else:\n             # http://docs.python.org/reference/datamodel.html\n             self.fileobj_type = filename_or_obj.__class__\n             if getattr(filename_or_obj, \"name\", False):\n                filename = filename_or_obj.name\n             elif getattr(filename_or_obj, \"filename\",\n                          False):  # gzip python <2.7?\n                filename = filename_or_obj.filename\n             else:\n                 # e.g. came from BytesIO or something file-like\n                self.filename = filename_or_obj\n\n         self.timestamp = None\n        if filename:\n            self.filename = filename\n            if os.path.exists(filename):\n                statinfo = os.stat(filename)\n                self.timestamp = statinfo.st_mtime\n \n     def __eq__(self, other):\n         \"\"\"Compare `self` and `other` for equality.\ndiff --git a/pydicom/filereader.py b/pydicom/filereader.py\n     Parameters\n     ----------\n     fp : file-like\n        The file-like to read from.\n     is_implicit_VR : bool\n         ``True`` if the data is encoded as implicit VR, ``False`` otherwise.\n     is_little_endian : bool\n     return offset\n \n \ndef read_deferred_data_element(fileobj_type, filename_or_obj, timestamp,\n                                raw_data_elem):\n     \"\"\"Read the previously deferred value from the file into memory\n    and return a raw data element.\n\n    .. note:\n\n        This is called internally by pydicom and will normally not be\n        needed in user code.\n\n    Parameters\n    ----------\n    fileobj_type : type\n        The type of the original file object.\n    filename_or_obj : str or file-like\n        The filename of the original file if one exists, or the file-like\n        object where the data element persists.\n    timestamp : time or None\n        The time the original file has been read, if not a file-like.\n    raw_data_elem : dataelem.RawDataElement\n        The raw data element with no value set.\n\n    Returns\n    -------\n    dataelem.RawDataElement\n        The data element with the value set.\n\n    Raises\n    ------\n    IOError\n        If `filename_or_obj` is ``None``.\n    IOError\n        If `filename_or_obj` is a filename and the corresponding file does\n        not exist.\n    ValueError\n        If the VR or tag of `raw_data_elem` does not match the read value.\n    \"\"\"\n     logger.debug(\"Reading deferred element %r\" % str(raw_data_elem.tag))\n     # If it wasn't read from a file, then return an error\n    if filename_or_obj is None:\n         raise IOError(\"Deferred read -- original filename not stored. \"\n                       \"Cannot re-open\")\n    is_filename = isinstance(filename_or_obj, compat.string_types)\n\n     # Check that the file is the same as when originally read\n    if is_filename and not os.path.exists(filename_or_obj):\n         raise IOError(u\"Deferred read -- original file \"\n                      \"{0:s} is missing\".format(filename_or_obj))\n     if timestamp is not None:\n        statinfo = os.stat(filename_or_obj)\n         if statinfo.st_mtime != timestamp:\n             warnings.warn(\"Deferred read warning -- file modification time \"\n                           \"has changed.\")\n \n     # Open the file, position to the right place\n    fp = (fileobj_type(filename_or_obj, 'rb')\n          if is_filename else filename_or_obj)\n     is_implicit_VR = raw_data_elem.is_implicit_VR\n     is_little_endian = raw_data_elem.is_little_endian\n     offset = data_element_offset_to_value(is_implicit_VR, raw_data_elem.VR)"
  },
  {
    "instruction": "OverflowError \"VR of 'DS' must be <= 16 characters long\" triggered when element is 16 characters long\n**Describe the bug**\r\n\r\n`OverflowError` triggered while accessing `PixelData`, which the values compliant with the standard. In the sample referenced in the example below, we have this, which satisfies DS VR:\r\n\r\n```\r\n(0028,0030) DS [.002006091181818\\.002006091181818]      #  34, 2 PixelSpacing\r\n```\r\n\r\nBut nevertheless the error is triggered while trying to access `PixelData`:\r\n\r\n```\r\nOverflowError: Values for elements with a VR of 'DS' must be <= 16 characters long, \r\nbut the float provided requires > 16 characters to be accurately represented. Use a \r\nsmaller string, set 'config.settings.reading_validation_mode' to 'WARN' to override \r\nthe length check, or explicitly construct a DS object with 'auto_format' set to True\r\n```\r\n\r\n**Expected behavior**\r\n\r\n`OverflowError` does not get triggered.\r\n\r\n**Steps To Reproduce**\r\n\r\nFollow the steps of this Colab notebook: https://colab.research.google.com/drive/1FcSgjBKazh0YN-jlJYdID0YUTh90CAvZ?usp=sharing\r\n\r\n**Your environment**\r\n\r\n```\r\nmodule       | version\r\n------       | -------\r\nplatform     | Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\nPython       | 3.7.13 (default, Mar 16 2022, 17:37:17)  [GCC 7.5.0]\r\npydicom      | 2.3.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.21.5\r\nPIL          | 9.1.0\r\npylibjpeg    | _module not found_\r\nopenjpeg     | _module not found_\r\nlibjpeg      | _module not found_\r\n```\r\n\r\nRelated issue: https://github.com/imi-bigpicture/wsidicom/issues/49\r\n\r\ncc: @DanielaSchacherer @dclunie @hackermd \n",
    "input": " \n         if (validation_mode == config.RAISE and\n                 not self.auto_format):\n            if len(repr(self)[1:-1]) > 16:\n                 raise OverflowError(\n                     \"Values for elements with a VR of 'DS' must be <= 16 \"\n                     \"characters long, but the float provided requires > 16 \"\n                     \"explicitly construct a DS object with 'auto_format' \"\n                     \"set to True\"\n                 )\n            if not is_valid_ds(repr(self)[1:-1]):\n                 # This will catch nan and inf\n                 raise ValueError(\n                     f'Value \"{str(self)}\" is not valid for elements with a VR '",
    "output": " \n         if (validation_mode == config.RAISE and\n                 not self.auto_format):\n            if len(str(self)) > 16:\n                 raise OverflowError(\n                     \"Values for elements with a VR of 'DS' must be <= 16 \"\n                     \"characters long, but the float provided requires > 16 \"\n                     \"explicitly construct a DS object with 'auto_format' \"\n                     \"set to True\"\n                 )\n            if not is_valid_ds(str(self)):\n                 # This will catch nan and inf\n                 raise ValueError(\n                     f'Value \"{str(self)}\" is not valid for elements with a VR '"
  },
  {
    "instruction": "Allow to search a list of elements in a `FileSet` while only loading instances once, to drastically improve execution time\n**Is your feature request related to a problem? Please describe.**\r\nCurrently, `fileset.FileSet.find_values` only allows for elements to be searched for one at a time. When executing this action while setting `load` to `True`, this results in a substantial overhead.\r\n\r\n**Describe the solution you'd like**\r\nThe following example code allows loading the instances once, and iterating over a list of elements to find:\r\n```python\r\ndef find_values_quick(self, elements, instances=None):\r\n    results = {element: [] for element in elements}\r\n    instances = instances or iter(self)\r\n    for instance in instances:\r\n        instance = instance.load()\r\n        for element in elements:\r\n            if element not in instance:\r\n                continue\r\n            val = instance[element].value\r\n            if val not in results[element]:\r\n                results[element].append(val)\r\n    return results\r\n```\n",
    "input": " \n     def find_values(\n         self,\n        element: Union[str, int],\n         instances: Optional[List[FileInstance]] = None,\n         load: bool = False\n    ) -> List[Any]:\n        \"\"\"Return a list of unique values for a given element.\n \n         Parameters\n         ----------\n        element : str, int or pydicom.tag.BaseTag\n            The keyword or tag of the element to search for.\n         instances : list of pydicom.fileset.FileInstance, optional\n             Search within the given instances. If not used then all available\n             instances will be searched.\n \n         Returns\n         -------\n        list of object\n            A list of value(s) for the element available in the instances.\n         \"\"\"\n        has_element = False\n        results = []\n         iter_instances = instances or iter(self)\n         instance: Union[Dataset, FileInstance]\n         for instance in iter_instances:\n             if load:\n                 instance = instance.load()\n\n            if element not in instance:\n                continue\n\n            has_element = True\n            val = instance[element].value\n            # Not very efficient, but we can't use set\n            if val not in results:\n                results.append(val)\n\n        if not load and not has_element:\n             warnings.warn(\n                 \"None of the records in the DICOMDIR dataset contain \"\n                \"the query element, consider using the 'load' parameter \"\n                 \"to expand the search to the corresponding SOP instances\"\n             )\n \n         return results\n ",
    "output": " \n     def find_values(\n         self,\n        elements: Union[str, int, List[Union[str, int]]],\n         instances: Optional[List[FileInstance]] = None,\n         load: bool = False\n    ) -> Union[List[Any], Dict[Union[str, int], List]]:\n        \"\"\"Return a list of unique values for given element(s).\n \n         Parameters\n         ----------\n        elements : str, int or pydicom.tag.BaseTag, or list of these\n            The keyword or tag of the element(s) to search for.\n         instances : list of pydicom.fileset.FileInstance, optional\n             Search within the given instances. If not used then all available\n             instances will be searched.\n \n         Returns\n         -------\n        list of object(s), or dict of lists of object(s)\n\n            * If single element was queried: A list of value(s) for the element\n              available in the instances.\n            * If list of elements was queried: A dict of element value pairs\n              with lists of value(s) for the elements available in the instances.\n         \"\"\"\n        element_list = elements if isinstance(elements, list) else [elements]\n        has_element = {element: False for element in element_list}\n        results: Dict = {element: [] for element in element_list}\n         iter_instances = instances or iter(self)\n         instance: Union[Dataset, FileInstance]\n         for instance in iter_instances:\n             if load:\n                 instance = instance.load()\n            for element in element_list:\n                if element not in instance:\n                    continue\n\n                has_element[element] = True\n                val = instance[element].value\n                # Not very efficient, but we can't use set\n                if val not in results[element]:\n                    results[element].append(val)\n        missing_elements = [\n            element for element, v in has_element.items() if not v\n        ]\n        if not load and missing_elements:\n             warnings.warn(\n                 \"None of the records in the DICOMDIR dataset contain \"\n                f\"{missing_elements}, consider using the 'load' parameter \"\n                 \"to expand the search to the corresponding SOP instances\"\n             )\n        if not isinstance(elements, list):\n            return results[element_list[0]]\n \n         return results\n "
  },
  {
    "instruction": "from_json does not correctly convert BulkDataURI's in SQ data elements\n**Describe the bug**\r\nWhen a DICOM object contains large data elements in SQ elements and is converted to JSON, those elements are correctly turned into BulkDataURI's. However, when the JSON is converted back to DICOM using from_json, the BulkDataURI's in SQ data elements are not converted back and warnings are thrown.\r\n\r\n**Expected behavior**\r\nThe BulkDataURI's in SQ data elements get converted back correctly.\r\n\r\n**Steps To Reproduce**\r\nTake the `waveform_ecg.dcm` in the test data, convert it to JSON, and then convert the JSON to DICOM\r\n\r\n**Your environment**\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.7-x86_64-i386-64bit\r\nPython       | 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18)  [Clang 6.0 (clang-600.0.57)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | _module not found_\r\nPIL          | _module not found_\r\n\r\nThe problem is in `jsonrep.py` at line 227. I plan on submitting a pull-request today for this.\n",
    "input": "                     value_key = unique_value_keys[0]\n                     elem = DataElement.from_json(\n                         self.dataset_class, key, vr,\n                        val[value_key], value_key\n                     )\n                 ds.add(elem)\n         return ds",
    "output": "                     value_key = unique_value_keys[0]\n                     elem = DataElement.from_json(\n                         self.dataset_class, key, vr,\n                        val[value_key], value_key,\n                        self.bulk_data_element_handler\n                     )\n                 ds.add(elem)\n         return ds"
  }
]